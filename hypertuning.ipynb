{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a67d9a",
   "metadata": {},
   "source": [
    "## bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16476992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> LR=1e-3,DO=0.0,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1668184\n",
      "\tspeed: 0.0232s/iter; left time: 261.6856s\n",
      "\titers: 200, epoch: 1 | loss: 0.1968358\n",
      "\tspeed: 0.0112s/iter; left time: 125.7434s\n",
      "\titers: 300, epoch: 1 | loss: 0.1832968\n",
      "\tspeed: 0.0114s/iter; left time: 126.2866s\n",
      "\titers: 400, epoch: 1 | loss: 0.1859085\n",
      "\tspeed: 0.0114s/iter; left time: 125.2163s\n",
      "\titers: 500, epoch: 1 | loss: 0.2158217\n",
      "\tspeed: 0.0114s/iter; left time: 124.4637s\n",
      "Epoch: 1 cost time: 7.703599452972412\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2083975 Vali Loss: 0.0515174 Test Loss: 0.1629780\n",
      "Validation loss decreased (inf --> 0.051517).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3291485\n",
      "\tspeed: 0.0384s/iter; left time: 411.6185s\n",
      "\titers: 200, epoch: 2 | loss: 0.2241254\n",
      "\tspeed: 0.0114s/iter; left time: 120.9441s\n",
      "\titers: 300, epoch: 2 | loss: 0.1987359\n",
      "\tspeed: 0.0114s/iter; left time: 119.7528s\n",
      "\titers: 400, epoch: 2 | loss: 0.2423836\n",
      "\tspeed: 0.0114s/iter; left time: 119.0787s\n",
      "\titers: 500, epoch: 2 | loss: 0.2867766\n",
      "\tspeed: 0.0114s/iter; left time: 117.5867s\n",
      "Epoch: 2 cost time: 6.824478387832642\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2576519 Vali Loss: 0.0684576 Test Loss: 0.1809007\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1952623\n",
      "\tspeed: 0.0381s/iter; left time: 387.1759s\n",
      "\titers: 200, epoch: 3 | loss: 0.2611968\n",
      "\tspeed: 0.0114s/iter; left time: 114.8335s\n",
      "\titers: 300, epoch: 3 | loss: 0.1721100\n",
      "\tspeed: 0.0114s/iter; left time: 113.2038s\n",
      "\titers: 400, epoch: 3 | loss: 0.2667773\n",
      "\tspeed: 0.0114s/iter; left time: 112.0488s\n",
      "\titers: 500, epoch: 3 | loss: 0.3224335\n",
      "\tspeed: 0.0113s/iter; left time: 110.5000s\n",
      "Epoch: 3 cost time: 6.6749584674835205\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2467329 Vali Loss: 0.0648856 Test Loss: 0.1647076\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3145069\n",
      "\tspeed: 0.0370s/iter; left time: 354.6019s\n",
      "\titers: 200, epoch: 4 | loss: 0.1908078\n",
      "\tspeed: 0.0111s/iter; left time: 105.0886s\n",
      "\titers: 300, epoch: 4 | loss: 0.2720823\n",
      "\tspeed: 0.0115s/iter; left time: 108.3005s\n",
      "\titers: 400, epoch: 4 | loss: 0.2160956\n",
      "\tspeed: 0.0122s/iter; left time: 113.0037s\n",
      "\titers: 500, epoch: 4 | loss: 0.2266363\n",
      "\tspeed: 0.0115s/iter; left time: 105.3902s\n",
      "Epoch: 4 cost time: 6.791365623474121\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2179058 Vali Loss: 0.0635907 Test Loss: 0.1608318\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1631806194782257, mae:0.26068833470344543\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1879944\n",
      "\tspeed: 0.0215s/iter; left time: 243.0087s\n",
      "\titers: 200, epoch: 1 | loss: 0.1874471\n",
      "\tspeed: 0.0098s/iter; left time: 109.7131s\n",
      "\titers: 300, epoch: 1 | loss: 0.2482666\n",
      "\tspeed: 0.0098s/iter; left time: 108.8628s\n",
      "\titers: 400, epoch: 1 | loss: 0.1488428\n",
      "\tspeed: 0.0098s/iter; left time: 107.5978s\n",
      "\titers: 500, epoch: 1 | loss: 0.1444412\n",
      "\tspeed: 0.0098s/iter; left time: 106.6762s\n",
      "Epoch: 1 cost time: 6.804545879364014\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1682830 Vali Loss: 0.0436042 Test Loss: 0.1377011\n",
      "Validation loss decreased (inf --> 0.043604).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1398871\n",
      "\tspeed: 0.0348s/iter; left time: 373.0056s\n",
      "\titers: 200, epoch: 2 | loss: 0.1850255\n",
      "\tspeed: 0.0096s/iter; left time: 102.5690s\n",
      "\titers: 300, epoch: 2 | loss: 0.1077379\n",
      "\tspeed: 0.0096s/iter; left time: 101.3430s\n",
      "\titers: 400, epoch: 2 | loss: 0.1096446\n",
      "\tspeed: 0.0096s/iter; left time: 100.6020s\n",
      "\titers: 500, epoch: 2 | loss: 0.2028266\n",
      "\tspeed: 0.0096s/iter; left time: 99.4225s\n",
      "Epoch: 2 cost time: 5.789113998413086\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1612543 Vali Loss: 0.0431032 Test Loss: 0.1407291\n",
      "Validation loss decreased (0.043604 --> 0.043103).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.0767745\n",
      "\tspeed: 0.0356s/iter; left time: 361.8705s\n",
      "\titers: 200, epoch: 3 | loss: 0.1207427\n",
      "\tspeed: 0.0097s/iter; left time: 98.0368s\n",
      "\titers: 300, epoch: 3 | loss: 0.2777740\n",
      "\tspeed: 0.0097s/iter; left time: 96.9716s\n",
      "\titers: 400, epoch: 3 | loss: 0.1337653\n",
      "\tspeed: 0.0097s/iter; left time: 95.8635s\n",
      "\titers: 500, epoch: 3 | loss: 0.1266848\n",
      "\tspeed: 0.0097s/iter; left time: 94.9676s\n",
      "Epoch: 3 cost time: 5.872825384140015\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1380364 Vali Loss: 0.0397742 Test Loss: 0.1264060\n",
      "Validation loss decreased (0.043103 --> 0.039774).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.0792852\n",
      "\tspeed: 0.0352s/iter; left time: 337.5416s\n",
      "\titers: 200, epoch: 4 | loss: 0.0799794\n",
      "\tspeed: 0.0097s/iter; left time: 91.9266s\n",
      "\titers: 300, epoch: 4 | loss: 0.0885246\n",
      "\tspeed: 0.0097s/iter; left time: 91.2989s\n",
      "\titers: 400, epoch: 4 | loss: 0.0728028\n",
      "\tspeed: 0.0097s/iter; left time: 90.2095s\n",
      "\titers: 500, epoch: 4 | loss: 0.1350426\n",
      "\tspeed: 0.0097s/iter; left time: 88.7340s\n",
      "Epoch: 4 cost time: 5.789398670196533\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1146851 Vali Loss: 0.0357869 Test Loss: 0.1162586\n",
      "Validation loss decreased (0.039774 --> 0.035787).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1119463\n",
      "\tspeed: 0.0359s/iter; left time: 323.6704s\n",
      "\titers: 200, epoch: 5 | loss: 0.1653396\n",
      "\tspeed: 0.0097s/iter; left time: 86.7166s\n",
      "\titers: 300, epoch: 5 | loss: 0.1019746\n",
      "\tspeed: 0.0097s/iter; left time: 85.7275s\n",
      "\titers: 400, epoch: 5 | loss: 0.1152093\n",
      "\tspeed: 0.0097s/iter; left time: 84.7205s\n",
      "\titers: 500, epoch: 5 | loss: 0.0985917\n",
      "\tspeed: 0.0097s/iter; left time: 83.7328s\n",
      "Epoch: 5 cost time: 5.8364338874816895\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1009639 Vali Loss: 0.0364314 Test Loss: 0.1136684\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0581600\n",
      "\tspeed: 0.0347s/iter; left time: 293.2471s\n",
      "\titers: 200, epoch: 6 | loss: 0.1095032\n",
      "\tspeed: 0.0097s/iter; left time: 80.9160s\n",
      "\titers: 300, epoch: 6 | loss: 0.1278585\n",
      "\tspeed: 0.0097s/iter; left time: 79.9372s\n",
      "\titers: 400, epoch: 6 | loss: 0.0641890\n",
      "\tspeed: 0.0097s/iter; left time: 78.9614s\n",
      "\titers: 500, epoch: 6 | loss: 0.0766375\n",
      "\tspeed: 0.0097s/iter; left time: 77.9750s\n",
      "Epoch: 6 cost time: 5.806777000427246\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0925622 Vali Loss: 0.0370014 Test Loss: 0.1155212\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0849662\n",
      "\tspeed: 0.0348s/iter; left time: 274.4081s\n",
      "\titers: 200, epoch: 7 | loss: 0.0992069\n",
      "\tspeed: 0.0097s/iter; left time: 75.4419s\n",
      "\titers: 300, epoch: 7 | loss: 0.1484343\n",
      "\tspeed: 0.0097s/iter; left time: 74.4624s\n",
      "\titers: 400, epoch: 7 | loss: 0.1500152\n",
      "\tspeed: 0.0097s/iter; left time: 73.4141s\n",
      "\titers: 500, epoch: 7 | loss: 0.1286581\n",
      "\tspeed: 0.0097s/iter; left time: 72.4792s\n",
      "Epoch: 7 cost time: 5.80428409576416\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0875537 Vali Loss: 0.0360128 Test Loss: 0.1157063\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11642131209373474, mae:0.20923534035682678\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2429955\n",
      "\tspeed: 0.0214s/iter; left time: 241.5398s\n",
      "\titers: 200, epoch: 1 | loss: 0.1262257\n",
      "\tspeed: 0.0098s/iter; left time: 109.9802s\n",
      "\titers: 300, epoch: 1 | loss: 0.1267452\n",
      "\tspeed: 0.0098s/iter; left time: 109.2423s\n",
      "\titers: 400, epoch: 1 | loss: 0.1871254\n",
      "\tspeed: 0.0098s/iter; left time: 107.9808s\n",
      "\titers: 500, epoch: 1 | loss: 0.2177178\n",
      "\tspeed: 0.0098s/iter; left time: 107.1591s\n",
      "Epoch: 1 cost time: 6.801011562347412\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2265803 Vali Loss: 0.0527771 Test Loss: 0.1677906\n",
      "Validation loss decreased (inf --> 0.052777).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1989672\n",
      "\tspeed: 0.0354s/iter; left time: 379.5368s\n",
      "\titers: 200, epoch: 2 | loss: 0.2996998\n",
      "\tspeed: 0.0097s/iter; left time: 103.5555s\n",
      "\titers: 300, epoch: 2 | loss: 0.1947185\n",
      "\tspeed: 0.0097s/iter; left time: 101.7320s\n",
      "\titers: 400, epoch: 2 | loss: 0.2522361\n",
      "\tspeed: 0.0097s/iter; left time: 100.9366s\n",
      "\titers: 500, epoch: 2 | loss: 0.2612756\n",
      "\tspeed: 0.0097s/iter; left time: 99.7859s\n",
      "Epoch: 2 cost time: 5.831964731216431\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2259816 Vali Loss: 0.0574514 Test Loss: 0.1699410\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1365477\n",
      "\tspeed: 0.0384s/iter; left time: 389.9169s\n",
      "\titers: 200, epoch: 3 | loss: 0.1770229\n",
      "\tspeed: 0.0098s/iter; left time: 98.7192s\n",
      "\titers: 300, epoch: 3 | loss: 0.2543505\n",
      "\tspeed: 0.0098s/iter; left time: 97.3624s\n",
      "\titers: 400, epoch: 3 | loss: 0.1783216\n",
      "\tspeed: 0.0098s/iter; left time: 96.4835s\n",
      "\titers: 500, epoch: 3 | loss: 0.1370625\n",
      "\tspeed: 0.0098s/iter; left time: 95.5055s\n",
      "Epoch: 3 cost time: 5.9805006980896\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2107144 Vali Loss: 0.0469428 Test Loss: 0.1451066\n",
      "Validation loss decreased (0.052777 --> 0.046943).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1333973\n",
      "\tspeed: 0.0356s/iter; left time: 341.0515s\n",
      "\titers: 200, epoch: 4 | loss: 0.0943953\n",
      "\tspeed: 0.0097s/iter; left time: 92.1147s\n",
      "\titers: 300, epoch: 4 | loss: 0.1297451\n",
      "\tspeed: 0.0097s/iter; left time: 91.1326s\n",
      "\titers: 400, epoch: 4 | loss: 0.2211343\n",
      "\tspeed: 0.0097s/iter; left time: 90.1527s\n",
      "\titers: 500, epoch: 4 | loss: 0.1433680\n",
      "\tspeed: 0.0097s/iter; left time: 88.9767s\n",
      "Epoch: 4 cost time: 5.840884208679199\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1749125 Vali Loss: 0.0473251 Test Loss: 0.1498124\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1313210\n",
      "\tspeed: 0.0352s/iter; left time: 317.5489s\n",
      "\titers: 200, epoch: 5 | loss: 0.1570131\n",
      "\tspeed: 0.0098s/iter; left time: 87.2822s\n",
      "\titers: 300, epoch: 5 | loss: 0.2370802\n",
      "\tspeed: 0.0098s/iter; left time: 86.4381s\n",
      "\titers: 400, epoch: 5 | loss: 0.1019689\n",
      "\tspeed: 0.0097s/iter; left time: 85.0051s\n",
      "\titers: 500, epoch: 5 | loss: 0.2425003\n",
      "\tspeed: 0.0097s/iter; left time: 83.9112s\n",
      "Epoch: 5 cost time: 5.827359914779663\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1612860 Vali Loss: 0.0438437 Test Loss: 0.1477919\n",
      "Validation loss decreased (0.046943 --> 0.043844).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1151529\n",
      "\tspeed: 0.0353s/iter; left time: 298.6379s\n",
      "\titers: 200, epoch: 6 | loss: 0.1784842\n",
      "\tspeed: 0.0097s/iter; left time: 80.8533s\n",
      "\titers: 300, epoch: 6 | loss: 0.1698503\n",
      "\tspeed: 0.0097s/iter; left time: 79.7852s\n",
      "\titers: 400, epoch: 6 | loss: 0.1325180\n",
      "\tspeed: 0.0097s/iter; left time: 78.8975s\n",
      "\titers: 500, epoch: 6 | loss: 0.1904864\n",
      "\tspeed: 0.0097s/iter; left time: 77.8109s\n",
      "Epoch: 6 cost time: 5.821336030960083\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1502975 Vali Loss: 0.0432721 Test Loss: 0.1421744\n",
      "Validation loss decreased (0.043844 --> 0.043272).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0912671\n",
      "\tspeed: 0.0353s/iter; left time: 278.4266s\n",
      "\titers: 200, epoch: 7 | loss: 0.1507141\n",
      "\tspeed: 0.0097s/iter; left time: 75.5434s\n",
      "\titers: 300, epoch: 7 | loss: 0.1111932\n",
      "\tspeed: 0.0097s/iter; left time: 74.5456s\n",
      "\titers: 400, epoch: 7 | loss: 0.0750316\n",
      "\tspeed: 0.0097s/iter; left time: 73.6544s\n",
      "\titers: 500, epoch: 7 | loss: 0.1784825\n",
      "\tspeed: 0.0097s/iter; left time: 72.6308s\n",
      "Epoch: 7 cost time: 5.84650731086731\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1443310 Vali Loss: 0.0433332 Test Loss: 0.1409613\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0878188\n",
      "\tspeed: 0.0354s/iter; left time: 258.8508s\n",
      "\titers: 200, epoch: 8 | loss: 0.1318479\n",
      "\tspeed: 0.0097s/iter; left time: 69.8767s\n",
      "\titers: 300, epoch: 8 | loss: 0.1270279\n",
      "\tspeed: 0.0097s/iter; left time: 68.8890s\n",
      "\titers: 400, epoch: 8 | loss: 0.1578188\n",
      "\tspeed: 0.0097s/iter; left time: 67.9465s\n",
      "\titers: 500, epoch: 8 | loss: 0.1500511\n",
      "\tspeed: 0.0097s/iter; left time: 66.9711s\n",
      "Epoch: 8 cost time: 5.831233501434326\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1401656 Vali Loss: 0.0427601 Test Loss: 0.1404220\n",
      "Validation loss decreased (0.043272 --> 0.042760).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1381372\n",
      "\tspeed: 0.0354s/iter; left time: 238.6854s\n",
      "\titers: 200, epoch: 9 | loss: 0.1403579\n",
      "\tspeed: 0.0097s/iter; left time: 64.6457s\n",
      "\titers: 300, epoch: 9 | loss: 0.1187568\n",
      "\tspeed: 0.0097s/iter; left time: 63.6444s\n",
      "\titers: 400, epoch: 9 | loss: 0.0885384\n",
      "\tspeed: 0.0097s/iter; left time: 62.6814s\n",
      "\titers: 500, epoch: 9 | loss: 0.1417777\n",
      "\tspeed: 0.0097s/iter; left time: 61.6799s\n",
      "Epoch: 9 cost time: 5.867711305618286\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1382025 Vali Loss: 0.0433924 Test Loss: 0.1427714\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1171558\n",
      "\tspeed: 0.0357s/iter; left time: 220.5606s\n",
      "\titers: 200, epoch: 10 | loss: 0.1783067\n",
      "\tspeed: 0.0097s/iter; left time: 59.0521s\n",
      "\titers: 300, epoch: 10 | loss: 0.1544783\n",
      "\tspeed: 0.0097s/iter; left time: 58.0650s\n",
      "\titers: 400, epoch: 10 | loss: 0.1501161\n",
      "\tspeed: 0.0097s/iter; left time: 57.0939s\n",
      "\titers: 500, epoch: 10 | loss: 0.1953642\n",
      "\tspeed: 0.0097s/iter; left time: 56.0842s\n",
      "Epoch: 10 cost time: 5.868743896484375\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1372004 Vali Loss: 0.0433279 Test Loss: 0.1432870\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1529302\n",
      "\tspeed: 0.0352s/iter; left time: 197.0089s\n",
      "\titers: 200, epoch: 11 | loss: 0.1255644\n",
      "\tspeed: 0.0097s/iter; left time: 53.4429s\n",
      "\titers: 300, epoch: 11 | loss: 0.1809491\n",
      "\tspeed: 0.0097s/iter; left time: 52.4519s\n",
      "\titers: 400, epoch: 11 | loss: 0.1512925\n",
      "\tspeed: 0.0097s/iter; left time: 51.4639s\n",
      "\titers: 500, epoch: 11 | loss: 0.1629266\n",
      "\tspeed: 0.0097s/iter; left time: 50.4600s\n",
      "Epoch: 11 cost time: 5.854115962982178\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1363042 Vali Loss: 0.0431477 Test Loss: 0.1432028\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1405969113111496, mae:0.24108478426933289\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2673554\n",
      "\tspeed: 0.0246s/iter; left time: 277.4486s\n",
      "\titers: 200, epoch: 1 | loss: 0.2358598\n",
      "\tspeed: 0.0124s/iter; left time: 139.2547s\n",
      "\titers: 300, epoch: 1 | loss: 0.1605896\n",
      "\tspeed: 0.0124s/iter; left time: 138.2013s\n",
      "\titers: 400, epoch: 1 | loss: 0.2092649\n",
      "\tspeed: 0.0124s/iter; left time: 136.5681s\n",
      "\titers: 500, epoch: 1 | loss: 0.3037598\n",
      "\tspeed: 0.0125s/iter; left time: 135.8223s\n",
      "Epoch: 1 cost time: 8.350462436676025\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2843307 Vali Loss: 0.0668196 Test Loss: 0.1870127\n",
      "Validation loss decreased (inf --> 0.066820).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2432782\n",
      "\tspeed: 0.0403s/iter; left time: 432.2204s\n",
      "\titers: 200, epoch: 2 | loss: 0.1925197\n",
      "\tspeed: 0.0114s/iter; left time: 121.0829s\n",
      "\titers: 300, epoch: 2 | loss: 0.1782432\n",
      "\tspeed: 0.0114s/iter; left time: 119.8990s\n",
      "\titers: 400, epoch: 2 | loss: 0.1232579\n",
      "\tspeed: 0.0114s/iter; left time: 118.7097s\n",
      "\titers: 500, epoch: 2 | loss: 0.2400337\n",
      "\tspeed: 0.0114s/iter; left time: 117.6306s\n",
      "Epoch: 2 cost time: 6.807101488113403\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2810881 Vali Loss: 0.0608437 Test Loss: 0.1798609\n",
      "Validation loss decreased (0.066820 --> 0.060844).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3295406\n",
      "\tspeed: 0.0404s/iter; left time: 410.9003s\n",
      "\titers: 200, epoch: 3 | loss: 0.4188828\n",
      "\tspeed: 0.0125s/iter; left time: 125.4526s\n",
      "\titers: 300, epoch: 3 | loss: 0.1578331\n",
      "\tspeed: 0.0124s/iter; left time: 123.9663s\n",
      "\titers: 400, epoch: 3 | loss: 0.2161615\n",
      "\tspeed: 0.0125s/iter; left time: 122.8127s\n",
      "\titers: 500, epoch: 3 | loss: 0.2143182\n",
      "\tspeed: 0.0125s/iter; left time: 121.6186s\n",
      "Epoch: 3 cost time: 7.421802043914795\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2520068 Vali Loss: 0.0636789 Test Loss: 0.1797697\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1728122\n",
      "\tspeed: 0.0381s/iter; left time: 365.7660s\n",
      "\titers: 200, epoch: 4 | loss: 0.2393166\n",
      "\tspeed: 0.0114s/iter; left time: 108.0863s\n",
      "\titers: 300, epoch: 4 | loss: 0.1626475\n",
      "\tspeed: 0.0114s/iter; left time: 107.0831s\n",
      "\titers: 400, epoch: 4 | loss: 0.1333545\n",
      "\tspeed: 0.0114s/iter; left time: 105.8298s\n",
      "\titers: 500, epoch: 4 | loss: 0.1654302\n",
      "\tspeed: 0.0114s/iter; left time: 104.7101s\n",
      "Epoch: 4 cost time: 6.775732755661011\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2222100 Vali Loss: 0.0608348 Test Loss: 0.1682695\n",
      "Validation loss decreased (0.060844 --> 0.060835).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1581249\n",
      "\tspeed: 0.0390s/iter; left time: 351.6616s\n",
      "\titers: 200, epoch: 5 | loss: 0.1555440\n",
      "\tspeed: 0.0114s/iter; left time: 101.8474s\n",
      "\titers: 300, epoch: 5 | loss: 0.2902546\n",
      "\tspeed: 0.0114s/iter; left time: 100.8013s\n",
      "\titers: 400, epoch: 5 | loss: 0.1749023\n",
      "\tspeed: 0.0114s/iter; left time: 99.5991s\n",
      "\titers: 500, epoch: 5 | loss: 0.2035597\n",
      "\tspeed: 0.0114s/iter; left time: 98.6323s\n",
      "Epoch: 5 cost time: 6.852463245391846\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2089508 Vali Loss: 0.0620068 Test Loss: 0.1694303\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1826471\n",
      "\tspeed: 0.0393s/iter; left time: 332.0718s\n",
      "\titers: 200, epoch: 6 | loss: 0.1136283\n",
      "\tspeed: 0.0114s/iter; left time: 95.5420s\n",
      "\titers: 300, epoch: 6 | loss: 0.2019654\n",
      "\tspeed: 0.0114s/iter; left time: 94.4027s\n",
      "\titers: 400, epoch: 6 | loss: 0.1772547\n",
      "\tspeed: 0.0115s/iter; left time: 93.4760s\n",
      "\titers: 500, epoch: 6 | loss: 0.1489908\n",
      "\tspeed: 0.0115s/iter; left time: 92.2022s\n",
      "Epoch: 6 cost time: 6.887919902801514\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2033361 Vali Loss: 0.0616341 Test Loss: 0.1669102\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2622835\n",
      "\tspeed: 0.0386s/iter; left time: 304.3293s\n",
      "\titers: 200, epoch: 7 | loss: 0.2151243\n",
      "\tspeed: 0.0116s/iter; left time: 90.2471s\n",
      "\titers: 300, epoch: 7 | loss: 0.2167960\n",
      "\tspeed: 0.0114s/iter; left time: 87.4714s\n",
      "\titers: 400, epoch: 7 | loss: 0.1562091\n",
      "\tspeed: 0.0114s/iter; left time: 86.2599s\n",
      "\titers: 500, epoch: 7 | loss: 0.2143835\n",
      "\tspeed: 0.0114s/iter; left time: 85.1803s\n",
      "Epoch: 7 cost time: 6.901947021484375\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1996851 Vali Loss: 0.0609210 Test Loss: 0.1667071\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1685466766357422, mae:0.27037519216537476\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1208526\n",
      "\tspeed: 0.0230s/iter; left time: 259.3597s\n",
      "\titers: 200, epoch: 1 | loss: 0.2859458\n",
      "\tspeed: 0.0110s/iter; left time: 123.1896s\n",
      "\titers: 300, epoch: 1 | loss: 0.1719414\n",
      "\tspeed: 0.0110s/iter; left time: 122.2438s\n",
      "\titers: 400, epoch: 1 | loss: 0.2156864\n",
      "\tspeed: 0.0110s/iter; left time: 120.9763s\n",
      "\titers: 500, epoch: 1 | loss: 0.1559027\n",
      "\tspeed: 0.0110s/iter; left time: 120.0080s\n",
      "Epoch: 1 cost time: 7.515220880508423\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2086973 Vali Loss: 0.0515904 Test Loss: 0.1532065\n",
      "Validation loss decreased (inf --> 0.051590).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2164857\n",
      "\tspeed: 0.0384s/iter; left time: 412.4486s\n",
      "\titers: 200, epoch: 2 | loss: 0.1621193\n",
      "\tspeed: 0.0110s/iter; left time: 117.0340s\n",
      "\titers: 300, epoch: 2 | loss: 0.2461625\n",
      "\tspeed: 0.0110s/iter; left time: 115.8736s\n",
      "\titers: 400, epoch: 2 | loss: 0.3248372\n",
      "\tspeed: 0.0110s/iter; left time: 114.4949s\n",
      "\titers: 500, epoch: 2 | loss: 0.3872446\n",
      "\tspeed: 0.0110s/iter; left time: 113.5162s\n",
      "Epoch: 2 cost time: 6.597521066665649\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2381216 Vali Loss: 0.0526147 Test Loss: 0.1589930\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2298876\n",
      "\tspeed: 0.0381s/iter; left time: 387.4630s\n",
      "\titers: 200, epoch: 3 | loss: 0.0892347\n",
      "\tspeed: 0.0111s/iter; left time: 111.2457s\n",
      "\titers: 300, epoch: 3 | loss: 0.2041319\n",
      "\tspeed: 0.0110s/iter; left time: 109.6310s\n",
      "\titers: 400, epoch: 3 | loss: 0.2050402\n",
      "\tspeed: 0.0110s/iter; left time: 108.4128s\n",
      "\titers: 500, epoch: 3 | loss: 0.2003963\n",
      "\tspeed: 0.0110s/iter; left time: 107.1588s\n",
      "Epoch: 3 cost time: 6.672077417373657\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1838980 Vali Loss: 0.0454281 Test Loss: 0.1411109\n",
      "Validation loss decreased (0.051590 --> 0.045428).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1862133\n",
      "\tspeed: 0.0394s/iter; left time: 378.0582s\n",
      "\titers: 200, epoch: 4 | loss: 0.1526703\n",
      "\tspeed: 0.0121s/iter; left time: 114.9258s\n",
      "\titers: 300, epoch: 4 | loss: 0.0859537\n",
      "\tspeed: 0.0121s/iter; left time: 113.3613s\n",
      "\titers: 400, epoch: 4 | loss: 0.1531971\n",
      "\tspeed: 0.0121s/iter; left time: 112.2108s\n",
      "\titers: 500, epoch: 4 | loss: 0.1257672\n",
      "\tspeed: 0.0115s/iter; left time: 105.5564s\n",
      "Epoch: 4 cost time: 7.066507577896118\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1588286 Vali Loss: 0.0467338 Test Loss: 0.1424807\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1165126\n",
      "\tspeed: 0.0375s/iter; left time: 338.0736s\n",
      "\titers: 200, epoch: 5 | loss: 0.1202716\n",
      "\tspeed: 0.0110s/iter; left time: 98.3584s\n",
      "\titers: 300, epoch: 5 | loss: 0.1846251\n",
      "\tspeed: 0.0111s/iter; left time: 97.7525s\n",
      "\titers: 400, epoch: 5 | loss: 0.1324604\n",
      "\tspeed: 0.0111s/iter; left time: 96.4726s\n",
      "\titers: 500, epoch: 5 | loss: 0.1090243\n",
      "\tspeed: 0.0110s/iter; left time: 95.2249s\n",
      "Epoch: 5 cost time: 6.622970104217529\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1417700 Vali Loss: 0.0407827 Test Loss: 0.1379961\n",
      "Validation loss decreased (0.045428 --> 0.040783).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1700202\n",
      "\tspeed: 0.0377s/iter; left time: 318.8579s\n",
      "\titers: 200, epoch: 6 | loss: 0.1496533\n",
      "\tspeed: 0.0110s/iter; left time: 92.0231s\n",
      "\titers: 300, epoch: 6 | loss: 0.0920784\n",
      "\tspeed: 0.0110s/iter; left time: 91.1246s\n",
      "\titers: 400, epoch: 6 | loss: 0.0841550\n",
      "\tspeed: 0.0110s/iter; left time: 89.9901s\n",
      "\titers: 500, epoch: 6 | loss: 0.1100778\n",
      "\tspeed: 0.0110s/iter; left time: 88.7411s\n",
      "Epoch: 6 cost time: 6.595178842544556\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1324151 Vali Loss: 0.0401964 Test Loss: 0.1361016\n",
      "Validation loss decreased (0.040783 --> 0.040196).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0873016\n",
      "\tspeed: 0.0397s/iter; left time: 312.8614s\n",
      "\titers: 200, epoch: 7 | loss: 0.1881062\n",
      "\tspeed: 0.0111s/iter; left time: 86.0055s\n",
      "\titers: 300, epoch: 7 | loss: 0.1087534\n",
      "\tspeed: 0.0110s/iter; left time: 84.6569s\n",
      "\titers: 400, epoch: 7 | loss: 0.1095457\n",
      "\tspeed: 0.0110s/iter; left time: 83.5582s\n",
      "\titers: 500, epoch: 7 | loss: 0.0909741\n",
      "\tspeed: 0.0110s/iter; left time: 82.5666s\n",
      "Epoch: 7 cost time: 6.594077825546265\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1267670 Vali Loss: 0.0402969 Test Loss: 0.1349403\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1074128\n",
      "\tspeed: 0.0385s/iter; left time: 281.5454s\n",
      "\titers: 200, epoch: 8 | loss: 0.1466217\n",
      "\tspeed: 0.0121s/iter; left time: 87.0233s\n",
      "\titers: 300, epoch: 8 | loss: 0.1092096\n",
      "\tspeed: 0.0120s/iter; left time: 85.6172s\n",
      "\titers: 400, epoch: 8 | loss: 0.0924052\n",
      "\tspeed: 0.0120s/iter; left time: 84.4063s\n",
      "\titers: 500, epoch: 8 | loss: 0.1204413\n",
      "\tspeed: 0.0120s/iter; left time: 83.2477s\n",
      "Epoch: 8 cost time: 7.180884838104248\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1230836 Vali Loss: 0.0399046 Test Loss: 0.1353944\n",
      "Validation loss decreased (0.040196 --> 0.039905).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1407759\n",
      "\tspeed: 0.0401s/iter; left time: 270.4314s\n",
      "\titers: 200, epoch: 9 | loss: 0.0852659\n",
      "\tspeed: 0.0110s/iter; left time: 73.1085s\n",
      "\titers: 300, epoch: 9 | loss: 0.1555552\n",
      "\tspeed: 0.0110s/iter; left time: 71.9556s\n",
      "\titers: 400, epoch: 9 | loss: 0.1433156\n",
      "\tspeed: 0.0110s/iter; left time: 71.0808s\n",
      "\titers: 500, epoch: 9 | loss: 0.1607147\n",
      "\tspeed: 0.0110s/iter; left time: 69.9153s\n",
      "Epoch: 9 cost time: 6.589288711547852\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1213381 Vali Loss: 0.0391797 Test Loss: 0.1352211\n",
      "Validation loss decreased (0.039905 --> 0.039180).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1191340\n",
      "\tspeed: 0.0380s/iter; left time: 234.4037s\n",
      "\titers: 200, epoch: 10 | loss: 0.0910964\n",
      "\tspeed: 0.0110s/iter; left time: 67.0451s\n",
      "\titers: 300, epoch: 10 | loss: 0.1056505\n",
      "\tspeed: 0.0111s/iter; left time: 66.0089s\n",
      "\titers: 400, epoch: 10 | loss: 0.1334907\n",
      "\tspeed: 0.0111s/iter; left time: 64.9510s\n",
      "\titers: 500, epoch: 10 | loss: 0.1398455\n",
      "\tspeed: 0.0111s/iter; left time: 63.8760s\n",
      "Epoch: 10 cost time: 6.587714672088623\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1202429 Vali Loss: 0.0395060 Test Loss: 0.1355136\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.0820738\n",
      "\tspeed: 0.0369s/iter; left time: 206.9044s\n",
      "\titers: 200, epoch: 11 | loss: 0.0953467\n",
      "\tspeed: 0.0110s/iter; left time: 60.4672s\n",
      "\titers: 300, epoch: 11 | loss: 0.1034635\n",
      "\tspeed: 0.0110s/iter; left time: 59.5186s\n",
      "\titers: 400, epoch: 11 | loss: 0.1124705\n",
      "\tspeed: 0.0110s/iter; left time: 58.2764s\n",
      "\titers: 500, epoch: 11 | loss: 0.1682471\n",
      "\tspeed: 0.0110s/iter; left time: 57.2027s\n",
      "Epoch: 11 cost time: 6.561575889587402\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1195659 Vali Loss: 0.0396204 Test Loss: 0.1359874\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1432239\n",
      "\tspeed: 0.0390s/iter; left time: 196.2183s\n",
      "\titers: 200, epoch: 12 | loss: 0.0984666\n",
      "\tspeed: 0.0120s/iter; left time: 59.3885s\n",
      "\titers: 300, epoch: 12 | loss: 0.0857073\n",
      "\tspeed: 0.0120s/iter; left time: 58.0878s\n",
      "\titers: 400, epoch: 12 | loss: 0.1472204\n",
      "\tspeed: 0.0121s/iter; left time: 57.0231s\n",
      "\titers: 500, epoch: 12 | loss: 0.1346401\n",
      "\tspeed: 0.0113s/iter; left time: 52.3649s\n",
      "Epoch: 12 cost time: 7.053299903869629\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1190357 Vali Loss: 0.0397651 Test Loss: 0.1360321\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13543544709682465, mae:0.23163580894470215\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2081373\n",
      "\tspeed: 0.0221s/iter; left time: 249.9206s\n",
      "\titers: 200, epoch: 1 | loss: 0.3531043\n",
      "\tspeed: 0.0102s/iter; left time: 114.6250s\n",
      "\titers: 300, epoch: 1 | loss: 0.3283095\n",
      "\tspeed: 0.0102s/iter; left time: 113.7782s\n",
      "\titers: 400, epoch: 1 | loss: 0.2961910\n",
      "\tspeed: 0.0102s/iter; left time: 112.4737s\n",
      "\titers: 500, epoch: 1 | loss: 0.2852784\n",
      "\tspeed: 0.0102s/iter; left time: 111.5174s\n",
      "Epoch: 1 cost time: 7.073476791381836\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2701385 Vali Loss: 0.0734769 Test Loss: 0.2123958\n",
      "Validation loss decreased (inf --> 0.073477).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2973227\n",
      "\tspeed: 0.0367s/iter; left time: 393.9319s\n",
      "\titers: 200, epoch: 2 | loss: 0.2906811\n",
      "\tspeed: 0.0102s/iter; left time: 108.9230s\n",
      "\titers: 300, epoch: 2 | loss: 0.1953369\n",
      "\tspeed: 0.0102s/iter; left time: 107.5501s\n",
      "\titers: 400, epoch: 2 | loss: 0.2043611\n",
      "\tspeed: 0.0102s/iter; left time: 106.6449s\n",
      "\titers: 500, epoch: 2 | loss: 0.2483530\n",
      "\tspeed: 0.0102s/iter; left time: 105.4227s\n",
      "Epoch: 2 cost time: 6.165688991546631\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2728299 Vali Loss: 0.0679701 Test Loss: 0.1997746\n",
      "Validation loss decreased (0.073477 --> 0.067970).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1554442\n",
      "\tspeed: 0.0373s/iter; left time: 378.6739s\n",
      "\titers: 200, epoch: 3 | loss: 0.1863188\n",
      "\tspeed: 0.0103s/iter; left time: 103.3202s\n",
      "\titers: 300, epoch: 3 | loss: 0.2836081\n",
      "\tspeed: 0.0103s/iter; left time: 102.5671s\n",
      "\titers: 400, epoch: 3 | loss: 0.2544675\n",
      "\tspeed: 0.0103s/iter; left time: 101.4825s\n",
      "\titers: 500, epoch: 3 | loss: 0.2589924\n",
      "\tspeed: 0.0103s/iter; left time: 100.2030s\n",
      "Epoch: 3 cost time: 6.174490213394165\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2451306 Vali Loss: 0.0637612 Test Loss: 0.1744619\n",
      "Validation loss decreased (0.067970 --> 0.063761).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2316469\n",
      "\tspeed: 0.0376s/iter; left time: 360.2339s\n",
      "\titers: 200, epoch: 4 | loss: 0.1120871\n",
      "\tspeed: 0.0103s/iter; left time: 97.4191s\n",
      "\titers: 300, epoch: 4 | loss: 0.2192818\n",
      "\tspeed: 0.0103s/iter; left time: 96.8030s\n",
      "\titers: 400, epoch: 4 | loss: 0.3106847\n",
      "\tspeed: 0.0103s/iter; left time: 95.8498s\n",
      "\titers: 500, epoch: 4 | loss: 0.2026028\n",
      "\tspeed: 0.0103s/iter; left time: 94.7682s\n",
      "Epoch: 4 cost time: 6.2024242877960205\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2175378 Vali Loss: 0.0593487 Test Loss: 0.1607444\n",
      "Validation loss decreased (0.063761 --> 0.059349).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1770361\n",
      "\tspeed: 0.0371s/iter; left time: 334.8778s\n",
      "\titers: 200, epoch: 5 | loss: 0.1810164\n",
      "\tspeed: 0.0103s/iter; left time: 91.8261s\n",
      "\titers: 300, epoch: 5 | loss: 0.2119149\n",
      "\tspeed: 0.0103s/iter; left time: 90.6111s\n",
      "\titers: 400, epoch: 5 | loss: 0.1309484\n",
      "\tspeed: 0.0103s/iter; left time: 89.4972s\n",
      "\titers: 500, epoch: 5 | loss: 0.2969440\n",
      "\tspeed: 0.0103s/iter; left time: 88.5535s\n",
      "Epoch: 5 cost time: 6.18122935295105\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1965188 Vali Loss: 0.0559909 Test Loss: 0.1539086\n",
      "Validation loss decreased (0.059349 --> 0.055991).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1419153\n",
      "\tspeed: 0.0381s/iter; left time: 322.2824s\n",
      "\titers: 200, epoch: 6 | loss: 0.1659421\n",
      "\tspeed: 0.0114s/iter; left time: 95.1239s\n",
      "\titers: 300, epoch: 6 | loss: 0.2197290\n",
      "\tspeed: 0.0114s/iter; left time: 94.0517s\n",
      "\titers: 400, epoch: 6 | loss: 0.1411058\n",
      "\tspeed: 0.0114s/iter; left time: 92.9358s\n",
      "\titers: 500, epoch: 6 | loss: 0.1455500\n",
      "\tspeed: 0.0114s/iter; left time: 91.8088s\n",
      "Epoch: 6 cost time: 6.735754489898682\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1863750 Vali Loss: 0.0566236 Test Loss: 0.1557629\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1294671\n",
      "\tspeed: 0.0366s/iter; left time: 288.4193s\n",
      "\titers: 200, epoch: 7 | loss: 0.1749182\n",
      "\tspeed: 0.0102s/iter; left time: 79.6240s\n",
      "\titers: 300, epoch: 7 | loss: 0.1338294\n",
      "\tspeed: 0.0102s/iter; left time: 78.5928s\n",
      "\titers: 400, epoch: 7 | loss: 0.2332579\n",
      "\tspeed: 0.0102s/iter; left time: 77.5106s\n",
      "\titers: 500, epoch: 7 | loss: 0.1354468\n",
      "\tspeed: 0.0102s/iter; left time: 76.4637s\n",
      "Epoch: 7 cost time: 6.12508487701416\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1792891 Vali Loss: 0.0548309 Test Loss: 0.1546936\n",
      "Validation loss decreased (0.055991 --> 0.054831).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1518206\n",
      "\tspeed: 0.0363s/iter; left time: 265.5282s\n",
      "\titers: 200, epoch: 8 | loss: 0.2262946\n",
      "\tspeed: 0.0102s/iter; left time: 73.9107s\n",
      "\titers: 300, epoch: 8 | loss: 0.2203848\n",
      "\tspeed: 0.0103s/iter; left time: 73.0005s\n",
      "\titers: 400, epoch: 8 | loss: 0.1756611\n",
      "\tspeed: 0.0103s/iter; left time: 71.9909s\n",
      "\titers: 500, epoch: 8 | loss: 0.1367280\n",
      "\tspeed: 0.0103s/iter; left time: 70.9266s\n",
      "Epoch: 8 cost time: 6.151458501815796\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1763261 Vali Loss: 0.0542998 Test Loss: 0.1541872\n",
      "Validation loss decreased (0.054831 --> 0.054300).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1023468\n",
      "\tspeed: 0.0373s/iter; left time: 251.6671s\n",
      "\titers: 200, epoch: 9 | loss: 0.1155551\n",
      "\tspeed: 0.0103s/iter; left time: 68.5821s\n",
      "\titers: 300, epoch: 9 | loss: 0.2510663\n",
      "\tspeed: 0.0102s/iter; left time: 67.0234s\n",
      "\titers: 400, epoch: 9 | loss: 0.2702790\n",
      "\tspeed: 0.0102s/iter; left time: 65.7576s\n",
      "\titers: 500, epoch: 9 | loss: 0.0944196\n",
      "\tspeed: 0.0102s/iter; left time: 64.7729s\n",
      "Epoch: 9 cost time: 6.1549718379974365\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1741916 Vali Loss: 0.0551400 Test Loss: 0.1549536\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2340378\n",
      "\tspeed: 0.0367s/iter; left time: 226.7567s\n",
      "\titers: 200, epoch: 10 | loss: 0.0925931\n",
      "\tspeed: 0.0102s/iter; left time: 62.2094s\n",
      "\titers: 300, epoch: 10 | loss: 0.1878417\n",
      "\tspeed: 0.0102s/iter; left time: 60.8247s\n",
      "\titers: 400, epoch: 10 | loss: 0.0969002\n",
      "\tspeed: 0.0102s/iter; left time: 59.8477s\n",
      "\titers: 500, epoch: 10 | loss: 0.1904809\n",
      "\tspeed: 0.0102s/iter; left time: 58.8705s\n",
      "Epoch: 10 cost time: 6.138896703720093\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1730067 Vali Loss: 0.0548287 Test Loss: 0.1543280\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1151982\n",
      "\tspeed: 0.0359s/iter; left time: 201.2769s\n",
      "\titers: 200, epoch: 11 | loss: 0.1987495\n",
      "\tspeed: 0.0103s/iter; left time: 56.5227s\n",
      "\titers: 300, epoch: 11 | loss: 0.1097980\n",
      "\tspeed: 0.0103s/iter; left time: 55.4841s\n",
      "\titers: 400, epoch: 11 | loss: 0.1088090\n",
      "\tspeed: 0.0103s/iter; left time: 54.5856s\n",
      "\titers: 500, epoch: 11 | loss: 0.1258720\n",
      "\tspeed: 0.0103s/iter; left time: 53.5481s\n",
      "Epoch: 11 cost time: 6.190441370010376\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1726455 Vali Loss: 0.0548595 Test Loss: 0.1545657\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15438935160636902, mae:0.2481307089328766\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1910524\n",
      "\tspeed: 0.0274s/iter; left time: 309.0834s\n",
      "\titers: 200, epoch: 1 | loss: 0.1724668\n",
      "\tspeed: 0.0154s/iter; left time: 172.3475s\n",
      "\titers: 300, epoch: 1 | loss: 0.2917525\n",
      "\tspeed: 0.0154s/iter; left time: 171.1032s\n",
      "\titers: 400, epoch: 1 | loss: 0.1924879\n",
      "\tspeed: 0.0155s/iter; left time: 170.0685s\n",
      "\titers: 500, epoch: 1 | loss: 0.1730633\n",
      "\tspeed: 0.0155s/iter; left time: 168.6396s\n",
      "Epoch: 1 cost time: 10.041582822799683\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2224579 Vali Loss: 0.0565231 Test Loss: 0.1590333\n",
      "Validation loss decreased (inf --> 0.056523).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2762584\n",
      "\tspeed: 0.0515s/iter; left time: 552.8862s\n",
      "\titers: 200, epoch: 2 | loss: 0.2946938\n",
      "\tspeed: 0.0155s/iter; left time: 165.0776s\n",
      "\titers: 300, epoch: 2 | loss: 0.2403137\n",
      "\tspeed: 0.0155s/iter; left time: 163.2629s\n",
      "\titers: 400, epoch: 2 | loss: 0.2267851\n",
      "\tspeed: 0.0155s/iter; left time: 161.8370s\n",
      "\titers: 500, epoch: 2 | loss: 0.3126200\n",
      "\tspeed: 0.0155s/iter; left time: 159.9422s\n",
      "Epoch: 2 cost time: 9.166327476501465\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2804122 Vali Loss: 0.0803362 Test Loss: 0.2180425\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.4174822\n",
      "\tspeed: 0.0457s/iter; left time: 464.7789s\n",
      "\titers: 200, epoch: 3 | loss: 0.3376241\n",
      "\tspeed: 0.0137s/iter; left time: 137.6963s\n",
      "\titers: 300, epoch: 3 | loss: 0.1651516\n",
      "\tspeed: 0.0137s/iter; left time: 136.2773s\n",
      "\titers: 400, epoch: 3 | loss: 0.2693733\n",
      "\tspeed: 0.0137s/iter; left time: 134.9807s\n",
      "\titers: 500, epoch: 3 | loss: 0.2479194\n",
      "\tspeed: 0.0137s/iter; left time: 133.7863s\n",
      "Epoch: 3 cost time: 8.090153217315674\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2874916 Vali Loss: 0.0774770 Test Loss: 0.1967589\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2508566\n",
      "\tspeed: 0.0458s/iter; left time: 438.9825s\n",
      "\titers: 200, epoch: 4 | loss: 0.2272388\n",
      "\tspeed: 0.0139s/iter; left time: 131.4616s\n",
      "\titers: 300, epoch: 4 | loss: 0.2643112\n",
      "\tspeed: 0.0138s/iter; left time: 129.9154s\n",
      "\titers: 400, epoch: 4 | loss: 0.2609861\n",
      "\tspeed: 0.0138s/iter; left time: 128.4933s\n",
      "\titers: 500, epoch: 4 | loss: 0.2996611\n",
      "\tspeed: 0.0138s/iter; left time: 126.8765s\n",
      "Epoch: 4 cost time: 8.185348987579346\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2648740 Vali Loss: 0.0613281 Test Loss: 0.1687082\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1592792421579361, mae:0.2639329433441162\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2919293\n",
      "\tspeed: 0.0271s/iter; left time: 305.8642s\n",
      "\titers: 200, epoch: 1 | loss: 0.1859017\n",
      "\tspeed: 0.0152s/iter; left time: 170.0001s\n",
      "\titers: 300, epoch: 1 | loss: 0.1094763\n",
      "\tspeed: 0.0152s/iter; left time: 168.2682s\n",
      "\titers: 400, epoch: 1 | loss: 0.2039593\n",
      "\tspeed: 0.0152s/iter; left time: 166.9819s\n",
      "\titers: 500, epoch: 1 | loss: 0.2330858\n",
      "\tspeed: 0.0151s/iter; left time: 164.9827s\n",
      "Epoch: 1 cost time: 9.88548493385315\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1849884 Vali Loss: 0.0479409 Test Loss: 0.1547139\n",
      "Validation loss decreased (inf --> 0.047941).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1143082\n",
      "\tspeed: 0.0489s/iter; left time: 524.4933s\n",
      "\titers: 200, epoch: 2 | loss: 0.3537095\n",
      "\tspeed: 0.0152s/iter; left time: 161.6597s\n",
      "\titers: 300, epoch: 2 | loss: 0.3505127\n",
      "\tspeed: 0.0152s/iter; left time: 159.6306s\n",
      "\titers: 400, epoch: 2 | loss: 0.2553155\n",
      "\tspeed: 0.0141s/iter; left time: 147.1699s\n",
      "\titers: 500, epoch: 2 | loss: 0.2997040\n",
      "\tspeed: 0.0135s/iter; left time: 139.1841s\n",
      "Epoch: 2 cost time: 8.584050416946411\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2630000 Vali Loss: 0.0724070 Test Loss: 0.1898956\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2477942\n",
      "\tspeed: 0.0452s/iter; left time: 458.7992s\n",
      "\titers: 200, epoch: 3 | loss: 0.1261086\n",
      "\tspeed: 0.0135s/iter; left time: 135.8987s\n",
      "\titers: 300, epoch: 3 | loss: 0.2619452\n",
      "\tspeed: 0.0135s/iter; left time: 134.3725s\n",
      "\titers: 400, epoch: 3 | loss: 0.1103649\n",
      "\tspeed: 0.0135s/iter; left time: 132.8750s\n",
      "\titers: 500, epoch: 3 | loss: 0.2083330\n",
      "\tspeed: 0.0135s/iter; left time: 131.7530s\n",
      "Epoch: 3 cost time: 8.001125812530518\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2439919 Vali Loss: 0.0668979 Test Loss: 0.1739569\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1651082\n",
      "\tspeed: 0.0441s/iter; left time: 423.1318s\n",
      "\titers: 200, epoch: 4 | loss: 0.2440787\n",
      "\tspeed: 0.0135s/iter; left time: 128.3493s\n",
      "\titers: 300, epoch: 4 | loss: 0.3100405\n",
      "\tspeed: 0.0135s/iter; left time: 127.1786s\n",
      "\titers: 400, epoch: 4 | loss: 0.1410217\n",
      "\tspeed: 0.0135s/iter; left time: 125.7899s\n",
      "\titers: 500, epoch: 4 | loss: 0.1318116\n",
      "\tspeed: 0.0135s/iter; left time: 124.2897s\n",
      "Epoch: 4 cost time: 8.02341604232788\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2142600 Vali Loss: 0.0600810 Test Loss: 0.1641871\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1549413502216339, mae:0.2512667179107666\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1190500\n",
      "\tspeed: 0.0249s/iter; left time: 281.3236s\n",
      "\titers: 200, epoch: 1 | loss: 0.3118449\n",
      "\tspeed: 0.0134s/iter; left time: 149.8381s\n",
      "\titers: 300, epoch: 1 | loss: 0.3297839\n",
      "\tspeed: 0.0134s/iter; left time: 148.2572s\n",
      "\titers: 400, epoch: 1 | loss: 0.2602028\n",
      "\tspeed: 0.0134s/iter; left time: 146.9362s\n",
      "\titers: 500, epoch: 1 | loss: 0.2521578\n",
      "\tspeed: 0.0134s/iter; left time: 145.8457s\n",
      "Epoch: 1 cost time: 8.82106637954712\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2388101 Vali Loss: 0.0638805 Test Loss: 0.1876526\n",
      "Validation loss decreased (inf --> 0.063880).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2265279\n",
      "\tspeed: 0.0447s/iter; left time: 479.8204s\n",
      "\titers: 200, epoch: 2 | loss: 0.1576295\n",
      "\tspeed: 0.0134s/iter; left time: 142.1581s\n",
      "\titers: 300, epoch: 2 | loss: 0.1770525\n",
      "\tspeed: 0.0134s/iter; left time: 140.8819s\n",
      "\titers: 400, epoch: 2 | loss: 0.2324228\n",
      "\tspeed: 0.0134s/iter; left time: 139.7155s\n",
      "\titers: 500, epoch: 2 | loss: 0.3294505\n",
      "\tspeed: 0.0134s/iter; left time: 138.3034s\n",
      "Epoch: 2 cost time: 7.933163642883301\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2661254 Vali Loss: 0.0713454 Test Loss: 0.1848145\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2670662\n",
      "\tspeed: 0.0471s/iter; left time: 478.9106s\n",
      "\titers: 200, epoch: 3 | loss: 0.2732421\n",
      "\tspeed: 0.0134s/iter; left time: 135.0131s\n",
      "\titers: 300, epoch: 3 | loss: 0.2004428\n",
      "\tspeed: 0.0134s/iter; left time: 133.5236s\n",
      "\titers: 400, epoch: 3 | loss: 0.1749303\n",
      "\tspeed: 0.0134s/iter; left time: 131.8350s\n",
      "\titers: 500, epoch: 3 | loss: 0.2124099\n",
      "\tspeed: 0.0134s/iter; left time: 130.3521s\n",
      "Epoch: 3 cost time: 7.914586067199707\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2391176 Vali Loss: 0.0647277 Test Loss: 0.1720510\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2196204\n",
      "\tspeed: 0.0442s/iter; left time: 424.1994s\n",
      "\titers: 200, epoch: 4 | loss: 0.1945581\n",
      "\tspeed: 0.0135s/iter; left time: 128.1547s\n",
      "\titers: 300, epoch: 4 | loss: 0.1817180\n",
      "\tspeed: 0.0135s/iter; left time: 126.8518s\n",
      "\titers: 400, epoch: 4 | loss: 0.1947605\n",
      "\tspeed: 0.0135s/iter; left time: 125.7008s\n",
      "\titers: 500, epoch: 4 | loss: 0.1333836\n",
      "\tspeed: 0.0135s/iter; left time: 123.7967s\n",
      "Epoch: 4 cost time: 7.973385334014893\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2129358 Vali Loss: 0.0605438 Test Loss: 0.1664300\n",
      "Validation loss decreased (0.063880 --> 0.060544).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1738715\n",
      "\tspeed: 0.0454s/iter; left time: 409.1702s\n",
      "\titers: 200, epoch: 5 | loss: 0.2195346\n",
      "\tspeed: 0.0135s/iter; left time: 120.0968s\n",
      "\titers: 300, epoch: 5 | loss: 0.1826030\n",
      "\tspeed: 0.0135s/iter; left time: 118.7707s\n",
      "\titers: 400, epoch: 5 | loss: 0.2756516\n",
      "\tspeed: 0.0134s/iter; left time: 117.2315s\n",
      "\titers: 500, epoch: 5 | loss: 0.1223429\n",
      "\tspeed: 0.0134s/iter; left time: 115.6241s\n",
      "Epoch: 5 cost time: 8.075530290603638\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1918286 Vali Loss: 0.0575880 Test Loss: 0.1577104\n",
      "Validation loss decreased (0.060544 --> 0.057588).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2719165\n",
      "\tspeed: 0.0478s/iter; left time: 403.8005s\n",
      "\titers: 200, epoch: 6 | loss: 0.1927407\n",
      "\tspeed: 0.0144s/iter; left time: 120.4611s\n",
      "\titers: 300, epoch: 6 | loss: 0.0962062\n",
      "\tspeed: 0.0135s/iter; left time: 111.2607s\n",
      "\titers: 400, epoch: 6 | loss: 0.2174937\n",
      "\tspeed: 0.0135s/iter; left time: 109.8067s\n",
      "\titers: 500, epoch: 6 | loss: 0.1846272\n",
      "\tspeed: 0.0135s/iter; left time: 108.5833s\n",
      "Epoch: 6 cost time: 8.243571519851685\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1781630 Vali Loss: 0.0573460 Test Loss: 0.1543588\n",
      "Validation loss decreased (0.057588 --> 0.057346).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1491029\n",
      "\tspeed: 0.0467s/iter; left time: 367.9004s\n",
      "\titers: 200, epoch: 7 | loss: 0.0993516\n",
      "\tspeed: 0.0151s/iter; left time: 117.2180s\n",
      "\titers: 300, epoch: 7 | loss: 0.1379019\n",
      "\tspeed: 0.0151s/iter; left time: 115.6785s\n",
      "\titers: 400, epoch: 7 | loss: 0.2205217\n",
      "\tspeed: 0.0151s/iter; left time: 114.2879s\n",
      "\titers: 500, epoch: 7 | loss: 0.1336048\n",
      "\tspeed: 0.0150s/iter; left time: 112.3944s\n",
      "Epoch: 7 cost time: 8.87835168838501\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1709848 Vali Loss: 0.0576333 Test Loss: 0.1552133\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1184507\n",
      "\tspeed: 0.0474s/iter; left time: 346.8295s\n",
      "\titers: 200, epoch: 8 | loss: 0.1265137\n",
      "\tspeed: 0.0134s/iter; left time: 96.5043s\n",
      "\titers: 300, epoch: 8 | loss: 0.1915678\n",
      "\tspeed: 0.0133s/iter; left time: 94.8034s\n",
      "\titers: 400, epoch: 8 | loss: 0.2026834\n",
      "\tspeed: 0.0133s/iter; left time: 93.3049s\n",
      "\titers: 500, epoch: 8 | loss: 0.1000716\n",
      "\tspeed: 0.0133s/iter; left time: 91.8657s\n",
      "Epoch: 8 cost time: 7.886708974838257\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1666280 Vali Loss: 0.0592988 Test Loss: 0.1568683\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1650104\n",
      "\tspeed: 0.0442s/iter; left time: 298.2428s\n",
      "\titers: 200, epoch: 9 | loss: 0.1470891\n",
      "\tspeed: 0.0133s/iter; left time: 88.5911s\n",
      "\titers: 300, epoch: 9 | loss: 0.1308380\n",
      "\tspeed: 0.0134s/iter; left time: 87.3612s\n",
      "\titers: 400, epoch: 9 | loss: 0.1730627\n",
      "\tspeed: 0.0134s/iter; left time: 86.2064s\n",
      "\titers: 500, epoch: 9 | loss: 0.1253441\n",
      "\tspeed: 0.0134s/iter; left time: 84.6815s\n",
      "Epoch: 9 cost time: 7.9226438999176025\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1645545 Vali Loss: 0.0580829 Test Loss: 0.1555741\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15459345281124115, mae:0.2520411014556885\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2702464\n",
      "\tspeed: 0.0298s/iter; left time: 337.0178s\n",
      "\titers: 200, epoch: 1 | loss: 0.2674955\n",
      "\tspeed: 0.0177s/iter; left time: 198.7197s\n",
      "\titers: 300, epoch: 1 | loss: 0.3488625\n",
      "\tspeed: 0.0170s/iter; left time: 188.7366s\n",
      "\titers: 400, epoch: 1 | loss: 0.2293683\n",
      "\tspeed: 0.0170s/iter; left time: 186.5746s\n",
      "\titers: 500, epoch: 1 | loss: 0.2553122\n",
      "\tspeed: 0.0178s/iter; left time: 193.8008s\n",
      "Epoch: 1 cost time: 11.225436925888062\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3194285 Vali Loss: 0.0671749 Test Loss: 0.1898966\n",
      "Validation loss decreased (inf --> 0.067175).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3389419\n",
      "\tspeed: 0.0517s/iter; left time: 554.8784s\n",
      "\titers: 200, epoch: 2 | loss: 0.3397476\n",
      "\tspeed: 0.0164s/iter; left time: 174.7887s\n",
      "\titers: 300, epoch: 2 | loss: 0.3092797\n",
      "\tspeed: 0.0164s/iter; left time: 172.8320s\n",
      "\titers: 400, epoch: 2 | loss: 0.2053859\n",
      "\tspeed: 0.0164s/iter; left time: 171.1342s\n",
      "\titers: 500, epoch: 2 | loss: 0.2841223\n",
      "\tspeed: 0.0164s/iter; left time: 169.4287s\n",
      "Epoch: 2 cost time: 9.691531419754028\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2695681 Vali Loss: 0.0684227 Test Loss: 0.1898351\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2059644\n",
      "\tspeed: 0.0497s/iter; left time: 505.4377s\n",
      "\titers: 200, epoch: 3 | loss: 0.2477117\n",
      "\tspeed: 0.0164s/iter; left time: 164.7611s\n",
      "\titers: 300, epoch: 3 | loss: 0.3639211\n",
      "\tspeed: 0.0164s/iter; left time: 163.2079s\n",
      "\titers: 400, epoch: 3 | loss: 0.2139793\n",
      "\tspeed: 0.0164s/iter; left time: 161.5522s\n",
      "\titers: 500, epoch: 3 | loss: 0.2109823\n",
      "\tspeed: 0.0164s/iter; left time: 159.7817s\n",
      "Epoch: 3 cost time: 9.617807149887085\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2380238 Vali Loss: 0.0639201 Test Loss: 0.1737082\n",
      "Validation loss decreased (0.067175 --> 0.063920).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2504411\n",
      "\tspeed: 0.0499s/iter; left time: 478.9693s\n",
      "\titers: 200, epoch: 4 | loss: 0.1834994\n",
      "\tspeed: 0.0164s/iter; left time: 155.3636s\n",
      "\titers: 300, epoch: 4 | loss: 0.2105092\n",
      "\tspeed: 0.0164s/iter; left time: 153.8845s\n",
      "\titers: 400, epoch: 4 | loss: 0.2755169\n",
      "\tspeed: 0.0164s/iter; left time: 152.0944s\n",
      "\titers: 500, epoch: 4 | loss: 0.2003397\n",
      "\tspeed: 0.0164s/iter; left time: 150.4920s\n",
      "Epoch: 4 cost time: 9.670193672180176\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2210738 Vali Loss: 0.0653202 Test Loss: 0.1676902\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1212686\n",
      "\tspeed: 0.0498s/iter; left time: 449.6692s\n",
      "\titers: 200, epoch: 5 | loss: 0.2460476\n",
      "\tspeed: 0.0164s/iter; left time: 146.3449s\n",
      "\titers: 300, epoch: 5 | loss: 0.1491101\n",
      "\tspeed: 0.0164s/iter; left time: 144.8363s\n",
      "\titers: 400, epoch: 5 | loss: 0.1874050\n",
      "\tspeed: 0.0164s/iter; left time: 143.0329s\n",
      "\titers: 500, epoch: 5 | loss: 0.1471154\n",
      "\tspeed: 0.0164s/iter; left time: 141.3561s\n",
      "Epoch: 5 cost time: 9.64632272720337\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2057814 Vali Loss: 0.0583800 Test Loss: 0.1598453\n",
      "Validation loss decreased (0.063920 --> 0.058380).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2401012\n",
      "\tspeed: 0.0501s/iter; left time: 423.5663s\n",
      "\titers: 200, epoch: 6 | loss: 0.1714445\n",
      "\tspeed: 0.0164s/iter; left time: 136.9902s\n",
      "\titers: 300, epoch: 6 | loss: 0.2459915\n",
      "\tspeed: 0.0164s/iter; left time: 135.2751s\n",
      "\titers: 400, epoch: 6 | loss: 0.1931977\n",
      "\tspeed: 0.0164s/iter; left time: 133.6797s\n",
      "\titers: 500, epoch: 6 | loss: 0.1885254\n",
      "\tspeed: 0.0164s/iter; left time: 131.9552s\n",
      "Epoch: 6 cost time: 9.673161029815674\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1941076 Vali Loss: 0.0599090 Test Loss: 0.1608659\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1485454\n",
      "\tspeed: 0.0497s/iter; left time: 391.5228s\n",
      "\titers: 200, epoch: 7 | loss: 0.2220893\n",
      "\tspeed: 0.0163s/iter; left time: 127.1430s\n",
      "\titers: 300, epoch: 7 | loss: 0.2589035\n",
      "\tspeed: 0.0163s/iter; left time: 125.4867s\n",
      "\titers: 400, epoch: 7 | loss: 0.1114334\n",
      "\tspeed: 0.0163s/iter; left time: 123.7435s\n",
      "\titers: 500, epoch: 7 | loss: 0.1242444\n",
      "\tspeed: 0.0163s/iter; left time: 122.2302s\n",
      "Epoch: 7 cost time: 9.623483896255493\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1875073 Vali Loss: 0.0603522 Test Loss: 0.1618410\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1701197\n",
      "\tspeed: 0.0515s/iter; left time: 376.4528s\n",
      "\titers: 200, epoch: 8 | loss: 0.1515106\n",
      "\tspeed: 0.0164s/iter; left time: 118.0863s\n",
      "\titers: 300, epoch: 8 | loss: 0.2233585\n",
      "\tspeed: 0.0164s/iter; left time: 116.4616s\n",
      "\titers: 400, epoch: 8 | loss: 0.0997724\n",
      "\tspeed: 0.0164s/iter; left time: 114.8483s\n",
      "\titers: 500, epoch: 8 | loss: 0.1245760\n",
      "\tspeed: 0.0164s/iter; left time: 113.2127s\n",
      "Epoch: 8 cost time: 9.64261245727539\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1838400 Vali Loss: 0.0598232 Test Loss: 0.1601604\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1600600779056549, mae:0.25847792625427246\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2592842\n",
      "\tspeed: 0.0274s/iter; left time: 310.1015s\n",
      "\titers: 200, epoch: 1 | loss: 0.1940206\n",
      "\tspeed: 0.0158s/iter; left time: 177.0292s\n",
      "\titers: 300, epoch: 1 | loss: 0.3411418\n",
      "\tspeed: 0.0158s/iter; left time: 175.1872s\n",
      "\titers: 400, epoch: 1 | loss: 0.2999641\n",
      "\tspeed: 0.0158s/iter; left time: 173.7033s\n",
      "\titers: 500, epoch: 1 | loss: 0.1544883\n",
      "\tspeed: 0.0158s/iter; left time: 172.6549s\n",
      "Epoch: 1 cost time: 10.22491192817688\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3029523 Vali Loss: 0.0649792 Test Loss: 0.1817163\n",
      "Validation loss decreased (inf --> 0.064979).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2204731\n",
      "\tspeed: 0.0507s/iter; left time: 543.6067s\n",
      "\titers: 200, epoch: 2 | loss: 0.2015020\n",
      "\tspeed: 0.0159s/iter; left time: 168.5615s\n",
      "\titers: 300, epoch: 2 | loss: 0.3120509\n",
      "\tspeed: 0.0159s/iter; left time: 167.1609s\n",
      "\titers: 400, epoch: 2 | loss: 0.4567918\n",
      "\tspeed: 0.0158s/iter; left time: 165.2598s\n",
      "\titers: 500, epoch: 2 | loss: 0.2119895\n",
      "\tspeed: 0.0159s/iter; left time: 163.9082s\n",
      "Epoch: 2 cost time: 9.354524374008179\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2555726 Vali Loss: 0.0691800 Test Loss: 0.1844113\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2225079\n",
      "\tspeed: 0.0511s/iter; left time: 518.9241s\n",
      "\titers: 200, epoch: 3 | loss: 0.2669656\n",
      "\tspeed: 0.0157s/iter; left time: 158.2933s\n",
      "\titers: 300, epoch: 3 | loss: 0.1537148\n",
      "\tspeed: 0.0157s/iter; left time: 156.5999s\n",
      "\titers: 400, epoch: 3 | loss: 0.2292681\n",
      "\tspeed: 0.0157s/iter; left time: 155.1448s\n",
      "\titers: 500, epoch: 3 | loss: 0.2172293\n",
      "\tspeed: 0.0157s/iter; left time: 153.5372s\n",
      "Epoch: 3 cost time: 9.27711820602417\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2331770 Vali Loss: 0.0565890 Test Loss: 0.1642408\n",
      "Validation loss decreased (0.064979 --> 0.056589).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2177852\n",
      "\tspeed: 0.0496s/iter; left time: 475.9562s\n",
      "\titers: 200, epoch: 4 | loss: 0.3362504\n",
      "\tspeed: 0.0158s/iter; left time: 150.1096s\n",
      "\titers: 300, epoch: 4 | loss: 0.2126502\n",
      "\tspeed: 0.0158s/iter; left time: 148.4289s\n",
      "\titers: 400, epoch: 4 | loss: 0.1504691\n",
      "\tspeed: 0.0158s/iter; left time: 146.6682s\n",
      "\titers: 500, epoch: 4 | loss: 0.3105541\n",
      "\tspeed: 0.0158s/iter; left time: 145.1756s\n",
      "Epoch: 4 cost time: 9.315791130065918\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2091449 Vali Loss: 0.0589450 Test Loss: 0.1651395\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2018463\n",
      "\tspeed: 0.0501s/iter; left time: 451.7945s\n",
      "\titers: 200, epoch: 5 | loss: 0.3313251\n",
      "\tspeed: 0.0159s/iter; left time: 141.8021s\n",
      "\titers: 300, epoch: 5 | loss: 0.2012002\n",
      "\tspeed: 0.0159s/iter; left time: 140.0136s\n",
      "\titers: 400, epoch: 5 | loss: 0.1302354\n",
      "\tspeed: 0.0159s/iter; left time: 138.4021s\n",
      "\titers: 500, epoch: 5 | loss: 0.2003354\n",
      "\tspeed: 0.0159s/iter; left time: 136.8616s\n",
      "Epoch: 5 cost time: 9.35083794593811\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1946756 Vali Loss: 0.0567082 Test Loss: 0.1582989\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2129356\n",
      "\tspeed: 0.0503s/iter; left time: 424.6683s\n",
      "\titers: 200, epoch: 6 | loss: 0.2304875\n",
      "\tspeed: 0.0158s/iter; left time: 131.8490s\n",
      "\titers: 300, epoch: 6 | loss: 0.2754611\n",
      "\tspeed: 0.0158s/iter; left time: 130.2631s\n",
      "\titers: 400, epoch: 6 | loss: 0.1710404\n",
      "\tspeed: 0.0158s/iter; left time: 128.6656s\n",
      "\titers: 500, epoch: 6 | loss: 0.1663898\n",
      "\tspeed: 0.0158s/iter; left time: 127.0884s\n",
      "Epoch: 6 cost time: 9.311377763748169\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1853511 Vali Loss: 0.0544491 Test Loss: 0.1538800\n",
      "Validation loss decreased (0.056589 --> 0.054449).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2040665\n",
      "\tspeed: 0.0499s/iter; left time: 393.5417s\n",
      "\titers: 200, epoch: 7 | loss: 0.1018395\n",
      "\tspeed: 0.0158s/iter; left time: 122.6769s\n",
      "\titers: 300, epoch: 7 | loss: 0.1823076\n",
      "\tspeed: 0.0158s/iter; left time: 121.1982s\n",
      "\titers: 400, epoch: 7 | loss: 0.1981986\n",
      "\tspeed: 0.0158s/iter; left time: 119.4372s\n",
      "\titers: 500, epoch: 7 | loss: 0.3003233\n",
      "\tspeed: 0.0157s/iter; left time: 117.7130s\n",
      "Epoch: 7 cost time: 9.295251369476318\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1801386 Vali Loss: 0.0555357 Test Loss: 0.1576312\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1524271\n",
      "\tspeed: 0.0483s/iter; left time: 353.4011s\n",
      "\titers: 200, epoch: 8 | loss: 0.2910529\n",
      "\tspeed: 0.0157s/iter; left time: 113.5314s\n",
      "\titers: 300, epoch: 8 | loss: 0.2003015\n",
      "\tspeed: 0.0158s/iter; left time: 112.0484s\n",
      "\titers: 400, epoch: 8 | loss: 0.1914104\n",
      "\tspeed: 0.0157s/iter; left time: 110.3477s\n",
      "\titers: 500, epoch: 8 | loss: 0.1669760\n",
      "\tspeed: 0.0157s/iter; left time: 108.8451s\n",
      "Epoch: 8 cost time: 9.261744737625122\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1769568 Vali Loss: 0.0551039 Test Loss: 0.1566411\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1595635\n",
      "\tspeed: 0.0485s/iter; left time: 326.9941s\n",
      "\titers: 200, epoch: 9 | loss: 0.1816410\n",
      "\tspeed: 0.0158s/iter; left time: 105.1071s\n",
      "\titers: 300, epoch: 9 | loss: 0.0986668\n",
      "\tspeed: 0.0158s/iter; left time: 103.4376s\n",
      "\titers: 400, epoch: 9 | loss: 0.1745072\n",
      "\tspeed: 0.0158s/iter; left time: 101.8514s\n",
      "\titers: 500, epoch: 9 | loss: 0.1041569\n",
      "\tspeed: 0.0158s/iter; left time: 100.2689s\n",
      "Epoch: 9 cost time: 9.30659532546997\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1748284 Vali Loss: 0.0550290 Test Loss: 0.1563157\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15406329929828644, mae:0.2528390884399414\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3087568\n",
      "\tspeed: 0.0282s/iter; left time: 318.8683s\n",
      "\titers: 200, epoch: 1 | loss: 0.3462889\n",
      "\tspeed: 0.0161s/iter; left time: 180.7187s\n",
      "\titers: 300, epoch: 1 | loss: 0.2530968\n",
      "\tspeed: 0.0161s/iter; left time: 178.8719s\n",
      "\titers: 400, epoch: 1 | loss: 0.3330573\n",
      "\tspeed: 0.0162s/iter; left time: 177.7042s\n",
      "\titers: 500, epoch: 1 | loss: 0.4840830\n",
      "\tspeed: 0.0162s/iter; left time: 176.4111s\n",
      "Epoch: 1 cost time: 10.460922718048096\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2991262 Vali Loss: 0.0749524 Test Loss: 0.2021642\n",
      "Validation loss decreased (inf --> 0.074952).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4184516\n",
      "\tspeed: 0.0480s/iter; left time: 515.2724s\n",
      "\titers: 200, epoch: 2 | loss: 0.1698274\n",
      "\tspeed: 0.0147s/iter; left time: 156.0762s\n",
      "\titers: 300, epoch: 2 | loss: 0.3770357\n",
      "\tspeed: 0.0147s/iter; left time: 154.5890s\n",
      "\titers: 400, epoch: 2 | loss: 0.2762246\n",
      "\tspeed: 0.0147s/iter; left time: 153.1603s\n",
      "\titers: 500, epoch: 2 | loss: 0.2991737\n",
      "\tspeed: 0.0147s/iter; left time: 151.7195s\n",
      "Epoch: 2 cost time: 8.684026956558228\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2748205 Vali Loss: 0.0604024 Test Loss: 0.1754508\n",
      "Validation loss decreased (0.074952 --> 0.060402).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2303173\n",
      "\tspeed: 0.0477s/iter; left time: 484.6669s\n",
      "\titers: 200, epoch: 3 | loss: 0.1447184\n",
      "\tspeed: 0.0146s/iter; left time: 146.9449s\n",
      "\titers: 300, epoch: 3 | loss: 0.2395865\n",
      "\tspeed: 0.0146s/iter; left time: 145.6360s\n",
      "\titers: 400, epoch: 3 | loss: 0.3493069\n",
      "\tspeed: 0.0146s/iter; left time: 144.2470s\n",
      "\titers: 500, epoch: 3 | loss: 0.2475004\n",
      "\tspeed: 0.0146s/iter; left time: 142.2912s\n",
      "Epoch: 3 cost time: 8.606404304504395\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2622324 Vali Loss: 0.0659564 Test Loss: 0.1945945\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1808744\n",
      "\tspeed: 0.0465s/iter; left time: 445.6756s\n",
      "\titers: 200, epoch: 4 | loss: 0.3168277\n",
      "\tspeed: 0.0147s/iter; left time: 139.1352s\n",
      "\titers: 300, epoch: 4 | loss: 0.3154862\n",
      "\tspeed: 0.0146s/iter; left time: 137.2595s\n",
      "\titers: 400, epoch: 4 | loss: 0.2198539\n",
      "\tspeed: 0.0146s/iter; left time: 135.6550s\n",
      "\titers: 500, epoch: 4 | loss: 0.2445161\n",
      "\tspeed: 0.0146s/iter; left time: 134.0429s\n",
      "Epoch: 4 cost time: 8.612568616867065\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2500855 Vali Loss: 0.0656367 Test Loss: 0.1755282\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2021067\n",
      "\tspeed: 0.0484s/iter; left time: 436.4588s\n",
      "\titers: 200, epoch: 5 | loss: 0.1661894\n",
      "\tspeed: 0.0163s/iter; left time: 145.7411s\n",
      "\titers: 300, epoch: 5 | loss: 0.1782910\n",
      "\tspeed: 0.0163s/iter; left time: 143.5725s\n",
      "\titers: 400, epoch: 5 | loss: 0.1894042\n",
      "\tspeed: 0.0162s/iter; left time: 141.3708s\n",
      "\titers: 500, epoch: 5 | loss: 0.2144025\n",
      "\tspeed: 0.0167s/iter; left time: 144.1865s\n",
      "Epoch: 5 cost time: 9.633060455322266\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2293930 Vali Loss: 0.0615150 Test Loss: 0.1702067\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.175662562251091, mae:0.26966238021850586\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2576005\n",
      "\tspeed: 0.0315s/iter; left time: 356.2860s\n",
      "\titers: 200, epoch: 1 | loss: 0.3177054\n",
      "\tspeed: 0.0196s/iter; left time: 219.6130s\n",
      "\titers: 300, epoch: 1 | loss: 0.4582160\n",
      "\tspeed: 0.0196s/iter; left time: 217.2706s\n",
      "\titers: 400, epoch: 1 | loss: 0.2453062\n",
      "\tspeed: 0.0196s/iter; left time: 215.5371s\n",
      "\titers: 500, epoch: 1 | loss: 0.1802820\n",
      "\tspeed: 0.0196s/iter; left time: 213.5347s\n",
      "Epoch: 1 cost time: 12.40468692779541\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2986423 Vali Loss: 0.0624207 Test Loss: 0.1859441\n",
      "Validation loss decreased (inf --> 0.062421).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3728313\n",
      "\tspeed: 0.0572s/iter; left time: 613.7432s\n",
      "\titers: 200, epoch: 2 | loss: 0.1625194\n",
      "\tspeed: 0.0176s/iter; left time: 186.8628s\n",
      "\titers: 300, epoch: 2 | loss: 0.2098111\n",
      "\tspeed: 0.0175s/iter; left time: 184.3792s\n",
      "\titers: 400, epoch: 2 | loss: 0.2316139\n",
      "\tspeed: 0.0175s/iter; left time: 183.0424s\n",
      "\titers: 500, epoch: 2 | loss: 0.2099084\n",
      "\tspeed: 0.0175s/iter; left time: 180.9769s\n",
      "Epoch: 2 cost time: 10.314679861068726\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2463334 Vali Loss: 0.0672368 Test Loss: 0.1854389\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1567222\n",
      "\tspeed: 0.0567s/iter; left time: 575.8538s\n",
      "\titers: 200, epoch: 3 | loss: 0.2328888\n",
      "\tspeed: 0.0175s/iter; left time: 175.9697s\n",
      "\titers: 300, epoch: 3 | loss: 0.2081664\n",
      "\tspeed: 0.0175s/iter; left time: 173.8918s\n",
      "\titers: 400, epoch: 3 | loss: 0.1160073\n",
      "\tspeed: 0.0175s/iter; left time: 172.2789s\n",
      "\titers: 500, epoch: 3 | loss: 0.1852383\n",
      "\tspeed: 0.0174s/iter; left time: 170.3212s\n",
      "Epoch: 3 cost time: 10.270918846130371\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2161097 Vali Loss: 0.0575502 Test Loss: 0.1623097\n",
      "Validation loss decreased (0.062421 --> 0.057550).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2091047\n",
      "\tspeed: 0.0556s/iter; left time: 533.4030s\n",
      "\titers: 200, epoch: 4 | loss: 0.1903538\n",
      "\tspeed: 0.0176s/iter; left time: 166.8254s\n",
      "\titers: 300, epoch: 4 | loss: 0.2823515\n",
      "\tspeed: 0.0175s/iter; left time: 164.6558s\n",
      "\titers: 400, epoch: 4 | loss: 0.1222386\n",
      "\tspeed: 0.0175s/iter; left time: 162.9539s\n",
      "\titers: 500, epoch: 4 | loss: 0.2279944\n",
      "\tspeed: 0.0175s/iter; left time: 161.2816s\n",
      "Epoch: 4 cost time: 10.309531927108765\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1895393 Vali Loss: 0.0610897 Test Loss: 0.1650018\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2419203\n",
      "\tspeed: 0.0558s/iter; left time: 503.6968s\n",
      "\titers: 200, epoch: 5 | loss: 0.1397381\n",
      "\tspeed: 0.0174s/iter; left time: 155.3954s\n",
      "\titers: 300, epoch: 5 | loss: 0.1961470\n",
      "\tspeed: 0.0175s/iter; left time: 154.1780s\n",
      "\titers: 400, epoch: 5 | loss: 0.2142258\n",
      "\tspeed: 0.0174s/iter; left time: 152.0249s\n",
      "\titers: 500, epoch: 5 | loss: 0.2020703\n",
      "\tspeed: 0.0174s/iter; left time: 150.3658s\n",
      "Epoch: 5 cost time: 10.23247241973877\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1776697 Vali Loss: 0.0597592 Test Loss: 0.1574803\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1256947\n",
      "\tspeed: 0.0542s/iter; left time: 458.4086s\n",
      "\titers: 200, epoch: 6 | loss: 0.1939256\n",
      "\tspeed: 0.0175s/iter; left time: 145.9394s\n",
      "\titers: 300, epoch: 6 | loss: 0.1321706\n",
      "\tspeed: 0.0174s/iter; left time: 143.8194s\n",
      "\titers: 400, epoch: 6 | loss: 0.1912255\n",
      "\tspeed: 0.0174s/iter; left time: 142.0392s\n",
      "\titers: 500, epoch: 6 | loss: 0.1801842\n",
      "\tspeed: 0.0174s/iter; left time: 140.2188s\n",
      "Epoch: 6 cost time: 10.225826501846313\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1691510 Vali Loss: 0.0597679 Test Loss: 0.1599545\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16255781054496765, mae:0.2579495906829834\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4043203\n",
      "\tspeed: 0.0313s/iter; left time: 353.4635s\n",
      "\titers: 200, epoch: 1 | loss: 0.1992670\n",
      "\tspeed: 0.0193s/iter; left time: 215.9496s\n",
      "\titers: 300, epoch: 1 | loss: 0.1797327\n",
      "\tspeed: 0.0193s/iter; left time: 214.2874s\n",
      "\titers: 400, epoch: 1 | loss: 0.1445734\n",
      "\tspeed: 0.0193s/iter; left time: 212.7271s\n",
      "\titers: 500, epoch: 1 | loss: 0.1533082\n",
      "\tspeed: 0.0189s/iter; left time: 206.1324s\n",
      "Epoch: 1 cost time: 12.066618919372559\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2059793 Vali Loss: 0.0638378 Test Loss: 0.1858505\n",
      "Validation loss decreased (inf --> 0.063838).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3128408\n",
      "\tspeed: 0.0544s/iter; left time: 583.4539s\n",
      "\titers: 200, epoch: 2 | loss: 0.3340852\n",
      "\tspeed: 0.0173s/iter; left time: 184.2655s\n",
      "\titers: 300, epoch: 2 | loss: 0.4815163\n",
      "\tspeed: 0.0174s/iter; left time: 182.7482s\n",
      "\titers: 400, epoch: 2 | loss: 0.2596833\n",
      "\tspeed: 0.0173s/iter; left time: 180.5538s\n",
      "\titers: 500, epoch: 2 | loss: 0.1839698\n",
      "\tspeed: 0.0173s/iter; left time: 178.4669s\n",
      "Epoch: 2 cost time: 10.182690620422363\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2772075 Vali Loss: 0.0627708 Test Loss: 0.1783495\n",
      "Validation loss decreased (0.063838 --> 0.062771).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1760170\n",
      "\tspeed: 0.0547s/iter; left time: 555.9249s\n",
      "\titers: 200, epoch: 3 | loss: 0.1966718\n",
      "\tspeed: 0.0172s/iter; left time: 172.9101s\n",
      "\titers: 300, epoch: 3 | loss: 0.1913995\n",
      "\tspeed: 0.0172s/iter; left time: 171.5888s\n",
      "\titers: 400, epoch: 3 | loss: 0.1781942\n",
      "\tspeed: 0.0172s/iter; left time: 169.4872s\n",
      "\titers: 500, epoch: 3 | loss: 0.1715818\n",
      "\tspeed: 0.0172s/iter; left time: 167.6294s\n",
      "Epoch: 3 cost time: 10.12765383720398\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2209164 Vali Loss: 0.0596031 Test Loss: 0.1728491\n",
      "Validation loss decreased (0.062771 --> 0.059603).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1805340\n",
      "\tspeed: 0.0588s/iter; left time: 564.3923s\n",
      "\titers: 200, epoch: 4 | loss: 0.0905904\n",
      "\tspeed: 0.0179s/iter; left time: 169.7601s\n",
      "\titers: 300, epoch: 4 | loss: 0.2791197\n",
      "\tspeed: 0.0173s/iter; left time: 162.2792s\n",
      "\titers: 400, epoch: 4 | loss: 0.1629296\n",
      "\tspeed: 0.0173s/iter; left time: 160.7180s\n",
      "\titers: 500, epoch: 4 | loss: 0.1282458\n",
      "\tspeed: 0.0173s/iter; left time: 158.9838s\n",
      "Epoch: 4 cost time: 10.439244031906128\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1975164 Vali Loss: 0.0578438 Test Loss: 0.1621911\n",
      "Validation loss decreased (0.059603 --> 0.057844).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1482425\n",
      "\tspeed: 0.0543s/iter; left time: 489.4556s\n",
      "\titers: 200, epoch: 5 | loss: 0.1782770\n",
      "\tspeed: 0.0173s/iter; left time: 153.9991s\n",
      "\titers: 300, epoch: 5 | loss: 0.1629610\n",
      "\tspeed: 0.0172s/iter; left time: 152.0288s\n",
      "\titers: 400, epoch: 5 | loss: 0.1764713\n",
      "\tspeed: 0.0172s/iter; left time: 150.3144s\n",
      "\titers: 500, epoch: 5 | loss: 0.1842960\n",
      "\tspeed: 0.0172s/iter; left time: 148.6850s\n",
      "Epoch: 5 cost time: 10.140869617462158\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1851035 Vali Loss: 0.0581966 Test Loss: 0.1593677\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2247585\n",
      "\tspeed: 0.0541s/iter; left time: 457.2094s\n",
      "\titers: 200, epoch: 6 | loss: 0.1118122\n",
      "\tspeed: 0.0173s/iter; left time: 144.2507s\n",
      "\titers: 300, epoch: 6 | loss: 0.1594950\n",
      "\tspeed: 0.0173s/iter; left time: 142.7202s\n",
      "\titers: 400, epoch: 6 | loss: 0.1419411\n",
      "\tspeed: 0.0173s/iter; left time: 140.8550s\n",
      "\titers: 500, epoch: 6 | loss: 0.1376313\n",
      "\tspeed: 0.0173s/iter; left time: 139.0808s\n",
      "Epoch: 6 cost time: 10.162371158599854\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1773759 Vali Loss: 0.0558292 Test Loss: 0.1570787\n",
      "Validation loss decreased (0.057844 --> 0.055829).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1612963\n",
      "\tspeed: 0.0541s/iter; left time: 426.6229s\n",
      "\titers: 200, epoch: 7 | loss: 0.1753921\n",
      "\tspeed: 0.0172s/iter; left time: 134.0789s\n",
      "\titers: 300, epoch: 7 | loss: 0.1152870\n",
      "\tspeed: 0.0172s/iter; left time: 132.2438s\n",
      "\titers: 400, epoch: 7 | loss: 0.1577708\n",
      "\tspeed: 0.0172s/iter; left time: 130.4922s\n",
      "\titers: 500, epoch: 7 | loss: 0.1604751\n",
      "\tspeed: 0.0172s/iter; left time: 128.7757s\n",
      "Epoch: 7 cost time: 10.156202554702759\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1730504 Vali Loss: 0.0571969 Test Loss: 0.1591050\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1169839\n",
      "\tspeed: 0.0536s/iter; left time: 391.5147s\n",
      "\titers: 200, epoch: 8 | loss: 0.1007759\n",
      "\tspeed: 0.0199s/iter; left time: 143.8191s\n",
      "\titers: 300, epoch: 8 | loss: 0.2006591\n",
      "\tspeed: 0.0194s/iter; left time: 137.9045s\n",
      "\titers: 400, epoch: 8 | loss: 0.2258077\n",
      "\tspeed: 0.0199s/iter; left time: 139.6781s\n",
      "\titers: 500, epoch: 8 | loss: 0.2066380\n",
      "\tspeed: 0.0199s/iter; left time: 137.7371s\n",
      "Epoch: 8 cost time: 11.33980107307434\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1705565 Vali Loss: 0.0568154 Test Loss: 0.1585337\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1093133\n",
      "\tspeed: 0.0589s/iter; left time: 397.0737s\n",
      "\titers: 200, epoch: 9 | loss: 0.1208550\n",
      "\tspeed: 0.0198s/iter; left time: 131.6364s\n",
      "\titers: 300, epoch: 9 | loss: 0.1547902\n",
      "\tspeed: 0.0198s/iter; left time: 129.6688s\n",
      "\titers: 400, epoch: 9 | loss: 0.1336278\n",
      "\tspeed: 0.0198s/iter; left time: 127.8479s\n",
      "\titers: 500, epoch: 9 | loss: 0.1185609\n",
      "\tspeed: 0.0198s/iter; left time: 125.8073s\n",
      "Epoch: 9 cost time: 11.635049104690552\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1690572 Vali Loss: 0.0568122 Test Loss: 0.1580576\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15728867053985596, mae:0.2547375559806824\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2556352\n",
      "\tspeed: 0.0311s/iter; left time: 350.9106s\n",
      "\titers: 200, epoch: 1 | loss: 0.1872244\n",
      "\tspeed: 0.0193s/iter; left time: 216.4136s\n",
      "\titers: 300, epoch: 1 | loss: 0.2462130\n",
      "\tspeed: 0.0193s/iter; left time: 214.3134s\n",
      "\titers: 400, epoch: 1 | loss: 0.2578883\n",
      "\tspeed: 0.0193s/iter; left time: 212.3728s\n",
      "\titers: 500, epoch: 1 | loss: 0.3866651\n",
      "\tspeed: 0.0193s/iter; left time: 210.5114s\n",
      "Epoch: 1 cost time: 12.235405921936035\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2572709 Vali Loss: 0.0606518 Test Loss: 0.1839672\n",
      "Validation loss decreased (inf --> 0.060652).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4579217\n",
      "\tspeed: 0.0582s/iter; left time: 624.3098s\n",
      "\titers: 200, epoch: 2 | loss: 0.3687385\n",
      "\tspeed: 0.0171s/iter; left time: 182.3182s\n",
      "\titers: 300, epoch: 2 | loss: 0.3783978\n",
      "\tspeed: 0.0172s/iter; left time: 181.1454s\n",
      "\titers: 400, epoch: 2 | loss: 0.2041424\n",
      "\tspeed: 0.0171s/iter; left time: 178.7737s\n",
      "\titers: 500, epoch: 2 | loss: 0.3030787\n",
      "\tspeed: 0.0172s/iter; left time: 177.3929s\n",
      "Epoch: 2 cost time: 10.086570978164673\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2547372 Vali Loss: 0.0611325 Test Loss: 0.1727885\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1552677\n",
      "\tspeed: 0.0536s/iter; left time: 544.5693s\n",
      "\titers: 200, epoch: 3 | loss: 0.2073597\n",
      "\tspeed: 0.0173s/iter; left time: 174.1127s\n",
      "\titers: 300, epoch: 3 | loss: 0.2164321\n",
      "\tspeed: 0.0173s/iter; left time: 172.1521s\n",
      "\titers: 400, epoch: 3 | loss: 0.2554320\n",
      "\tspeed: 0.0173s/iter; left time: 170.3324s\n",
      "\titers: 500, epoch: 3 | loss: 0.2286625\n",
      "\tspeed: 0.0173s/iter; left time: 168.5008s\n",
      "Epoch: 3 cost time: 10.11748218536377\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2108641 Vali Loss: 0.0606534 Test Loss: 0.1671809\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1449989\n",
      "\tspeed: 0.0557s/iter; left time: 533.8752s\n",
      "\titers: 200, epoch: 4 | loss: 0.1873359\n",
      "\tspeed: 0.0172s/iter; left time: 162.8986s\n",
      "\titers: 300, epoch: 4 | loss: 0.1576527\n",
      "\tspeed: 0.0171s/iter; left time: 161.0213s\n",
      "\titers: 400, epoch: 4 | loss: 0.2044394\n",
      "\tspeed: 0.0171s/iter; left time: 159.2798s\n",
      "\titers: 500, epoch: 4 | loss: 0.1120493\n",
      "\tspeed: 0.0172s/iter; left time: 157.6555s\n",
      "Epoch: 4 cost time: 10.051607131958008\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1912532 Vali Loss: 0.0578707 Test Loss: 0.1583105\n",
      "Validation loss decreased (0.060652 --> 0.057871).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1316964\n",
      "\tspeed: 0.0533s/iter; left time: 480.7882s\n",
      "\titers: 200, epoch: 5 | loss: 0.1546748\n",
      "\tspeed: 0.0173s/iter; left time: 154.3099s\n",
      "\titers: 300, epoch: 5 | loss: 0.1557725\n",
      "\tspeed: 0.0173s/iter; left time: 152.3911s\n",
      "\titers: 400, epoch: 5 | loss: 0.2160063\n",
      "\tspeed: 0.0173s/iter; left time: 150.5254s\n",
      "\titers: 500, epoch: 5 | loss: 0.1194131\n",
      "\tspeed: 0.0173s/iter; left time: 148.7704s\n",
      "Epoch: 5 cost time: 10.136269569396973\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1778205 Vali Loss: 0.0576764 Test Loss: 0.1575596\n",
      "Validation loss decreased (0.057871 --> 0.057676).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2322009\n",
      "\tspeed: 0.0540s/iter; left time: 456.3205s\n",
      "\titers: 200, epoch: 6 | loss: 0.1299272\n",
      "\tspeed: 0.0172s/iter; left time: 143.5143s\n",
      "\titers: 300, epoch: 6 | loss: 0.3989783\n",
      "\tspeed: 0.0172s/iter; left time: 141.5518s\n",
      "\titers: 400, epoch: 6 | loss: 0.1361643\n",
      "\tspeed: 0.0172s/iter; left time: 139.9575s\n",
      "\titers: 500, epoch: 6 | loss: 0.2072702\n",
      "\tspeed: 0.0172s/iter; left time: 138.3540s\n",
      "Epoch: 6 cost time: 10.071016550064087\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1666638 Vali Loss: 0.0567906 Test Loss: 0.1548798\n",
      "Validation loss decreased (0.057676 --> 0.056791).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1350531\n",
      "\tspeed: 0.0562s/iter; left time: 442.7479s\n",
      "\titers: 200, epoch: 7 | loss: 0.1668256\n",
      "\tspeed: 0.0174s/iter; left time: 135.0755s\n",
      "\titers: 300, epoch: 7 | loss: 0.1984801\n",
      "\tspeed: 0.0173s/iter; left time: 133.1492s\n",
      "\titers: 400, epoch: 7 | loss: 0.1426664\n",
      "\tspeed: 0.0173s/iter; left time: 131.3446s\n",
      "\titers: 500, epoch: 7 | loss: 0.1303774\n",
      "\tspeed: 0.0173s/iter; left time: 129.6043s\n",
      "Epoch: 7 cost time: 10.193880081176758\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1604715 Vali Loss: 0.0566994 Test Loss: 0.1539143\n",
      "Validation loss decreased (0.056791 --> 0.056699).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0793495\n",
      "\tspeed: 0.0555s/iter; left time: 405.8468s\n",
      "\titers: 200, epoch: 8 | loss: 0.0816441\n",
      "\tspeed: 0.0172s/iter; left time: 123.7689s\n",
      "\titers: 300, epoch: 8 | loss: 0.1428428\n",
      "\tspeed: 0.0172s/iter; left time: 122.1539s\n",
      "\titers: 400, epoch: 8 | loss: 0.1213741\n",
      "\tspeed: 0.0172s/iter; left time: 120.3683s\n",
      "\titers: 500, epoch: 8 | loss: 0.1387340\n",
      "\tspeed: 0.0172s/iter; left time: 118.6156s\n",
      "Epoch: 8 cost time: 10.098626613616943\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1568428 Vali Loss: 0.0571787 Test Loss: 0.1558457\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2026256\n",
      "\tspeed: 0.0534s/iter; left time: 359.7216s\n",
      "\titers: 200, epoch: 9 | loss: 0.1673106\n",
      "\tspeed: 0.0173s/iter; left time: 114.9227s\n",
      "\titers: 300, epoch: 9 | loss: 0.1801844\n",
      "\tspeed: 0.0173s/iter; left time: 113.1252s\n",
      "\titers: 400, epoch: 9 | loss: 0.2959049\n",
      "\tspeed: 0.0173s/iter; left time: 111.4560s\n",
      "\titers: 500, epoch: 9 | loss: 0.1356361\n",
      "\tspeed: 0.0173s/iter; left time: 109.6931s\n",
      "Epoch: 9 cost time: 10.126974105834961\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1549962 Vali Loss: 0.0568263 Test Loss: 0.1550211\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1370055\n",
      "\tspeed: 0.0531s/iter; left time: 327.4834s\n",
      "\titers: 200, epoch: 10 | loss: 0.1289415\n",
      "\tspeed: 0.0173s/iter; left time: 105.2008s\n",
      "\titers: 300, epoch: 10 | loss: 0.1983363\n",
      "\tspeed: 0.0174s/iter; left time: 103.6458s\n",
      "\titers: 400, epoch: 10 | loss: 0.1467326\n",
      "\tspeed: 0.0173s/iter; left time: 101.8222s\n",
      "\titers: 500, epoch: 10 | loss: 0.1663210\n",
      "\tspeed: 0.0174s/iter; left time: 100.3661s\n",
      "Epoch: 10 cost time: 10.15054965019226\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1536804 Vali Loss: 0.0567571 Test Loss: 0.1549835\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15411075949668884, mae:0.2524541914463043\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2633941\n",
      "\tspeed: 0.0352s/iter; left time: 397.3620s\n",
      "\titers: 200, epoch: 1 | loss: 0.2882230\n",
      "\tspeed: 0.0232s/iter; left time: 259.9095s\n",
      "\titers: 300, epoch: 1 | loss: 0.3079577\n",
      "\tspeed: 0.0231s/iter; left time: 256.8693s\n",
      "\titers: 400, epoch: 1 | loss: 0.4719545\n",
      "\tspeed: 0.0231s/iter; left time: 254.0437s\n",
      "\titers: 500, epoch: 1 | loss: 0.3473301\n",
      "\tspeed: 0.0231s/iter; left time: 251.5872s\n",
      "Epoch: 1 cost time: 14.443045616149902\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3496201 Vali Loss: 0.0691530 Test Loss: 0.2032876\n",
      "Validation loss decreased (inf --> 0.069153).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3026515\n",
      "\tspeed: 0.0636s/iter; left time: 682.8651s\n",
      "\titers: 200, epoch: 2 | loss: 0.2586778\n",
      "\tspeed: 0.0214s/iter; left time: 227.1344s\n",
      "\titers: 300, epoch: 2 | loss: 0.2658741\n",
      "\tspeed: 0.0214s/iter; left time: 224.9063s\n",
      "\titers: 400, epoch: 2 | loss: 0.3067841\n",
      "\tspeed: 0.0213s/iter; left time: 222.6413s\n",
      "\titers: 500, epoch: 2 | loss: 0.2336292\n",
      "\tspeed: 0.0214s/iter; left time: 220.7424s\n",
      "Epoch: 2 cost time: 12.466941118240356\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2770482 Vali Loss: 0.0684681 Test Loss: 0.1803966\n",
      "Validation loss decreased (0.069153 --> 0.068468).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.5014133\n",
      "\tspeed: 0.0635s/iter; left time: 644.7410s\n",
      "\titers: 200, epoch: 3 | loss: 0.3132436\n",
      "\tspeed: 0.0213s/iter; left time: 214.6039s\n",
      "\titers: 300, epoch: 3 | loss: 0.3100383\n",
      "\tspeed: 0.0213s/iter; left time: 212.1743s\n",
      "\titers: 400, epoch: 3 | loss: 0.2355167\n",
      "\tspeed: 0.0213s/iter; left time: 210.1502s\n",
      "\titers: 500, epoch: 3 | loss: 0.2293372\n",
      "\tspeed: 0.0213s/iter; left time: 208.2519s\n",
      "Epoch: 3 cost time: 12.45596170425415\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2852790 Vali Loss: 0.0618621 Test Loss: 0.1858376\n",
      "Validation loss decreased (0.068468 --> 0.061862).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1910913\n",
      "\tspeed: 0.0613s/iter; left time: 587.7800s\n",
      "\titers: 200, epoch: 4 | loss: 0.2064937\n",
      "\tspeed: 0.0214s/iter; left time: 202.6936s\n",
      "\titers: 300, epoch: 4 | loss: 0.3060553\n",
      "\tspeed: 0.0213s/iter; left time: 200.4788s\n",
      "\titers: 400, epoch: 4 | loss: 0.2156951\n",
      "\tspeed: 0.0214s/iter; left time: 198.4220s\n",
      "\titers: 500, epoch: 4 | loss: 0.2605067\n",
      "\tspeed: 0.0213s/iter; left time: 196.2225s\n",
      "Epoch: 4 cost time: 12.473122358322144\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2557708 Vali Loss: 0.0617414 Test Loss: 0.1765215\n",
      "Validation loss decreased (0.061862 --> 0.061741).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2731111\n",
      "\tspeed: 0.0639s/iter; left time: 576.1748s\n",
      "\titers: 200, epoch: 5 | loss: 0.1924176\n",
      "\tspeed: 0.0213s/iter; left time: 190.0214s\n",
      "\titers: 300, epoch: 5 | loss: 0.1932702\n",
      "\tspeed: 0.0213s/iter; left time: 188.1911s\n",
      "\titers: 400, epoch: 5 | loss: 0.2116029\n",
      "\tspeed: 0.0213s/iter; left time: 185.9539s\n",
      "\titers: 500, epoch: 5 | loss: 0.2595659\n",
      "\tspeed: 0.0213s/iter; left time: 183.6982s\n",
      "Epoch: 5 cost time: 12.47373080253601\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2348923 Vali Loss: 0.0627406 Test Loss: 0.1726262\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2130222\n",
      "\tspeed: 0.0608s/iter; left time: 514.1418s\n",
      "\titers: 200, epoch: 6 | loss: 0.3572890\n",
      "\tspeed: 0.0214s/iter; left time: 178.5749s\n",
      "\titers: 300, epoch: 6 | loss: 0.2064936\n",
      "\tspeed: 0.0214s/iter; left time: 176.4720s\n",
      "\titers: 400, epoch: 6 | loss: 0.2617048\n",
      "\tspeed: 0.0214s/iter; left time: 174.1751s\n",
      "\titers: 500, epoch: 6 | loss: 0.3124670\n",
      "\tspeed: 0.0214s/iter; left time: 172.0661s\n",
      "Epoch: 6 cost time: 12.449581146240234\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2221600 Vali Loss: 0.0617320 Test Loss: 0.1794897\n",
      "Validation loss decreased (0.061741 --> 0.061732).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2988350\n",
      "\tspeed: 0.0612s/iter; left time: 481.9871s\n",
      "\titers: 200, epoch: 7 | loss: 0.1856506\n",
      "\tspeed: 0.0213s/iter; left time: 165.7617s\n",
      "\titers: 300, epoch: 7 | loss: 0.1715053\n",
      "\tspeed: 0.0213s/iter; left time: 163.6100s\n",
      "\titers: 400, epoch: 7 | loss: 0.2193801\n",
      "\tspeed: 0.0213s/iter; left time: 161.5462s\n",
      "\titers: 500, epoch: 7 | loss: 0.2082414\n",
      "\tspeed: 0.0213s/iter; left time: 159.2258s\n",
      "Epoch: 7 cost time: 12.447759866714478\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2201508 Vali Loss: 0.0617448 Test Loss: 0.1733547\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1307898\n",
      "\tspeed: 0.0627s/iter; left time: 458.7546s\n",
      "\titers: 200, epoch: 8 | loss: 0.3508490\n",
      "\tspeed: 0.0231s/iter; left time: 166.8731s\n",
      "\titers: 300, epoch: 8 | loss: 0.1427951\n",
      "\tspeed: 0.0232s/iter; left time: 164.7588s\n",
      "\titers: 400, epoch: 8 | loss: 0.1989607\n",
      "\tspeed: 0.0232s/iter; left time: 162.4993s\n",
      "\titers: 500, epoch: 8 | loss: 0.2051648\n",
      "\tspeed: 0.0231s/iter; left time: 159.7937s\n",
      "Epoch: 8 cost time: 13.332557201385498\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2198184 Vali Loss: 0.0605993 Test Loss: 0.1703959\n",
      "Validation loss decreased (0.061732 --> 0.060599).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2086617\n",
      "\tspeed: 0.0656s/iter; left time: 441.9386s\n",
      "\titers: 200, epoch: 9 | loss: 0.1701879\n",
      "\tspeed: 0.0232s/iter; left time: 153.7824s\n",
      "\titers: 300, epoch: 9 | loss: 0.2340400\n",
      "\tspeed: 0.0232s/iter; left time: 151.6869s\n",
      "\titers: 400, epoch: 9 | loss: 0.2145180\n",
      "\tspeed: 0.0232s/iter; left time: 149.1860s\n",
      "\titers: 500, epoch: 9 | loss: 0.1757907\n",
      "\tspeed: 0.0232s/iter; left time: 147.0663s\n",
      "Epoch: 9 cost time: 13.537116289138794\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2117511 Vali Loss: 0.0595498 Test Loss: 0.1690634\n",
      "Validation loss decreased (0.060599 --> 0.059550).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1913969\n",
      "\tspeed: 0.0629s/iter; left time: 388.0867s\n",
      "\titers: 200, epoch: 10 | loss: 0.1887120\n",
      "\tspeed: 0.0214s/iter; left time: 129.7419s\n",
      "\titers: 300, epoch: 10 | loss: 0.1581029\n",
      "\tspeed: 0.0214s/iter; left time: 127.5056s\n",
      "\titers: 400, epoch: 10 | loss: 0.3914342\n",
      "\tspeed: 0.0214s/iter; left time: 125.5075s\n",
      "\titers: 500, epoch: 10 | loss: 0.2320722\n",
      "\tspeed: 0.0214s/iter; left time: 123.2797s\n",
      "Epoch: 10 cost time: 12.487029552459717\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2091234 Vali Loss: 0.0599943 Test Loss: 0.1690989\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1834012\n",
      "\tspeed: 0.0623s/iter; left time: 348.7611s\n",
      "\titers: 200, epoch: 11 | loss: 0.1807611\n",
      "\tspeed: 0.0213s/iter; left time: 117.4365s\n",
      "\titers: 300, epoch: 11 | loss: 0.1376544\n",
      "\tspeed: 0.0214s/iter; left time: 115.3364s\n",
      "\titers: 400, epoch: 11 | loss: 0.1764182\n",
      "\tspeed: 0.0214s/iter; left time: 113.1866s\n",
      "\titers: 500, epoch: 11 | loss: 0.1822242\n",
      "\tspeed: 0.0214s/iter; left time: 111.0460s\n",
      "Epoch: 11 cost time: 12.49424695968628\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2081567 Vali Loss: 0.0601298 Test Loss: 0.1693076\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2462718\n",
      "\tspeed: 0.0608s/iter; left time: 305.9487s\n",
      "\titers: 200, epoch: 12 | loss: 0.1744411\n",
      "\tspeed: 0.0213s/iter; left time: 105.2169s\n",
      "\titers: 300, epoch: 12 | loss: 0.3393806\n",
      "\tspeed: 0.0213s/iter; left time: 103.0944s\n",
      "\titers: 400, epoch: 12 | loss: 0.1140367\n",
      "\tspeed: 0.0213s/iter; left time: 100.9530s\n",
      "\titers: 500, epoch: 12 | loss: 0.2412114\n",
      "\tspeed: 0.0213s/iter; left time: 98.8452s\n",
      "Epoch: 12 cost time: 12.461491107940674\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2077105 Vali Loss: 0.0600915 Test Loss: 0.1693314\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16930536925792694, mae:0.26500973105430603\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.5048662\n",
      "\tspeed: 0.0329s/iter; left time: 371.6249s\n",
      "\titers: 200, epoch: 1 | loss: 0.1369040\n",
      "\tspeed: 0.0207s/iter; left time: 231.7189s\n",
      "\titers: 300, epoch: 1 | loss: 0.2355949\n",
      "\tspeed: 0.0207s/iter; left time: 229.2944s\n",
      "\titers: 400, epoch: 1 | loss: 0.2787115\n",
      "\tspeed: 0.0206s/iter; left time: 227.1397s\n",
      "\titers: 500, epoch: 1 | loss: 0.2199036\n",
      "\tspeed: 0.0206s/iter; left time: 225.0469s\n",
      "Epoch: 1 cost time: 13.053419589996338\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3321772 Vali Loss: 0.0685171 Test Loss: 0.1871986\n",
      "Validation loss decreased (inf --> 0.068517).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2593084\n",
      "\tspeed: 0.0595s/iter; left time: 638.9177s\n",
      "\titers: 200, epoch: 2 | loss: 0.1782596\n",
      "\tspeed: 0.0206s/iter; left time: 219.0154s\n",
      "\titers: 300, epoch: 2 | loss: 0.3906453\n",
      "\tspeed: 0.0206s/iter; left time: 216.7526s\n",
      "\titers: 400, epoch: 2 | loss: 0.1749899\n",
      "\tspeed: 0.0206s/iter; left time: 215.0112s\n",
      "\titers: 500, epoch: 2 | loss: 0.2072232\n",
      "\tspeed: 0.0206s/iter; left time: 212.7726s\n",
      "Epoch: 2 cost time: 12.013784408569336\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2578903 Vali Loss: 0.0668498 Test Loss: 0.1912969\n",
      "Validation loss decreased (0.068517 --> 0.066850).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2249382\n",
      "\tspeed: 0.0593s/iter; left time: 602.9503s\n",
      "\titers: 200, epoch: 3 | loss: 0.1607056\n",
      "\tspeed: 0.0206s/iter; left time: 206.9130s\n",
      "\titers: 300, epoch: 3 | loss: 0.1565936\n",
      "\tspeed: 0.0206s/iter; left time: 204.7882s\n",
      "\titers: 400, epoch: 3 | loss: 0.1524085\n",
      "\tspeed: 0.0206s/iter; left time: 202.8051s\n",
      "\titers: 500, epoch: 3 | loss: 0.1550296\n",
      "\tspeed: 0.0206s/iter; left time: 200.8758s\n",
      "Epoch: 3 cost time: 12.00016188621521\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2275447 Vali Loss: 0.0636690 Test Loss: 0.1738327\n",
      "Validation loss decreased (0.066850 --> 0.063669).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1582057\n",
      "\tspeed: 0.0599s/iter; left time: 574.4288s\n",
      "\titers: 200, epoch: 4 | loss: 0.2079531\n",
      "\tspeed: 0.0206s/iter; left time: 195.8931s\n",
      "\titers: 300, epoch: 4 | loss: 0.1579667\n",
      "\tspeed: 0.0206s/iter; left time: 193.8836s\n",
      "\titers: 400, epoch: 4 | loss: 0.2145253\n",
      "\tspeed: 0.0206s/iter; left time: 191.8073s\n",
      "\titers: 500, epoch: 4 | loss: 0.1629684\n",
      "\tspeed: 0.0207s/iter; left time: 189.8165s\n",
      "Epoch: 4 cost time: 12.051313400268555\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1952149 Vali Loss: 0.0597343 Test Loss: 0.1581925\n",
      "Validation loss decreased (0.063669 --> 0.059734).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2804677\n",
      "\tspeed: 0.0599s/iter; left time: 540.5499s\n",
      "\titers: 200, epoch: 5 | loss: 0.1796376\n",
      "\tspeed: 0.0207s/iter; left time: 184.7721s\n",
      "\titers: 300, epoch: 5 | loss: 0.1583485\n",
      "\tspeed: 0.0207s/iter; left time: 182.7355s\n",
      "\titers: 400, epoch: 5 | loss: 0.1763235\n",
      "\tspeed: 0.0207s/iter; left time: 180.1649s\n",
      "\titers: 500, epoch: 5 | loss: 0.1554981\n",
      "\tspeed: 0.0206s/iter; left time: 177.7260s\n",
      "Epoch: 5 cost time: 12.061827421188354\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1784757 Vali Loss: 0.0619457 Test Loss: 0.1574815\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1472059\n",
      "\tspeed: 0.0582s/iter; left time: 491.4584s\n",
      "\titers: 200, epoch: 6 | loss: 0.1663551\n",
      "\tspeed: 0.0206s/iter; left time: 172.1909s\n",
      "\titers: 300, epoch: 6 | loss: 0.1644344\n",
      "\tspeed: 0.0206s/iter; left time: 170.0670s\n",
      "\titers: 400, epoch: 6 | loss: 0.1136185\n",
      "\tspeed: 0.0206s/iter; left time: 167.8508s\n",
      "\titers: 500, epoch: 6 | loss: 0.1272855\n",
      "\tspeed: 0.0205s/iter; left time: 164.8979s\n",
      "Epoch: 6 cost time: 11.977345705032349\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1690723 Vali Loss: 0.0634737 Test Loss: 0.1590462\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1238686\n",
      "\tspeed: 0.0584s/iter; left time: 460.4659s\n",
      "\titers: 200, epoch: 7 | loss: 0.1984115\n",
      "\tspeed: 0.0207s/iter; left time: 160.9426s\n",
      "\titers: 300, epoch: 7 | loss: 0.1817501\n",
      "\tspeed: 0.0206s/iter; left time: 158.6064s\n",
      "\titers: 400, epoch: 7 | loss: 0.1394706\n",
      "\tspeed: 0.0207s/iter; left time: 156.6326s\n",
      "\titers: 500, epoch: 7 | loss: 0.1559870\n",
      "\tspeed: 0.0207s/iter; left time: 154.5063s\n",
      "Epoch: 7 cost time: 12.02893877029419\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1623321 Vali Loss: 0.0616060 Test Loss: 0.1566058\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1584259569644928, mae:0.2586020827293396\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3459812\n",
      "\tspeed: 0.0333s/iter; left time: 376.0633s\n",
      "\titers: 200, epoch: 1 | loss: 0.2204933\n",
      "\tspeed: 0.0210s/iter; left time: 234.7481s\n",
      "\titers: 300, epoch: 1 | loss: 0.3470292\n",
      "\tspeed: 0.0210s/iter; left time: 233.3353s\n",
      "\titers: 400, epoch: 1 | loss: 0.3688976\n",
      "\tspeed: 0.0210s/iter; left time: 230.9924s\n",
      "\titers: 500, epoch: 1 | loss: 0.3242333\n",
      "\tspeed: 0.0210s/iter; left time: 228.4110s\n",
      "Epoch: 1 cost time: 13.244526624679565\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3072776 Vali Loss: 0.0635447 Test Loss: 0.1786053\n",
      "Validation loss decreased (inf --> 0.063545).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1897742\n",
      "\tspeed: 0.0593s/iter; left time: 636.4411s\n",
      "\titers: 200, epoch: 2 | loss: 0.5891153\n",
      "\tspeed: 0.0190s/iter; left time: 201.8619s\n",
      "\titers: 300, epoch: 2 | loss: 0.2883355\n",
      "\tspeed: 0.0190s/iter; left time: 199.7946s\n",
      "\titers: 400, epoch: 2 | loss: 0.1507711\n",
      "\tspeed: 0.0190s/iter; left time: 198.6357s\n",
      "\titers: 500, epoch: 2 | loss: 0.1833827\n",
      "\tspeed: 0.0190s/iter; left time: 196.7061s\n",
      "Epoch: 2 cost time: 11.163628578186035\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2681090 Vali Loss: 0.0624694 Test Loss: 0.1786789\n",
      "Validation loss decreased (0.063545 --> 0.062469).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1649375\n",
      "\tspeed: 0.0574s/iter; left time: 583.3955s\n",
      "\titers: 200, epoch: 3 | loss: 0.1788609\n",
      "\tspeed: 0.0190s/iter; left time: 190.9959s\n",
      "\titers: 300, epoch: 3 | loss: 0.1947482\n",
      "\tspeed: 0.0190s/iter; left time: 189.1120s\n",
      "\titers: 400, epoch: 3 | loss: 0.2425351\n",
      "\tspeed: 0.0190s/iter; left time: 186.9471s\n",
      "\titers: 500, epoch: 3 | loss: 0.2817035\n",
      "\tspeed: 0.0190s/iter; left time: 185.2719s\n",
      "Epoch: 3 cost time: 11.126565217971802\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2410358 Vali Loss: 0.0631183 Test Loss: 0.1702424\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1540900\n",
      "\tspeed: 0.0568s/iter; left time: 544.9970s\n",
      "\titers: 200, epoch: 4 | loss: 0.2941101\n",
      "\tspeed: 0.0190s/iter; left time: 180.1468s\n",
      "\titers: 300, epoch: 4 | loss: 0.1636501\n",
      "\tspeed: 0.0190s/iter; left time: 178.5742s\n",
      "\titers: 400, epoch: 4 | loss: 0.2417484\n",
      "\tspeed: 0.0190s/iter; left time: 176.2847s\n",
      "\titers: 500, epoch: 4 | loss: 0.1856163\n",
      "\tspeed: 0.0190s/iter; left time: 174.2905s\n",
      "Epoch: 4 cost time: 11.116266250610352\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2178987 Vali Loss: 0.0632832 Test Loss: 0.1700603\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1762531\n",
      "\tspeed: 0.0572s/iter; left time: 516.2396s\n",
      "\titers: 200, epoch: 5 | loss: 0.3220015\n",
      "\tspeed: 0.0190s/iter; left time: 169.8490s\n",
      "\titers: 300, epoch: 5 | loss: 0.1489344\n",
      "\tspeed: 0.0189s/iter; left time: 166.8760s\n",
      "\titers: 400, epoch: 5 | loss: 0.1708648\n",
      "\tspeed: 0.0189s/iter; left time: 165.0910s\n",
      "\titers: 500, epoch: 5 | loss: 0.1593254\n",
      "\tspeed: 0.0190s/iter; left time: 163.3875s\n",
      "Epoch: 5 cost time: 11.109708547592163\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2111658 Vali Loss: 0.0593761 Test Loss: 0.1635990\n",
      "Validation loss decreased (0.062469 --> 0.059376).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2397203\n",
      "\tspeed: 0.0576s/iter; left time: 487.0778s\n",
      "\titers: 200, epoch: 6 | loss: 0.2457219\n",
      "\tspeed: 0.0191s/iter; left time: 159.8599s\n",
      "\titers: 300, epoch: 6 | loss: 0.1633635\n",
      "\tspeed: 0.0191s/iter; left time: 157.3469s\n",
      "\titers: 400, epoch: 6 | loss: 0.1521265\n",
      "\tspeed: 0.0191s/iter; left time: 155.8222s\n",
      "\titers: 500, epoch: 6 | loss: 0.2515264\n",
      "\tspeed: 0.0191s/iter; left time: 153.4764s\n",
      "Epoch: 6 cost time: 11.206693410873413\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2006975 Vali Loss: 0.0586759 Test Loss: 0.1642092\n",
      "Validation loss decreased (0.059376 --> 0.058676).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2224349\n",
      "\tspeed: 0.0599s/iter; left time: 471.9218s\n",
      "\titers: 200, epoch: 7 | loss: 0.2338411\n",
      "\tspeed: 0.0190s/iter; left time: 148.1932s\n",
      "\titers: 300, epoch: 7 | loss: 0.2719557\n",
      "\tspeed: 0.0190s/iter; left time: 146.2796s\n",
      "\titers: 400, epoch: 7 | loss: 0.3161416\n",
      "\tspeed: 0.0190s/iter; left time: 144.0270s\n",
      "\titers: 500, epoch: 7 | loss: 0.1665621\n",
      "\tspeed: 0.0190s/iter; left time: 141.9812s\n",
      "Epoch: 7 cost time: 11.134984254837036\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1945284 Vali Loss: 0.0596985 Test Loss: 0.1651743\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2725866\n",
      "\tspeed: 0.0595s/iter; left time: 434.6592s\n",
      "\titers: 200, epoch: 8 | loss: 0.2585613\n",
      "\tspeed: 0.0191s/iter; left time: 137.5102s\n",
      "\titers: 300, epoch: 8 | loss: 0.2563990\n",
      "\tspeed: 0.0191s/iter; left time: 135.8787s\n",
      "\titers: 400, epoch: 8 | loss: 0.1603610\n",
      "\tspeed: 0.0191s/iter; left time: 133.6520s\n",
      "\titers: 500, epoch: 8 | loss: 0.2072012\n",
      "\tspeed: 0.0191s/iter; left time: 131.7084s\n",
      "Epoch: 8 cost time: 11.188389778137207\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1912372 Vali Loss: 0.0585950 Test Loss: 0.1643800\n",
      "Validation loss decreased (0.058676 --> 0.058595).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1484015\n",
      "\tspeed: 0.0574s/iter; left time: 387.2089s\n",
      "\titers: 200, epoch: 9 | loss: 0.1572453\n",
      "\tspeed: 0.0190s/iter; left time: 126.0428s\n",
      "\titers: 300, epoch: 9 | loss: 0.2903929\n",
      "\tspeed: 0.0190s/iter; left time: 124.0938s\n",
      "\titers: 400, epoch: 9 | loss: 0.1727037\n",
      "\tspeed: 0.0190s/iter; left time: 122.1237s\n",
      "\titers: 500, epoch: 9 | loss: 0.1098155\n",
      "\tspeed: 0.0190s/iter; left time: 120.3210s\n",
      "Epoch: 9 cost time: 11.095708847045898\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1892703 Vali Loss: 0.0585207 Test Loss: 0.1643788\n",
      "Validation loss decreased (0.058595 --> 0.058521).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2391348\n",
      "\tspeed: 0.0571s/iter; left time: 352.4778s\n",
      "\titers: 200, epoch: 10 | loss: 0.1191047\n",
      "\tspeed: 0.0190s/iter; left time: 115.1776s\n",
      "\titers: 300, epoch: 10 | loss: 0.2457750\n",
      "\tspeed: 0.0189s/iter; left time: 113.0569s\n",
      "\titers: 400, epoch: 10 | loss: 0.2172185\n",
      "\tspeed: 0.0189s/iter; left time: 111.2023s\n",
      "\titers: 500, epoch: 10 | loss: 0.1814567\n",
      "\tspeed: 0.0189s/iter; left time: 109.2850s\n",
      "Epoch: 10 cost time: 11.124875545501709\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1883511 Vali Loss: 0.0584853 Test Loss: 0.1639752\n",
      "Validation loss decreased (0.058521 --> 0.058485).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1710032\n",
      "\tspeed: 0.0575s/iter; left time: 322.1879s\n",
      "\titers: 200, epoch: 11 | loss: 0.1714001\n",
      "\tspeed: 0.0191s/iter; left time: 105.1636s\n",
      "\titers: 300, epoch: 11 | loss: 0.1431176\n",
      "\tspeed: 0.0191s/iter; left time: 103.0011s\n",
      "\titers: 400, epoch: 11 | loss: 0.2365137\n",
      "\tspeed: 0.0191s/iter; left time: 101.3149s\n",
      "\titers: 500, epoch: 11 | loss: 0.1492564\n",
      "\tspeed: 0.0191s/iter; left time: 99.5828s\n",
      "Epoch: 11 cost time: 11.18666386604309\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1880311 Vali Loss: 0.0585991 Test Loss: 0.1641587\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1611707\n",
      "\tspeed: 0.0589s/iter; left time: 296.4489s\n",
      "\titers: 200, epoch: 12 | loss: 0.1814352\n",
      "\tspeed: 0.0210s/iter; left time: 103.4300s\n",
      "\titers: 300, epoch: 12 | loss: 0.3350646\n",
      "\tspeed: 0.0210s/iter; left time: 101.4233s\n",
      "\titers: 400, epoch: 12 | loss: 0.2441531\n",
      "\tspeed: 0.0210s/iter; left time: 99.2175s\n",
      "\titers: 500, epoch: 12 | loss: 0.2772302\n",
      "\tspeed: 0.0210s/iter; left time: 97.2910s\n",
      "Epoch: 12 cost time: 12.20010495185852\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1877626 Vali Loss: 0.0584666 Test Loss: 0.1642219\n",
      "Validation loss decreased (0.058485 --> 0.058467).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1725542\n",
      "\tspeed: 0.0602s/iter; left time: 268.5739s\n",
      "\titers: 200, epoch: 13 | loss: 0.1736616\n",
      "\tspeed: 0.0190s/iter; left time: 82.9481s\n",
      "\titers: 300, epoch: 13 | loss: 0.1994443\n",
      "\tspeed: 0.0190s/iter; left time: 81.0118s\n",
      "\titers: 400, epoch: 13 | loss: 0.1422995\n",
      "\tspeed: 0.0190s/iter; left time: 79.0694s\n",
      "\titers: 500, epoch: 13 | loss: 0.1748704\n",
      "\tspeed: 0.0190s/iter; left time: 77.2258s\n",
      "Epoch: 13 cost time: 11.179510593414307\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1873977 Vali Loss: 0.0586605 Test Loss: 0.1642319\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2812103\n",
      "\tspeed: 0.0560s/iter; left time: 218.0879s\n",
      "\titers: 200, epoch: 14 | loss: 0.2077650\n",
      "\tspeed: 0.0190s/iter; left time: 72.1819s\n",
      "\titers: 300, epoch: 14 | loss: 0.1620535\n",
      "\tspeed: 0.0190s/iter; left time: 70.3119s\n",
      "\titers: 400, epoch: 14 | loss: 0.1186182\n",
      "\tspeed: 0.0190s/iter; left time: 68.4071s\n",
      "\titers: 500, epoch: 14 | loss: 0.1526008\n",
      "\tspeed: 0.0190s/iter; left time: 66.4830s\n",
      "Epoch: 14 cost time: 11.130144357681274\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1874303 Vali Loss: 0.0584769 Test Loss: 0.1642397\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1220352\n",
      "\tspeed: 0.0574s/iter; left time: 190.5608s\n",
      "\titers: 200, epoch: 15 | loss: 0.2429871\n",
      "\tspeed: 0.0191s/iter; left time: 61.4561s\n",
      "\titers: 300, epoch: 15 | loss: 0.1776120\n",
      "\tspeed: 0.0191s/iter; left time: 59.5207s\n",
      "\titers: 400, epoch: 15 | loss: 0.1808693\n",
      "\tspeed: 0.0191s/iter; left time: 57.6255s\n",
      "\titers: 500, epoch: 15 | loss: 0.1404826\n",
      "\tspeed: 0.0191s/iter; left time: 55.7275s\n",
      "Epoch: 15 cost time: 11.212652444839478\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1875341 Vali Loss: 0.0586241 Test Loss: 0.1642427\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16442342102527618, mae:0.2625240385532379\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1777191\n",
      "\tspeed: 0.0236s/iter; left time: 266.1433s\n",
      "\titers: 200, epoch: 1 | loss: 0.1935108\n",
      "\tspeed: 0.0119s/iter; left time: 133.2886s\n",
      "\titers: 300, epoch: 1 | loss: 0.2110804\n",
      "\tspeed: 0.0109s/iter; left time: 121.3602s\n",
      "\titers: 400, epoch: 1 | loss: 0.2278834\n",
      "\tspeed: 0.0106s/iter; left time: 116.3988s\n",
      "\titers: 500, epoch: 1 | loss: 0.2286975\n",
      "\tspeed: 0.0106s/iter; left time: 115.2906s\n",
      "Epoch: 1 cost time: 7.545633316040039\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2393754 Vali Loss: 0.0499301 Test Loss: 0.1551164\n",
      "Validation loss decreased (inf --> 0.049930).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3729154\n",
      "\tspeed: 0.0366s/iter; left time: 392.5611s\n",
      "\titers: 200, epoch: 2 | loss: 0.3245559\n",
      "\tspeed: 0.0106s/iter; left time: 112.2214s\n",
      "\titers: 300, epoch: 2 | loss: 0.2269728\n",
      "\tspeed: 0.0106s/iter; left time: 111.2207s\n",
      "\titers: 400, epoch: 2 | loss: 0.1790629\n",
      "\tspeed: 0.0105s/iter; left time: 109.8423s\n",
      "\titers: 500, epoch: 2 | loss: 0.2005681\n",
      "\tspeed: 0.0105s/iter; left time: 108.9586s\n",
      "Epoch: 2 cost time: 6.328356742858887\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2194005 Vali Loss: 0.0467984 Test Loss: 0.1458801\n",
      "Validation loss decreased (0.049930 --> 0.046798).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1609079\n",
      "\tspeed: 0.0363s/iter; left time: 368.3548s\n",
      "\titers: 200, epoch: 3 | loss: 0.2279218\n",
      "\tspeed: 0.0105s/iter; left time: 105.6526s\n",
      "\titers: 300, epoch: 3 | loss: 0.1517598\n",
      "\tspeed: 0.0105s/iter; left time: 104.7625s\n",
      "\titers: 400, epoch: 3 | loss: 0.1926924\n",
      "\tspeed: 0.0105s/iter; left time: 103.6667s\n",
      "\titers: 500, epoch: 3 | loss: 0.2619040\n",
      "\tspeed: 0.0105s/iter; left time: 102.3897s\n",
      "Epoch: 3 cost time: 6.2816174030303955\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1928285 Vali Loss: 0.0434940 Test Loss: 0.1316906\n",
      "Validation loss decreased (0.046798 --> 0.043494).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1974011\n",
      "\tspeed: 0.0379s/iter; left time: 363.8252s\n",
      "\titers: 200, epoch: 4 | loss: 0.0997751\n",
      "\tspeed: 0.0105s/iter; left time: 100.1135s\n",
      "\titers: 300, epoch: 4 | loss: 0.1813466\n",
      "\tspeed: 0.0105s/iter; left time: 98.8256s\n",
      "\titers: 400, epoch: 4 | loss: 0.1814042\n",
      "\tspeed: 0.0105s/iter; left time: 97.8537s\n",
      "\titers: 500, epoch: 4 | loss: 0.1456451\n",
      "\tspeed: 0.0105s/iter; left time: 96.9351s\n",
      "Epoch: 4 cost time: 6.326786041259766\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1741642 Vali Loss: 0.0419364 Test Loss: 0.1319740\n",
      "Validation loss decreased (0.043494 --> 0.041936).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2005041\n",
      "\tspeed: 0.0384s/iter; left time: 346.2140s\n",
      "\titers: 200, epoch: 5 | loss: 0.1612382\n",
      "\tspeed: 0.0106s/iter; left time: 94.5116s\n",
      "\titers: 300, epoch: 5 | loss: 0.1828484\n",
      "\tspeed: 0.0106s/iter; left time: 93.7661s\n",
      "\titers: 400, epoch: 5 | loss: 0.1539630\n",
      "\tspeed: 0.0106s/iter; left time: 92.7666s\n",
      "\titers: 500, epoch: 5 | loss: 0.1531187\n",
      "\tspeed: 0.0106s/iter; left time: 91.6864s\n",
      "Epoch: 5 cost time: 6.355587959289551\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1631698 Vali Loss: 0.0412901 Test Loss: 0.1334837\n",
      "Validation loss decreased (0.041936 --> 0.041290).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1511397\n",
      "\tspeed: 0.0365s/iter; left time: 308.0851s\n",
      "\titers: 200, epoch: 6 | loss: 0.1123220\n",
      "\tspeed: 0.0106s/iter; left time: 88.3079s\n",
      "\titers: 300, epoch: 6 | loss: 0.2148315\n",
      "\tspeed: 0.0106s/iter; left time: 87.5644s\n",
      "\titers: 400, epoch: 6 | loss: 0.1687021\n",
      "\tspeed: 0.0106s/iter; left time: 86.3210s\n",
      "\titers: 500, epoch: 6 | loss: 0.1469876\n",
      "\tspeed: 0.0114s/iter; left time: 91.6978s\n",
      "Epoch: 6 cost time: 6.510421991348267\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1537006 Vali Loss: 0.0401119 Test Loss: 0.1313482\n",
      "Validation loss decreased (0.041290 --> 0.040112).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1674063\n",
      "\tspeed: 0.0408s/iter; left time: 321.4466s\n",
      "\titers: 200, epoch: 7 | loss: 0.2014802\n",
      "\tspeed: 0.0121s/iter; left time: 94.0672s\n",
      "\titers: 300, epoch: 7 | loss: 0.1516421\n",
      "\tspeed: 0.0121s/iter; left time: 92.6287s\n",
      "\titers: 400, epoch: 7 | loss: 0.1144426\n",
      "\tspeed: 0.0120s/iter; left time: 91.1781s\n",
      "\titers: 500, epoch: 7 | loss: 0.1227431\n",
      "\tspeed: 0.0120s/iter; left time: 89.9743s\n",
      "Epoch: 7 cost time: 7.187563419342041\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1511042 Vali Loss: 0.0403949 Test Loss: 0.1302827\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1535092\n",
      "\tspeed: 0.0393s/iter; left time: 287.1929s\n",
      "\titers: 200, epoch: 8 | loss: 0.1330004\n",
      "\tspeed: 0.0120s/iter; left time: 86.5201s\n",
      "\titers: 300, epoch: 8 | loss: 0.1745556\n",
      "\tspeed: 0.0120s/iter; left time: 85.4150s\n",
      "\titers: 400, epoch: 8 | loss: 0.2076908\n",
      "\tspeed: 0.0106s/iter; left time: 74.1341s\n",
      "\titers: 500, epoch: 8 | loss: 0.1386951\n",
      "\tspeed: 0.0105s/iter; left time: 72.6999s\n",
      "Epoch: 8 cost time: 6.742974519729614\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1473867 Vali Loss: 0.0400983 Test Loss: 0.1308237\n",
      "Validation loss decreased (0.040112 --> 0.040098).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2033126\n",
      "\tspeed: 0.0379s/iter; left time: 255.7400s\n",
      "\titers: 200, epoch: 9 | loss: 0.1391253\n",
      "\tspeed: 0.0105s/iter; left time: 69.8863s\n",
      "\titers: 300, epoch: 9 | loss: 0.0781884\n",
      "\tspeed: 0.0113s/iter; left time: 73.6096s\n",
      "\titers: 400, epoch: 9 | loss: 0.1423060\n",
      "\tspeed: 0.0120s/iter; left time: 77.3829s\n",
      "\titers: 500, epoch: 9 | loss: 0.1406676\n",
      "\tspeed: 0.0120s/iter; left time: 76.2090s\n",
      "Epoch: 9 cost time: 6.768323659896851\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1468509 Vali Loss: 0.0405770 Test Loss: 0.1315916\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1316060\n",
      "\tspeed: 0.0388s/iter; left time: 239.2590s\n",
      "\titers: 200, epoch: 10 | loss: 0.1492279\n",
      "\tspeed: 0.0120s/iter; left time: 72.7464s\n",
      "\titers: 300, epoch: 10 | loss: 0.1004063\n",
      "\tspeed: 0.0120s/iter; left time: 71.4599s\n",
      "\titers: 400, epoch: 10 | loss: 0.1272062\n",
      "\tspeed: 0.0120s/iter; left time: 70.6478s\n",
      "\titers: 500, epoch: 10 | loss: 0.1947585\n",
      "\tspeed: 0.0120s/iter; left time: 69.4094s\n",
      "Epoch: 10 cost time: 7.11151909828186\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1463307 Vali Loss: 0.0401553 Test Loss: 0.1311446\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1390495\n",
      "\tspeed: 0.0393s/iter; left time: 220.3936s\n",
      "\titers: 200, epoch: 11 | loss: 0.1162399\n",
      "\tspeed: 0.0120s/iter; left time: 66.0204s\n",
      "\titers: 300, epoch: 11 | loss: 0.1146599\n",
      "\tspeed: 0.0120s/iter; left time: 64.8200s\n",
      "\titers: 400, epoch: 11 | loss: 0.1900888\n",
      "\tspeed: 0.0120s/iter; left time: 63.6200s\n",
      "\titers: 500, epoch: 11 | loss: 0.1377741\n",
      "\tspeed: 0.0120s/iter; left time: 62.5150s\n",
      "Epoch: 11 cost time: 7.155396938323975\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1467670 Vali Loss: 0.0401198 Test Loss: 0.1309380\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1310017704963684, mae:0.22413523495197296\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2011288\n",
      "\tspeed: 0.0220s/iter; left time: 248.1608s\n",
      "\titers: 200, epoch: 1 | loss: 0.2121019\n",
      "\tspeed: 0.0103s/iter; left time: 115.8290s\n",
      "\titers: 300, epoch: 1 | loss: 0.2554605\n",
      "\tspeed: 0.0104s/iter; left time: 115.0610s\n",
      "\titers: 400, epoch: 1 | loss: 0.2007774\n",
      "\tspeed: 0.0103s/iter; left time: 113.8543s\n",
      "\titers: 500, epoch: 1 | loss: 0.1596668\n",
      "\tspeed: 0.0104s/iter; left time: 112.9831s\n",
      "Epoch: 1 cost time: 7.112987041473389\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1986092 Vali Loss: 0.0408514 Test Loss: 0.1363988\n",
      "Validation loss decreased (inf --> 0.040851).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1544909\n",
      "\tspeed: 0.0353s/iter; left time: 378.2975s\n",
      "\titers: 200, epoch: 2 | loss: 0.1977520\n",
      "\tspeed: 0.0103s/iter; left time: 109.5473s\n",
      "\titers: 300, epoch: 2 | loss: 0.1335832\n",
      "\tspeed: 0.0104s/iter; left time: 109.5863s\n",
      "\titers: 400, epoch: 2 | loss: 0.1909222\n",
      "\tspeed: 0.0104s/iter; left time: 108.5082s\n",
      "\titers: 500, epoch: 2 | loss: 0.1895629\n",
      "\tspeed: 0.0103s/iter; left time: 106.8931s\n",
      "Epoch: 2 cost time: 6.145948648452759\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1816486 Vali Loss: 0.0411289 Test Loss: 0.1282441\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.0831251\n",
      "\tspeed: 0.0349s/iter; left time: 354.6057s\n",
      "\titers: 200, epoch: 3 | loss: 0.1447144\n",
      "\tspeed: 0.0104s/iter; left time: 104.7075s\n",
      "\titers: 300, epoch: 3 | loss: 0.3137289\n",
      "\tspeed: 0.0104s/iter; left time: 103.3650s\n",
      "\titers: 400, epoch: 3 | loss: 0.1911944\n",
      "\tspeed: 0.0104s/iter; left time: 102.5886s\n",
      "\titers: 500, epoch: 3 | loss: 0.1429696\n",
      "\tspeed: 0.0104s/iter; left time: 101.2718s\n",
      "Epoch: 3 cost time: 6.15851092338562\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1624195 Vali Loss: 0.0374713 Test Loss: 0.1170176\n",
      "Validation loss decreased (0.040851 --> 0.037471).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1103782\n",
      "\tspeed: 0.0351s/iter; left time: 336.4517s\n",
      "\titers: 200, epoch: 4 | loss: 0.1187697\n",
      "\tspeed: 0.0103s/iter; left time: 98.0373s\n",
      "\titers: 300, epoch: 4 | loss: 0.0884880\n",
      "\tspeed: 0.0104s/iter; left time: 97.3973s\n",
      "\titers: 400, epoch: 4 | loss: 0.0976685\n",
      "\tspeed: 0.0104s/iter; left time: 96.3311s\n",
      "\titers: 500, epoch: 4 | loss: 0.1811225\n",
      "\tspeed: 0.0102s/iter; left time: 94.1264s\n",
      "Epoch: 4 cost time: 6.142638921737671\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1434960 Vali Loss: 0.0341161 Test Loss: 0.1104835\n",
      "Validation loss decreased (0.037471 --> 0.034116).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1103035\n",
      "\tspeed: 0.0345s/iter; left time: 311.1085s\n",
      "\titers: 200, epoch: 5 | loss: 0.1790779\n",
      "\tspeed: 0.0102s/iter; left time: 91.1114s\n",
      "\titers: 300, epoch: 5 | loss: 0.1173813\n",
      "\tspeed: 0.0103s/iter; left time: 90.5466s\n",
      "\titers: 400, epoch: 5 | loss: 0.1384535\n",
      "\tspeed: 0.0103s/iter; left time: 89.7487s\n",
      "\titers: 500, epoch: 5 | loss: 0.1380685\n",
      "\tspeed: 0.0103s/iter; left time: 88.4119s\n",
      "Epoch: 5 cost time: 6.091769456863403\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1332018 Vali Loss: 0.0354310 Test Loss: 0.1137024\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0893482\n",
      "\tspeed: 0.0351s/iter; left time: 296.8090s\n",
      "\titers: 200, epoch: 6 | loss: 0.1794420\n",
      "\tspeed: 0.0103s/iter; left time: 85.7462s\n",
      "\titers: 300, epoch: 6 | loss: 0.1483492\n",
      "\tspeed: 0.0103s/iter; left time: 84.7140s\n",
      "\titers: 400, epoch: 6 | loss: 0.0914391\n",
      "\tspeed: 0.0103s/iter; left time: 83.7423s\n",
      "\titers: 500, epoch: 6 | loss: 0.0890388\n",
      "\tspeed: 0.0103s/iter; left time: 83.0541s\n",
      "Epoch: 6 cost time: 6.122091293334961\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1277808 Vali Loss: 0.0344818 Test Loss: 0.1114692\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1217993\n",
      "\tspeed: 0.0343s/iter; left time: 270.0012s\n",
      "\titers: 200, epoch: 7 | loss: 0.1507055\n",
      "\tspeed: 0.0103s/iter; left time: 79.7674s\n",
      "\titers: 300, epoch: 7 | loss: 0.2469890\n",
      "\tspeed: 0.0103s/iter; left time: 78.7820s\n",
      "\titers: 400, epoch: 7 | loss: 0.1859451\n",
      "\tspeed: 0.0103s/iter; left time: 77.8831s\n",
      "\titers: 500, epoch: 7 | loss: 0.1438853\n",
      "\tspeed: 0.0103s/iter; left time: 76.6955s\n",
      "Epoch: 7 cost time: 6.094702482223511\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1256190 Vali Loss: 0.0339808 Test Loss: 0.1099915\n",
      "Validation loss decreased (0.034116 --> 0.033981).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1184343\n",
      "\tspeed: 0.0347s/iter; left time: 253.7964s\n",
      "\titers: 200, epoch: 8 | loss: 0.0885658\n",
      "\tspeed: 0.0104s/iter; left time: 75.1712s\n",
      "\titers: 300, epoch: 8 | loss: 0.1112555\n",
      "\tspeed: 0.0104s/iter; left time: 73.8762s\n",
      "\titers: 400, epoch: 8 | loss: 0.1127180\n",
      "\tspeed: 0.0103s/iter; left time: 72.5619s\n",
      "\titers: 500, epoch: 8 | loss: 0.0920499\n",
      "\tspeed: 0.0104s/iter; left time: 71.5468s\n",
      "Epoch: 8 cost time: 6.168047666549683\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1232764 Vali Loss: 0.0343710 Test Loss: 0.1123049\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1152850\n",
      "\tspeed: 0.0345s/iter; left time: 232.2962s\n",
      "\titers: 200, epoch: 9 | loss: 0.1366268\n",
      "\tspeed: 0.0103s/iter; left time: 68.0796s\n",
      "\titers: 300, epoch: 9 | loss: 0.1035083\n",
      "\tspeed: 0.0103s/iter; left time: 67.0494s\n",
      "\titers: 400, epoch: 9 | loss: 0.1578759\n",
      "\tspeed: 0.0102s/iter; left time: 65.9086s\n",
      "\titers: 500, epoch: 9 | loss: 0.1022582\n",
      "\tspeed: 0.0102s/iter; left time: 64.9063s\n",
      "Epoch: 9 cost time: 6.085042476654053\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1214196 Vali Loss: 0.0341249 Test Loss: 0.1114252\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1141844\n",
      "\tspeed: 0.0346s/iter; left time: 213.7459s\n",
      "\titers: 200, epoch: 10 | loss: 0.1014069\n",
      "\tspeed: 0.0102s/iter; left time: 61.7690s\n",
      "\titers: 300, epoch: 10 | loss: 0.1240180\n",
      "\tspeed: 0.0103s/iter; left time: 61.3894s\n",
      "\titers: 400, epoch: 10 | loss: 0.0875974\n",
      "\tspeed: 0.0103s/iter; left time: 60.4837s\n",
      "\titers: 500, epoch: 10 | loss: 0.0761100\n",
      "\tspeed: 0.0103s/iter; left time: 59.4337s\n",
      "Epoch: 10 cost time: 6.094587087631226\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1217547 Vali Loss: 0.0341139 Test Loss: 0.1113909\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11014990508556366, mae:0.20213402807712555\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2760210\n",
      "\tspeed: 0.0221s/iter; left time: 249.6275s\n",
      "\titers: 200, epoch: 1 | loss: 0.1569620\n",
      "\tspeed: 0.0104s/iter; left time: 116.1553s\n",
      "\titers: 300, epoch: 1 | loss: 0.1703339\n",
      "\tspeed: 0.0104s/iter; left time: 114.9946s\n",
      "\titers: 400, epoch: 1 | loss: 0.1834075\n",
      "\tspeed: 0.0103s/iter; left time: 113.5458s\n",
      "\titers: 500, epoch: 1 | loss: 0.2316654\n",
      "\tspeed: 0.0103s/iter; left time: 112.6484s\n",
      "Epoch: 1 cost time: 7.1259284019470215\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2487522 Vali Loss: 0.0535518 Test Loss: 0.1598241\n",
      "Validation loss decreased (inf --> 0.053552).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2102094\n",
      "\tspeed: 0.0363s/iter; left time: 389.2785s\n",
      "\titers: 200, epoch: 2 | loss: 0.5037255\n",
      "\tspeed: 0.0104s/iter; left time: 110.5636s\n",
      "\titers: 300, epoch: 2 | loss: 0.3371377\n",
      "\tspeed: 0.0104s/iter; left time: 109.6513s\n",
      "\titers: 400, epoch: 2 | loss: 0.4152882\n",
      "\tspeed: 0.0104s/iter; left time: 108.2592s\n",
      "\titers: 500, epoch: 2 | loss: 0.3067176\n",
      "\tspeed: 0.0104s/iter; left time: 107.3752s\n",
      "Epoch: 2 cost time: 6.217277765274048\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2717112 Vali Loss: 0.0605821 Test Loss: 0.1763985\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1737072\n",
      "\tspeed: 0.0365s/iter; left time: 370.8607s\n",
      "\titers: 200, epoch: 3 | loss: 0.2173738\n",
      "\tspeed: 0.0103s/iter; left time: 103.9524s\n",
      "\titers: 300, epoch: 3 | loss: 0.3133671\n",
      "\tspeed: 0.0103s/iter; left time: 102.9243s\n",
      "\titers: 400, epoch: 3 | loss: 0.2098190\n",
      "\tspeed: 0.0104s/iter; left time: 102.1212s\n",
      "\titers: 500, epoch: 3 | loss: 0.2057551\n",
      "\tspeed: 0.0103s/iter; left time: 100.9544s\n",
      "Epoch: 3 cost time: 6.221595287322998\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2512181 Vali Loss: 0.0534191 Test Loss: 0.1621263\n",
      "Validation loss decreased (0.053552 --> 0.053419).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1413024\n",
      "\tspeed: 0.0365s/iter; left time: 350.1955s\n",
      "\titers: 200, epoch: 4 | loss: 0.1723188\n",
      "\tspeed: 0.0103s/iter; left time: 97.2911s\n",
      "\titers: 300, epoch: 4 | loss: 0.1920038\n",
      "\tspeed: 0.0103s/iter; left time: 96.4166s\n",
      "\titers: 400, epoch: 4 | loss: 0.3015375\n",
      "\tspeed: 0.0102s/iter; left time: 95.2151s\n",
      "\titers: 500, epoch: 4 | loss: 0.1878215\n",
      "\tspeed: 0.0103s/iter; left time: 94.2326s\n",
      "Epoch: 4 cost time: 6.165150880813599\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2239809 Vali Loss: 0.0535721 Test Loss: 0.1619853\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2102711\n",
      "\tspeed: 0.0360s/iter; left time: 324.5524s\n",
      "\titers: 200, epoch: 5 | loss: 0.1812565\n",
      "\tspeed: 0.0103s/iter; left time: 91.6072s\n",
      "\titers: 300, epoch: 5 | loss: 0.2270057\n",
      "\tspeed: 0.0103s/iter; left time: 90.4180s\n",
      "\titers: 400, epoch: 5 | loss: 0.1802817\n",
      "\tspeed: 0.0103s/iter; left time: 89.4781s\n",
      "\titers: 500, epoch: 5 | loss: 0.3264125\n",
      "\tspeed: 0.0103s/iter; left time: 88.5428s\n",
      "Epoch: 5 cost time: 6.105363368988037\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2111830 Vali Loss: 0.0497295 Test Loss: 0.1540494\n",
      "Validation loss decreased (0.053419 --> 0.049729).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1856858\n",
      "\tspeed: 0.0357s/iter; left time: 301.7106s\n",
      "\titers: 200, epoch: 6 | loss: 0.2089431\n",
      "\tspeed: 0.0103s/iter; left time: 86.1090s\n",
      "\titers: 300, epoch: 6 | loss: 0.1692455\n",
      "\tspeed: 0.0103s/iter; left time: 85.1572s\n",
      "\titers: 400, epoch: 6 | loss: 0.1765668\n",
      "\tspeed: 0.0103s/iter; left time: 84.0178s\n",
      "\titers: 500, epoch: 6 | loss: 0.2485469\n",
      "\tspeed: 0.0103s/iter; left time: 83.0665s\n",
      "Epoch: 6 cost time: 6.20879054069519\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2018977 Vali Loss: 0.0494379 Test Loss: 0.1522202\n",
      "Validation loss decreased (0.049729 --> 0.049438).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1340874\n",
      "\tspeed: 0.0384s/iter; left time: 302.7710s\n",
      "\titers: 200, epoch: 7 | loss: 0.1357253\n",
      "\tspeed: 0.0116s/iter; left time: 90.6325s\n",
      "\titers: 300, epoch: 7 | loss: 0.1856140\n",
      "\tspeed: 0.0116s/iter; left time: 89.2359s\n",
      "\titers: 400, epoch: 7 | loss: 0.1292744\n",
      "\tspeed: 0.0116s/iter; left time: 88.0704s\n",
      "\titers: 500, epoch: 7 | loss: 0.2150543\n",
      "\tspeed: 0.0116s/iter; left time: 86.9231s\n",
      "Epoch: 7 cost time: 6.927873611450195\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1983729 Vali Loss: 0.0494550 Test Loss: 0.1508071\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1108174\n",
      "\tspeed: 0.0374s/iter; left time: 273.6816s\n",
      "\titers: 200, epoch: 8 | loss: 0.1593016\n",
      "\tspeed: 0.0104s/iter; left time: 74.6510s\n",
      "\titers: 300, epoch: 8 | loss: 0.1904207\n",
      "\tspeed: 0.0103s/iter; left time: 73.5963s\n",
      "\titers: 400, epoch: 8 | loss: 0.1893917\n",
      "\tspeed: 0.0104s/iter; left time: 72.5981s\n",
      "\titers: 500, epoch: 8 | loss: 0.1773364\n",
      "\tspeed: 0.0104s/iter; left time: 71.5383s\n",
      "Epoch: 8 cost time: 6.225088834762573\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1954682 Vali Loss: 0.0490171 Test Loss: 0.1501433\n",
      "Validation loss decreased (0.049438 --> 0.049017).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1811720\n",
      "\tspeed: 0.0365s/iter; left time: 245.8161s\n",
      "\titers: 200, epoch: 9 | loss: 0.2337634\n",
      "\tspeed: 0.0103s/iter; left time: 68.6211s\n",
      "\titers: 300, epoch: 9 | loss: 0.1826662\n",
      "\tspeed: 0.0103s/iter; left time: 67.6745s\n",
      "\titers: 400, epoch: 9 | loss: 0.1685027\n",
      "\tspeed: 0.0104s/iter; left time: 66.7988s\n",
      "\titers: 500, epoch: 9 | loss: 0.2114587\n",
      "\tspeed: 0.0103s/iter; left time: 65.0349s\n",
      "Epoch: 9 cost time: 6.207677602767944\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1932855 Vali Loss: 0.0489839 Test Loss: 0.1497300\n",
      "Validation loss decreased (0.049017 --> 0.048984).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1468959\n",
      "\tspeed: 0.0370s/iter; left time: 228.4504s\n",
      "\titers: 200, epoch: 10 | loss: 0.2943932\n",
      "\tspeed: 0.0104s/iter; left time: 63.1307s\n",
      "\titers: 300, epoch: 10 | loss: 0.2734065\n",
      "\tspeed: 0.0104s/iter; left time: 61.8112s\n",
      "\titers: 400, epoch: 10 | loss: 0.2248660\n",
      "\tspeed: 0.0103s/iter; left time: 60.7223s\n",
      "\titers: 500, epoch: 10 | loss: 0.2291971\n",
      "\tspeed: 0.0104s/iter; left time: 59.9693s\n",
      "Epoch: 10 cost time: 6.239911079406738\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1929867 Vali Loss: 0.0490412 Test Loss: 0.1497344\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2016304\n",
      "\tspeed: 0.0363s/iter; left time: 203.0829s\n",
      "\titers: 200, epoch: 11 | loss: 0.1890531\n",
      "\tspeed: 0.0104s/iter; left time: 57.0521s\n",
      "\titers: 300, epoch: 11 | loss: 0.2562253\n",
      "\tspeed: 0.0104s/iter; left time: 56.0377s\n",
      "\titers: 400, epoch: 11 | loss: 0.1936106\n",
      "\tspeed: 0.0103s/iter; left time: 54.8067s\n",
      "\titers: 500, epoch: 11 | loss: 0.1959444\n",
      "\tspeed: 0.0103s/iter; left time: 53.7505s\n",
      "Epoch: 11 cost time: 6.185500860214233\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1928420 Vali Loss: 0.0488997 Test Loss: 0.1497000\n",
      "Validation loss decreased (0.048984 --> 0.048900).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1751735\n",
      "\tspeed: 0.0358s/iter; left time: 179.8907s\n",
      "\titers: 200, epoch: 12 | loss: 0.2051178\n",
      "\tspeed: 0.0102s/iter; left time: 50.5045s\n",
      "\titers: 300, epoch: 12 | loss: 0.1418368\n",
      "\tspeed: 0.0103s/iter; left time: 49.5596s\n",
      "\titers: 400, epoch: 12 | loss: 0.1730569\n",
      "\tspeed: 0.0103s/iter; left time: 48.5569s\n",
      "\titers: 500, epoch: 12 | loss: 0.0879592\n",
      "\tspeed: 0.0103s/iter; left time: 47.6042s\n",
      "Epoch: 12 cost time: 6.12819504737854\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1935263 Vali Loss: 0.0489174 Test Loss: 0.1497241\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1612099\n",
      "\tspeed: 0.0367s/iter; left time: 163.5257s\n",
      "\titers: 200, epoch: 13 | loss: 0.2018778\n",
      "\tspeed: 0.0104s/iter; left time: 45.3403s\n",
      "\titers: 300, epoch: 13 | loss: 0.1750134\n",
      "\tspeed: 0.0104s/iter; left time: 44.1676s\n",
      "\titers: 400, epoch: 13 | loss: 0.2200643\n",
      "\tspeed: 0.0104s/iter; left time: 43.1754s\n",
      "\titers: 500, epoch: 13 | loss: 0.1767964\n",
      "\tspeed: 0.0104s/iter; left time: 42.1988s\n",
      "Epoch: 13 cost time: 6.198461532592773\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1924223 Vali Loss: 0.0490513 Test Loss: 0.1496656\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1728437\n",
      "\tspeed: 0.0367s/iter; left time: 142.8753s\n",
      "\titers: 200, epoch: 14 | loss: 0.2402483\n",
      "\tspeed: 0.0103s/iter; left time: 39.1956s\n",
      "\titers: 300, epoch: 14 | loss: 0.1398079\n",
      "\tspeed: 0.0104s/iter; left time: 38.2435s\n",
      "\titers: 400, epoch: 14 | loss: 0.1153657\n",
      "\tspeed: 0.0104s/iter; left time: 37.1823s\n",
      "\titers: 500, epoch: 14 | loss: 0.2750791\n",
      "\tspeed: 0.0103s/iter; left time: 36.1039s\n",
      "Epoch: 14 cost time: 6.181631088256836\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1909967 Vali Loss: 0.0488073 Test Loss: 0.1496678\n",
      "Validation loss decreased (0.048900 --> 0.048807).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1617509\n",
      "\tspeed: 0.0364s/iter; left time: 120.7817s\n",
      "\titers: 200, epoch: 15 | loss: 0.2353400\n",
      "\tspeed: 0.0104s/iter; left time: 33.3615s\n",
      "\titers: 300, epoch: 15 | loss: 0.1058330\n",
      "\tspeed: 0.0104s/iter; left time: 32.3428s\n",
      "\titers: 400, epoch: 15 | loss: 0.0849429\n",
      "\tspeed: 0.0104s/iter; left time: 31.2978s\n",
      "\titers: 500, epoch: 15 | loss: 0.2182889\n",
      "\tspeed: 0.0103s/iter; left time: 30.1139s\n",
      "Epoch: 15 cost time: 6.1944990158081055\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1929903 Vali Loss: 0.0488566 Test Loss: 0.1496770\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1995316\n",
      "\tspeed: 0.0364s/iter; left time: 100.1507s\n",
      "\titers: 200, epoch: 16 | loss: 0.1273779\n",
      "\tspeed: 0.0103s/iter; left time: 27.2608s\n",
      "\titers: 300, epoch: 16 | loss: 0.1738085\n",
      "\tspeed: 0.0103s/iter; left time: 26.1846s\n",
      "\titers: 400, epoch: 16 | loss: 0.1989763\n",
      "\tspeed: 0.0103s/iter; left time: 25.1576s\n",
      "\titers: 500, epoch: 16 | loss: 0.1810515\n",
      "\tspeed: 0.0103s/iter; left time: 24.1430s\n",
      "Epoch: 16 cost time: 6.132954120635986\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1922915 Vali Loss: 0.0491062 Test Loss: 0.1496811\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1572271\n",
      "\tspeed: 0.0358s/iter; left time: 77.9948s\n",
      "\titers: 200, epoch: 17 | loss: 0.1318000\n",
      "\tspeed: 0.0104s/iter; left time: 21.7323s\n",
      "\titers: 300, epoch: 17 | loss: 0.1215536\n",
      "\tspeed: 0.0103s/iter; left time: 20.3828s\n",
      "\titers: 400, epoch: 17 | loss: 0.1946611\n",
      "\tspeed: 0.0103s/iter; left time: 19.3525s\n",
      "\titers: 500, epoch: 17 | loss: 0.1164205\n",
      "\tspeed: 0.0103s/iter; left time: 18.3447s\n",
      "Epoch: 17 cost time: 6.1892664432525635\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1921213 Vali Loss: 0.0491891 Test Loss: 0.1496846\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14987124502658844, mae:0.24645471572875977\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3265904\n",
      "\tspeed: 0.0234s/iter; left time: 264.2897s\n",
      "\titers: 200, epoch: 1 | loss: 0.2824314\n",
      "\tspeed: 0.0118s/iter; left time: 132.2920s\n",
      "\titers: 300, epoch: 1 | loss: 0.2027302\n",
      "\tspeed: 0.0118s/iter; left time: 131.1623s\n",
      "\titers: 400, epoch: 1 | loss: 0.1829416\n",
      "\tspeed: 0.0118s/iter; left time: 129.8434s\n",
      "\titers: 500, epoch: 1 | loss: 0.2720419\n",
      "\tspeed: 0.0118s/iter; left time: 128.7465s\n",
      "Epoch: 1 cost time: 7.936777353286743\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2819023 Vali Loss: 0.0710527 Test Loss: 0.1796554\n",
      "Validation loss decreased (inf --> 0.071053).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2995609\n",
      "\tspeed: 0.0399s/iter; left time: 428.6000s\n",
      "\titers: 200, epoch: 2 | loss: 0.2093853\n",
      "\tspeed: 0.0118s/iter; left time: 125.8983s\n",
      "\titers: 300, epoch: 2 | loss: 0.2064758\n",
      "\tspeed: 0.0119s/iter; left time: 124.8264s\n",
      "\titers: 400, epoch: 2 | loss: 0.1652062\n",
      "\tspeed: 0.0119s/iter; left time: 123.8096s\n",
      "\titers: 500, epoch: 2 | loss: 0.2448857\n",
      "\tspeed: 0.0118s/iter; left time: 122.4130s\n",
      "Epoch: 2 cost time: 7.090646743774414\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3054748 Vali Loss: 0.0641825 Test Loss: 0.1975380\n",
      "Validation loss decreased (0.071053 --> 0.064182).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3358017\n",
      "\tspeed: 0.0390s/iter; left time: 395.8983s\n",
      "\titers: 200, epoch: 3 | loss: 0.4676081\n",
      "\tspeed: 0.0118s/iter; left time: 119.2204s\n",
      "\titers: 300, epoch: 3 | loss: 0.1594378\n",
      "\tspeed: 0.0119s/iter; left time: 118.1408s\n",
      "\titers: 400, epoch: 3 | loss: 0.2910548\n",
      "\tspeed: 0.0119s/iter; left time: 116.8730s\n",
      "\titers: 500, epoch: 3 | loss: 0.2464300\n",
      "\tspeed: 0.0119s/iter; left time: 115.6725s\n",
      "Epoch: 3 cost time: 7.076974153518677\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2865843 Vali Loss: 0.0679678 Test Loss: 0.1888290\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1879116\n",
      "\tspeed: 0.0399s/iter; left time: 382.5808s\n",
      "\titers: 200, epoch: 4 | loss: 0.3424387\n",
      "\tspeed: 0.0129s/iter; left time: 122.7279s\n",
      "\titers: 300, epoch: 4 | loss: 0.1733771\n",
      "\tspeed: 0.0129s/iter; left time: 121.4030s\n",
      "\titers: 400, epoch: 4 | loss: 0.1620296\n",
      "\tspeed: 0.0129s/iter; left time: 120.1100s\n",
      "\titers: 500, epoch: 4 | loss: 0.2221527\n",
      "\tspeed: 0.0129s/iter; left time: 118.8398s\n",
      "Epoch: 4 cost time: 7.636206388473511\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2680516 Vali Loss: 0.0610722 Test Loss: 0.1731480\n",
      "Validation loss decreased (0.064182 --> 0.061072).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2432520\n",
      "\tspeed: 0.0406s/iter; left time: 366.5708s\n",
      "\titers: 200, epoch: 5 | loss: 0.2601934\n",
      "\tspeed: 0.0119s/iter; left time: 106.5654s\n",
      "\titers: 300, epoch: 5 | loss: 0.2983795\n",
      "\tspeed: 0.0119s/iter; left time: 105.0648s\n",
      "\titers: 400, epoch: 5 | loss: 0.2202662\n",
      "\tspeed: 0.0119s/iter; left time: 103.7478s\n",
      "\titers: 500, epoch: 5 | loss: 0.2291308\n",
      "\tspeed: 0.0119s/iter; left time: 102.8923s\n",
      "Epoch: 5 cost time: 7.117336750030518\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2547676 Vali Loss: 0.0613823 Test Loss: 0.1707520\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2284081\n",
      "\tspeed: 0.0412s/iter; left time: 347.8764s\n",
      "\titers: 200, epoch: 6 | loss: 0.1313824\n",
      "\tspeed: 0.0119s/iter; left time: 99.6252s\n",
      "\titers: 300, epoch: 6 | loss: 0.2413198\n",
      "\tspeed: 0.0119s/iter; left time: 97.9891s\n",
      "\titers: 400, epoch: 6 | loss: 0.3221701\n",
      "\tspeed: 0.0119s/iter; left time: 96.8580s\n",
      "\titers: 500, epoch: 6 | loss: 0.1760595\n",
      "\tspeed: 0.0119s/iter; left time: 95.7902s\n",
      "Epoch: 6 cost time: 7.091256618499756\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2456368 Vali Loss: 0.0611322 Test Loss: 0.1657845\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2951231\n",
      "\tspeed: 0.0391s/iter; left time: 308.2787s\n",
      "\titers: 200, epoch: 7 | loss: 0.2469725\n",
      "\tspeed: 0.0119s/iter; left time: 92.4948s\n",
      "\titers: 300, epoch: 7 | loss: 0.2372079\n",
      "\tspeed: 0.0119s/iter; left time: 91.3821s\n",
      "\titers: 400, epoch: 7 | loss: 0.1685892\n",
      "\tspeed: 0.0119s/iter; left time: 90.2972s\n",
      "\titers: 500, epoch: 7 | loss: 0.2855080\n",
      "\tspeed: 0.0119s/iter; left time: 88.8895s\n",
      "Epoch: 7 cost time: 7.08808708190918\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2416486 Vali Loss: 0.0592097 Test Loss: 0.1649025\n",
      "Validation loss decreased (0.061072 --> 0.059210).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1336527\n",
      "\tspeed: 0.0400s/iter; left time: 292.2528s\n",
      "\titers: 200, epoch: 8 | loss: 0.2625571\n",
      "\tspeed: 0.0119s/iter; left time: 85.7933s\n",
      "\titers: 300, epoch: 8 | loss: 0.1553656\n",
      "\tspeed: 0.0119s/iter; left time: 84.7073s\n",
      "\titers: 400, epoch: 8 | loss: 0.1543682\n",
      "\tspeed: 0.0118s/iter; left time: 82.9662s\n",
      "\titers: 500, epoch: 8 | loss: 0.2160325\n",
      "\tspeed: 0.0119s/iter; left time: 82.0296s\n",
      "Epoch: 8 cost time: 7.087522268295288\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2399890 Vali Loss: 0.0591067 Test Loss: 0.1641041\n",
      "Validation loss decreased (0.059210 --> 0.059107).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1801660\n",
      "\tspeed: 0.0414s/iter; left time: 278.8654s\n",
      "\titers: 200, epoch: 9 | loss: 0.1819342\n",
      "\tspeed: 0.0121s/iter; left time: 80.1549s\n",
      "\titers: 300, epoch: 9 | loss: 0.2147153\n",
      "\tspeed: 0.0119s/iter; left time: 77.9347s\n",
      "\titers: 400, epoch: 9 | loss: 0.2052983\n",
      "\tspeed: 0.0119s/iter; left time: 76.9416s\n",
      "\titers: 500, epoch: 9 | loss: 0.2915892\n",
      "\tspeed: 0.0119s/iter; left time: 75.6071s\n",
      "Epoch: 9 cost time: 7.26894736289978\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2384743 Vali Loss: 0.0582299 Test Loss: 0.1634074\n",
      "Validation loss decreased (0.059107 --> 0.058230).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1885498\n",
      "\tspeed: 0.0412s/iter; left time: 253.9782s\n",
      "\titers: 200, epoch: 10 | loss: 0.2248306\n",
      "\tspeed: 0.0130s/iter; left time: 78.9659s\n",
      "\titers: 300, epoch: 10 | loss: 0.1583959\n",
      "\tspeed: 0.0130s/iter; left time: 77.4008s\n",
      "\titers: 400, epoch: 10 | loss: 0.2134813\n",
      "\tspeed: 0.0129s/iter; left time: 76.0024s\n",
      "\titers: 500, epoch: 10 | loss: 0.4080054\n",
      "\tspeed: 0.0129s/iter; left time: 74.6826s\n",
      "Epoch: 10 cost time: 7.690529823303223\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2358646 Vali Loss: 0.0583463 Test Loss: 0.1634760\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1674199\n",
      "\tspeed: 0.0414s/iter; left time: 231.7299s\n",
      "\titers: 200, epoch: 11 | loss: 0.3668125\n",
      "\tspeed: 0.0119s/iter; left time: 65.2996s\n",
      "\titers: 300, epoch: 11 | loss: 0.2715604\n",
      "\tspeed: 0.0119s/iter; left time: 64.1036s\n",
      "\titers: 400, epoch: 11 | loss: 0.2415816\n",
      "\tspeed: 0.0119s/iter; left time: 62.9370s\n",
      "\titers: 500, epoch: 11 | loss: 0.2843077\n",
      "\tspeed: 0.0119s/iter; left time: 61.7204s\n",
      "Epoch: 11 cost time: 7.0616326332092285\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2364390 Vali Loss: 0.0585822 Test Loss: 0.1633486\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2286063\n",
      "\tspeed: 0.0403s/iter; left time: 202.7325s\n",
      "\titers: 200, epoch: 12 | loss: 0.1283193\n",
      "\tspeed: 0.0129s/iter; left time: 63.8055s\n",
      "\titers: 300, epoch: 12 | loss: 0.2065744\n",
      "\tspeed: 0.0129s/iter; left time: 62.5056s\n",
      "\titers: 400, epoch: 12 | loss: 0.2635843\n",
      "\tspeed: 0.0129s/iter; left time: 61.2123s\n",
      "\titers: 500, epoch: 12 | loss: 0.2058234\n",
      "\tspeed: 0.0124s/iter; left time: 57.2785s\n",
      "Epoch: 12 cost time: 7.540778636932373\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2372949 Vali Loss: 0.0585811 Test Loss: 0.1633064\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16357727348804474, mae:0.25873905420303345\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1796685\n",
      "\tspeed: 0.0247s/iter; left time: 278.7058s\n",
      "\titers: 200, epoch: 1 | loss: 0.3203726\n",
      "\tspeed: 0.0126s/iter; left time: 141.1232s\n",
      "\titers: 300, epoch: 1 | loss: 0.2118618\n",
      "\tspeed: 0.0126s/iter; left time: 139.5380s\n",
      "\titers: 400, epoch: 1 | loss: 0.2701736\n",
      "\tspeed: 0.0126s/iter; left time: 138.5407s\n",
      "\titers: 500, epoch: 1 | loss: 0.2387635\n",
      "\tspeed: 0.0126s/iter; left time: 137.8519s\n",
      "Epoch: 1 cost time: 8.441968441009521\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2496155 Vali Loss: 0.0789338 Test Loss: 0.2181921\n",
      "Validation loss decreased (inf --> 0.078934).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2929499\n",
      "\tspeed: 0.0401s/iter; left time: 430.2132s\n",
      "\titers: 200, epoch: 2 | loss: 0.2174159\n",
      "\tspeed: 0.0115s/iter; left time: 121.8703s\n",
      "\titers: 300, epoch: 2 | loss: 0.2474419\n",
      "\tspeed: 0.0115s/iter; left time: 120.9723s\n",
      "\titers: 400, epoch: 2 | loss: 0.3588068\n",
      "\tspeed: 0.0115s/iter; left time: 119.6934s\n",
      "\titers: 500, epoch: 2 | loss: 0.5979308\n",
      "\tspeed: 0.0115s/iter; left time: 118.4081s\n",
      "Epoch: 2 cost time: 6.873464822769165\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3421846 Vali Loss: 0.0665686 Test Loss: 0.1938915\n",
      "Validation loss decreased (0.078934 --> 0.066569).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3956637\n",
      "\tspeed: 0.0394s/iter; left time: 400.1699s\n",
      "\titers: 200, epoch: 3 | loss: 0.1582731\n",
      "\tspeed: 0.0115s/iter; left time: 115.8283s\n",
      "\titers: 300, epoch: 3 | loss: 0.3669603\n",
      "\tspeed: 0.0115s/iter; left time: 114.5688s\n",
      "\titers: 400, epoch: 3 | loss: 0.2793542\n",
      "\tspeed: 0.0115s/iter; left time: 113.4541s\n",
      "\titers: 500, epoch: 3 | loss: 0.3144830\n",
      "\tspeed: 0.0115s/iter; left time: 112.4047s\n",
      "Epoch: 3 cost time: 6.904554605484009\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2952531 Vali Loss: 0.0628071 Test Loss: 0.1831682\n",
      "Validation loss decreased (0.066569 --> 0.062807).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2594198\n",
      "\tspeed: 0.0384s/iter; left time: 367.8684s\n",
      "\titers: 200, epoch: 4 | loss: 0.1618074\n",
      "\tspeed: 0.0115s/iter; left time: 108.7779s\n",
      "\titers: 300, epoch: 4 | loss: 0.1491181\n",
      "\tspeed: 0.0115s/iter; left time: 107.6711s\n",
      "\titers: 400, epoch: 4 | loss: 0.2702274\n",
      "\tspeed: 0.0115s/iter; left time: 106.5877s\n",
      "\titers: 500, epoch: 4 | loss: 0.2366442\n",
      "\tspeed: 0.0115s/iter; left time: 105.3176s\n",
      "Epoch: 4 cost time: 6.855206489562988\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2701998 Vali Loss: 0.0637959 Test Loss: 0.1755745\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2900212\n",
      "\tspeed: 0.0394s/iter; left time: 355.1959s\n",
      "\titers: 200, epoch: 5 | loss: 0.2182794\n",
      "\tspeed: 0.0115s/iter; left time: 102.6770s\n",
      "\titers: 300, epoch: 5 | loss: 0.3421086\n",
      "\tspeed: 0.0115s/iter; left time: 101.4444s\n",
      "\titers: 400, epoch: 5 | loss: 0.1955701\n",
      "\tspeed: 0.0115s/iter; left time: 100.3447s\n",
      "\titers: 500, epoch: 5 | loss: 0.2884355\n",
      "\tspeed: 0.0115s/iter; left time: 99.1132s\n",
      "Epoch: 5 cost time: 6.8312294483184814\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2586808 Vali Loss: 0.0617454 Test Loss: 0.1717319\n",
      "Validation loss decreased (0.062807 --> 0.061745).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2905227\n",
      "\tspeed: 0.0389s/iter; left time: 328.6812s\n",
      "\titers: 200, epoch: 6 | loss: 0.2730651\n",
      "\tspeed: 0.0115s/iter; left time: 95.8443s\n",
      "\titers: 300, epoch: 6 | loss: 0.1626481\n",
      "\tspeed: 0.0122s/iter; left time: 100.6955s\n",
      "\titers: 400, epoch: 6 | loss: 0.1758117\n",
      "\tspeed: 0.0126s/iter; left time: 102.7584s\n",
      "\titers: 500, epoch: 6 | loss: 0.2027135\n",
      "\tspeed: 0.0126s/iter; left time: 101.2582s\n",
      "Epoch: 6 cost time: 7.2682623863220215\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2503739 Vali Loss: 0.0603163 Test Loss: 0.1696076\n",
      "Validation loss decreased (0.061745 --> 0.060316).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1629372\n",
      "\tspeed: 0.0400s/iter; left time: 315.5678s\n",
      "\titers: 200, epoch: 7 | loss: 0.4093698\n",
      "\tspeed: 0.0115s/iter; left time: 89.7954s\n",
      "\titers: 300, epoch: 7 | loss: 0.2375754\n",
      "\tspeed: 0.0116s/iter; left time: 88.8021s\n",
      "\titers: 400, epoch: 7 | loss: 0.1748221\n",
      "\tspeed: 0.0116s/iter; left time: 87.5896s\n",
      "\titers: 500, epoch: 7 | loss: 0.1773041\n",
      "\tspeed: 0.0115s/iter; left time: 86.0133s\n",
      "Epoch: 7 cost time: 6.8954126834869385\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2470633 Vali Loss: 0.0601705 Test Loss: 0.1683172\n",
      "Validation loss decreased (0.060316 --> 0.060171).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1559660\n",
      "\tspeed: 0.0405s/iter; left time: 296.0186s\n",
      "\titers: 200, epoch: 8 | loss: 0.2440483\n",
      "\tspeed: 0.0115s/iter; left time: 83.1260s\n",
      "\titers: 300, epoch: 8 | loss: 0.2035100\n",
      "\tspeed: 0.0115s/iter; left time: 81.9826s\n",
      "\titers: 400, epoch: 8 | loss: 0.2517019\n",
      "\tspeed: 0.0115s/iter; left time: 80.8048s\n",
      "\titers: 500, epoch: 8 | loss: 0.1877975\n",
      "\tspeed: 0.0115s/iter; left time: 79.6925s\n",
      "Epoch: 8 cost time: 6.906211853027344\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2481832 Vali Loss: 0.0595144 Test Loss: 0.1678012\n",
      "Validation loss decreased (0.060171 --> 0.059514).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3043793\n",
      "\tspeed: 0.0417s/iter; left time: 281.1454s\n",
      "\titers: 200, epoch: 9 | loss: 0.1733602\n",
      "\tspeed: 0.0115s/iter; left time: 76.5444s\n",
      "\titers: 300, epoch: 9 | loss: 0.3501782\n",
      "\tspeed: 0.0115s/iter; left time: 75.4126s\n",
      "\titers: 400, epoch: 9 | loss: 0.3207723\n",
      "\tspeed: 0.0115s/iter; left time: 74.1635s\n",
      "\titers: 500, epoch: 9 | loss: 0.3596117\n",
      "\tspeed: 0.0115s/iter; left time: 72.9450s\n",
      "Epoch: 9 cost time: 6.8962085247039795\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2453948 Vali Loss: 0.0594102 Test Loss: 0.1675649\n",
      "Validation loss decreased (0.059514 --> 0.059410).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1794396\n",
      "\tspeed: 0.0401s/iter; left time: 247.2466s\n",
      "\titers: 200, epoch: 10 | loss: 0.2279155\n",
      "\tspeed: 0.0115s/iter; left time: 69.7413s\n",
      "\titers: 300, epoch: 10 | loss: 0.1558017\n",
      "\tspeed: 0.0115s/iter; left time: 68.5841s\n",
      "\titers: 400, epoch: 10 | loss: 0.2411603\n",
      "\tspeed: 0.0115s/iter; left time: 67.4029s\n",
      "\titers: 500, epoch: 10 | loss: 0.2668256\n",
      "\tspeed: 0.0115s/iter; left time: 66.2734s\n",
      "Epoch: 10 cost time: 6.88175106048584\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2453270 Vali Loss: 0.0596609 Test Loss: 0.1673820\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1222677\n",
      "\tspeed: 0.0395s/iter; left time: 221.3738s\n",
      "\titers: 200, epoch: 11 | loss: 0.1957473\n",
      "\tspeed: 0.0115s/iter; left time: 62.9936s\n",
      "\titers: 300, epoch: 11 | loss: 0.2241580\n",
      "\tspeed: 0.0115s/iter; left time: 61.9691s\n",
      "\titers: 400, epoch: 11 | loss: 0.2861233\n",
      "\tspeed: 0.0115s/iter; left time: 60.7823s\n",
      "\titers: 500, epoch: 11 | loss: 0.3761418\n",
      "\tspeed: 0.0115s/iter; left time: 59.6821s\n",
      "Epoch: 11 cost time: 6.832703113555908\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2456568 Vali Loss: 0.0596132 Test Loss: 0.1673657\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2111523\n",
      "\tspeed: 0.0391s/iter; left time: 196.8936s\n",
      "\titers: 200, epoch: 12 | loss: 0.1821611\n",
      "\tspeed: 0.0115s/iter; left time: 56.4926s\n",
      "\titers: 300, epoch: 12 | loss: 0.1647033\n",
      "\tspeed: 0.0115s/iter; left time: 55.3849s\n",
      "\titers: 400, epoch: 12 | loss: 0.3011793\n",
      "\tspeed: 0.0115s/iter; left time: 54.2016s\n",
      "\titers: 500, epoch: 12 | loss: 0.2388800\n",
      "\tspeed: 0.0115s/iter; left time: 53.0567s\n",
      "Epoch: 12 cost time: 6.816465377807617\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2440781 Vali Loss: 0.0597405 Test Loss: 0.1674265\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16781757771968842, mae:0.2611628472805023\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3327174\n",
      "\tspeed: 0.0225s/iter; left time: 254.1465s\n",
      "\titers: 200, epoch: 1 | loss: 0.3182364\n",
      "\tspeed: 0.0108s/iter; left time: 121.0584s\n",
      "\titers: 300, epoch: 1 | loss: 0.3426073\n",
      "\tspeed: 0.0108s/iter; left time: 120.1339s\n",
      "\titers: 400, epoch: 1 | loss: 0.3059278\n",
      "\tspeed: 0.0108s/iter; left time: 118.8458s\n",
      "\titers: 500, epoch: 1 | loss: 0.4027936\n",
      "\tspeed: 0.0108s/iter; left time: 117.8089s\n",
      "Epoch: 1 cost time: 7.38435173034668\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3039419 Vali Loss: 0.0807907 Test Loss: 0.2209334\n",
      "Validation loss decreased (inf --> 0.080791).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3536313\n",
      "\tspeed: 0.0392s/iter; left time: 420.6887s\n",
      "\titers: 200, epoch: 2 | loss: 0.3966191\n",
      "\tspeed: 0.0108s/iter; left time: 115.2626s\n",
      "\titers: 300, epoch: 2 | loss: 0.2305571\n",
      "\tspeed: 0.0108s/iter; left time: 113.8976s\n",
      "\titers: 400, epoch: 2 | loss: 0.2279711\n",
      "\tspeed: 0.0108s/iter; left time: 112.6158s\n",
      "\titers: 500, epoch: 2 | loss: 0.2820280\n",
      "\tspeed: 0.0108s/iter; left time: 111.5863s\n",
      "Epoch: 2 cost time: 6.4819018840789795\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3421060 Vali Loss: 0.0724195 Test Loss: 0.1958509\n",
      "Validation loss decreased (0.080791 --> 0.072419).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2353123\n",
      "\tspeed: 0.0389s/iter; left time: 394.9965s\n",
      "\titers: 200, epoch: 3 | loss: 0.2257837\n",
      "\tspeed: 0.0108s/iter; left time: 108.8133s\n",
      "\titers: 300, epoch: 3 | loss: 0.2265708\n",
      "\tspeed: 0.0108s/iter; left time: 107.5865s\n",
      "\titers: 400, epoch: 3 | loss: 0.2759785\n",
      "\tspeed: 0.0109s/iter; left time: 107.1905s\n",
      "\titers: 500, epoch: 3 | loss: 0.2378863\n",
      "\tspeed: 0.0109s/iter; left time: 106.0180s\n",
      "Epoch: 3 cost time: 6.473947525024414\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2737085 Vali Loss: 0.0630089 Test Loss: 0.1675890\n",
      "Validation loss decreased (0.072419 --> 0.063009).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2410200\n",
      "\tspeed: 0.0370s/iter; left time: 355.0066s\n",
      "\titers: 200, epoch: 4 | loss: 0.1390311\n",
      "\tspeed: 0.0108s/iter; left time: 102.6927s\n",
      "\titers: 300, epoch: 4 | loss: 0.2606155\n",
      "\tspeed: 0.0108s/iter; left time: 101.6468s\n",
      "\titers: 400, epoch: 4 | loss: 0.4015678\n",
      "\tspeed: 0.0108s/iter; left time: 100.5444s\n",
      "\titers: 500, epoch: 4 | loss: 0.2342789\n",
      "\tspeed: 0.0108s/iter; left time: 99.5030s\n",
      "Epoch: 4 cost time: 6.484527349472046\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2503709 Vali Loss: 0.0647304 Test Loss: 0.1658181\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2422166\n",
      "\tspeed: 0.0373s/iter; left time: 336.0405s\n",
      "\titers: 200, epoch: 5 | loss: 0.1894854\n",
      "\tspeed: 0.0108s/iter; left time: 96.1085s\n",
      "\titers: 300, epoch: 5 | loss: 0.2011713\n",
      "\tspeed: 0.0108s/iter; left time: 95.0381s\n",
      "\titers: 400, epoch: 5 | loss: 0.1729721\n",
      "\tspeed: 0.0108s/iter; left time: 94.2527s\n",
      "\titers: 500, epoch: 5 | loss: 0.3241611\n",
      "\tspeed: 0.0108s/iter; left time: 92.8559s\n",
      "Epoch: 5 cost time: 6.444623947143555\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2379840 Vali Loss: 0.0632330 Test Loss: 0.1627935\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1657743\n",
      "\tspeed: 0.0366s/iter; left time: 309.0723s\n",
      "\titers: 200, epoch: 6 | loss: 0.2104838\n",
      "\tspeed: 0.0108s/iter; left time: 90.2418s\n",
      "\titers: 300, epoch: 6 | loss: 0.2671312\n",
      "\tspeed: 0.0108s/iter; left time: 89.1504s\n",
      "\titers: 400, epoch: 6 | loss: 0.2211233\n",
      "\tspeed: 0.0108s/iter; left time: 88.1613s\n",
      "\titers: 500, epoch: 6 | loss: 0.1637257\n",
      "\tspeed: 0.0108s/iter; left time: 86.9881s\n",
      "Epoch: 6 cost time: 6.447439432144165\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2299352 Vali Loss: 0.0637382 Test Loss: 0.1610361\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16782337427139282, mae:0.265283465385437\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2021424\n",
      "\tspeed: 0.0263s/iter; left time: 296.8362s\n",
      "\titers: 200, epoch: 1 | loss: 0.1925161\n",
      "\tspeed: 0.0146s/iter; left time: 163.4683s\n",
      "\titers: 300, epoch: 1 | loss: 0.3646648\n",
      "\tspeed: 0.0145s/iter; left time: 161.3904s\n",
      "\titers: 400, epoch: 1 | loss: 0.3172990\n",
      "\tspeed: 0.0145s/iter; left time: 159.6204s\n",
      "\titers: 500, epoch: 1 | loss: 0.1780760\n",
      "\tspeed: 0.0146s/iter; left time: 158.9163s\n",
      "Epoch: 1 cost time: 9.520718812942505\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2514787 Vali Loss: 0.0618211 Test Loss: 0.1670834\n",
      "Validation loss decreased (inf --> 0.061821).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3758127\n",
      "\tspeed: 0.0478s/iter; left time: 512.9086s\n",
      "\titers: 200, epoch: 2 | loss: 0.2328117\n",
      "\tspeed: 0.0147s/iter; left time: 156.1674s\n",
      "\titers: 300, epoch: 2 | loss: 0.2166753\n",
      "\tspeed: 0.0146s/iter; left time: 154.2770s\n",
      "\titers: 400, epoch: 2 | loss: 0.1600483\n",
      "\tspeed: 0.0147s/iter; left time: 153.0867s\n",
      "\titers: 500, epoch: 2 | loss: 0.2440310\n",
      "\tspeed: 0.0146s/iter; left time: 151.2053s\n",
      "Epoch: 2 cost time: 8.67594313621521\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2545696 Vali Loss: 0.0566529 Test Loss: 0.1636013\n",
      "Validation loss decreased (0.061821 --> 0.056653).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3808836\n",
      "\tspeed: 0.0475s/iter; left time: 482.3335s\n",
      "\titers: 200, epoch: 3 | loss: 0.2466947\n",
      "\tspeed: 0.0146s/iter; left time: 146.5876s\n",
      "\titers: 300, epoch: 3 | loss: 0.1148460\n",
      "\tspeed: 0.0146s/iter; left time: 145.2056s\n",
      "\titers: 400, epoch: 3 | loss: 0.1709522\n",
      "\tspeed: 0.0146s/iter; left time: 143.6059s\n",
      "\titers: 500, epoch: 3 | loss: 0.1981073\n",
      "\tspeed: 0.0146s/iter; left time: 142.1807s\n",
      "Epoch: 3 cost time: 8.694891452789307\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2202817 Vali Loss: 0.0512793 Test Loss: 0.1501765\n",
      "Validation loss decreased (0.056653 --> 0.051279).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1944005\n",
      "\tspeed: 0.0476s/iter; left time: 456.9583s\n",
      "\titers: 200, epoch: 4 | loss: 0.1566377\n",
      "\tspeed: 0.0146s/iter; left time: 138.4783s\n",
      "\titers: 300, epoch: 4 | loss: 0.2081069\n",
      "\tspeed: 0.0146s/iter; left time: 136.8303s\n",
      "\titers: 400, epoch: 4 | loss: 0.1892206\n",
      "\tspeed: 0.0146s/iter; left time: 135.2711s\n",
      "\titers: 500, epoch: 4 | loss: 0.2124998\n",
      "\tspeed: 0.0146s/iter; left time: 133.8030s\n",
      "Epoch: 4 cost time: 8.613909244537354\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1995000 Vali Loss: 0.0463360 Test Loss: 0.1446078\n",
      "Validation loss decreased (0.051279 --> 0.046336).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1876682\n",
      "\tspeed: 0.0465s/iter; left time: 419.5015s\n",
      "\titers: 200, epoch: 5 | loss: 0.1668259\n",
      "\tspeed: 0.0146s/iter; left time: 129.8457s\n",
      "\titers: 300, epoch: 5 | loss: 0.1381411\n",
      "\tspeed: 0.0146s/iter; left time: 128.3651s\n",
      "\titers: 400, epoch: 5 | loss: 0.2317812\n",
      "\tspeed: 0.0146s/iter; left time: 126.9796s\n",
      "\titers: 500, epoch: 5 | loss: 0.1862955\n",
      "\tspeed: 0.0146s/iter; left time: 125.4827s\n",
      "Epoch: 5 cost time: 8.596188068389893\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1858906 Vali Loss: 0.0453545 Test Loss: 0.1382442\n",
      "Validation loss decreased (0.046336 --> 0.045354).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1635985\n",
      "\tspeed: 0.0469s/iter; left time: 396.2248s\n",
      "\titers: 200, epoch: 6 | loss: 0.1883183\n",
      "\tspeed: 0.0146s/iter; left time: 121.7540s\n",
      "\titers: 300, epoch: 6 | loss: 0.1240506\n",
      "\tspeed: 0.0146s/iter; left time: 120.3248s\n",
      "\titers: 400, epoch: 6 | loss: 0.1124600\n",
      "\tspeed: 0.0146s/iter; left time: 118.8672s\n",
      "\titers: 500, epoch: 6 | loss: 0.1650752\n",
      "\tspeed: 0.0146s/iter; left time: 117.3474s\n",
      "Epoch: 6 cost time: 8.62702465057373\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1777709 Vali Loss: 0.0447077 Test Loss: 0.1378238\n",
      "Validation loss decreased (0.045354 --> 0.044708).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1421961\n",
      "\tspeed: 0.0478s/iter; left time: 377.0995s\n",
      "\titers: 200, epoch: 7 | loss: 0.1592079\n",
      "\tspeed: 0.0146s/iter; left time: 113.2456s\n",
      "\titers: 300, epoch: 7 | loss: 0.1536149\n",
      "\tspeed: 0.0145s/iter; left time: 111.6701s\n",
      "\titers: 400, epoch: 7 | loss: 0.1435184\n",
      "\tspeed: 0.0145s/iter; left time: 110.0885s\n",
      "\titers: 500, epoch: 7 | loss: 0.1989434\n",
      "\tspeed: 0.0145s/iter; left time: 108.6191s\n",
      "Epoch: 7 cost time: 8.573155879974365\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1743078 Vali Loss: 0.0442103 Test Loss: 0.1381981\n",
      "Validation loss decreased (0.044708 --> 0.044210).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2754486\n",
      "\tspeed: 0.0483s/iter; left time: 352.7767s\n",
      "\titers: 200, epoch: 8 | loss: 0.1697963\n",
      "\tspeed: 0.0145s/iter; left time: 104.7873s\n",
      "\titers: 300, epoch: 8 | loss: 0.2003881\n",
      "\tspeed: 0.0145s/iter; left time: 103.0513s\n",
      "\titers: 400, epoch: 8 | loss: 0.1058447\n",
      "\tspeed: 0.0145s/iter; left time: 101.5738s\n",
      "\titers: 500, epoch: 8 | loss: 0.2092163\n",
      "\tspeed: 0.0145s/iter; left time: 100.3131s\n",
      "Epoch: 8 cost time: 8.578622341156006\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1720699 Vali Loss: 0.0444646 Test Loss: 0.1381192\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2775129\n",
      "\tspeed: 0.0458s/iter; left time: 308.9405s\n",
      "\titers: 200, epoch: 9 | loss: 0.1869513\n",
      "\tspeed: 0.0147s/iter; left time: 97.3422s\n",
      "\titers: 300, epoch: 9 | loss: 0.2404243\n",
      "\tspeed: 0.0147s/iter; left time: 95.8838s\n",
      "\titers: 400, epoch: 9 | loss: 0.1877683\n",
      "\tspeed: 0.0147s/iter; left time: 94.4225s\n",
      "\titers: 500, epoch: 9 | loss: 0.1491519\n",
      "\tspeed: 0.0147s/iter; left time: 93.1813s\n",
      "Epoch: 9 cost time: 8.6262948513031\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1709610 Vali Loss: 0.0442922 Test Loss: 0.1373717\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1937200\n",
      "\tspeed: 0.0492s/iter; left time: 303.3231s\n",
      "\titers: 200, epoch: 10 | loss: 0.1355558\n",
      "\tspeed: 0.0146s/iter; left time: 88.5715s\n",
      "\titers: 300, epoch: 10 | loss: 0.1136709\n",
      "\tspeed: 0.0146s/iter; left time: 87.0624s\n",
      "\titers: 400, epoch: 10 | loss: 0.1265586\n",
      "\tspeed: 0.0146s/iter; left time: 85.6768s\n",
      "\titers: 500, epoch: 10 | loss: 0.2174215\n",
      "\tspeed: 0.0146s/iter; left time: 84.1958s\n",
      "Epoch: 10 cost time: 8.631231307983398\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1704257 Vali Loss: 0.0441107 Test Loss: 0.1373031\n",
      "Validation loss decreased (0.044210 --> 0.044111).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1637001\n",
      "\tspeed: 0.0465s/iter; left time: 260.6175s\n",
      "\titers: 200, epoch: 11 | loss: 0.1189625\n",
      "\tspeed: 0.0146s/iter; left time: 80.1828s\n",
      "\titers: 300, epoch: 11 | loss: 0.1546430\n",
      "\tspeed: 0.0146s/iter; left time: 78.7249s\n",
      "\titers: 400, epoch: 11 | loss: 0.1115248\n",
      "\tspeed: 0.0146s/iter; left time: 77.2704s\n",
      "\titers: 500, epoch: 11 | loss: 0.1244606\n",
      "\tspeed: 0.0146s/iter; left time: 75.7880s\n",
      "Epoch: 11 cost time: 8.617980003356934\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1703624 Vali Loss: 0.0442272 Test Loss: 0.1372467\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1294420\n",
      "\tspeed: 0.0474s/iter; left time: 238.5052s\n",
      "\titers: 200, epoch: 12 | loss: 0.1040739\n",
      "\tspeed: 0.0163s/iter; left time: 80.4595s\n",
      "\titers: 300, epoch: 12 | loss: 0.1994811\n",
      "\tspeed: 0.0163s/iter; left time: 78.8603s\n",
      "\titers: 400, epoch: 12 | loss: 0.1641083\n",
      "\tspeed: 0.0163s/iter; left time: 77.1955s\n",
      "\titers: 500, epoch: 12 | loss: 0.1259927\n",
      "\tspeed: 0.0163s/iter; left time: 75.6012s\n",
      "Epoch: 12 cost time: 9.574938297271729\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1692939 Vali Loss: 0.0441475 Test Loss: 0.1371971\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2209660\n",
      "\tspeed: 0.0477s/iter; left time: 212.7996s\n",
      "\titers: 200, epoch: 13 | loss: 0.2390860\n",
      "\tspeed: 0.0147s/iter; left time: 64.0170s\n",
      "\titers: 300, epoch: 13 | loss: 0.2171553\n",
      "\tspeed: 0.0147s/iter; left time: 62.5122s\n",
      "\titers: 400, epoch: 13 | loss: 0.1052573\n",
      "\tspeed: 0.0147s/iter; left time: 61.0521s\n",
      "\titers: 500, epoch: 13 | loss: 0.1743914\n",
      "\tspeed: 0.0147s/iter; left time: 59.6608s\n",
      "Epoch: 13 cost time: 8.68053126335144\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1694824 Vali Loss: 0.0442130 Test Loss: 0.1371884\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13750919699668884, mae:0.2309766709804535\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3579784\n",
      "\tspeed: 0.0280s/iter; left time: 316.0873s\n",
      "\titers: 200, epoch: 1 | loss: 0.2492652\n",
      "\tspeed: 0.0154s/iter; left time: 173.0549s\n",
      "\titers: 300, epoch: 1 | loss: 0.1267075\n",
      "\tspeed: 0.0144s/iter; left time: 159.8214s\n",
      "\titers: 400, epoch: 1 | loss: 0.2885725\n",
      "\tspeed: 0.0144s/iter; left time: 158.6667s\n",
      "\titers: 500, epoch: 1 | loss: 0.1961450\n",
      "\tspeed: 0.0144s/iter; left time: 157.3870s\n",
      "Epoch: 1 cost time: 9.728955268859863\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2143978 Vali Loss: 0.0422654 Test Loss: 0.1404254\n",
      "Validation loss decreased (inf --> 0.042265).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1853428\n",
      "\tspeed: 0.0508s/iter; left time: 544.8091s\n",
      "\titers: 200, epoch: 2 | loss: 0.3092574\n",
      "\tspeed: 0.0166s/iter; left time: 176.3662s\n",
      "\titers: 300, epoch: 2 | loss: 0.2416447\n",
      "\tspeed: 0.0145s/iter; left time: 152.8807s\n",
      "\titers: 400, epoch: 2 | loss: 0.1907769\n",
      "\tspeed: 0.0144s/iter; left time: 150.4672s\n",
      "\titers: 500, epoch: 2 | loss: 0.3004417\n",
      "\tspeed: 0.0144s/iter; left time: 148.8610s\n",
      "Epoch: 2 cost time: 8.981372356414795\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2455699 Vali Loss: 0.0700985 Test Loss: 0.1969756\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3261602\n",
      "\tspeed: 0.0453s/iter; left time: 460.2564s\n",
      "\titers: 200, epoch: 3 | loss: 0.1153121\n",
      "\tspeed: 0.0143s/iter; left time: 143.4582s\n",
      "\titers: 300, epoch: 3 | loss: 0.3054507\n",
      "\tspeed: 0.0143s/iter; left time: 142.1340s\n",
      "\titers: 400, epoch: 3 | loss: 0.1318701\n",
      "\tspeed: 0.0143s/iter; left time: 140.8352s\n",
      "\titers: 500, epoch: 3 | loss: 0.2147380\n",
      "\tspeed: 0.0143s/iter; left time: 139.4629s\n",
      "Epoch: 3 cost time: 8.418151617050171\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2506641 Vali Loss: 0.0531762 Test Loss: 0.1599999\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2274706\n",
      "\tspeed: 0.0472s/iter; left time: 452.2841s\n",
      "\titers: 200, epoch: 4 | loss: 0.1663541\n",
      "\tspeed: 0.0143s/iter; left time: 135.5332s\n",
      "\titers: 300, epoch: 4 | loss: 0.3156635\n",
      "\tspeed: 0.0143s/iter; left time: 134.3928s\n",
      "\titers: 400, epoch: 4 | loss: 0.1417001\n",
      "\tspeed: 0.0143s/iter; left time: 132.8292s\n",
      "\titers: 500, epoch: 4 | loss: 0.1505014\n",
      "\tspeed: 0.0143s/iter; left time: 131.2975s\n",
      "Epoch: 4 cost time: 8.432449102401733\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2101283 Vali Loss: 0.0509230 Test Loss: 0.1516359\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1406233012676239, mae:0.23075206577777863\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1255287\n",
      "\tspeed: 0.0280s/iter; left time: 316.2098s\n",
      "\titers: 200, epoch: 1 | loss: 0.3506041\n",
      "\tspeed: 0.0161s/iter; left time: 180.0018s\n",
      "\titers: 300, epoch: 1 | loss: 0.3245904\n",
      "\tspeed: 0.0147s/iter; left time: 163.7001s\n",
      "\titers: 400, epoch: 1 | loss: 0.2325457\n",
      "\tspeed: 0.0143s/iter; left time: 157.8053s\n",
      "\titers: 500, epoch: 1 | loss: 0.2726045\n",
      "\tspeed: 0.0143s/iter; left time: 155.7659s\n",
      "Epoch: 1 cost time: 9.800244808197021\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2607614 Vali Loss: 0.0578785 Test Loss: 0.1718232\n",
      "Validation loss decreased (inf --> 0.057879).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2770151\n",
      "\tspeed: 0.0462s/iter; left time: 495.4485s\n",
      "\titers: 200, epoch: 2 | loss: 0.2334131\n",
      "\tspeed: 0.0143s/iter; left time: 151.8562s\n",
      "\titers: 300, epoch: 2 | loss: 0.1974951\n",
      "\tspeed: 0.0143s/iter; left time: 150.1260s\n",
      "\titers: 400, epoch: 2 | loss: 0.2227008\n",
      "\tspeed: 0.0143s/iter; left time: 148.9876s\n",
      "\titers: 500, epoch: 2 | loss: 0.3233249\n",
      "\tspeed: 0.0143s/iter; left time: 147.4253s\n",
      "Epoch: 2 cost time: 8.456159830093384\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2962000 Vali Loss: 0.0627234 Test Loss: 0.1698018\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2439906\n",
      "\tspeed: 0.0468s/iter; left time: 475.7972s\n",
      "\titers: 200, epoch: 3 | loss: 0.3127278\n",
      "\tspeed: 0.0144s/iter; left time: 144.3938s\n",
      "\titers: 300, epoch: 3 | loss: 0.2239275\n",
      "\tspeed: 0.0144s/iter; left time: 143.0111s\n",
      "\titers: 400, epoch: 3 | loss: 0.2060677\n",
      "\tspeed: 0.0157s/iter; left time: 154.7470s\n",
      "\titers: 500, epoch: 3 | loss: 0.2406228\n",
      "\tspeed: 0.0144s/iter; left time: 140.3397s\n",
      "Epoch: 3 cost time: 8.591138124465942\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2465655 Vali Loss: 0.0627705 Test Loss: 0.1686419\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3114236\n",
      "\tspeed: 0.0473s/iter; left time: 454.0787s\n",
      "\titers: 200, epoch: 4 | loss: 0.1990649\n",
      "\tspeed: 0.0144s/iter; left time: 136.2158s\n",
      "\titers: 300, epoch: 4 | loss: 0.2273496\n",
      "\tspeed: 0.0144s/iter; left time: 134.9313s\n",
      "\titers: 400, epoch: 4 | loss: 0.2008995\n",
      "\tspeed: 0.0143s/iter; left time: 132.8287s\n",
      "\titers: 500, epoch: 4 | loss: 0.1867630\n",
      "\tspeed: 0.0143s/iter; left time: 131.7265s\n",
      "Epoch: 4 cost time: 8.439200162887573\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2317419 Vali Loss: 0.0565305 Test Loss: 0.1541988\n",
      "Validation loss decreased (0.057879 --> 0.056530).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1827768\n",
      "\tspeed: 0.0483s/iter; left time: 435.5009s\n",
      "\titers: 200, epoch: 5 | loss: 0.2424145\n",
      "\tspeed: 0.0143s/iter; left time: 127.3777s\n",
      "\titers: 300, epoch: 5 | loss: 0.1769633\n",
      "\tspeed: 0.0165s/iter; left time: 145.7364s\n",
      "\titers: 400, epoch: 5 | loss: 0.3183462\n",
      "\tspeed: 0.0164s/iter; left time: 143.4048s\n",
      "\titers: 500, epoch: 5 | loss: 0.1517796\n",
      "\tspeed: 0.0152s/iter; left time: 131.3386s\n",
      "Epoch: 5 cost time: 8.962648391723633\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2206718 Vali Loss: 0.0559052 Test Loss: 0.1531190\n",
      "Validation loss decreased (0.056530 --> 0.055905).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2887405\n",
      "\tspeed: 0.0461s/iter; left time: 389.7776s\n",
      "\titers: 200, epoch: 6 | loss: 0.2290369\n",
      "\tspeed: 0.0142s/iter; left time: 118.6633s\n",
      "\titers: 300, epoch: 6 | loss: 0.1388823\n",
      "\tspeed: 0.0142s/iter; left time: 117.2411s\n",
      "\titers: 400, epoch: 6 | loss: 0.2632432\n",
      "\tspeed: 0.0142s/iter; left time: 115.7841s\n",
      "\titers: 500, epoch: 6 | loss: 0.2012624\n",
      "\tspeed: 0.0142s/iter; left time: 114.3449s\n",
      "Epoch: 6 cost time: 8.407126903533936\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2157504 Vali Loss: 0.0545692 Test Loss: 0.1535233\n",
      "Validation loss decreased (0.055905 --> 0.054569).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2609769\n",
      "\tspeed: 0.0464s/iter; left time: 365.8039s\n",
      "\titers: 200, epoch: 7 | loss: 0.1198593\n",
      "\tspeed: 0.0142s/iter; left time: 110.7750s\n",
      "\titers: 300, epoch: 7 | loss: 0.1548982\n",
      "\tspeed: 0.0142s/iter; left time: 109.3524s\n",
      "\titers: 400, epoch: 7 | loss: 0.2881845\n",
      "\tspeed: 0.0142s/iter; left time: 107.9363s\n",
      "\titers: 500, epoch: 7 | loss: 0.1886747\n",
      "\tspeed: 0.0142s/iter; left time: 106.4953s\n",
      "Epoch: 7 cost time: 8.419147729873657\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2104953 Vali Loss: 0.0552722 Test Loss: 0.1542632\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1520666\n",
      "\tspeed: 0.0465s/iter; left time: 339.9314s\n",
      "\titers: 200, epoch: 8 | loss: 0.1610115\n",
      "\tspeed: 0.0143s/iter; left time: 103.2733s\n",
      "\titers: 300, epoch: 8 | loss: 0.2201933\n",
      "\tspeed: 0.0143s/iter; left time: 101.8869s\n",
      "\titers: 400, epoch: 8 | loss: 0.1989685\n",
      "\tspeed: 0.0143s/iter; left time: 100.3769s\n",
      "\titers: 500, epoch: 8 | loss: 0.1495541\n",
      "\tspeed: 0.0144s/iter; left time: 99.4023s\n",
      "Epoch: 8 cost time: 8.499264001846313\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2098225 Vali Loss: 0.0553594 Test Loss: 0.1540214\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2585093\n",
      "\tspeed: 0.0474s/iter; left time: 319.2472s\n",
      "\titers: 200, epoch: 9 | loss: 0.1992339\n",
      "\tspeed: 0.0164s/iter; left time: 108.6401s\n",
      "\titers: 300, epoch: 9 | loss: 0.1346188\n",
      "\tspeed: 0.0155s/iter; left time: 101.2095s\n",
      "\titers: 400, epoch: 9 | loss: 0.1820253\n",
      "\tspeed: 0.0143s/iter; left time: 92.3034s\n",
      "\titers: 500, epoch: 9 | loss: 0.1771368\n",
      "\tspeed: 0.0143s/iter; left time: 90.8612s\n",
      "Epoch: 9 cost time: 8.943140983581543\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2070904 Vali Loss: 0.0549729 Test Loss: 0.1534572\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15372373163700104, mae:0.25145941972732544\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3330847\n",
      "\tspeed: 0.0288s/iter; left time: 324.9156s\n",
      "\titers: 200, epoch: 1 | loss: 0.4218915\n",
      "\tspeed: 0.0169s/iter; left time: 189.1219s\n",
      "\titers: 300, epoch: 1 | loss: 0.2805955\n",
      "\tspeed: 0.0169s/iter; left time: 187.4978s\n",
      "\titers: 400, epoch: 1 | loss: 0.3035471\n",
      "\tspeed: 0.0169s/iter; left time: 186.2265s\n",
      "\titers: 500, epoch: 1 | loss: 0.2725039\n",
      "\tspeed: 0.0169s/iter; left time: 184.1043s\n",
      "Epoch: 1 cost time: 10.87680721282959\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3454965 Vali Loss: 0.0697863 Test Loss: 0.1908302\n",
      "Validation loss decreased (inf --> 0.069786).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4603559\n",
      "\tspeed: 0.0526s/iter; left time: 564.8563s\n",
      "\titers: 200, epoch: 2 | loss: 0.3383348\n",
      "\tspeed: 0.0169s/iter; left time: 179.6173s\n",
      "\titers: 300, epoch: 2 | loss: 0.2927231\n",
      "\tspeed: 0.0169s/iter; left time: 177.5351s\n",
      "\titers: 400, epoch: 2 | loss: 0.3209269\n",
      "\tspeed: 0.0169s/iter; left time: 175.9175s\n",
      "\titers: 500, epoch: 2 | loss: 0.3092749\n",
      "\tspeed: 0.0169s/iter; left time: 174.3392s\n",
      "Epoch: 2 cost time: 9.947781324386597\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2911564 Vali Loss: 0.0641279 Test Loss: 0.1778177\n",
      "Validation loss decreased (0.069786 --> 0.064128).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2104033\n",
      "\tspeed: 0.0529s/iter; left time: 537.6355s\n",
      "\titers: 200, epoch: 3 | loss: 0.2310549\n",
      "\tspeed: 0.0169s/iter; left time: 169.9030s\n",
      "\titers: 300, epoch: 3 | loss: 0.4158018\n",
      "\tspeed: 0.0169s/iter; left time: 167.8791s\n",
      "\titers: 400, epoch: 3 | loss: 0.2733926\n",
      "\tspeed: 0.0169s/iter; left time: 166.4417s\n",
      "\titers: 500, epoch: 3 | loss: 0.2293164\n",
      "\tspeed: 0.0169s/iter; left time: 164.5858s\n",
      "Epoch: 3 cost time: 9.967597246170044\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2670521 Vali Loss: 0.0600581 Test Loss: 0.1713327\n",
      "Validation loss decreased (0.064128 --> 0.060058).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3341044\n",
      "\tspeed: 0.0519s/iter; left time: 498.1464s\n",
      "\titers: 200, epoch: 4 | loss: 0.2223510\n",
      "\tspeed: 0.0170s/iter; left time: 161.2565s\n",
      "\titers: 300, epoch: 4 | loss: 0.2641203\n",
      "\tspeed: 0.0169s/iter; left time: 159.0700s\n",
      "\titers: 400, epoch: 4 | loss: 0.3035638\n",
      "\tspeed: 0.0169s/iter; left time: 157.0756s\n",
      "\titers: 500, epoch: 4 | loss: 0.2579872\n",
      "\tspeed: 0.0169s/iter; left time: 155.6794s\n",
      "Epoch: 4 cost time: 10.000986576080322\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2549842 Vali Loss: 0.0583454 Test Loss: 0.1644505\n",
      "Validation loss decreased (0.060058 --> 0.058345).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1485283\n",
      "\tspeed: 0.0524s/iter; left time: 472.8873s\n",
      "\titers: 200, epoch: 5 | loss: 0.3071730\n",
      "\tspeed: 0.0168s/iter; left time: 150.1587s\n",
      "\titers: 300, epoch: 5 | loss: 0.2212565\n",
      "\tspeed: 0.0169s/iter; left time: 148.8001s\n",
      "\titers: 400, epoch: 5 | loss: 0.2692057\n",
      "\tspeed: 0.0168s/iter; left time: 146.7965s\n",
      "\titers: 500, epoch: 5 | loss: 0.1740633\n",
      "\tspeed: 0.0169s/iter; left time: 145.3776s\n",
      "Epoch: 5 cost time: 9.929325580596924\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2406700 Vali Loss: 0.0549469 Test Loss: 0.1616737\n",
      "Validation loss decreased (0.058345 --> 0.054947).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3004594\n",
      "\tspeed: 0.0520s/iter; left time: 439.6270s\n",
      "\titers: 200, epoch: 6 | loss: 0.2355074\n",
      "\tspeed: 0.0170s/iter; left time: 141.7596s\n",
      "\titers: 300, epoch: 6 | loss: 0.3626229\n",
      "\tspeed: 0.0170s/iter; left time: 139.9440s\n",
      "\titers: 400, epoch: 6 | loss: 0.2579860\n",
      "\tspeed: 0.0169s/iter; left time: 138.1127s\n",
      "\titers: 500, epoch: 6 | loss: 0.2163518\n",
      "\tspeed: 0.0169s/iter; left time: 136.1207s\n",
      "Epoch: 6 cost time: 9.982840299606323\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2339336 Vali Loss: 0.0543129 Test Loss: 0.1576901\n",
      "Validation loss decreased (0.054947 --> 0.054313).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1825576\n",
      "\tspeed: 0.0520s/iter; left time: 410.1943s\n",
      "\titers: 200, epoch: 7 | loss: 0.2221727\n",
      "\tspeed: 0.0169s/iter; left time: 131.7520s\n",
      "\titers: 300, epoch: 7 | loss: 0.3218852\n",
      "\tspeed: 0.0170s/iter; left time: 130.1968s\n",
      "\titers: 400, epoch: 7 | loss: 0.1682148\n",
      "\tspeed: 0.0170s/iter; left time: 128.6139s\n",
      "\titers: 500, epoch: 7 | loss: 0.1819262\n",
      "\tspeed: 0.0169s/iter; left time: 126.5412s\n",
      "Epoch: 7 cost time: 9.989230394363403\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2284274 Vali Loss: 0.0537960 Test Loss: 0.1573032\n",
      "Validation loss decreased (0.054313 --> 0.053796).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2675963\n",
      "\tspeed: 0.0517s/iter; left time: 378.1556s\n",
      "\titers: 200, epoch: 8 | loss: 0.1502899\n",
      "\tspeed: 0.0169s/iter; left time: 121.9491s\n",
      "\titers: 300, epoch: 8 | loss: 0.2123189\n",
      "\tspeed: 0.0169s/iter; left time: 120.2156s\n",
      "\titers: 400, epoch: 8 | loss: 0.1475389\n",
      "\tspeed: 0.0169s/iter; left time: 118.4812s\n",
      "\titers: 500, epoch: 8 | loss: 0.1974163\n",
      "\tspeed: 0.0169s/iter; left time: 116.7901s\n",
      "Epoch: 8 cost time: 9.979647397994995\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2272331 Vali Loss: 0.0537581 Test Loss: 0.1568641\n",
      "Validation loss decreased (0.053796 --> 0.053758).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2166151\n",
      "\tspeed: 0.0527s/iter; left time: 355.4052s\n",
      "\titers: 200, epoch: 9 | loss: 0.1553343\n",
      "\tspeed: 0.0169s/iter; left time: 112.5585s\n",
      "\titers: 300, epoch: 9 | loss: 0.2586129\n",
      "\tspeed: 0.0169s/iter; left time: 110.7320s\n",
      "\titers: 400, epoch: 9 | loss: 0.2952958\n",
      "\tspeed: 0.0169s/iter; left time: 109.0550s\n",
      "\titers: 500, epoch: 9 | loss: 0.3034004\n",
      "\tspeed: 0.0169s/iter; left time: 107.3487s\n",
      "Epoch: 9 cost time: 10.003300666809082\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2247944 Vali Loss: 0.0537391 Test Loss: 0.1563775\n",
      "Validation loss decreased (0.053758 --> 0.053739).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3168563\n",
      "\tspeed: 0.0517s/iter; left time: 319.0436s\n",
      "\titers: 200, epoch: 10 | loss: 0.0878856\n",
      "\tspeed: 0.0169s/iter; left time: 102.5931s\n",
      "\titers: 300, epoch: 10 | loss: 0.3373968\n",
      "\tspeed: 0.0169s/iter; left time: 100.8938s\n",
      "\titers: 400, epoch: 10 | loss: 0.2689933\n",
      "\tspeed: 0.0169s/iter; left time: 99.3905s\n",
      "\titers: 500, epoch: 10 | loss: 0.2301403\n",
      "\tspeed: 0.0170s/iter; left time: 97.8254s\n",
      "Epoch: 10 cost time: 9.979855060577393\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2245514 Vali Loss: 0.0535284 Test Loss: 0.1562473\n",
      "Validation loss decreased (0.053739 --> 0.053528).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1871051\n",
      "\tspeed: 0.0531s/iter; left time: 297.4122s\n",
      "\titers: 200, epoch: 11 | loss: 0.2793106\n",
      "\tspeed: 0.0169s/iter; left time: 92.9101s\n",
      "\titers: 300, epoch: 11 | loss: 0.1377579\n",
      "\tspeed: 0.0169s/iter; left time: 91.1119s\n",
      "\titers: 400, epoch: 11 | loss: 0.2009258\n",
      "\tspeed: 0.0168s/iter; left time: 89.2558s\n",
      "\titers: 500, epoch: 11 | loss: 0.3015958\n",
      "\tspeed: 0.0168s/iter; left time: 87.6068s\n",
      "Epoch: 11 cost time: 9.946401834487915\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2249263 Vali Loss: 0.0536926 Test Loss: 0.1561432\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2742327\n",
      "\tspeed: 0.0518s/iter; left time: 260.5698s\n",
      "\titers: 200, epoch: 12 | loss: 0.2219787\n",
      "\tspeed: 0.0169s/iter; left time: 83.1830s\n",
      "\titers: 300, epoch: 12 | loss: 0.1666441\n",
      "\tspeed: 0.0169s/iter; left time: 81.7021s\n",
      "\titers: 400, epoch: 12 | loss: 0.2601201\n",
      "\tspeed: 0.0169s/iter; left time: 80.1760s\n",
      "\titers: 500, epoch: 12 | loss: 0.2562214\n",
      "\tspeed: 0.0169s/iter; left time: 78.4356s\n",
      "Epoch: 12 cost time: 9.965986490249634\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2220818 Vali Loss: 0.0534947 Test Loss: 0.1561710\n",
      "Validation loss decreased (0.053528 --> 0.053495).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2680194\n",
      "\tspeed: 0.0522s/iter; left time: 233.0724s\n",
      "\titers: 200, epoch: 13 | loss: 0.2794768\n",
      "\tspeed: 0.0169s/iter; left time: 73.5231s\n",
      "\titers: 300, epoch: 13 | loss: 0.2121294\n",
      "\tspeed: 0.0169s/iter; left time: 71.7982s\n",
      "\titers: 400, epoch: 13 | loss: 0.1974308\n",
      "\tspeed: 0.0168s/iter; left time: 70.0995s\n",
      "\titers: 500, epoch: 13 | loss: 0.1572743\n",
      "\tspeed: 0.0168s/iter; left time: 68.4236s\n",
      "Epoch: 13 cost time: 9.946398973464966\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2236310 Vali Loss: 0.0535584 Test Loss: 0.1561416\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2315268\n",
      "\tspeed: 0.0517s/iter; left time: 201.2303s\n",
      "\titers: 200, epoch: 14 | loss: 0.1732250\n",
      "\tspeed: 0.0168s/iter; left time: 63.7727s\n",
      "\titers: 300, epoch: 14 | loss: 0.3574424\n",
      "\tspeed: 0.0168s/iter; left time: 62.0904s\n",
      "\titers: 400, epoch: 14 | loss: 0.1625539\n",
      "\tspeed: 0.0168s/iter; left time: 60.4024s\n",
      "\titers: 500, epoch: 14 | loss: 0.1681940\n",
      "\tspeed: 0.0168s/iter; left time: 58.7248s\n",
      "Epoch: 14 cost time: 9.894289016723633\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2241748 Vali Loss: 0.0537431 Test Loss: 0.1561242\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2904820\n",
      "\tspeed: 0.0510s/iter; left time: 169.4281s\n",
      "\titers: 200, epoch: 15 | loss: 0.2192653\n",
      "\tspeed: 0.0170s/iter; left time: 54.6811s\n",
      "\titers: 300, epoch: 15 | loss: 0.2122760\n",
      "\tspeed: 0.0170s/iter; left time: 52.9550s\n",
      "\titers: 400, epoch: 15 | loss: 0.2134637\n",
      "\tspeed: 0.0170s/iter; left time: 51.2131s\n",
      "\titers: 500, epoch: 15 | loss: 0.1997483\n",
      "\tspeed: 0.0170s/iter; left time: 49.6092s\n",
      "Epoch: 15 cost time: 10.001797676086426\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2233492 Vali Loss: 0.0536967 Test Loss: 0.1561356\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15627369284629822, mae:0.25418999791145325\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2144737\n",
      "\tspeed: 0.0300s/iter; left time: 339.0269s\n",
      "\titers: 200, epoch: 1 | loss: 0.3402271\n",
      "\tspeed: 0.0170s/iter; left time: 189.8981s\n",
      "\titers: 300, epoch: 1 | loss: 0.4058509\n",
      "\tspeed: 0.0164s/iter; left time: 182.1504s\n",
      "\titers: 400, epoch: 1 | loss: 0.2971203\n",
      "\tspeed: 0.0164s/iter; left time: 180.7162s\n",
      "\titers: 500, epoch: 1 | loss: 0.1734860\n",
      "\tspeed: 0.0165s/iter; left time: 179.3437s\n",
      "Epoch: 1 cost time: 10.828068494796753\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3428784 Vali Loss: 0.0651933 Test Loss: 0.1866074\n",
      "Validation loss decreased (inf --> 0.065193).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2711714\n",
      "\tspeed: 0.0519s/iter; left time: 556.6135s\n",
      "\titers: 200, epoch: 2 | loss: 0.1883248\n",
      "\tspeed: 0.0164s/iter; left time: 174.1201s\n",
      "\titers: 300, epoch: 2 | loss: 0.2948382\n",
      "\tspeed: 0.0164s/iter; left time: 172.4340s\n",
      "\titers: 400, epoch: 2 | loss: 0.3790845\n",
      "\tspeed: 0.0163s/iter; left time: 170.5341s\n",
      "\titers: 500, epoch: 2 | loss: 0.2702579\n",
      "\tspeed: 0.0163s/iter; left time: 168.8924s\n",
      "Epoch: 2 cost time: 9.679903984069824\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2971498 Vali Loss: 0.0673031 Test Loss: 0.1794859\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2691230\n",
      "\tspeed: 0.0506s/iter; left time: 514.2141s\n",
      "\titers: 200, epoch: 3 | loss: 0.2847893\n",
      "\tspeed: 0.0164s/iter; left time: 165.2632s\n",
      "\titers: 300, epoch: 3 | loss: 0.1823582\n",
      "\tspeed: 0.0165s/iter; left time: 163.9660s\n",
      "\titers: 400, epoch: 3 | loss: 0.3311642\n",
      "\tspeed: 0.0164s/iter; left time: 161.8889s\n",
      "\titers: 500, epoch: 3 | loss: 0.1644257\n",
      "\tspeed: 0.0164s/iter; left time: 160.5037s\n",
      "Epoch: 3 cost time: 9.681769371032715\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2645965 Vali Loss: 0.0609325 Test Loss: 0.1718027\n",
      "Validation loss decreased (0.065193 --> 0.060933).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2358768\n",
      "\tspeed: 0.0520s/iter; left time: 498.5706s\n",
      "\titers: 200, epoch: 4 | loss: 0.5337734\n",
      "\tspeed: 0.0164s/iter; left time: 156.1127s\n",
      "\titers: 300, epoch: 4 | loss: 0.2043590\n",
      "\tspeed: 0.0164s/iter; left time: 153.9982s\n",
      "\titers: 400, epoch: 4 | loss: 0.1743312\n",
      "\tspeed: 0.0164s/iter; left time: 152.6949s\n",
      "\titers: 500, epoch: 4 | loss: 0.3348444\n",
      "\tspeed: 0.0164s/iter; left time: 150.9156s\n",
      "Epoch: 4 cost time: 9.689658403396606\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2481150 Vali Loss: 0.0584789 Test Loss: 0.1644980\n",
      "Validation loss decreased (0.060933 --> 0.058479).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2391319\n",
      "\tspeed: 0.0523s/iter; left time: 472.2239s\n",
      "\titers: 200, epoch: 5 | loss: 0.3235899\n",
      "\tspeed: 0.0164s/iter; left time: 146.5559s\n",
      "\titers: 300, epoch: 5 | loss: 0.1979586\n",
      "\tspeed: 0.0165s/iter; left time: 145.2771s\n",
      "\titers: 400, epoch: 5 | loss: 0.1689177\n",
      "\tspeed: 0.0165s/iter; left time: 143.4834s\n",
      "\titers: 500, epoch: 5 | loss: 0.2987221\n",
      "\tspeed: 0.0164s/iter; left time: 141.6611s\n",
      "Epoch: 5 cost time: 9.69760775566101\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2362265 Vali Loss: 0.0601912 Test Loss: 0.1622988\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2104980\n",
      "\tspeed: 0.0505s/iter; left time: 427.1781s\n",
      "\titers: 200, epoch: 6 | loss: 0.3277321\n",
      "\tspeed: 0.0165s/iter; left time: 137.6332s\n",
      "\titers: 300, epoch: 6 | loss: 0.4561567\n",
      "\tspeed: 0.0165s/iter; left time: 135.9402s\n",
      "\titers: 400, epoch: 6 | loss: 0.2088858\n",
      "\tspeed: 0.0165s/iter; left time: 134.2362s\n",
      "\titers: 500, epoch: 6 | loss: 0.2088178\n",
      "\tspeed: 0.0165s/iter; left time: 132.5865s\n",
      "Epoch: 6 cost time: 9.717436075210571\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2292538 Vali Loss: 0.0588901 Test Loss: 0.1615104\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3115033\n",
      "\tspeed: 0.0516s/iter; left time: 407.0440s\n",
      "\titers: 200, epoch: 7 | loss: 0.1426938\n",
      "\tspeed: 0.0180s/iter; left time: 139.7563s\n",
      "\titers: 300, epoch: 7 | loss: 0.2557999\n",
      "\tspeed: 0.0179s/iter; left time: 137.8173s\n",
      "\titers: 400, epoch: 7 | loss: 0.3230401\n",
      "\tspeed: 0.0179s/iter; left time: 135.9202s\n",
      "\titers: 500, epoch: 7 | loss: 0.3296988\n",
      "\tspeed: 0.0172s/iter; left time: 128.9594s\n",
      "Epoch: 7 cost time: 10.368970155715942\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2278809 Vali Loss: 0.0594008 Test Loss: 0.1616243\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16471463441848755, mae:0.2595165967941284\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2977015\n",
      "\tspeed: 0.0286s/iter; left time: 322.8431s\n",
      "\titers: 200, epoch: 1 | loss: 0.3025633\n",
      "\tspeed: 0.0155s/iter; left time: 173.1965s\n",
      "\titers: 300, epoch: 1 | loss: 0.2813328\n",
      "\tspeed: 0.0154s/iter; left time: 171.3234s\n",
      "\titers: 400, epoch: 1 | loss: 0.3453762\n",
      "\tspeed: 0.0154s/iter; left time: 169.8357s\n",
      "\titers: 500, epoch: 1 | loss: 0.6158192\n",
      "\tspeed: 0.0155s/iter; left time: 168.7397s\n",
      "Epoch: 1 cost time: 10.169834852218628\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3398425 Vali Loss: 0.0795060 Test Loss: 0.2115769\n",
      "Validation loss decreased (inf --> 0.079506).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4421618\n",
      "\tspeed: 0.0495s/iter; left time: 531.5759s\n",
      "\titers: 200, epoch: 2 | loss: 0.1749569\n",
      "\tspeed: 0.0155s/iter; left time: 165.2398s\n",
      "\titers: 300, epoch: 2 | loss: 0.2967185\n",
      "\tspeed: 0.0155s/iter; left time: 163.3051s\n",
      "\titers: 400, epoch: 2 | loss: 0.2667936\n",
      "\tspeed: 0.0155s/iter; left time: 161.7975s\n",
      "\titers: 500, epoch: 2 | loss: 0.3372166\n",
      "\tspeed: 0.0155s/iter; left time: 159.8435s\n",
      "Epoch: 2 cost time: 9.141471862792969\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2999388 Vali Loss: 0.0653191 Test Loss: 0.1817264\n",
      "Validation loss decreased (0.079506 --> 0.065319).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2608772\n",
      "\tspeed: 0.0494s/iter; left time: 501.5137s\n",
      "\titers: 200, epoch: 3 | loss: 0.1706884\n",
      "\tspeed: 0.0155s/iter; left time: 155.9068s\n",
      "\titers: 300, epoch: 3 | loss: 0.3815733\n",
      "\tspeed: 0.0155s/iter; left time: 154.0087s\n",
      "\titers: 400, epoch: 3 | loss: 0.2664266\n",
      "\tspeed: 0.0155s/iter; left time: 152.6269s\n",
      "\titers: 500, epoch: 3 | loss: 0.2473663\n",
      "\tspeed: 0.0155s/iter; left time: 150.8636s\n",
      "Epoch: 3 cost time: 9.108070373535156\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2685149 Vali Loss: 0.0612395 Test Loss: 0.1667740\n",
      "Validation loss decreased (0.065319 --> 0.061240).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2239064\n",
      "\tspeed: 0.0476s/iter; left time: 456.5434s\n",
      "\titers: 200, epoch: 4 | loss: 0.2824219\n",
      "\tspeed: 0.0155s/iter; left time: 147.4512s\n",
      "\titers: 300, epoch: 4 | loss: 0.3538493\n",
      "\tspeed: 0.0155s/iter; left time: 145.9167s\n",
      "\titers: 400, epoch: 4 | loss: 0.2288669\n",
      "\tspeed: 0.0162s/iter; left time: 150.1215s\n",
      "\titers: 500, epoch: 4 | loss: 0.3610906\n",
      "\tspeed: 0.0178s/iter; left time: 164.0065s\n",
      "Epoch: 4 cost time: 9.600301027297974\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2505508 Vali Loss: 0.0605412 Test Loss: 0.1656972\n",
      "Validation loss decreased (0.061240 --> 0.060541).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2240075\n",
      "\tspeed: 0.0528s/iter; left time: 476.6724s\n",
      "\titers: 200, epoch: 5 | loss: 0.2118904\n",
      "\tspeed: 0.0178s/iter; left time: 158.5509s\n",
      "\titers: 300, epoch: 5 | loss: 0.2205756\n",
      "\tspeed: 0.0178s/iter; left time: 156.5945s\n",
      "\titers: 400, epoch: 5 | loss: 0.2091028\n",
      "\tspeed: 0.0178s/iter; left time: 154.8849s\n",
      "\titers: 500, epoch: 5 | loss: 0.1949403\n",
      "\tspeed: 0.0177s/iter; left time: 152.9924s\n",
      "Epoch: 5 cost time: 10.436367750167847\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2422824 Vali Loss: 0.0583978 Test Loss: 0.1641003\n",
      "Validation loss decreased (0.060541 --> 0.058398).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2339019\n",
      "\tspeed: 0.0522s/iter; left time: 441.2825s\n",
      "\titers: 200, epoch: 6 | loss: 0.1755083\n",
      "\tspeed: 0.0178s/iter; left time: 148.4457s\n",
      "\titers: 300, epoch: 6 | loss: 0.1500492\n",
      "\tspeed: 0.0178s/iter; left time: 146.7308s\n",
      "\titers: 400, epoch: 6 | loss: 0.1486233\n",
      "\tspeed: 0.0178s/iter; left time: 144.9127s\n",
      "\titers: 500, epoch: 6 | loss: 0.2213362\n",
      "\tspeed: 0.0178s/iter; left time: 143.2092s\n",
      "Epoch: 6 cost time: 10.446432828903198\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2372961 Vali Loss: 0.0585612 Test Loss: 0.1626155\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3091866\n",
      "\tspeed: 0.0504s/iter; left time: 397.4824s\n",
      "\titers: 200, epoch: 7 | loss: 0.2188091\n",
      "\tspeed: 0.0167s/iter; left time: 130.1581s\n",
      "\titers: 300, epoch: 7 | loss: 0.1809100\n",
      "\tspeed: 0.0154s/iter; left time: 118.5125s\n",
      "\titers: 400, epoch: 7 | loss: 0.2696238\n",
      "\tspeed: 0.0154s/iter; left time: 116.9739s\n",
      "\titers: 500, epoch: 7 | loss: 0.3083701\n",
      "\tspeed: 0.0154s/iter; left time: 115.4205s\n",
      "Epoch: 7 cost time: 9.36635446548462\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2330361 Vali Loss: 0.0587148 Test Loss: 0.1625889\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2236156\n",
      "\tspeed: 0.0498s/iter; left time: 363.7860s\n",
      "\titers: 200, epoch: 8 | loss: 0.2934226\n",
      "\tspeed: 0.0177s/iter; left time: 127.9749s\n",
      "\titers: 300, epoch: 8 | loss: 0.1606408\n",
      "\tspeed: 0.0177s/iter; left time: 126.1457s\n",
      "\titers: 400, epoch: 8 | loss: 0.2432509\n",
      "\tspeed: 0.0177s/iter; left time: 124.3595s\n",
      "\titers: 500, epoch: 8 | loss: 0.1970395\n",
      "\tspeed: 0.0177s/iter; left time: 122.5923s\n",
      "Epoch: 8 cost time: 10.153965711593628\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2315208 Vali Loss: 0.0578444 Test Loss: 0.1620613\n",
      "Validation loss decreased (0.058398 --> 0.057844).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1934782\n",
      "\tspeed: 0.0522s/iter; left time: 351.8914s\n",
      "\titers: 200, epoch: 9 | loss: 0.2204364\n",
      "\tspeed: 0.0178s/iter; left time: 118.2163s\n",
      "\titers: 300, epoch: 9 | loss: 0.2244284\n",
      "\tspeed: 0.0178s/iter; left time: 116.2774s\n",
      "\titers: 400, epoch: 9 | loss: 0.1628443\n",
      "\tspeed: 0.0178s/iter; left time: 114.5288s\n",
      "\titers: 500, epoch: 9 | loss: 0.2247437\n",
      "\tspeed: 0.0178s/iter; left time: 112.7513s\n",
      "Epoch: 9 cost time: 10.43529987335205\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2303209 Vali Loss: 0.0581975 Test Loss: 0.1621438\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2562991\n",
      "\tspeed: 0.0512s/iter; left time: 315.7515s\n",
      "\titers: 200, epoch: 10 | loss: 0.2500834\n",
      "\tspeed: 0.0178s/iter; left time: 108.3417s\n",
      "\titers: 300, epoch: 10 | loss: 0.2114974\n",
      "\tspeed: 0.0178s/iter; left time: 106.5411s\n",
      "\titers: 400, epoch: 10 | loss: 0.2712291\n",
      "\tspeed: 0.0178s/iter; left time: 104.6796s\n",
      "\titers: 500, epoch: 10 | loss: 0.3080527\n",
      "\tspeed: 0.0178s/iter; left time: 102.9163s\n",
      "Epoch: 10 cost time: 10.456926107406616\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2312544 Vali Loss: 0.0581431 Test Loss: 0.1621729\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2140190\n",
      "\tspeed: 0.0511s/iter; left time: 286.0084s\n",
      "\titers: 200, epoch: 11 | loss: 0.1459406\n",
      "\tspeed: 0.0177s/iter; left time: 97.4713s\n",
      "\titers: 300, epoch: 11 | loss: 0.2325401\n",
      "\tspeed: 0.0177s/iter; left time: 95.6786s\n",
      "\titers: 400, epoch: 11 | loss: 0.2320054\n",
      "\tspeed: 0.0177s/iter; left time: 93.9277s\n",
      "\titers: 500, epoch: 11 | loss: 0.3245713\n",
      "\tspeed: 0.0177s/iter; left time: 92.1598s\n",
      "Epoch: 11 cost time: 10.39211368560791\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2307453 Vali Loss: 0.0582252 Test Loss: 0.1622383\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16231337189674377, mae:0.25703009963035583\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1897985\n",
      "\tspeed: 0.0328s/iter; left time: 371.1689s\n",
      "\titers: 200, epoch: 1 | loss: 0.2887057\n",
      "\tspeed: 0.0209s/iter; left time: 234.1929s\n",
      "\titers: 300, epoch: 1 | loss: 0.4413811\n",
      "\tspeed: 0.0209s/iter; left time: 232.5541s\n",
      "\titers: 400, epoch: 1 | loss: 0.2888007\n",
      "\tspeed: 0.0210s/iter; left time: 231.0601s\n",
      "\titers: 500, epoch: 1 | loss: 0.2090833\n",
      "\tspeed: 0.0210s/iter; left time: 228.5008s\n",
      "Epoch: 1 cost time: 13.18981385231018\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2503787 Vali Loss: 0.0560616 Test Loss: 0.1611836\n",
      "Validation loss decreased (inf --> 0.056062).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3634200\n",
      "\tspeed: 0.0581s/iter; left time: 623.8268s\n",
      "\titers: 200, epoch: 2 | loss: 0.2976031\n",
      "\tspeed: 0.0187s/iter; left time: 198.4495s\n",
      "\titers: 300, epoch: 2 | loss: 0.2022951\n",
      "\tspeed: 0.0187s/iter; left time: 196.5146s\n",
      "\titers: 400, epoch: 2 | loss: 0.2796339\n",
      "\tspeed: 0.0187s/iter; left time: 194.6377s\n",
      "\titers: 500, epoch: 2 | loss: 0.2375820\n",
      "\tspeed: 0.0187s/iter; left time: 192.7336s\n",
      "Epoch: 2 cost time: 10.943004608154297\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2878703 Vali Loss: 0.0673999 Test Loss: 0.1945417\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1964109\n",
      "\tspeed: 0.0572s/iter; left time: 581.6766s\n",
      "\titers: 200, epoch: 3 | loss: 0.2394517\n",
      "\tspeed: 0.0188s/iter; left time: 189.0653s\n",
      "\titers: 300, epoch: 3 | loss: 0.2163593\n",
      "\tspeed: 0.0188s/iter; left time: 187.0157s\n",
      "\titers: 400, epoch: 3 | loss: 0.1630104\n",
      "\tspeed: 0.0188s/iter; left time: 185.0011s\n",
      "\titers: 500, epoch: 3 | loss: 0.2052260\n",
      "\tspeed: 0.0188s/iter; left time: 183.3919s\n",
      "Epoch: 3 cost time: 11.016671895980835\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2572694 Vali Loss: 0.0582203 Test Loss: 0.1707456\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2638965\n",
      "\tspeed: 0.0558s/iter; left time: 535.3289s\n",
      "\titers: 200, epoch: 4 | loss: 0.2769852\n",
      "\tspeed: 0.0188s/iter; left time: 178.7631s\n",
      "\titers: 300, epoch: 4 | loss: 0.2741510\n",
      "\tspeed: 0.0188s/iter; left time: 176.8290s\n",
      "\titers: 400, epoch: 4 | loss: 0.1484838\n",
      "\tspeed: 0.0188s/iter; left time: 174.8902s\n",
      "\titers: 500, epoch: 4 | loss: 0.2438200\n",
      "\tspeed: 0.0189s/iter; left time: 173.2674s\n",
      "Epoch: 4 cost time: 11.037014722824097\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2353540 Vali Loss: 0.0568876 Test Loss: 0.1671759\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16143308579921722, mae:0.261800080537796\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4577753\n",
      "\tspeed: 0.0324s/iter; left time: 366.3505s\n",
      "\titers: 200, epoch: 1 | loss: 0.1889921\n",
      "\tspeed: 0.0205s/iter; left time: 229.1178s\n",
      "\titers: 300, epoch: 1 | loss: 0.1929015\n",
      "\tspeed: 0.0204s/iter; left time: 226.9439s\n",
      "\titers: 400, epoch: 1 | loss: 0.1672915\n",
      "\tspeed: 0.0204s/iter; left time: 224.6368s\n",
      "\titers: 500, epoch: 1 | loss: 0.3148077\n",
      "\tspeed: 0.0205s/iter; left time: 223.6794s\n",
      "Epoch: 1 cost time: 12.912228107452393\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2371157 Vali Loss: 0.0581155 Test Loss: 0.1931640\n",
      "Validation loss decreased (inf --> 0.058115).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1904044\n",
      "\tspeed: 0.0598s/iter; left time: 641.4927s\n",
      "\titers: 200, epoch: 2 | loss: 0.3319464\n",
      "\tspeed: 0.0205s/iter; left time: 217.7749s\n",
      "\titers: 300, epoch: 2 | loss: 0.5493437\n",
      "\tspeed: 0.0205s/iter; left time: 215.7289s\n",
      "\titers: 400, epoch: 2 | loss: 0.2594875\n",
      "\tspeed: 0.0205s/iter; left time: 213.7115s\n",
      "\titers: 500, epoch: 2 | loss: 0.1825258\n",
      "\tspeed: 0.0205s/iter; left time: 211.8898s\n",
      "Epoch: 2 cost time: 11.998517990112305\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2861869 Vali Loss: 0.0610071 Test Loss: 0.1707035\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1784080\n",
      "\tspeed: 0.0601s/iter; left time: 610.9059s\n",
      "\titers: 200, epoch: 3 | loss: 0.1907181\n",
      "\tspeed: 0.0211s/iter; left time: 212.1749s\n",
      "\titers: 300, epoch: 3 | loss: 0.2150978\n",
      "\tspeed: 0.0211s/iter; left time: 209.7690s\n",
      "\titers: 400, epoch: 3 | loss: 0.1987578\n",
      "\tspeed: 0.0211s/iter; left time: 207.5847s\n",
      "\titers: 500, epoch: 3 | loss: 0.1934327\n",
      "\tspeed: 0.0211s/iter; left time: 205.5198s\n",
      "Epoch: 3 cost time: 12.324219703674316\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2373628 Vali Loss: 0.0554212 Test Loss: 0.1654473\n",
      "Validation loss decreased (0.058115 --> 0.055421).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2528630\n",
      "\tspeed: 0.0614s/iter; left time: 588.9901s\n",
      "\titers: 200, epoch: 4 | loss: 0.1235906\n",
      "\tspeed: 0.0210s/iter; left time: 199.4112s\n",
      "\titers: 300, epoch: 4 | loss: 0.2539161\n",
      "\tspeed: 0.0210s/iter; left time: 197.3897s\n",
      "\titers: 400, epoch: 4 | loss: 0.1838807\n",
      "\tspeed: 0.0210s/iter; left time: 195.4634s\n",
      "\titers: 500, epoch: 4 | loss: 0.1295810\n",
      "\tspeed: 0.0210s/iter; left time: 193.0568s\n",
      "Epoch: 4 cost time: 12.312519073486328\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2169620 Vali Loss: 0.0554162 Test Loss: 0.1523077\n",
      "Validation loss decreased (0.055421 --> 0.055416).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1547552\n",
      "\tspeed: 0.0612s/iter; left time: 552.2792s\n",
      "\titers: 200, epoch: 5 | loss: 0.2026396\n",
      "\tspeed: 0.0210s/iter; left time: 187.6662s\n",
      "\titers: 300, epoch: 5 | loss: 0.1919032\n",
      "\tspeed: 0.0210s/iter; left time: 185.2839s\n",
      "\titers: 400, epoch: 5 | loss: 0.2043490\n",
      "\tspeed: 0.0210s/iter; left time: 182.7867s\n",
      "\titers: 500, epoch: 5 | loss: 0.1990732\n",
      "\tspeed: 0.0210s/iter; left time: 180.7783s\n",
      "Epoch: 5 cost time: 12.266993761062622\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2062961 Vali Loss: 0.0577610 Test Loss: 0.1532470\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2716459\n",
      "\tspeed: 0.0598s/iter; left time: 505.2795s\n",
      "\titers: 200, epoch: 6 | loss: 0.1615687\n",
      "\tspeed: 0.0210s/iter; left time: 175.0961s\n",
      "\titers: 300, epoch: 6 | loss: 0.2168526\n",
      "\tspeed: 0.0210s/iter; left time: 173.0481s\n",
      "\titers: 400, epoch: 6 | loss: 0.1593640\n",
      "\tspeed: 0.0209s/iter; left time: 170.7335s\n",
      "\titers: 500, epoch: 6 | loss: 0.1377405\n",
      "\tspeed: 0.0209s/iter; left time: 168.5486s\n",
      "Epoch: 6 cost time: 12.224653720855713\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2025473 Vali Loss: 0.0542774 Test Loss: 0.1501711\n",
      "Validation loss decreased (0.055416 --> 0.054277).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1793858\n",
      "\tspeed: 0.0609s/iter; left time: 480.0012s\n",
      "\titers: 200, epoch: 7 | loss: 0.3043558\n",
      "\tspeed: 0.0210s/iter; left time: 163.7271s\n",
      "\titers: 300, epoch: 7 | loss: 0.1265976\n",
      "\tspeed: 0.0210s/iter; left time: 161.2509s\n",
      "\titers: 400, epoch: 7 | loss: 0.2142242\n",
      "\tspeed: 0.0210s/iter; left time: 159.0914s\n",
      "\titers: 500, epoch: 7 | loss: 0.1831573\n",
      "\tspeed: 0.0210s/iter; left time: 157.0073s\n",
      "Epoch: 7 cost time: 12.294317722320557\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1984276 Vali Loss: 0.0547133 Test Loss: 0.1501346\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1364244\n",
      "\tspeed: 0.0593s/iter; left time: 433.4725s\n",
      "\titers: 200, epoch: 8 | loss: 0.1428776\n",
      "\tspeed: 0.0210s/iter; left time: 151.1998s\n",
      "\titers: 300, epoch: 8 | loss: 0.2285191\n",
      "\tspeed: 0.0210s/iter; left time: 149.1103s\n",
      "\titers: 400, epoch: 8 | loss: 0.2212646\n",
      "\tspeed: 0.0210s/iter; left time: 147.0149s\n",
      "\titers: 500, epoch: 8 | loss: 0.2518257\n",
      "\tspeed: 0.0210s/iter; left time: 144.9128s\n",
      "Epoch: 8 cost time: 12.23184323310852\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1964491 Vali Loss: 0.0545418 Test Loss: 0.1491979\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1176716\n",
      "\tspeed: 0.0608s/iter; left time: 409.7838s\n",
      "\titers: 200, epoch: 9 | loss: 0.2062732\n",
      "\tspeed: 0.0210s/iter; left time: 139.2764s\n",
      "\titers: 300, epoch: 9 | loss: 0.2390584\n",
      "\tspeed: 0.0210s/iter; left time: 137.0546s\n",
      "\titers: 400, epoch: 9 | loss: 0.1789776\n",
      "\tspeed: 0.0210s/iter; left time: 135.0752s\n",
      "\titers: 500, epoch: 9 | loss: 0.1672723\n",
      "\tspeed: 0.0209s/iter; left time: 132.6759s\n",
      "Epoch: 9 cost time: 12.24669337272644\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1971224 Vali Loss: 0.0542637 Test Loss: 0.1488945\n",
      "Validation loss decreased (0.054277 --> 0.054264).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1279312\n",
      "\tspeed: 0.0647s/iter; left time: 398.9706s\n",
      "\titers: 200, epoch: 10 | loss: 0.1533802\n",
      "\tspeed: 0.0211s/iter; left time: 128.1492s\n",
      "\titers: 300, epoch: 10 | loss: 0.2471815\n",
      "\tspeed: 0.0211s/iter; left time: 125.7132s\n",
      "\titers: 400, epoch: 10 | loss: 0.2247956\n",
      "\tspeed: 0.0211s/iter; left time: 123.8059s\n",
      "\titers: 500, epoch: 10 | loss: 0.2233932\n",
      "\tspeed: 0.0193s/iter; left time: 111.2872s\n",
      "Epoch: 10 cost time: 12.051947116851807\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1957112 Vali Loss: 0.0542442 Test Loss: 0.1490749\n",
      "Validation loss decreased (0.054264 --> 0.054244).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2585023\n",
      "\tspeed: 0.0593s/iter; left time: 332.0056s\n",
      "\titers: 200, epoch: 11 | loss: 0.2559385\n",
      "\tspeed: 0.0210s/iter; left time: 115.5468s\n",
      "\titers: 300, epoch: 11 | loss: 0.1487543\n",
      "\tspeed: 0.0210s/iter; left time: 113.5195s\n",
      "\titers: 400, epoch: 11 | loss: 0.2045055\n",
      "\tspeed: 0.0210s/iter; left time: 111.2546s\n",
      "\titers: 500, epoch: 11 | loss: 0.1912563\n",
      "\tspeed: 0.0210s/iter; left time: 109.0162s\n",
      "Epoch: 11 cost time: 12.263322114944458\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1963690 Vali Loss: 0.0544495 Test Loss: 0.1492017\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2479386\n",
      "\tspeed: 0.0594s/iter; left time: 298.7573s\n",
      "\titers: 200, epoch: 12 | loss: 0.2418561\n",
      "\tspeed: 0.0209s/iter; left time: 103.2362s\n",
      "\titers: 300, epoch: 12 | loss: 0.1374248\n",
      "\tspeed: 0.0209s/iter; left time: 101.1149s\n",
      "\titers: 400, epoch: 12 | loss: 0.1622343\n",
      "\tspeed: 0.0209s/iter; left time: 98.9449s\n",
      "\titers: 500, epoch: 12 | loss: 0.1916432\n",
      "\tspeed: 0.0209s/iter; left time: 96.8776s\n",
      "Epoch: 12 cost time: 12.218897819519043\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1951744 Vali Loss: 0.0542789 Test Loss: 0.1491429\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1677894\n",
      "\tspeed: 0.0618s/iter; left time: 275.6192s\n",
      "\titers: 200, epoch: 13 | loss: 0.2673208\n",
      "\tspeed: 0.0210s/iter; left time: 91.6471s\n",
      "\titers: 300, epoch: 13 | loss: 0.1751967\n",
      "\tspeed: 0.0210s/iter; left time: 89.4249s\n",
      "\titers: 400, epoch: 13 | loss: 0.2055026\n",
      "\tspeed: 0.0210s/iter; left time: 87.4141s\n",
      "\titers: 500, epoch: 13 | loss: 0.2717972\n",
      "\tspeed: 0.0210s/iter; left time: 85.3076s\n",
      "Epoch: 13 cost time: 12.30660343170166\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1961132 Vali Loss: 0.0543760 Test Loss: 0.1491305\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14925047755241394, mae:0.24429486691951752\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3445646\n",
      "\tspeed: 0.0327s/iter; left time: 370.0454s\n",
      "\titers: 200, epoch: 1 | loss: 0.1877764\n",
      "\tspeed: 0.0206s/iter; left time: 230.7582s\n",
      "\titers: 300, epoch: 1 | loss: 0.3729849\n",
      "\tspeed: 0.0206s/iter; left time: 228.5416s\n",
      "\titers: 400, epoch: 1 | loss: 0.3171844\n",
      "\tspeed: 0.0206s/iter; left time: 226.2911s\n",
      "\titers: 500, epoch: 1 | loss: 0.3730814\n",
      "\tspeed: 0.0206s/iter; left time: 224.9581s\n",
      "Epoch: 1 cost time: 13.010758876800537\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2813609 Vali Loss: 0.0633417 Test Loss: 0.1786923\n",
      "Validation loss decreased (inf --> 0.063342).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3185824\n",
      "\tspeed: 0.0596s/iter; left time: 639.2799s\n",
      "\titers: 200, epoch: 2 | loss: 0.3732510\n",
      "\tspeed: 0.0184s/iter; left time: 195.4386s\n",
      "\titers: 300, epoch: 2 | loss: 0.4587632\n",
      "\tspeed: 0.0184s/iter; left time: 193.4643s\n",
      "\titers: 400, epoch: 2 | loss: 0.2056504\n",
      "\tspeed: 0.0183s/iter; left time: 191.3464s\n",
      "\titers: 500, epoch: 2 | loss: 0.1887688\n",
      "\tspeed: 0.0184s/iter; left time: 189.9993s\n",
      "Epoch: 2 cost time: 10.775303602218628\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2697050 Vali Loss: 0.0622225 Test Loss: 0.1683851\n",
      "Validation loss decreased (0.063342 --> 0.062223).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1565359\n",
      "\tspeed: 0.0563s/iter; left time: 572.5641s\n",
      "\titers: 200, epoch: 3 | loss: 0.2706599\n",
      "\tspeed: 0.0183s/iter; left time: 184.2294s\n",
      "\titers: 300, epoch: 3 | loss: 0.2815258\n",
      "\tspeed: 0.0183s/iter; left time: 182.6117s\n",
      "\titers: 400, epoch: 3 | loss: 0.3146517\n",
      "\tspeed: 0.0183s/iter; left time: 180.5477s\n",
      "\titers: 500, epoch: 3 | loss: 0.2551386\n",
      "\tspeed: 0.0183s/iter; left time: 178.7122s\n",
      "Epoch: 3 cost time: 10.730594873428345\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2377717 Vali Loss: 0.0598904 Test Loss: 0.1628170\n",
      "Validation loss decreased (0.062223 --> 0.059890).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2135967\n",
      "\tspeed: 0.0556s/iter; left time: 533.3106s\n",
      "\titers: 200, epoch: 4 | loss: 0.2030511\n",
      "\tspeed: 0.0212s/iter; left time: 201.2583s\n",
      "\titers: 300, epoch: 4 | loss: 0.1808579\n",
      "\tspeed: 0.0212s/iter; left time: 198.9006s\n",
      "\titers: 400, epoch: 4 | loss: 0.2235925\n",
      "\tspeed: 0.0212s/iter; left time: 196.8083s\n",
      "\titers: 500, epoch: 4 | loss: 0.1414864\n",
      "\tspeed: 0.0212s/iter; left time: 194.4926s\n",
      "Epoch: 4 cost time: 12.093401670455933\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2207926 Vali Loss: 0.0544014 Test Loss: 0.1543365\n",
      "Validation loss decreased (0.059890 --> 0.054401).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1340903\n",
      "\tspeed: 0.0614s/iter; left time: 553.8832s\n",
      "\titers: 200, epoch: 5 | loss: 0.2077597\n",
      "\tspeed: 0.0212s/iter; left time: 188.7091s\n",
      "\titers: 300, epoch: 5 | loss: 0.1357080\n",
      "\tspeed: 0.0211s/iter; left time: 186.1102s\n",
      "\titers: 400, epoch: 5 | loss: 0.1882963\n",
      "\tspeed: 0.0211s/iter; left time: 183.9317s\n",
      "\titers: 500, epoch: 5 | loss: 0.1163111\n",
      "\tspeed: 0.0211s/iter; left time: 182.2118s\n",
      "Epoch: 5 cost time: 12.378915309906006\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2083121 Vali Loss: 0.0520057 Test Loss: 0.1467012\n",
      "Validation loss decreased (0.054401 --> 0.052006).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2263478\n",
      "\tspeed: 0.0610s/iter; left time: 515.2119s\n",
      "\titers: 200, epoch: 6 | loss: 0.1294562\n",
      "\tspeed: 0.0196s/iter; left time: 163.2677s\n",
      "\titers: 300, epoch: 6 | loss: 0.4127450\n",
      "\tspeed: 0.0184s/iter; left time: 151.4877s\n",
      "\titers: 400, epoch: 6 | loss: 0.1523789\n",
      "\tspeed: 0.0184s/iter; left time: 149.7022s\n",
      "\titers: 500, epoch: 6 | loss: 0.2137214\n",
      "\tspeed: 0.0183s/iter; left time: 147.6999s\n",
      "Epoch: 6 cost time: 11.169806718826294\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1999501 Vali Loss: 0.0507813 Test Loss: 0.1440697\n",
      "Validation loss decreased (0.052006 --> 0.050781).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1522244\n",
      "\tspeed: 0.0593s/iter; left time: 467.1045s\n",
      "\titers: 200, epoch: 7 | loss: 0.1638225\n",
      "\tspeed: 0.0211s/iter; left time: 164.3731s\n",
      "\titers: 300, epoch: 7 | loss: 0.2337635\n",
      "\tspeed: 0.0211s/iter; left time: 162.0424s\n",
      "\titers: 400, epoch: 7 | loss: 0.1708054\n",
      "\tspeed: 0.0211s/iter; left time: 160.3249s\n",
      "\titers: 500, epoch: 7 | loss: 0.1548209\n",
      "\tspeed: 0.0211s/iter; left time: 157.9601s\n",
      "Epoch: 7 cost time: 12.357621669769287\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1966750 Vali Loss: 0.0515746 Test Loss: 0.1437420\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1198446\n",
      "\tspeed: 0.0617s/iter; left time: 450.8157s\n",
      "\titers: 200, epoch: 8 | loss: 0.1399414\n",
      "\tspeed: 0.0211s/iter; left time: 151.9004s\n",
      "\titers: 300, epoch: 8 | loss: 0.1922701\n",
      "\tspeed: 0.0192s/iter; left time: 136.5666s\n",
      "\titers: 400, epoch: 8 | loss: 0.2479041\n",
      "\tspeed: 0.0183s/iter; left time: 128.1241s\n",
      "\titers: 500, epoch: 8 | loss: 0.1562815\n",
      "\tspeed: 0.0183s/iter; left time: 126.5396s\n",
      "Epoch: 8 cost time: 11.400978326797485\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1939699 Vali Loss: 0.0508912 Test Loss: 0.1437241\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2116522\n",
      "\tspeed: 0.0577s/iter; left time: 389.0061s\n",
      "\titers: 200, epoch: 9 | loss: 0.2552299\n",
      "\tspeed: 0.0183s/iter; left time: 121.3209s\n",
      "\titers: 300, epoch: 9 | loss: 0.2623025\n",
      "\tspeed: 0.0183s/iter; left time: 119.5833s\n",
      "\titers: 400, epoch: 9 | loss: 0.3542818\n",
      "\tspeed: 0.0182s/iter; left time: 117.5277s\n",
      "\titers: 500, epoch: 9 | loss: 0.1560402\n",
      "\tspeed: 0.0190s/iter; left time: 120.3370s\n",
      "Epoch: 9 cost time: 10.987032890319824\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1923798 Vali Loss: 0.0505789 Test Loss: 0.1431689\n",
      "Validation loss decreased (0.050781 --> 0.050579).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1748650\n",
      "\tspeed: 0.0604s/iter; left time: 372.9998s\n",
      "\titers: 200, epoch: 10 | loss: 0.1834387\n",
      "\tspeed: 0.0212s/iter; left time: 128.8555s\n",
      "\titers: 300, epoch: 10 | loss: 0.2298550\n",
      "\tspeed: 0.0212s/iter; left time: 126.6627s\n",
      "\titers: 400, epoch: 10 | loss: 0.2008950\n",
      "\tspeed: 0.0212s/iter; left time: 124.5598s\n",
      "\titers: 500, epoch: 10 | loss: 0.2051430\n",
      "\tspeed: 0.0212s/iter; left time: 122.6309s\n",
      "Epoch: 10 cost time: 12.411158084869385\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1923975 Vali Loss: 0.0504339 Test Loss: 0.1432166\n",
      "Validation loss decreased (0.050579 --> 0.050434).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1846491\n",
      "\tspeed: 0.0631s/iter; left time: 353.2344s\n",
      "\titers: 200, epoch: 11 | loss: 0.2004280\n",
      "\tspeed: 0.0206s/iter; left time: 113.3789s\n",
      "\titers: 300, epoch: 11 | loss: 0.1794293\n",
      "\tspeed: 0.0206s/iter; left time: 111.3012s\n",
      "\titers: 400, epoch: 11 | loss: 0.1951572\n",
      "\tspeed: 0.0207s/iter; left time: 109.5230s\n",
      "\titers: 500, epoch: 11 | loss: 0.1286800\n",
      "\tspeed: 0.0207s/iter; left time: 107.5376s\n",
      "Epoch: 11 cost time: 11.933206796646118\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1942597 Vali Loss: 0.0503781 Test Loss: 0.1431744\n",
      "Validation loss decreased (0.050434 --> 0.050378).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1566080\n",
      "\tspeed: 0.0574s/iter; left time: 288.7348s\n",
      "\titers: 200, epoch: 12 | loss: 0.2663315\n",
      "\tspeed: 0.0183s/iter; left time: 90.1493s\n",
      "\titers: 300, epoch: 12 | loss: 0.1894167\n",
      "\tspeed: 0.0183s/iter; left time: 88.6208s\n",
      "\titers: 400, epoch: 12 | loss: 0.1221318\n",
      "\tspeed: 0.0212s/iter; left time: 100.2801s\n",
      "\titers: 500, epoch: 12 | loss: 0.2709911\n",
      "\tspeed: 0.0212s/iter; left time: 98.2372s\n",
      "Epoch: 12 cost time: 11.524925947189331\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1917216 Vali Loss: 0.0504367 Test Loss: 0.1431719\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1706440\n",
      "\tspeed: 0.0599s/iter; left time: 267.1146s\n",
      "\titers: 200, epoch: 13 | loss: 0.1085223\n",
      "\tspeed: 0.0211s/iter; left time: 92.1074s\n",
      "\titers: 300, epoch: 13 | loss: 0.1748090\n",
      "\tspeed: 0.0211s/iter; left time: 89.9501s\n",
      "\titers: 400, epoch: 13 | loss: 0.1320335\n",
      "\tspeed: 0.0211s/iter; left time: 87.9002s\n",
      "\titers: 500, epoch: 13 | loss: 0.1230229\n",
      "\tspeed: 0.0212s/iter; left time: 85.9271s\n",
      "Epoch: 13 cost time: 12.327481508255005\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1911351 Vali Loss: 0.0506621 Test Loss: 0.1431873\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1293672\n",
      "\tspeed: 0.0608s/iter; left time: 236.5227s\n",
      "\titers: 200, epoch: 14 | loss: 0.1320582\n",
      "\tspeed: 0.0212s/iter; left time: 80.3628s\n",
      "\titers: 300, epoch: 14 | loss: 0.1745252\n",
      "\tspeed: 0.0212s/iter; left time: 78.2111s\n",
      "\titers: 400, epoch: 14 | loss: 0.1491092\n",
      "\tspeed: 0.0212s/iter; left time: 76.1790s\n",
      "\titers: 500, epoch: 14 | loss: 0.1423359\n",
      "\tspeed: 0.0212s/iter; left time: 74.0141s\n",
      "Epoch: 14 cost time: 12.367737293243408\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1921796 Vali Loss: 0.0504895 Test Loss: 0.1431945\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14335070550441742, mae:0.24025031924247742\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2392795\n",
      "\tspeed: 0.0342s/iter; left time: 385.9590s\n",
      "\titers: 200, epoch: 1 | loss: 0.3235706\n",
      "\tspeed: 0.0220s/iter; left time: 246.4293s\n",
      "\titers: 300, epoch: 1 | loss: 0.2933561\n",
      "\tspeed: 0.0220s/iter; left time: 244.1635s\n",
      "\titers: 400, epoch: 1 | loss: 0.4406134\n",
      "\tspeed: 0.0220s/iter; left time: 241.7903s\n",
      "\titers: 500, epoch: 1 | loss: 0.4308271\n",
      "\tspeed: 0.0220s/iter; left time: 239.9149s\n",
      "Epoch: 1 cost time: 13.80452275276184\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3484697 Vali Loss: 0.0637454 Test Loss: 0.1881898\n",
      "Validation loss decreased (inf --> 0.063745).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3287198\n",
      "\tspeed: 0.0654s/iter; left time: 701.9004s\n",
      "\titers: 200, epoch: 2 | loss: 0.2529510\n",
      "\tspeed: 0.0222s/iter; left time: 235.4774s\n",
      "\titers: 300, epoch: 2 | loss: 0.2642837\n",
      "\tspeed: 0.0221s/iter; left time: 232.5161s\n",
      "\titers: 400, epoch: 2 | loss: 0.2760209\n",
      "\tspeed: 0.0221s/iter; left time: 230.2935s\n",
      "\titers: 500, epoch: 2 | loss: 0.2563839\n",
      "\tspeed: 0.0221s/iter; left time: 228.0722s\n",
      "Epoch: 2 cost time: 12.935076713562012\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2911943 Vali Loss: 0.0618177 Test Loss: 0.1784082\n",
      "Validation loss decreased (0.063745 --> 0.061818).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.5450932\n",
      "\tspeed: 0.0656s/iter; left time: 666.6968s\n",
      "\titers: 200, epoch: 3 | loss: 0.2047932\n",
      "\tspeed: 0.0221s/iter; left time: 222.0679s\n",
      "\titers: 300, epoch: 3 | loss: 0.3956991\n",
      "\tspeed: 0.0221s/iter; left time: 220.0184s\n",
      "\titers: 400, epoch: 3 | loss: 0.2368403\n",
      "\tspeed: 0.0221s/iter; left time: 217.7204s\n",
      "\titers: 500, epoch: 3 | loss: 0.2099386\n",
      "\tspeed: 0.0221s/iter; left time: 215.5113s\n",
      "Epoch: 3 cost time: 12.90898585319519\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2846029 Vali Loss: 0.0579255 Test Loss: 0.1641819\n",
      "Validation loss decreased (0.061818 --> 0.057925).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2046410\n",
      "\tspeed: 0.0651s/iter; left time: 624.7047s\n",
      "\titers: 200, epoch: 4 | loss: 0.2222010\n",
      "\tspeed: 0.0220s/iter; left time: 209.2450s\n",
      "\titers: 300, epoch: 4 | loss: 0.2790974\n",
      "\tspeed: 0.0221s/iter; left time: 207.3320s\n",
      "\titers: 400, epoch: 4 | loss: 0.2107026\n",
      "\tspeed: 0.0221s/iter; left time: 204.9920s\n",
      "\titers: 500, epoch: 4 | loss: 0.3143312\n",
      "\tspeed: 0.0221s/iter; left time: 203.1836s\n",
      "Epoch: 4 cost time: 12.904078483581543\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2583972 Vali Loss: 0.0568851 Test Loss: 0.1662834\n",
      "Validation loss decreased (0.057925 --> 0.056885).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2783979\n",
      "\tspeed: 0.0648s/iter; left time: 584.1163s\n",
      "\titers: 200, epoch: 5 | loss: 0.2492633\n",
      "\tspeed: 0.0221s/iter; left time: 197.3167s\n",
      "\titers: 300, epoch: 5 | loss: 0.1835204\n",
      "\tspeed: 0.0221s/iter; left time: 194.6991s\n",
      "\titers: 400, epoch: 5 | loss: 0.2364521\n",
      "\tspeed: 0.0221s/iter; left time: 192.5366s\n",
      "\titers: 500, epoch: 5 | loss: 0.2786642\n",
      "\tspeed: 0.0221s/iter; left time: 190.3170s\n",
      "Epoch: 5 cost time: 12.921745777130127\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2471538 Vali Loss: 0.0547942 Test Loss: 0.1584070\n",
      "Validation loss decreased (0.056885 --> 0.054794).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2043900\n",
      "\tspeed: 0.0633s/iter; left time: 534.9719s\n",
      "\titers: 200, epoch: 6 | loss: 0.3686359\n",
      "\tspeed: 0.0221s/iter; left time: 184.5360s\n",
      "\titers: 300, epoch: 6 | loss: 0.2122341\n",
      "\tspeed: 0.0221s/iter; left time: 182.2477s\n",
      "\titers: 400, epoch: 6 | loss: 0.1944636\n",
      "\tspeed: 0.0221s/iter; left time: 180.0200s\n",
      "\titers: 500, epoch: 6 | loss: 0.3643658\n",
      "\tspeed: 0.0221s/iter; left time: 177.8983s\n",
      "Epoch: 6 cost time: 12.916518688201904\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2404939 Vali Loss: 0.0540997 Test Loss: 0.1588474\n",
      "Validation loss decreased (0.054794 --> 0.054100).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2972192\n",
      "\tspeed: 0.0637s/iter; left time: 501.9093s\n",
      "\titers: 200, epoch: 7 | loss: 0.1989725\n",
      "\tspeed: 0.0221s/iter; left time: 172.1979s\n",
      "\titers: 300, epoch: 7 | loss: 0.1605517\n",
      "\tspeed: 0.0221s/iter; left time: 170.0532s\n",
      "\titers: 400, epoch: 7 | loss: 0.1990369\n",
      "\tspeed: 0.0221s/iter; left time: 167.6656s\n",
      "\titers: 500, epoch: 7 | loss: 0.1980347\n",
      "\tspeed: 0.0222s/iter; left time: 165.7722s\n",
      "Epoch: 7 cost time: 12.940999507904053\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2361313 Vali Loss: 0.0534070 Test Loss: 0.1574572\n",
      "Validation loss decreased (0.054100 --> 0.053407).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1609631\n",
      "\tspeed: 0.0637s/iter; left time: 465.8280s\n",
      "\titers: 200, epoch: 8 | loss: 0.6042265\n",
      "\tspeed: 0.0221s/iter; left time: 159.4811s\n",
      "\titers: 300, epoch: 8 | loss: 0.1642970\n",
      "\tspeed: 0.0221s/iter; left time: 157.3366s\n",
      "\titers: 400, epoch: 8 | loss: 0.2039292\n",
      "\tspeed: 0.0221s/iter; left time: 154.9695s\n",
      "\titers: 500, epoch: 8 | loss: 0.2035870\n",
      "\tspeed: 0.0221s/iter; left time: 152.6820s\n",
      "Epoch: 8 cost time: 12.941941499710083\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2348030 Vali Loss: 0.0534884 Test Loss: 0.1569922\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2348224\n",
      "\tspeed: 0.0618s/iter; left time: 416.7054s\n",
      "\titers: 200, epoch: 9 | loss: 0.1998831\n",
      "\tspeed: 0.0222s/iter; left time: 147.1069s\n",
      "\titers: 300, epoch: 9 | loss: 0.3595006\n",
      "\tspeed: 0.0221s/iter; left time: 144.6450s\n",
      "\titers: 400, epoch: 9 | loss: 0.1851474\n",
      "\tspeed: 0.0221s/iter; left time: 142.4405s\n",
      "\titers: 500, epoch: 9 | loss: 0.2093672\n",
      "\tspeed: 0.0222s/iter; left time: 140.4762s\n",
      "Epoch: 9 cost time: 12.896367073059082\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2342727 Vali Loss: 0.0532538 Test Loss: 0.1566123\n",
      "Validation loss decreased (0.053407 --> 0.053254).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2134637\n",
      "\tspeed: 0.0625s/iter; left time: 385.6281s\n",
      "\titers: 200, epoch: 10 | loss: 0.2708219\n",
      "\tspeed: 0.0221s/iter; left time: 133.8729s\n",
      "\titers: 300, epoch: 10 | loss: 0.2378317\n",
      "\tspeed: 0.0220s/iter; left time: 131.6573s\n",
      "\titers: 400, epoch: 10 | loss: 0.3833191\n",
      "\tspeed: 0.0221s/iter; left time: 129.6624s\n",
      "\titers: 500, epoch: 10 | loss: 0.2913111\n",
      "\tspeed: 0.0221s/iter; left time: 127.4175s\n",
      "Epoch: 10 cost time: 12.85918116569519\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2342428 Vali Loss: 0.0534238 Test Loss: 0.1564279\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1943801\n",
      "\tspeed: 0.0648s/iter; left time: 362.8753s\n",
      "\titers: 200, epoch: 11 | loss: 0.1655978\n",
      "\tspeed: 0.0222s/iter; left time: 121.8974s\n",
      "\titers: 300, epoch: 11 | loss: 0.1294974\n",
      "\tspeed: 0.0222s/iter; left time: 119.6503s\n",
      "\titers: 400, epoch: 11 | loss: 0.2129554\n",
      "\tspeed: 0.0221s/iter; left time: 117.3431s\n",
      "\titers: 500, epoch: 11 | loss: 0.1963433\n",
      "\tspeed: 0.0222s/iter; left time: 115.2600s\n",
      "Epoch: 11 cost time: 12.946214437484741\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2345260 Vali Loss: 0.0532203 Test Loss: 0.1563416\n",
      "Validation loss decreased (0.053254 --> 0.053220).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2545201\n",
      "\tspeed: 0.0634s/iter; left time: 318.7655s\n",
      "\titers: 200, epoch: 12 | loss: 0.1851709\n",
      "\tspeed: 0.0221s/iter; left time: 108.8287s\n",
      "\titers: 300, epoch: 12 | loss: 0.3137481\n",
      "\tspeed: 0.0221s/iter; left time: 106.7310s\n",
      "\titers: 400, epoch: 12 | loss: 0.1479778\n",
      "\tspeed: 0.0221s/iter; left time: 104.3485s\n",
      "\titers: 500, epoch: 12 | loss: 0.2617011\n",
      "\tspeed: 0.0221s/iter; left time: 102.1873s\n",
      "Epoch: 12 cost time: 12.901498794555664\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2333996 Vali Loss: 0.0531982 Test Loss: 0.1563964\n",
      "Validation loss decreased (0.053220 --> 0.053198).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.3771702\n",
      "\tspeed: 0.0641s/iter; left time: 286.1465s\n",
      "\titers: 200, epoch: 13 | loss: 0.1694183\n",
      "\tspeed: 0.0221s/iter; left time: 96.3193s\n",
      "\titers: 300, epoch: 13 | loss: 0.1437278\n",
      "\tspeed: 0.0221s/iter; left time: 94.0685s\n",
      "\titers: 400, epoch: 13 | loss: 0.2160377\n",
      "\tspeed: 0.0221s/iter; left time: 91.8655s\n",
      "\titers: 500, epoch: 13 | loss: 0.4250709\n",
      "\tspeed: 0.0221s/iter; left time: 89.6246s\n",
      "Epoch: 13 cost time: 12.897320985794067\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2338304 Vali Loss: 0.0532326 Test Loss: 0.1563705\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2406413\n",
      "\tspeed: 0.0629s/iter; left time: 244.8650s\n",
      "\titers: 200, epoch: 14 | loss: 0.3010644\n",
      "\tspeed: 0.0221s/iter; left time: 83.6072s\n",
      "\titers: 300, epoch: 14 | loss: 0.2287150\n",
      "\tspeed: 0.0220s/iter; left time: 81.3009s\n",
      "\titers: 400, epoch: 14 | loss: 0.1924923\n",
      "\tspeed: 0.0221s/iter; left time: 79.1847s\n",
      "\titers: 500, epoch: 14 | loss: 0.1605249\n",
      "\tspeed: 0.0221s/iter; left time: 77.0535s\n",
      "Epoch: 14 cost time: 12.902438879013062\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2321255 Vali Loss: 0.0534521 Test Loss: 0.1563781\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2224185\n",
      "\tspeed: 0.0624s/iter; left time: 207.1422s\n",
      "\titers: 200, epoch: 15 | loss: 0.2126149\n",
      "\tspeed: 0.0222s/iter; left time: 71.5026s\n",
      "\titers: 300, epoch: 15 | loss: 0.1061153\n",
      "\tspeed: 0.0222s/iter; left time: 69.1318s\n",
      "\titers: 400, epoch: 15 | loss: 0.2380839\n",
      "\tspeed: 0.0222s/iter; left time: 66.9363s\n",
      "\titers: 500, epoch: 15 | loss: 0.2103924\n",
      "\tspeed: 0.0222s/iter; left time: 64.7959s\n",
      "Epoch: 15 cost time: 12.917304277420044\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2347845 Vali Loss: 0.0533193 Test Loss: 0.1563731\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1566171944141388, mae:0.2542649507522583\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3377392\n",
      "\tspeed: 0.0352s/iter; left time: 397.8634s\n",
      "\titers: 200, epoch: 1 | loss: 0.1646500\n",
      "\tspeed: 0.0232s/iter; left time: 259.6305s\n",
      "\titers: 300, epoch: 1 | loss: 0.2190873\n",
      "\tspeed: 0.0232s/iter; left time: 257.2016s\n",
      "\titers: 400, epoch: 1 | loss: 0.2957370\n",
      "\tspeed: 0.0232s/iter; left time: 255.0003s\n",
      "\titers: 500, epoch: 1 | loss: 0.2291556\n",
      "\tspeed: 0.0232s/iter; left time: 252.4068s\n",
      "Epoch: 1 cost time: 14.469799757003784\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3399138 Vali Loss: 0.0699984 Test Loss: 0.1897158\n",
      "Validation loss decreased (inf --> 0.069998).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2779487\n",
      "\tspeed: 0.0638s/iter; left time: 684.8520s\n",
      "\titers: 200, epoch: 2 | loss: 0.2231273\n",
      "\tspeed: 0.0213s/iter; left time: 226.0006s\n",
      "\titers: 300, epoch: 2 | loss: 0.3795634\n",
      "\tspeed: 0.0213s/iter; left time: 223.7867s\n",
      "\titers: 400, epoch: 2 | loss: 0.2643101\n",
      "\tspeed: 0.0213s/iter; left time: 221.6896s\n",
      "\titers: 500, epoch: 2 | loss: 0.2047420\n",
      "\tspeed: 0.0213s/iter; left time: 219.5466s\n",
      "Epoch: 2 cost time: 12.469574689865112\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2844722 Vali Loss: 0.0626669 Test Loss: 0.1898932\n",
      "Validation loss decreased (0.069998 --> 0.062667).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2739033\n",
      "\tspeed: 0.0638s/iter; left time: 648.3477s\n",
      "\titers: 200, epoch: 3 | loss: 0.2690250\n",
      "\tspeed: 0.0214s/iter; left time: 214.9253s\n",
      "\titers: 300, epoch: 3 | loss: 0.1724478\n",
      "\tspeed: 0.0213s/iter; left time: 212.2107s\n",
      "\titers: 400, epoch: 3 | loss: 0.1621042\n",
      "\tspeed: 0.0213s/iter; left time: 210.4041s\n",
      "\titers: 500, epoch: 3 | loss: 0.2729535\n",
      "\tspeed: 0.0213s/iter; left time: 207.9798s\n",
      "Epoch: 3 cost time: 12.503925085067749\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2619222 Vali Loss: 0.0608310 Test Loss: 0.1616126\n",
      "Validation loss decreased (0.062667 --> 0.060831).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1626700\n",
      "\tspeed: 0.0621s/iter; left time: 595.1566s\n",
      "\titers: 200, epoch: 4 | loss: 0.2158362\n",
      "\tspeed: 0.0213s/iter; left time: 202.3355s\n",
      "\titers: 300, epoch: 4 | loss: 0.2240596\n",
      "\tspeed: 0.0213s/iter; left time: 200.2567s\n",
      "\titers: 400, epoch: 4 | loss: 0.3337071\n",
      "\tspeed: 0.0213s/iter; left time: 198.1321s\n",
      "\titers: 500, epoch: 4 | loss: 0.1876449\n",
      "\tspeed: 0.0213s/iter; left time: 195.8009s\n",
      "Epoch: 4 cost time: 12.476000785827637\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2398474 Vali Loss: 0.0581919 Test Loss: 0.1548366\n",
      "Validation loss decreased (0.060831 --> 0.058192).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.3035271\n",
      "\tspeed: 0.0619s/iter; left time: 558.1060s\n",
      "\titers: 200, epoch: 5 | loss: 0.1793818\n",
      "\tspeed: 0.0214s/iter; left time: 190.8531s\n",
      "\titers: 300, epoch: 5 | loss: 0.2417612\n",
      "\tspeed: 0.0214s/iter; left time: 188.9051s\n",
      "\titers: 400, epoch: 5 | loss: 0.2484414\n",
      "\tspeed: 0.0214s/iter; left time: 186.4470s\n",
      "\titers: 500, epoch: 5 | loss: 0.1850076\n",
      "\tspeed: 0.0214s/iter; left time: 184.3399s\n",
      "Epoch: 5 cost time: 12.500789880752563\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2259612 Vali Loss: 0.0581790 Test Loss: 0.1517878\n",
      "Validation loss decreased (0.058192 --> 0.058179).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2080330\n",
      "\tspeed: 0.0647s/iter; left time: 546.6759s\n",
      "\titers: 200, epoch: 6 | loss: 0.3204569\n",
      "\tspeed: 0.0213s/iter; left time: 178.0492s\n",
      "\titers: 300, epoch: 6 | loss: 0.2294517\n",
      "\tspeed: 0.0213s/iter; left time: 175.8708s\n",
      "\titers: 400, epoch: 6 | loss: 0.1415012\n",
      "\tspeed: 0.0213s/iter; left time: 173.5542s\n",
      "\titers: 500, epoch: 6 | loss: 0.1335572\n",
      "\tspeed: 0.0213s/iter; left time: 171.3920s\n",
      "Epoch: 6 cost time: 12.436667442321777\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2220644 Vali Loss: 0.0585967 Test Loss: 0.1529160\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1969465\n",
      "\tspeed: 0.0602s/iter; left time: 474.4837s\n",
      "\titers: 200, epoch: 7 | loss: 0.2675973\n",
      "\tspeed: 0.0213s/iter; left time: 165.6491s\n",
      "\titers: 300, epoch: 7 | loss: 0.2175160\n",
      "\tspeed: 0.0213s/iter; left time: 163.4312s\n",
      "\titers: 400, epoch: 7 | loss: 0.1372379\n",
      "\tspeed: 0.0213s/iter; left time: 161.2407s\n",
      "\titers: 500, epoch: 7 | loss: 0.1875855\n",
      "\tspeed: 0.0213s/iter; left time: 159.3479s\n",
      "Epoch: 7 cost time: 12.456549167633057\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2186270 Vali Loss: 0.0573958 Test Loss: 0.1515518\n",
      "Validation loss decreased (0.058179 --> 0.057396).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2150478\n",
      "\tspeed: 0.0627s/iter; left time: 458.0898s\n",
      "\titers: 200, epoch: 8 | loss: 0.2058710\n",
      "\tspeed: 0.0214s/iter; left time: 154.2618s\n",
      "\titers: 300, epoch: 8 | loss: 0.2225778\n",
      "\tspeed: 0.0222s/iter; left time: 158.1591s\n",
      "\titers: 400, epoch: 8 | loss: 0.2368623\n",
      "\tspeed: 0.0234s/iter; left time: 163.8474s\n",
      "\titers: 500, epoch: 8 | loss: 0.2915786\n",
      "\tspeed: 0.0218s/iter; left time: 150.4513s\n",
      "Epoch: 8 cost time: 12.8038969039917\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2145763 Vali Loss: 0.0559872 Test Loss: 0.1507481\n",
      "Validation loss decreased (0.057396 --> 0.055987).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1523317\n",
      "\tspeed: 0.0642s/iter; left time: 432.8172s\n",
      "\titers: 200, epoch: 9 | loss: 0.3080234\n",
      "\tspeed: 0.0214s/iter; left time: 141.8015s\n",
      "\titers: 300, epoch: 9 | loss: 0.1853736\n",
      "\tspeed: 0.0213s/iter; left time: 139.5946s\n",
      "\titers: 400, epoch: 9 | loss: 0.1888307\n",
      "\tspeed: 0.0213s/iter; left time: 137.4624s\n",
      "\titers: 500, epoch: 9 | loss: 0.1443109\n",
      "\tspeed: 0.0213s/iter; left time: 135.3236s\n",
      "Epoch: 9 cost time: 12.511424779891968\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2150063 Vali Loss: 0.0561404 Test Loss: 0.1507259\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1206441\n",
      "\tspeed: 0.0623s/iter; left time: 384.4464s\n",
      "\titers: 200, epoch: 10 | loss: 0.2277396\n",
      "\tspeed: 0.0214s/iter; left time: 129.7889s\n",
      "\titers: 300, epoch: 10 | loss: 0.1638017\n",
      "\tspeed: 0.0214s/iter; left time: 127.5904s\n",
      "\titers: 400, epoch: 10 | loss: 0.2121173\n",
      "\tspeed: 0.0214s/iter; left time: 125.4975s\n",
      "\titers: 500, epoch: 10 | loss: 0.2538242\n",
      "\tspeed: 0.0214s/iter; left time: 123.3371s\n",
      "Epoch: 10 cost time: 12.482203245162964\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2131337 Vali Loss: 0.0561769 Test Loss: 0.1508286\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2837865\n",
      "\tspeed: 0.0603s/iter; left time: 337.6466s\n",
      "\titers: 200, epoch: 11 | loss: 0.2379410\n",
      "\tspeed: 0.0213s/iter; left time: 117.2869s\n",
      "\titers: 300, epoch: 11 | loss: 0.1742121\n",
      "\tspeed: 0.0213s/iter; left time: 114.8593s\n",
      "\titers: 400, epoch: 11 | loss: 0.1710873\n",
      "\tspeed: 0.0213s/iter; left time: 112.7079s\n",
      "\titers: 500, epoch: 11 | loss: 0.2518443\n",
      "\tspeed: 0.0213s/iter; left time: 110.7142s\n",
      "Epoch: 11 cost time: 12.441160440444946\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2131534 Vali Loss: 0.0561333 Test Loss: 0.1507716\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15090881288051605, mae:0.2482869029045105\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3404231\n",
      "\tspeed: 0.0319s/iter; left time: 360.3543s\n",
      "\titers: 200, epoch: 1 | loss: 0.2426745\n",
      "\tspeed: 0.0201s/iter; left time: 225.5962s\n",
      "\titers: 300, epoch: 1 | loss: 0.4168512\n",
      "\tspeed: 0.0202s/iter; left time: 223.7566s\n",
      "\titers: 400, epoch: 1 | loss: 0.3796895\n",
      "\tspeed: 0.0201s/iter; left time: 221.6380s\n",
      "\titers: 500, epoch: 1 | loss: 0.2751806\n",
      "\tspeed: 0.0201s/iter; left time: 219.6221s\n",
      "Epoch: 1 cost time: 12.714741945266724\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3250057 Vali Loss: 0.0661804 Test Loss: 0.1876831\n",
      "Validation loss decreased (inf --> 0.066180).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1786113\n",
      "\tspeed: 0.0617s/iter; left time: 661.8826s\n",
      "\titers: 200, epoch: 2 | loss: 0.5230356\n",
      "\tspeed: 0.0221s/iter; left time: 234.7253s\n",
      "\titers: 300, epoch: 2 | loss: 0.3196713\n",
      "\tspeed: 0.0206s/iter; left time: 216.5512s\n",
      "\titers: 400, epoch: 2 | loss: 0.1660084\n",
      "\tspeed: 0.0223s/iter; left time: 232.1341s\n",
      "\titers: 500, epoch: 2 | loss: 0.1820250\n",
      "\tspeed: 0.0223s/iter; left time: 229.9416s\n",
      "Epoch: 2 cost time: 12.815780401229858\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2875961 Vali Loss: 0.0598336 Test Loss: 0.1854444\n",
      "Validation loss decreased (0.066180 --> 0.059834).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1814147\n",
      "\tspeed: 0.0634s/iter; left time: 643.8120s\n",
      "\titers: 200, epoch: 3 | loss: 0.1833535\n",
      "\tspeed: 0.0202s/iter; left time: 203.1545s\n",
      "\titers: 300, epoch: 3 | loss: 0.2722208\n",
      "\tspeed: 0.0202s/iter; left time: 201.1450s\n",
      "\titers: 400, epoch: 3 | loss: 0.2516907\n",
      "\tspeed: 0.0202s/iter; left time: 198.8643s\n",
      "\titers: 500, epoch: 3 | loss: 0.2900542\n",
      "\tspeed: 0.0202s/iter; left time: 196.9889s\n",
      "Epoch: 3 cost time: 11.800395011901855\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2670503 Vali Loss: 0.0646542 Test Loss: 0.1743281\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1391644\n",
      "\tspeed: 0.0583s/iter; left time: 559.3144s\n",
      "\titers: 200, epoch: 4 | loss: 0.4543982\n",
      "\tspeed: 0.0202s/iter; left time: 191.6981s\n",
      "\titers: 300, epoch: 4 | loss: 0.1633909\n",
      "\tspeed: 0.0202s/iter; left time: 189.4333s\n",
      "\titers: 400, epoch: 4 | loss: 0.2394994\n",
      "\tspeed: 0.0202s/iter; left time: 187.5083s\n",
      "\titers: 500, epoch: 4 | loss: 0.2080919\n",
      "\tspeed: 0.0202s/iter; left time: 185.4172s\n",
      "Epoch: 4 cost time: 11.792239904403687\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2490871 Vali Loss: 0.0620873 Test Loss: 0.1656219\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2104890\n",
      "\tspeed: 0.0608s/iter; left time: 548.7528s\n",
      "\titers: 200, epoch: 5 | loss: 0.4366803\n",
      "\tspeed: 0.0202s/iter; left time: 180.6438s\n",
      "\titers: 300, epoch: 5 | loss: 0.2191622\n",
      "\tspeed: 0.0202s/iter; left time: 178.2018s\n",
      "\titers: 400, epoch: 5 | loss: 0.2156383\n",
      "\tspeed: 0.0202s/iter; left time: 176.3327s\n",
      "\titers: 500, epoch: 5 | loss: 0.2265786\n",
      "\tspeed: 0.0202s/iter; left time: 174.3555s\n",
      "Epoch: 5 cost time: 11.826763391494751\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2388819 Vali Loss: 0.0566011 Test Loss: 0.1586045\n",
      "Validation loss decreased (0.059834 --> 0.056601).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2559460\n",
      "\tspeed: 0.0599s/iter; left time: 506.2979s\n",
      "\titers: 200, epoch: 6 | loss: 0.2333039\n",
      "\tspeed: 0.0202s/iter; left time: 168.3319s\n",
      "\titers: 300, epoch: 6 | loss: 0.2261849\n",
      "\tspeed: 0.0202s/iter; left time: 166.3882s\n",
      "\titers: 400, epoch: 6 | loss: 0.1795142\n",
      "\tspeed: 0.0202s/iter; left time: 164.2468s\n",
      "\titers: 500, epoch: 6 | loss: 0.3686235\n",
      "\tspeed: 0.0201s/iter; left time: 162.1186s\n",
      "Epoch: 6 cost time: 11.77818775177002\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2322234 Vali Loss: 0.0568356 Test Loss: 0.1558356\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2439669\n",
      "\tspeed: 0.0587s/iter; left time: 462.3218s\n",
      "\titers: 200, epoch: 7 | loss: 0.2223335\n",
      "\tspeed: 0.0202s/iter; left time: 156.8173s\n",
      "\titers: 300, epoch: 7 | loss: 0.2759081\n",
      "\tspeed: 0.0202s/iter; left time: 154.8007s\n",
      "\titers: 400, epoch: 7 | loss: 0.4412869\n",
      "\tspeed: 0.0202s/iter; left time: 152.8559s\n",
      "\titers: 500, epoch: 7 | loss: 0.1374561\n",
      "\tspeed: 0.0202s/iter; left time: 151.0141s\n",
      "Epoch: 7 cost time: 11.787142038345337\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2276247 Vali Loss: 0.0568543 Test Loss: 0.1559284\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2687363\n",
      "\tspeed: 0.0589s/iter; left time: 430.5944s\n",
      "\titers: 200, epoch: 8 | loss: 0.3044820\n",
      "\tspeed: 0.0201s/iter; left time: 145.0846s\n",
      "\titers: 300, epoch: 8 | loss: 0.2953758\n",
      "\tspeed: 0.0201s/iter; left time: 142.6401s\n",
      "\titers: 400, epoch: 8 | loss: 0.1874395\n",
      "\tspeed: 0.0201s/iter; left time: 140.9104s\n",
      "\titers: 500, epoch: 8 | loss: 0.2586952\n",
      "\tspeed: 0.0201s/iter; left time: 138.8563s\n",
      "Epoch: 8 cost time: 11.70957899093628\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2268142 Vali Loss: 0.0565078 Test Loss: 0.1549985\n",
      "Validation loss decreased (0.056601 --> 0.056508).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2243491\n",
      "\tspeed: 0.0616s/iter; left time: 415.4247s\n",
      "\titers: 200, epoch: 9 | loss: 0.1885431\n",
      "\tspeed: 0.0202s/iter; left time: 134.3113s\n",
      "\titers: 300, epoch: 9 | loss: 0.2734574\n",
      "\tspeed: 0.0202s/iter; left time: 132.0640s\n",
      "\titers: 400, epoch: 9 | loss: 0.1793066\n",
      "\tspeed: 0.0202s/iter; left time: 130.1710s\n",
      "\titers: 500, epoch: 9 | loss: 0.1427067\n",
      "\tspeed: 0.0202s/iter; left time: 127.7789s\n",
      "Epoch: 9 cost time: 11.831654071807861\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2260832 Vali Loss: 0.0563967 Test Loss: 0.1547564\n",
      "Validation loss decreased (0.056508 --> 0.056397).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2403950\n",
      "\tspeed: 0.0596s/iter; left time: 367.8769s\n",
      "\titers: 200, epoch: 10 | loss: 0.1651448\n",
      "\tspeed: 0.0204s/iter; left time: 123.6051s\n",
      "\titers: 300, epoch: 10 | loss: 0.2896049\n",
      "\tspeed: 0.0204s/iter; left time: 121.5125s\n",
      "\titers: 400, epoch: 10 | loss: 0.3092924\n",
      "\tspeed: 0.0204s/iter; left time: 119.6092s\n",
      "\titers: 500, epoch: 10 | loss: 0.2361620\n",
      "\tspeed: 0.0203s/iter; left time: 117.3936s\n",
      "Epoch: 10 cost time: 11.909939050674438\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2255608 Vali Loss: 0.0564119 Test Loss: 0.1546019\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2059657\n",
      "\tspeed: 0.0616s/iter; left time: 344.9056s\n",
      "\titers: 200, epoch: 11 | loss: 0.1982250\n",
      "\tspeed: 0.0202s/iter; left time: 110.9073s\n",
      "\titers: 300, epoch: 11 | loss: 0.1643462\n",
      "\tspeed: 0.0201s/iter; left time: 108.7908s\n",
      "\titers: 400, epoch: 11 | loss: 0.2687558\n",
      "\tspeed: 0.0201s/iter; left time: 106.7675s\n",
      "\titers: 500, epoch: 11 | loss: 0.1734656\n",
      "\tspeed: 0.0201s/iter; left time: 104.7518s\n",
      "Epoch: 11 cost time: 11.746526718139648\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2242707 Vali Loss: 0.0564297 Test Loss: 0.1546341\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2062238\n",
      "\tspeed: 0.0606s/iter; left time: 305.1180s\n",
      "\titers: 200, epoch: 12 | loss: 0.1974917\n",
      "\tspeed: 0.0223s/iter; left time: 110.1033s\n",
      "\titers: 300, epoch: 12 | loss: 0.3693649\n",
      "\tspeed: 0.0223s/iter; left time: 107.6365s\n",
      "\titers: 400, epoch: 12 | loss: 0.2459300\n",
      "\tspeed: 0.0223s/iter; left time: 105.3821s\n",
      "\titers: 500, epoch: 12 | loss: 0.3968561\n",
      "\tspeed: 0.0223s/iter; left time: 103.4442s\n",
      "Epoch: 12 cost time: 13.001673936843872\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2242734 Vali Loss: 0.0563085 Test Loss: 0.1546287\n",
      "Validation loss decreased (0.056397 --> 0.056308).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2222096\n",
      "\tspeed: 0.0620s/iter; left time: 276.5809s\n",
      "\titers: 200, epoch: 13 | loss: 0.1883194\n",
      "\tspeed: 0.0201s/iter; left time: 87.7128s\n",
      "\titers: 300, epoch: 13 | loss: 0.2055316\n",
      "\tspeed: 0.0201s/iter; left time: 85.5570s\n",
      "\titers: 400, epoch: 13 | loss: 0.1621581\n",
      "\tspeed: 0.0201s/iter; left time: 83.5888s\n",
      "\titers: 500, epoch: 13 | loss: 0.3007304\n",
      "\tspeed: 0.0201s/iter; left time: 81.5378s\n",
      "Epoch: 13 cost time: 11.771286010742188\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2250128 Vali Loss: 0.0565566 Test Loss: 0.1546288\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2919002\n",
      "\tspeed: 0.0618s/iter; left time: 240.4059s\n",
      "\titers: 200, epoch: 14 | loss: 0.2277680\n",
      "\tspeed: 0.0202s/iter; left time: 76.5152s\n",
      "\titers: 300, epoch: 14 | loss: 0.2307416\n",
      "\tspeed: 0.0202s/iter; left time: 74.4955s\n",
      "\titers: 400, epoch: 14 | loss: 0.1529334\n",
      "\tspeed: 0.0202s/iter; left time: 72.4418s\n",
      "\titers: 500, epoch: 14 | loss: 0.1951609\n",
      "\tspeed: 0.0202s/iter; left time: 70.3940s\n",
      "Epoch: 14 cost time: 11.785974502563477\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2229430 Vali Loss: 0.0564357 Test Loss: 0.1546331\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1179616\n",
      "\tspeed: 0.0586s/iter; left time: 194.5586s\n",
      "\titers: 200, epoch: 15 | loss: 0.2970698\n",
      "\tspeed: 0.0202s/iter; left time: 65.0475s\n",
      "\titers: 300, epoch: 15 | loss: 0.2058809\n",
      "\tspeed: 0.0202s/iter; left time: 63.0737s\n",
      "\titers: 400, epoch: 15 | loss: 0.2033745\n",
      "\tspeed: 0.0202s/iter; left time: 61.0155s\n",
      "\titers: 500, epoch: 15 | loss: 0.2084622\n",
      "\tspeed: 0.0202s/iter; left time: 59.0030s\n",
      "Epoch: 15 cost time: 11.788908004760742\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2243458 Vali Loss: 0.0565327 Test Loss: 0.1546283\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15482033789157867, mae:0.25201061367988586\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2117237\n",
      "\tspeed: 0.0223s/iter; left time: 251.5261s\n",
      "\titers: 200, epoch: 1 | loss: 0.1962214\n",
      "\tspeed: 0.0106s/iter; left time: 118.2162s\n",
      "\titers: 300, epoch: 1 | loss: 0.2084974\n",
      "\tspeed: 0.0106s/iter; left time: 117.3029s\n",
      "\titers: 400, epoch: 1 | loss: 0.2296560\n",
      "\tspeed: 0.0106s/iter; left time: 116.0777s\n",
      "\titers: 500, epoch: 1 | loss: 0.2272920\n",
      "\tspeed: 0.0106s/iter; left time: 115.1762s\n",
      "Epoch: 1 cost time: 7.239390850067139\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2698223 Vali Loss: 0.0505259 Test Loss: 0.1521214\n",
      "Validation loss decreased (inf --> 0.050526).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.5054663\n",
      "\tspeed: 0.0384s/iter; left time: 411.9819s\n",
      "\titers: 200, epoch: 2 | loss: 0.3730318\n",
      "\tspeed: 0.0109s/iter; left time: 115.7382s\n",
      "\titers: 300, epoch: 2 | loss: 0.2677314\n",
      "\tspeed: 0.0104s/iter; left time: 109.8929s\n",
      "\titers: 400, epoch: 2 | loss: 0.2034914\n",
      "\tspeed: 0.0105s/iter; left time: 109.4446s\n",
      "\titers: 500, epoch: 2 | loss: 0.2365679\n",
      "\tspeed: 0.0106s/iter; left time: 109.2065s\n",
      "Epoch: 2 cost time: 6.461136341094971\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2636143 Vali Loss: 0.0675508 Test Loss: 0.1951685\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2460580\n",
      "\tspeed: 0.0376s/iter; left time: 382.1713s\n",
      "\titers: 200, epoch: 3 | loss: 0.3752654\n",
      "\tspeed: 0.0118s/iter; left time: 118.9396s\n",
      "\titers: 300, epoch: 3 | loss: 0.2134835\n",
      "\tspeed: 0.0118s/iter; left time: 118.0270s\n",
      "\titers: 400, epoch: 3 | loss: 0.2111763\n",
      "\tspeed: 0.0118s/iter; left time: 116.3969s\n",
      "\titers: 500, epoch: 3 | loss: 0.2905028\n",
      "\tspeed: 0.0118s/iter; left time: 115.0224s\n",
      "Epoch: 3 cost time: 7.002861022949219\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2672945 Vali Loss: 0.0502162 Test Loss: 0.1495679\n",
      "Validation loss decreased (0.050526 --> 0.050216).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.4105504\n",
      "\tspeed: 0.0383s/iter; left time: 367.2943s\n",
      "\titers: 200, epoch: 4 | loss: 0.1554852\n",
      "\tspeed: 0.0105s/iter; left time: 100.0772s\n",
      "\titers: 300, epoch: 4 | loss: 0.2539790\n",
      "\tspeed: 0.0105s/iter; left time: 98.9913s\n",
      "\titers: 400, epoch: 4 | loss: 0.2332794\n",
      "\tspeed: 0.0105s/iter; left time: 97.8672s\n",
      "\titers: 500, epoch: 4 | loss: 0.2072173\n",
      "\tspeed: 0.0105s/iter; left time: 96.7575s\n",
      "Epoch: 4 cost time: 6.321913480758667\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2372657 Vali Loss: 0.0477580 Test Loss: 0.1426489\n",
      "Validation loss decreased (0.050216 --> 0.047758).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2328312\n",
      "\tspeed: 0.0360s/iter; left time: 325.2012s\n",
      "\titers: 200, epoch: 5 | loss: 0.2216953\n",
      "\tspeed: 0.0106s/iter; left time: 94.8716s\n",
      "\titers: 300, epoch: 5 | loss: 0.1907568\n",
      "\tspeed: 0.0106s/iter; left time: 93.7870s\n",
      "\titers: 400, epoch: 5 | loss: 0.1760155\n",
      "\tspeed: 0.0106s/iter; left time: 92.4009s\n",
      "\titers: 500, epoch: 5 | loss: 0.2505337\n",
      "\tspeed: 0.0106s/iter; left time: 91.5475s\n",
      "Epoch: 5 cost time: 6.344281196594238\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2250188 Vali Loss: 0.0460322 Test Loss: 0.1390025\n",
      "Validation loss decreased (0.047758 --> 0.046032).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1989035\n",
      "\tspeed: 0.0383s/iter; left time: 323.9107s\n",
      "\titers: 200, epoch: 6 | loss: 0.1535221\n",
      "\tspeed: 0.0106s/iter; left time: 88.5298s\n",
      "\titers: 300, epoch: 6 | loss: 0.2412390\n",
      "\tspeed: 0.0117s/iter; left time: 96.3282s\n",
      "\titers: 400, epoch: 6 | loss: 0.2632490\n",
      "\tspeed: 0.0118s/iter; left time: 96.4333s\n",
      "\titers: 500, epoch: 6 | loss: 0.2396662\n",
      "\tspeed: 0.0118s/iter; left time: 95.3610s\n",
      "Epoch: 6 cost time: 6.75518536567688\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2193443 Vali Loss: 0.0450908 Test Loss: 0.1399323\n",
      "Validation loss decreased (0.046032 --> 0.045091).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2661636\n",
      "\tspeed: 0.0372s/iter; left time: 293.1139s\n",
      "\titers: 200, epoch: 7 | loss: 0.2804905\n",
      "\tspeed: 0.0105s/iter; left time: 81.3983s\n",
      "\titers: 300, epoch: 7 | loss: 0.1554718\n",
      "\tspeed: 0.0105s/iter; left time: 80.7634s\n",
      "\titers: 400, epoch: 7 | loss: 0.1617900\n",
      "\tspeed: 0.0105s/iter; left time: 79.5770s\n",
      "\titers: 500, epoch: 7 | loss: 0.1971248\n",
      "\tspeed: 0.0105s/iter; left time: 78.4045s\n",
      "Epoch: 7 cost time: 6.289736032485962\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2169546 Vali Loss: 0.0455527 Test Loss: 0.1369280\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2547485\n",
      "\tspeed: 0.0362s/iter; left time: 264.4103s\n",
      "\titers: 200, epoch: 8 | loss: 0.2095753\n",
      "\tspeed: 0.0105s/iter; left time: 75.9925s\n",
      "\titers: 300, epoch: 8 | loss: 0.2478575\n",
      "\tspeed: 0.0106s/iter; left time: 75.0948s\n",
      "\titers: 400, epoch: 8 | loss: 0.2990359\n",
      "\tspeed: 0.0105s/iter; left time: 73.9083s\n",
      "\titers: 500, epoch: 8 | loss: 0.1562864\n",
      "\tspeed: 0.0105s/iter; left time: 72.6244s\n",
      "Epoch: 8 cost time: 6.2791736125946045\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2126635 Vali Loss: 0.0449546 Test Loss: 0.1366721\n",
      "Validation loss decreased (0.045091 --> 0.044955).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2795043\n",
      "\tspeed: 0.0362s/iter; left time: 243.8338s\n",
      "\titers: 200, epoch: 9 | loss: 0.1916270\n",
      "\tspeed: 0.0106s/iter; left time: 70.0720s\n",
      "\titers: 300, epoch: 9 | loss: 0.1030944\n",
      "\tspeed: 0.0105s/iter; left time: 68.7009s\n",
      "\titers: 400, epoch: 9 | loss: 0.1902943\n",
      "\tspeed: 0.0105s/iter; left time: 67.6884s\n",
      "\titers: 500, epoch: 9 | loss: 0.1884133\n",
      "\tspeed: 0.0105s/iter; left time: 66.8527s\n",
      "Epoch: 9 cost time: 6.272051811218262\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2139503 Vali Loss: 0.0453385 Test Loss: 0.1365460\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1995925\n",
      "\tspeed: 0.0364s/iter; left time: 224.5547s\n",
      "\titers: 200, epoch: 10 | loss: 0.1834594\n",
      "\tspeed: 0.0105s/iter; left time: 63.5206s\n",
      "\titers: 300, epoch: 10 | loss: 0.1525384\n",
      "\tspeed: 0.0104s/iter; left time: 62.3664s\n",
      "\titers: 400, epoch: 10 | loss: 0.1993958\n",
      "\tspeed: 0.0105s/iter; left time: 61.5610s\n",
      "\titers: 500, epoch: 10 | loss: 0.3330832\n",
      "\tspeed: 0.0105s/iter; left time: 60.5066s\n",
      "Epoch: 10 cost time: 6.216636657714844\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2125980 Vali Loss: 0.0449574 Test Loss: 0.1366582\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2353677\n",
      "\tspeed: 0.0377s/iter; left time: 210.9072s\n",
      "\titers: 200, epoch: 11 | loss: 0.1963727\n",
      "\tspeed: 0.0120s/iter; left time: 65.9562s\n",
      "\titers: 300, epoch: 11 | loss: 0.1679645\n",
      "\tspeed: 0.0111s/iter; left time: 60.0019s\n",
      "\titers: 400, epoch: 11 | loss: 0.3239220\n",
      "\tspeed: 0.0106s/iter; left time: 56.0750s\n",
      "\titers: 500, epoch: 11 | loss: 0.1685197\n",
      "\tspeed: 0.0106s/iter; left time: 54.9997s\n",
      "Epoch: 11 cost time: 6.6328699588775635\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2125218 Vali Loss: 0.0449107 Test Loss: 0.1366072\n",
      "Validation loss decreased (0.044955 --> 0.044911).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2075755\n",
      "\tspeed: 0.0361s/iter; left time: 181.4058s\n",
      "\titers: 200, epoch: 12 | loss: 0.2189093\n",
      "\tspeed: 0.0106s/iter; left time: 52.0453s\n",
      "\titers: 300, epoch: 12 | loss: 0.1716382\n",
      "\tspeed: 0.0105s/iter; left time: 50.9386s\n",
      "\titers: 400, epoch: 12 | loss: 0.1820216\n",
      "\tspeed: 0.0105s/iter; left time: 49.8678s\n",
      "\titers: 500, epoch: 12 | loss: 0.3031299\n",
      "\tspeed: 0.0105s/iter; left time: 48.8455s\n",
      "Epoch: 12 cost time: 6.3062779903411865\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2114540 Vali Loss: 0.0450061 Test Loss: 0.1366529\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1022834\n",
      "\tspeed: 0.0376s/iter; left time: 167.5658s\n",
      "\titers: 200, epoch: 13 | loss: 0.2675892\n",
      "\tspeed: 0.0118s/iter; left time: 51.6001s\n",
      "\titers: 300, epoch: 13 | loss: 0.1558159\n",
      "\tspeed: 0.0118s/iter; left time: 50.3838s\n",
      "\titers: 400, epoch: 13 | loss: 0.1571575\n",
      "\tspeed: 0.0118s/iter; left time: 49.1817s\n",
      "\titers: 500, epoch: 13 | loss: 0.2214960\n",
      "\tspeed: 0.0118s/iter; left time: 47.9702s\n",
      "Epoch: 13 cost time: 7.017001390457153\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2133635 Vali Loss: 0.0450953 Test Loss: 0.1366360\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1524723\n",
      "\tspeed: 0.0377s/iter; left time: 146.7559s\n",
      "\titers: 200, epoch: 14 | loss: 0.1994179\n",
      "\tspeed: 0.0105s/iter; left time: 39.8972s\n",
      "\titers: 300, epoch: 14 | loss: 0.2454288\n",
      "\tspeed: 0.0105s/iter; left time: 38.8370s\n",
      "\titers: 400, epoch: 14 | loss: 0.2762757\n",
      "\tspeed: 0.0105s/iter; left time: 37.7912s\n",
      "\titers: 500, epoch: 14 | loss: 0.2121911\n",
      "\tspeed: 0.0105s/iter; left time: 36.7452s\n",
      "Epoch: 14 cost time: 6.254024028778076\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2123307 Vali Loss: 0.0449673 Test Loss: 0.1366275\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13682235777378082, mae:0.23013094067573547\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2720160\n",
      "\tspeed: 0.0238s/iter; left time: 268.4708s\n",
      "\titers: 200, epoch: 1 | loss: 0.2550612\n",
      "\tspeed: 0.0117s/iter; left time: 130.6602s\n",
      "\titers: 300, epoch: 1 | loss: 0.2996303\n",
      "\tspeed: 0.0116s/iter; left time: 129.1860s\n",
      "\titers: 400, epoch: 1 | loss: 0.2450242\n",
      "\tspeed: 0.0117s/iter; left time: 128.3198s\n",
      "\titers: 500, epoch: 1 | loss: 0.2024878\n",
      "\tspeed: 0.0117s/iter; left time: 127.2749s\n",
      "Epoch: 1 cost time: 7.903503894805908\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2292420 Vali Loss: 0.0406269 Test Loss: 0.1289614\n",
      "Validation loss decreased (inf --> 0.040627).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1660670\n",
      "\tspeed: 0.0380s/iter; left time: 407.7618s\n",
      "\titers: 200, epoch: 2 | loss: 0.2225348\n",
      "\tspeed: 0.0103s/iter; left time: 109.9934s\n",
      "\titers: 300, epoch: 2 | loss: 0.1696781\n",
      "\tspeed: 0.0104s/iter; left time: 109.2627s\n",
      "\titers: 400, epoch: 2 | loss: 0.2024043\n",
      "\tspeed: 0.0104s/iter; left time: 108.1326s\n",
      "\titers: 500, epoch: 2 | loss: 0.2608290\n",
      "\tspeed: 0.0104s/iter; left time: 107.1948s\n",
      "Epoch: 2 cost time: 6.2236082553863525\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2151188 Vali Loss: 0.0432774 Test Loss: 0.1319082\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1073155\n",
      "\tspeed: 0.0365s/iter; left time: 370.8846s\n",
      "\titers: 200, epoch: 3 | loss: 0.2045198\n",
      "\tspeed: 0.0104s/iter; left time: 104.8516s\n",
      "\titers: 300, epoch: 3 | loss: 0.3562557\n",
      "\tspeed: 0.0104s/iter; left time: 103.6907s\n",
      "\titers: 400, epoch: 3 | loss: 0.1984722\n",
      "\tspeed: 0.0104s/iter; left time: 102.9338s\n",
      "\titers: 500, epoch: 3 | loss: 0.1355830\n",
      "\tspeed: 0.0104s/iter; left time: 101.5661s\n",
      "Epoch: 3 cost time: 6.227769374847412\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1942021 Vali Loss: 0.0354472 Test Loss: 0.1168530\n",
      "Validation loss decreased (0.040627 --> 0.035447).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1595048\n",
      "\tspeed: 0.0363s/iter; left time: 347.7308s\n",
      "\titers: 200, epoch: 4 | loss: 0.1545954\n",
      "\tspeed: 0.0118s/iter; left time: 112.2550s\n",
      "\titers: 300, epoch: 4 | loss: 0.1039984\n",
      "\tspeed: 0.0118s/iter; left time: 110.8012s\n",
      "\titers: 400, epoch: 4 | loss: 0.1309091\n",
      "\tspeed: 0.0118s/iter; left time: 109.6554s\n",
      "\titers: 500, epoch: 4 | loss: 0.2197086\n",
      "\tspeed: 0.0118s/iter; left time: 108.6643s\n",
      "Epoch: 4 cost time: 6.95213508605957\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1774322 Vali Loss: 0.0345319 Test Loss: 0.1144773\n",
      "Validation loss decreased (0.035447 --> 0.034532).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1512218\n",
      "\tspeed: 0.0390s/iter; left time: 351.4861s\n",
      "\titers: 200, epoch: 5 | loss: 0.2394757\n",
      "\tspeed: 0.0116s/iter; left time: 103.6184s\n",
      "\titers: 300, epoch: 5 | loss: 0.1671902\n",
      "\tspeed: 0.0116s/iter; left time: 102.7444s\n",
      "\titers: 400, epoch: 5 | loss: 0.1594759\n",
      "\tspeed: 0.0116s/iter; left time: 101.4899s\n",
      "\titers: 500, epoch: 5 | loss: 0.1756693\n",
      "\tspeed: 0.0116s/iter; left time: 100.2519s\n",
      "Epoch: 5 cost time: 6.969079971313477\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1706449 Vali Loss: 0.0340808 Test Loss: 0.1143026\n",
      "Validation loss decreased (0.034532 --> 0.034081).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1132660\n",
      "\tspeed: 0.0380s/iter; left time: 321.1730s\n",
      "\titers: 200, epoch: 6 | loss: 0.2210791\n",
      "\tspeed: 0.0103s/iter; left time: 86.2357s\n",
      "\titers: 300, epoch: 6 | loss: 0.1718553\n",
      "\tspeed: 0.0103s/iter; left time: 85.0338s\n",
      "\titers: 400, epoch: 6 | loss: 0.1357596\n",
      "\tspeed: 0.0103s/iter; left time: 83.9698s\n",
      "\titers: 500, epoch: 6 | loss: 0.1047990\n",
      "\tspeed: 0.0103s/iter; left time: 82.6564s\n",
      "Epoch: 6 cost time: 6.154315948486328\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1650022 Vali Loss: 0.0340683 Test Loss: 0.1121477\n",
      "Validation loss decreased (0.034081 --> 0.034068).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1775665\n",
      "\tspeed: 0.0355s/iter; left time: 279.9533s\n",
      "\titers: 200, epoch: 7 | loss: 0.1840083\n",
      "\tspeed: 0.0103s/iter; left time: 80.3397s\n",
      "\titers: 300, epoch: 7 | loss: 0.3098571\n",
      "\tspeed: 0.0103s/iter; left time: 79.2376s\n",
      "\titers: 400, epoch: 7 | loss: 0.2166393\n",
      "\tspeed: 0.0103s/iter; left time: 78.1830s\n",
      "\titers: 500, epoch: 7 | loss: 0.1808318\n",
      "\tspeed: 0.0103s/iter; left time: 77.1619s\n",
      "Epoch: 7 cost time: 6.172895431518555\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1634188 Vali Loss: 0.0334270 Test Loss: 0.1127313\n",
      "Validation loss decreased (0.034068 --> 0.033427).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1337667\n",
      "\tspeed: 0.0375s/iter; left time: 273.9533s\n",
      "\titers: 200, epoch: 8 | loss: 0.1220730\n",
      "\tspeed: 0.0103s/iter; left time: 74.3001s\n",
      "\titers: 300, epoch: 8 | loss: 0.1137987\n",
      "\tspeed: 0.0103s/iter; left time: 73.2675s\n",
      "\titers: 400, epoch: 8 | loss: 0.1611990\n",
      "\tspeed: 0.0103s/iter; left time: 72.4656s\n",
      "\titers: 500, epoch: 8 | loss: 0.1009670\n",
      "\tspeed: 0.0103s/iter; left time: 71.5059s\n",
      "Epoch: 8 cost time: 6.180626153945923\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1629380 Vali Loss: 0.0332468 Test Loss: 0.1121480\n",
      "Validation loss decreased (0.033427 --> 0.033247).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1463883\n",
      "\tspeed: 0.0375s/iter; left time: 252.4726s\n",
      "\titers: 200, epoch: 9 | loss: 0.1533737\n",
      "\tspeed: 0.0103s/iter; left time: 68.5942s\n",
      "\titers: 300, epoch: 9 | loss: 0.1479795\n",
      "\tspeed: 0.0103s/iter; left time: 67.6010s\n",
      "\titers: 400, epoch: 9 | loss: 0.2016435\n",
      "\tspeed: 0.0103s/iter; left time: 66.5265s\n",
      "\titers: 500, epoch: 9 | loss: 0.1387911\n",
      "\tspeed: 0.0103s/iter; left time: 65.5093s\n",
      "Epoch: 9 cost time: 6.2134270668029785\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1612467 Vali Loss: 0.0334340 Test Loss: 0.1120927\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2028267\n",
      "\tspeed: 0.0362s/iter; left time: 223.1869s\n",
      "\titers: 200, epoch: 10 | loss: 0.1325454\n",
      "\tspeed: 0.0102s/iter; left time: 62.0378s\n",
      "\titers: 300, epoch: 10 | loss: 0.1501399\n",
      "\tspeed: 0.0102s/iter; left time: 61.0097s\n",
      "\titers: 400, epoch: 10 | loss: 0.1044933\n",
      "\tspeed: 0.0102s/iter; left time: 59.9516s\n",
      "\titers: 500, epoch: 10 | loss: 0.1074002\n",
      "\tspeed: 0.0102s/iter; left time: 58.9925s\n",
      "Epoch: 10 cost time: 6.089453935623169\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1624086 Vali Loss: 0.0333549 Test Loss: 0.1120792\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1523762\n",
      "\tspeed: 0.0359s/iter; left time: 201.3244s\n",
      "\titers: 200, epoch: 11 | loss: 0.1533564\n",
      "\tspeed: 0.0104s/iter; left time: 56.9486s\n",
      "\titers: 300, epoch: 11 | loss: 0.1951946\n",
      "\tspeed: 0.0104s/iter; left time: 55.9065s\n",
      "\titers: 400, epoch: 11 | loss: 0.1608341\n",
      "\tspeed: 0.0104s/iter; left time: 54.9291s\n",
      "\titers: 500, epoch: 11 | loss: 0.1590894\n",
      "\tspeed: 0.0103s/iter; left time: 53.7961s\n",
      "Epoch: 11 cost time: 6.184031248092651\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1606826 Vali Loss: 0.0332173 Test Loss: 0.1120477\n",
      "Validation loss decreased (0.033247 --> 0.033217).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1488593\n",
      "\tspeed: 0.0381s/iter; left time: 191.7601s\n",
      "\titers: 200, epoch: 12 | loss: 0.1528861\n",
      "\tspeed: 0.0104s/iter; left time: 51.3225s\n",
      "\titers: 300, epoch: 12 | loss: 0.1186375\n",
      "\tspeed: 0.0103s/iter; left time: 49.9947s\n",
      "\titers: 400, epoch: 12 | loss: 0.2596448\n",
      "\tspeed: 0.0104s/iter; left time: 49.0523s\n",
      "\titers: 500, epoch: 12 | loss: 0.1406447\n",
      "\tspeed: 0.0104s/iter; left time: 48.0941s\n",
      "Epoch: 12 cost time: 6.222867250442505\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1614960 Vali Loss: 0.0332660 Test Loss: 0.1121420\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2797479\n",
      "\tspeed: 0.0362s/iter; left time: 161.4507s\n",
      "\titers: 200, epoch: 13 | loss: 0.2159488\n",
      "\tspeed: 0.0104s/iter; left time: 45.1846s\n",
      "\titers: 300, epoch: 13 | loss: 0.1232705\n",
      "\tspeed: 0.0103s/iter; left time: 44.0659s\n",
      "\titers: 400, epoch: 13 | loss: 0.1761489\n",
      "\tspeed: 0.0104s/iter; left time: 43.1564s\n",
      "\titers: 500, epoch: 13 | loss: 0.1888717\n",
      "\tspeed: 0.0104s/iter; left time: 42.1540s\n",
      "Epoch: 13 cost time: 6.183291435241699\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1612591 Vali Loss: 0.0332204 Test Loss: 0.1121158\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1898113\n",
      "\tspeed: 0.0375s/iter; left time: 145.9151s\n",
      "\titers: 200, epoch: 14 | loss: 0.1162388\n",
      "\tspeed: 0.0103s/iter; left time: 39.2055s\n",
      "\titers: 300, epoch: 14 | loss: 0.1605986\n",
      "\tspeed: 0.0103s/iter; left time: 38.1144s\n",
      "\titers: 400, epoch: 14 | loss: 0.1655512\n",
      "\tspeed: 0.0103s/iter; left time: 37.1419s\n",
      "\titers: 500, epoch: 14 | loss: 0.1636503\n",
      "\tspeed: 0.0103s/iter; left time: 36.0586s\n",
      "Epoch: 14 cost time: 6.151569604873657\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1613626 Vali Loss: 0.0332831 Test Loss: 0.1121289\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11220263689756393, mae:0.20432178676128387\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2665744\n",
      "\tspeed: 0.0238s/iter; left time: 268.5686s\n",
      "\titers: 200, epoch: 1 | loss: 0.1625039\n",
      "\tspeed: 0.0116s/iter; left time: 130.3553s\n",
      "\titers: 300, epoch: 1 | loss: 0.1715275\n",
      "\tspeed: 0.0116s/iter; left time: 128.8272s\n",
      "\titers: 400, epoch: 1 | loss: 0.2192196\n",
      "\tspeed: 0.0116s/iter; left time: 127.5620s\n",
      "\titers: 500, epoch: 1 | loss: 0.2436446\n",
      "\tspeed: 0.0116s/iter; left time: 126.6041s\n",
      "Epoch: 1 cost time: 7.8839898109436035\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2679370 Vali Loss: 0.0518775 Test Loss: 0.1528189\n",
      "Validation loss decreased (inf --> 0.051878).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3995857\n",
      "\tspeed: 0.0374s/iter; left time: 401.3050s\n",
      "\titers: 200, epoch: 2 | loss: 0.3813215\n",
      "\tspeed: 0.0103s/iter; left time: 109.9289s\n",
      "\titers: 300, epoch: 2 | loss: 0.2108474\n",
      "\tspeed: 0.0103s/iter; left time: 108.3243s\n",
      "\titers: 400, epoch: 2 | loss: 0.3423056\n",
      "\tspeed: 0.0103s/iter; left time: 107.5345s\n",
      "\titers: 500, epoch: 2 | loss: 0.3010347\n",
      "\tspeed: 0.0103s/iter; left time: 106.0617s\n",
      "Epoch: 2 cost time: 6.198144197463989\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2640337 Vali Loss: 0.0516810 Test Loss: 0.1591369\n",
      "Validation loss decreased (0.051878 --> 0.051681).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1683755\n",
      "\tspeed: 0.0369s/iter; left time: 374.8009s\n",
      "\titers: 200, epoch: 3 | loss: 0.2236179\n",
      "\tspeed: 0.0103s/iter; left time: 103.7710s\n",
      "\titers: 300, epoch: 3 | loss: 0.2537792\n",
      "\tspeed: 0.0103s/iter; left time: 102.8964s\n",
      "\titers: 400, epoch: 3 | loss: 0.1891318\n",
      "\tspeed: 0.0103s/iter; left time: 101.8164s\n",
      "\titers: 500, epoch: 3 | loss: 0.1693430\n",
      "\tspeed: 0.0103s/iter; left time: 100.7406s\n",
      "Epoch: 3 cost time: 6.193397283554077\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2363307 Vali Loss: 0.0479477 Test Loss: 0.1425689\n",
      "Validation loss decreased (0.051681 --> 0.047948).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1491272\n",
      "\tspeed: 0.0362s/iter; left time: 347.1986s\n",
      "\titers: 200, epoch: 4 | loss: 0.1335760\n",
      "\tspeed: 0.0103s/iter; left time: 97.8524s\n",
      "\titers: 300, epoch: 4 | loss: 0.1788428\n",
      "\tspeed: 0.0104s/iter; left time: 97.2392s\n",
      "\titers: 400, epoch: 4 | loss: 0.2666694\n",
      "\tspeed: 0.0103s/iter; left time: 95.9990s\n",
      "\titers: 500, epoch: 4 | loss: 0.1612497\n",
      "\tspeed: 0.0103s/iter; left time: 94.7399s\n",
      "Epoch: 4 cost time: 6.177092552185059\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2194571 Vali Loss: 0.0480028 Test Loss: 0.1399659\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1819476\n",
      "\tspeed: 0.0363s/iter; left time: 327.2021s\n",
      "\titers: 200, epoch: 5 | loss: 0.1414169\n",
      "\tspeed: 0.0103s/iter; left time: 91.6610s\n",
      "\titers: 300, epoch: 5 | loss: 0.2882839\n",
      "\tspeed: 0.0103s/iter; left time: 90.8866s\n",
      "\titers: 400, epoch: 5 | loss: 0.1805757\n",
      "\tspeed: 0.0103s/iter; left time: 89.8999s\n",
      "\titers: 500, epoch: 5 | loss: 0.3003975\n",
      "\tspeed: 0.0103s/iter; left time: 88.6588s\n",
      "Epoch: 5 cost time: 6.187060117721558\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2111736 Vali Loss: 0.0444598 Test Loss: 0.1373426\n",
      "Validation loss decreased (0.047948 --> 0.044460).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1481559\n",
      "\tspeed: 0.0383s/iter; left time: 323.4452s\n",
      "\titers: 200, epoch: 6 | loss: 0.2157323\n",
      "\tspeed: 0.0103s/iter; left time: 85.9478s\n",
      "\titers: 300, epoch: 6 | loss: 0.2078682\n",
      "\tspeed: 0.0103s/iter; left time: 85.1614s\n",
      "\titers: 400, epoch: 6 | loss: 0.1530166\n",
      "\tspeed: 0.0103s/iter; left time: 84.2143s\n",
      "\titers: 500, epoch: 6 | loss: 0.1808219\n",
      "\tspeed: 0.0103s/iter; left time: 82.9722s\n",
      "Epoch: 6 cost time: 6.284023761749268\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2026788 Vali Loss: 0.0443465 Test Loss: 0.1363007\n",
      "Validation loss decreased (0.044460 --> 0.044347).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2109874\n",
      "\tspeed: 0.0381s/iter; left time: 300.1086s\n",
      "\titers: 200, epoch: 7 | loss: 0.1694206\n",
      "\tspeed: 0.0104s/iter; left time: 80.6958s\n",
      "\titers: 300, epoch: 7 | loss: 0.1643476\n",
      "\tspeed: 0.0104s/iter; left time: 79.6035s\n",
      "\titers: 400, epoch: 7 | loss: 0.1253503\n",
      "\tspeed: 0.0104s/iter; left time: 78.5946s\n",
      "\titers: 500, epoch: 7 | loss: 0.2321343\n",
      "\tspeed: 0.0104s/iter; left time: 77.5691s\n",
      "Epoch: 7 cost time: 6.247174263000488\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2003247 Vali Loss: 0.0449832 Test Loss: 0.1369952\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1437660\n",
      "\tspeed: 0.0375s/iter; left time: 274.1090s\n",
      "\titers: 200, epoch: 8 | loss: 0.1740367\n",
      "\tspeed: 0.0103s/iter; left time: 74.0168s\n",
      "\titers: 300, epoch: 8 | loss: 0.2138445\n",
      "\tspeed: 0.0103s/iter; left time: 73.0096s\n",
      "\titers: 400, epoch: 8 | loss: 0.2274852\n",
      "\tspeed: 0.0103s/iter; left time: 71.9792s\n",
      "\titers: 500, epoch: 8 | loss: 0.2640571\n",
      "\tspeed: 0.0103s/iter; left time: 70.9471s\n",
      "Epoch: 8 cost time: 6.128490209579468\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1992223 Vali Loss: 0.0450322 Test Loss: 0.1366178\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1558319\n",
      "\tspeed: 0.0360s/iter; left time: 242.5639s\n",
      "\titers: 200, epoch: 9 | loss: 0.2155426\n",
      "\tspeed: 0.0103s/iter; left time: 68.1641s\n",
      "\titers: 300, epoch: 9 | loss: 0.1660830\n",
      "\tspeed: 0.0103s/iter; left time: 67.0805s\n",
      "\titers: 400, epoch: 9 | loss: 0.1353658\n",
      "\tspeed: 0.0103s/iter; left time: 66.0729s\n",
      "\titers: 500, epoch: 9 | loss: 0.2189843\n",
      "\tspeed: 0.0103s/iter; left time: 65.0490s\n",
      "Epoch: 9 cost time: 6.120204448699951\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1988193 Vali Loss: 0.0453159 Test Loss: 0.1367790\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1364724338054657, mae:0.23290593922138214\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3146958\n",
      "\tspeed: 0.0250s/iter; left time: 282.2361s\n",
      "\titers: 200, epoch: 1 | loss: 0.2838693\n",
      "\tspeed: 0.0129s/iter; left time: 145.0489s\n",
      "\titers: 300, epoch: 1 | loss: 0.1965312\n",
      "\tspeed: 0.0129s/iter; left time: 143.3113s\n",
      "\titers: 400, epoch: 1 | loss: 0.1757296\n",
      "\tspeed: 0.0129s/iter; left time: 142.2966s\n",
      "\titers: 500, epoch: 1 | loss: 0.2426057\n",
      "\tspeed: 0.0130s/iter; left time: 141.2103s\n",
      "Epoch: 1 cost time: 8.621425867080688\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3048955 Vali Loss: 0.0569524 Test Loss: 0.1670164\n",
      "Validation loss decreased (inf --> 0.056952).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3564685\n",
      "\tspeed: 0.0404s/iter; left time: 433.6651s\n",
      "\titers: 200, epoch: 2 | loss: 0.2490194\n",
      "\tspeed: 0.0119s/iter; left time: 125.9837s\n",
      "\titers: 300, epoch: 2 | loss: 0.2912205\n",
      "\tspeed: 0.0119s/iter; left time: 125.0700s\n",
      "\titers: 400, epoch: 2 | loss: 0.2075615\n",
      "\tspeed: 0.0119s/iter; left time: 123.7893s\n",
      "\titers: 500, epoch: 2 | loss: 0.3259264\n",
      "\tspeed: 0.0119s/iter; left time: 122.4567s\n",
      "Epoch: 2 cost time: 7.099339485168457\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3621097 Vali Loss: 0.0655616 Test Loss: 0.2010690\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.4145622\n",
      "\tspeed: 0.0396s/iter; left time: 402.4706s\n",
      "\titers: 200, epoch: 3 | loss: 0.4263114\n",
      "\tspeed: 0.0119s/iter; left time: 119.7856s\n",
      "\titers: 300, epoch: 3 | loss: 0.1655468\n",
      "\tspeed: 0.0119s/iter; left time: 118.3887s\n",
      "\titers: 400, epoch: 3 | loss: 0.3101531\n",
      "\tspeed: 0.0119s/iter; left time: 117.1992s\n",
      "\titers: 500, epoch: 3 | loss: 0.2383604\n",
      "\tspeed: 0.0119s/iter; left time: 116.1098s\n",
      "Epoch: 3 cost time: 7.126617431640625\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3143142 Vali Loss: 0.0663429 Test Loss: 0.1853718\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2161061\n",
      "\tspeed: 0.0400s/iter; left time: 384.0456s\n",
      "\titers: 200, epoch: 4 | loss: 0.3722821\n",
      "\tspeed: 0.0120s/iter; left time: 113.5798s\n",
      "\titers: 300, epoch: 4 | loss: 0.2015136\n",
      "\tspeed: 0.0119s/iter; left time: 112.0137s\n",
      "\titers: 400, epoch: 4 | loss: 0.1925986\n",
      "\tspeed: 0.0123s/iter; left time: 114.4210s\n",
      "\titers: 500, epoch: 4 | loss: 0.2151652\n",
      "\tspeed: 0.0130s/iter; left time: 119.1478s\n",
      "Epoch: 4 cost time: 7.370324611663818\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2983768 Vali Loss: 0.0602570 Test Loss: 0.1767666\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16728396713733673, mae:0.26014572381973267\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1621534\n",
      "\tspeed: 0.0243s/iter; left time: 274.4358s\n",
      "\titers: 200, epoch: 1 | loss: 0.4235111\n",
      "\tspeed: 0.0126s/iter; left time: 140.6674s\n",
      "\titers: 300, epoch: 1 | loss: 0.2352856\n",
      "\tspeed: 0.0125s/iter; left time: 139.2953s\n",
      "\titers: 400, epoch: 1 | loss: 0.4525290\n",
      "\tspeed: 0.0126s/iter; left time: 138.0757s\n",
      "\titers: 500, epoch: 1 | loss: 0.2929428\n",
      "\tspeed: 0.0126s/iter; left time: 137.1519s\n",
      "Epoch: 1 cost time: 8.386117219924927\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3089340 Vali Loss: 0.0703744 Test Loss: 0.2047924\n",
      "Validation loss decreased (inf --> 0.070374).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2462121\n",
      "\tspeed: 0.0425s/iter; left time: 456.2376s\n",
      "\titers: 200, epoch: 2 | loss: 0.1961503\n",
      "\tspeed: 0.0115s/iter; left time: 122.1841s\n",
      "\titers: 300, epoch: 2 | loss: 0.1915278\n",
      "\tspeed: 0.0119s/iter; left time: 125.4259s\n",
      "\titers: 400, epoch: 2 | loss: 0.3650100\n",
      "\tspeed: 0.0126s/iter; left time: 131.0318s\n",
      "\titers: 500, epoch: 2 | loss: 0.5563440\n",
      "\tspeed: 0.0126s/iter; left time: 129.7735s\n",
      "Epoch: 2 cost time: 7.2849812507629395\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3197718 Vali Loss: 0.0647909 Test Loss: 0.1861613\n",
      "Validation loss decreased (0.070374 --> 0.064791).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3419395\n",
      "\tspeed: 0.0404s/iter; left time: 410.0566s\n",
      "\titers: 200, epoch: 3 | loss: 0.1741049\n",
      "\tspeed: 0.0115s/iter; left time: 116.0088s\n",
      "\titers: 300, epoch: 3 | loss: 0.3266390\n",
      "\tspeed: 0.0115s/iter; left time: 114.9936s\n",
      "\titers: 400, epoch: 3 | loss: 0.2794829\n",
      "\tspeed: 0.0115s/iter; left time: 113.5811s\n",
      "\titers: 500, epoch: 3 | loss: 0.4134959\n",
      "\tspeed: 0.0115s/iter; left time: 112.7265s\n",
      "Epoch: 3 cost time: 6.915549278259277\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3013770 Vali Loss: 0.0672429 Test Loss: 0.1973960\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2597378\n",
      "\tspeed: 0.0390s/iter; left time: 373.5879s\n",
      "\titers: 200, epoch: 4 | loss: 0.1957911\n",
      "\tspeed: 0.0115s/iter; left time: 109.1132s\n",
      "\titers: 300, epoch: 4 | loss: 0.2042681\n",
      "\tspeed: 0.0115s/iter; left time: 108.1493s\n",
      "\titers: 400, epoch: 4 | loss: 0.2713029\n",
      "\tspeed: 0.0115s/iter; left time: 106.7402s\n",
      "\titers: 500, epoch: 4 | loss: 0.2831668\n",
      "\tspeed: 0.0115s/iter; left time: 105.7765s\n",
      "Epoch: 4 cost time: 6.853731393814087\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.3059611 Vali Loss: 0.0696376 Test Loss: 0.1842965\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.3049182\n",
      "\tspeed: 0.0373s/iter; left time: 336.8962s\n",
      "\titers: 200, epoch: 5 | loss: 0.2351806\n",
      "\tspeed: 0.0115s/iter; left time: 102.3564s\n",
      "\titers: 300, epoch: 5 | loss: 0.4495172\n",
      "\tspeed: 0.0115s/iter; left time: 101.3686s\n",
      "\titers: 400, epoch: 5 | loss: 0.2376069\n",
      "\tspeed: 0.0115s/iter; left time: 100.1688s\n",
      "\titers: 500, epoch: 5 | loss: 0.2409071\n",
      "\tspeed: 0.0115s/iter; left time: 98.9924s\n",
      "Epoch: 5 cost time: 6.812418460845947\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2914947 Vali Loss: 0.0645789 Test Loss: 0.1771692\n",
      "Validation loss decreased (0.064791 --> 0.064579).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3137941\n",
      "\tspeed: 0.0387s/iter; left time: 327.3756s\n",
      "\titers: 200, epoch: 6 | loss: 0.3051760\n",
      "\tspeed: 0.0114s/iter; left time: 95.5769s\n",
      "\titers: 300, epoch: 6 | loss: 0.1748664\n",
      "\tspeed: 0.0115s/iter; left time: 94.5825s\n",
      "\titers: 400, epoch: 6 | loss: 0.2137271\n",
      "\tspeed: 0.0115s/iter; left time: 93.3676s\n",
      "\titers: 500, epoch: 6 | loss: 0.2155635\n",
      "\tspeed: 0.0117s/iter; left time: 93.9591s\n",
      "Epoch: 6 cost time: 6.966285467147827\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2856273 Vali Loss: 0.0634681 Test Loss: 0.1763937\n",
      "Validation loss decreased (0.064579 --> 0.063468).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2200189\n",
      "\tspeed: 0.0394s/iter; left time: 310.4943s\n",
      "\titers: 200, epoch: 7 | loss: 0.3610333\n",
      "\tspeed: 0.0115s/iter; left time: 89.5880s\n",
      "\titers: 300, epoch: 7 | loss: 0.2429709\n",
      "\tspeed: 0.0115s/iter; left time: 88.3471s\n",
      "\titers: 400, epoch: 7 | loss: 0.2551625\n",
      "\tspeed: 0.0115s/iter; left time: 87.3280s\n",
      "\titers: 500, epoch: 7 | loss: 0.2693877\n",
      "\tspeed: 0.0115s/iter; left time: 85.9410s\n",
      "Epoch: 7 cost time: 6.869879722595215\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2844154 Vali Loss: 0.0636506 Test Loss: 0.1754195\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1919257\n",
      "\tspeed: 0.0385s/iter; left time: 281.6687s\n",
      "\titers: 200, epoch: 8 | loss: 0.2944022\n",
      "\tspeed: 0.0115s/iter; left time: 83.1066s\n",
      "\titers: 300, epoch: 8 | loss: 0.2542833\n",
      "\tspeed: 0.0115s/iter; left time: 81.6238s\n",
      "\titers: 400, epoch: 8 | loss: 0.2601533\n",
      "\tspeed: 0.0115s/iter; left time: 80.4566s\n",
      "\titers: 500, epoch: 8 | loss: 0.2570642\n",
      "\tspeed: 0.0115s/iter; left time: 79.3134s\n",
      "Epoch: 8 cost time: 6.83769416809082\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2814559 Vali Loss: 0.0634190 Test Loss: 0.1761353\n",
      "Validation loss decreased (0.063468 --> 0.063419).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3800250\n",
      "\tspeed: 0.0392s/iter; left time: 264.5671s\n",
      "\titers: 200, epoch: 9 | loss: 0.1612675\n",
      "\tspeed: 0.0122s/iter; left time: 81.3097s\n",
      "\titers: 300, epoch: 9 | loss: 0.3236064\n",
      "\tspeed: 0.0114s/iter; left time: 74.8794s\n",
      "\titers: 400, epoch: 9 | loss: 0.3164397\n",
      "\tspeed: 0.0114s/iter; left time: 73.7350s\n",
      "\titers: 500, epoch: 9 | loss: 0.3997166\n",
      "\tspeed: 0.0115s/iter; left time: 72.6159s\n",
      "Epoch: 9 cost time: 7.036710739135742\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2817927 Vali Loss: 0.0627447 Test Loss: 0.1758721\n",
      "Validation loss decreased (0.063419 --> 0.062745).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2104461\n",
      "\tspeed: 0.0382s/iter; left time: 235.7708s\n",
      "\titers: 200, epoch: 10 | loss: 0.2363057\n",
      "\tspeed: 0.0114s/iter; left time: 69.4060s\n",
      "\titers: 300, epoch: 10 | loss: 0.2046684\n",
      "\tspeed: 0.0115s/iter; left time: 68.4971s\n",
      "\titers: 400, epoch: 10 | loss: 0.2702019\n",
      "\tspeed: 0.0115s/iter; left time: 67.3419s\n",
      "\titers: 500, epoch: 10 | loss: 0.2903891\n",
      "\tspeed: 0.0114s/iter; left time: 65.9359s\n",
      "Epoch: 10 cost time: 6.830387115478516\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2824371 Vali Loss: 0.0631095 Test Loss: 0.1755162\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1698792\n",
      "\tspeed: 0.0396s/iter; left time: 221.7396s\n",
      "\titers: 200, epoch: 11 | loss: 0.2862641\n",
      "\tspeed: 0.0114s/iter; left time: 62.8940s\n",
      "\titers: 300, epoch: 11 | loss: 0.2626987\n",
      "\tspeed: 0.0115s/iter; left time: 61.8995s\n",
      "\titers: 400, epoch: 11 | loss: 0.3237939\n",
      "\tspeed: 0.0115s/iter; left time: 60.7629s\n",
      "\titers: 500, epoch: 11 | loss: 0.4603405\n",
      "\tspeed: 0.0115s/iter; left time: 59.6536s\n",
      "Epoch: 11 cost time: 6.8439812660217285\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2805423 Vali Loss: 0.0631211 Test Loss: 0.1755228\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2366856\n",
      "\tspeed: 0.0385s/iter; left time: 193.7130s\n",
      "\titers: 200, epoch: 12 | loss: 0.2605909\n",
      "\tspeed: 0.0115s/iter; left time: 56.6890s\n",
      "\titers: 300, epoch: 12 | loss: 0.1787814\n",
      "\tspeed: 0.0115s/iter; left time: 55.7159s\n",
      "\titers: 400, epoch: 12 | loss: 0.3991050\n",
      "\tspeed: 0.0118s/iter; left time: 56.0141s\n",
      "\titers: 500, epoch: 12 | loss: 0.2452077\n",
      "\tspeed: 0.0125s/iter; left time: 58.0412s\n",
      "Epoch: 12 cost time: 7.086598634719849\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2828113 Vali Loss: 0.0631685 Test Loss: 0.1755223\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17613042891025543, mae:0.26716727018356323\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3310875\n",
      "\tspeed: 0.0227s/iter; left time: 256.6779s\n",
      "\titers: 200, epoch: 1 | loss: 0.3205411\n",
      "\tspeed: 0.0109s/iter; left time: 121.5818s\n",
      "\titers: 300, epoch: 1 | loss: 0.4598767\n",
      "\tspeed: 0.0108s/iter; left time: 120.1761s\n",
      "\titers: 400, epoch: 1 | loss: 0.3615898\n",
      "\tspeed: 0.0108s/iter; left time: 118.9683s\n",
      "\titers: 500, epoch: 1 | loss: 0.2669631\n",
      "\tspeed: 0.0108s/iter; left time: 117.8779s\n",
      "Epoch: 1 cost time: 7.409855365753174\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3139204 Vali Loss: 0.0577420 Test Loss: 0.1718128\n",
      "Validation loss decreased (inf --> 0.057742).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2738406\n",
      "\tspeed: 0.0376s/iter; left time: 403.1893s\n",
      "\titers: 200, epoch: 2 | loss: 0.3333329\n",
      "\tspeed: 0.0109s/iter; left time: 115.6976s\n",
      "\titers: 300, epoch: 2 | loss: 0.1958640\n",
      "\tspeed: 0.0109s/iter; left time: 114.5756s\n",
      "\titers: 400, epoch: 2 | loss: 0.2421278\n",
      "\tspeed: 0.0109s/iter; left time: 113.6028s\n",
      "\titers: 500, epoch: 2 | loss: 0.2657471\n",
      "\tspeed: 0.0109s/iter; left time: 112.1883s\n",
      "Epoch: 2 cost time: 6.524508714675903\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3191139 Vali Loss: 0.0683331 Test Loss: 0.1963060\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1974169\n",
      "\tspeed: 0.0377s/iter; left time: 383.5223s\n",
      "\titers: 200, epoch: 3 | loss: 0.2441244\n",
      "\tspeed: 0.0109s/iter; left time: 109.1723s\n",
      "\titers: 300, epoch: 3 | loss: 0.3068202\n",
      "\tspeed: 0.0108s/iter; left time: 107.7658s\n",
      "\titers: 400, epoch: 3 | loss: 0.3356909\n",
      "\tspeed: 0.0108s/iter; left time: 106.9533s\n",
      "\titers: 500, epoch: 3 | loss: 0.3248774\n",
      "\tspeed: 0.0108s/iter; left time: 105.7025s\n",
      "Epoch: 3 cost time: 6.500051736831665\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2982767 Vali Loss: 0.0630827 Test Loss: 0.1717639\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2849285\n",
      "\tspeed: 0.0361s/iter; left time: 346.4220s\n",
      "\titers: 200, epoch: 4 | loss: 0.1341906\n",
      "\tspeed: 0.0109s/iter; left time: 103.0152s\n",
      "\titers: 300, epoch: 4 | loss: 0.3374710\n",
      "\tspeed: 0.0108s/iter; left time: 101.7590s\n",
      "\titers: 400, epoch: 4 | loss: 0.4772728\n",
      "\tspeed: 0.0109s/iter; left time: 100.9208s\n",
      "\titers: 500, epoch: 4 | loss: 0.3229633\n",
      "\tspeed: 0.0108s/iter; left time: 99.6354s\n",
      "Epoch: 4 cost time: 6.459392309188843\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2769645 Vali Loss: 0.0623952 Test Loss: 0.1707577\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17203237116336823, mae:0.27029332518577576\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2435709\n",
      "\tspeed: 0.0282s/iter; left time: 318.5640s\n",
      "\titers: 200, epoch: 1 | loss: 0.1911342\n",
      "\tspeed: 0.0163s/iter; left time: 182.0859s\n",
      "\titers: 300, epoch: 1 | loss: 0.3567736\n",
      "\tspeed: 0.0163s/iter; left time: 180.9973s\n",
      "\titers: 400, epoch: 1 | loss: 0.2810073\n",
      "\tspeed: 0.0163s/iter; left time: 178.8012s\n",
      "\titers: 500, epoch: 1 | loss: 0.2252716\n",
      "\tspeed: 0.0163s/iter; left time: 177.7329s\n",
      "Epoch: 1 cost time: 10.522274494171143\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2713187 Vali Loss: 0.0512629 Test Loss: 0.1473980\n",
      "Validation loss decreased (inf --> 0.051263).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3252781\n",
      "\tspeed: 0.0512s/iter; left time: 549.7177s\n",
      "\titers: 200, epoch: 2 | loss: 0.2741295\n",
      "\tspeed: 0.0163s/iter; left time: 173.8073s\n",
      "\titers: 300, epoch: 2 | loss: 0.2366938\n",
      "\tspeed: 0.0149s/iter; left time: 156.8605s\n",
      "\titers: 400, epoch: 2 | loss: 0.1830042\n",
      "\tspeed: 0.0161s/iter; left time: 168.1395s\n",
      "\titers: 500, epoch: 2 | loss: 0.3432406\n",
      "\tspeed: 0.0163s/iter; left time: 168.4525s\n",
      "Epoch: 2 cost time: 9.43977665901184\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2852060 Vali Loss: 0.0700450 Test Loss: 0.2050198\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.5130218\n",
      "\tspeed: 0.0469s/iter; left time: 476.5521s\n",
      "\titers: 200, epoch: 3 | loss: 0.3435275\n",
      "\tspeed: 0.0145s/iter; left time: 145.6535s\n",
      "\titers: 300, epoch: 3 | loss: 0.1621601\n",
      "\tspeed: 0.0145s/iter; left time: 144.3435s\n",
      "\titers: 400, epoch: 3 | loss: 0.2179836\n",
      "\tspeed: 0.0145s/iter; left time: 142.7286s\n",
      "\titers: 500, epoch: 3 | loss: 0.2560741\n",
      "\tspeed: 0.0145s/iter; left time: 141.2412s\n",
      "Epoch: 3 cost time: 8.533675909042358\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3024391 Vali Loss: 0.0615400 Test Loss: 0.1726220\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2690701\n",
      "\tspeed: 0.0455s/iter; left time: 436.5145s\n",
      "\titers: 200, epoch: 4 | loss: 0.2230475\n",
      "\tspeed: 0.0145s/iter; left time: 137.6738s\n",
      "\titers: 300, epoch: 4 | loss: 0.2580901\n",
      "\tspeed: 0.0145s/iter; left time: 136.1329s\n",
      "\titers: 400, epoch: 4 | loss: 0.2845585\n",
      "\tspeed: 0.0145s/iter; left time: 134.6614s\n",
      "\titers: 500, epoch: 4 | loss: 0.2436915\n",
      "\tspeed: 0.0146s/iter; left time: 133.8758s\n",
      "Epoch: 4 cost time: 8.534708976745605\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2748295 Vali Loss: 0.0567464 Test Loss: 0.1649906\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14764295518398285, mae:0.2447245717048645\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3579422\n",
      "\tspeed: 0.0260s/iter; left time: 293.9761s\n",
      "\titers: 200, epoch: 1 | loss: 0.3188561\n",
      "\tspeed: 0.0142s/iter; left time: 159.3032s\n",
      "\titers: 300, epoch: 1 | loss: 0.1412449\n",
      "\tspeed: 0.0142s/iter; left time: 157.3305s\n",
      "\titers: 400, epoch: 1 | loss: 0.3408230\n",
      "\tspeed: 0.0142s/iter; left time: 156.5359s\n",
      "\titers: 500, epoch: 1 | loss: 0.2369466\n",
      "\tspeed: 0.0142s/iter; left time: 154.4089s\n",
      "Epoch: 1 cost time: 9.32254958152771\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2422368 Vali Loss: 0.0484057 Test Loss: 0.1495427\n",
      "Validation loss decreased (inf --> 0.048406).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1942520\n",
      "\tspeed: 0.0461s/iter; left time: 494.3414s\n",
      "\titers: 200, epoch: 2 | loss: 0.2967184\n",
      "\tspeed: 0.0143s/iter; left time: 151.8953s\n",
      "\titers: 300, epoch: 2 | loss: 0.2182838\n",
      "\tspeed: 0.0142s/iter; left time: 149.8494s\n",
      "\titers: 400, epoch: 2 | loss: 0.2311323\n",
      "\tspeed: 0.0143s/iter; left time: 148.7084s\n",
      "\titers: 500, epoch: 2 | loss: 0.2057789\n",
      "\tspeed: 0.0142s/iter; left time: 147.1770s\n",
      "Epoch: 2 cost time: 8.450984239578247\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2377276 Vali Loss: 0.0728795 Test Loss: 0.2161092\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3237036\n",
      "\tspeed: 0.0477s/iter; left time: 484.6333s\n",
      "\titers: 200, epoch: 3 | loss: 0.1564135\n",
      "\tspeed: 0.0143s/iter; left time: 144.0003s\n",
      "\titers: 300, epoch: 3 | loss: 0.3661249\n",
      "\tspeed: 0.0143s/iter; left time: 142.3225s\n",
      "\titers: 400, epoch: 3 | loss: 0.1727637\n",
      "\tspeed: 0.0143s/iter; left time: 140.9605s\n",
      "\titers: 500, epoch: 3 | loss: 0.2421882\n",
      "\tspeed: 0.0143s/iter; left time: 139.6944s\n",
      "Epoch: 3 cost time: 8.454082489013672\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2786699 Vali Loss: 0.0570485 Test Loss: 0.1763236\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2777259\n",
      "\tspeed: 0.0450s/iter; left time: 431.5252s\n",
      "\titers: 200, epoch: 4 | loss: 0.2660138\n",
      "\tspeed: 0.0143s/iter; left time: 135.2996s\n",
      "\titers: 300, epoch: 4 | loss: 0.3011681\n",
      "\tspeed: 0.0143s/iter; left time: 134.1504s\n",
      "\titers: 400, epoch: 4 | loss: 0.1803908\n",
      "\tspeed: 0.0142s/iter; left time: 132.2711s\n",
      "\titers: 500, epoch: 4 | loss: 0.1449819\n",
      "\tspeed: 0.0143s/iter; left time: 131.0388s\n",
      "Epoch: 4 cost time: 8.4141526222229\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2481087 Vali Loss: 0.0525879 Test Loss: 0.1558835\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.149746373295784, mae:0.24280180037021637\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1354365\n",
      "\tspeed: 0.0258s/iter; left time: 292.0768s\n",
      "\titers: 200, epoch: 1 | loss: 0.3682787\n",
      "\tspeed: 0.0142s/iter; left time: 158.9420s\n",
      "\titers: 300, epoch: 1 | loss: 0.2754350\n",
      "\tspeed: 0.0142s/iter; left time: 157.2296s\n",
      "\titers: 400, epoch: 1 | loss: 0.2626590\n",
      "\tspeed: 0.0142s/iter; left time: 156.1920s\n",
      "\titers: 500, epoch: 1 | loss: 0.2970988\n",
      "\tspeed: 0.0142s/iter; left time: 154.6230s\n",
      "Epoch: 1 cost time: 9.303525447845459\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2840688 Vali Loss: 0.0640136 Test Loss: 0.1812901\n",
      "Validation loss decreased (inf --> 0.064014).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2767709\n",
      "\tspeed: 0.0461s/iter; left time: 494.8299s\n",
      "\titers: 200, epoch: 2 | loss: 0.1961362\n",
      "\tspeed: 0.0142s/iter; left time: 151.0802s\n",
      "\titers: 300, epoch: 2 | loss: 0.1926715\n",
      "\tspeed: 0.0142s/iter; left time: 149.4947s\n",
      "\titers: 400, epoch: 2 | loss: 0.2650085\n",
      "\tspeed: 0.0142s/iter; left time: 148.0944s\n",
      "\titers: 500, epoch: 2 | loss: 0.3407041\n",
      "\tspeed: 0.0142s/iter; left time: 146.8379s\n",
      "Epoch: 2 cost time: 8.41737675666809\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2870881 Vali Loss: 0.0582459 Test Loss: 0.1693398\n",
      "Validation loss decreased (0.064014 --> 0.058246).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2637312\n",
      "\tspeed: 0.0462s/iter; left time: 469.5869s\n",
      "\titers: 200, epoch: 3 | loss: 0.3161782\n",
      "\tspeed: 0.0142s/iter; left time: 142.7490s\n",
      "\titers: 300, epoch: 3 | loss: 0.2022959\n",
      "\tspeed: 0.0142s/iter; left time: 141.7467s\n",
      "\titers: 400, epoch: 3 | loss: 0.2054545\n",
      "\tspeed: 0.0142s/iter; left time: 139.8869s\n",
      "\titers: 500, epoch: 3 | loss: 0.2705434\n",
      "\tspeed: 0.0142s/iter; left time: 138.6895s\n",
      "Epoch: 3 cost time: 8.363610029220581\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2598173 Vali Loss: 0.0578059 Test Loss: 0.1630261\n",
      "Validation loss decreased (0.058246 --> 0.057806).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2894474\n",
      "\tspeed: 0.0453s/iter; left time: 434.7892s\n",
      "\titers: 200, epoch: 4 | loss: 0.2260659\n",
      "\tspeed: 0.0143s/iter; left time: 135.7626s\n",
      "\titers: 300, epoch: 4 | loss: 0.2424885\n",
      "\tspeed: 0.0143s/iter; left time: 134.1040s\n",
      "\titers: 400, epoch: 4 | loss: 0.2020031\n",
      "\tspeed: 0.0142s/iter; left time: 132.2669s\n",
      "\titers: 500, epoch: 4 | loss: 0.1781809\n",
      "\tspeed: 0.0141s/iter; left time: 129.9907s\n",
      "Epoch: 4 cost time: 8.393113613128662\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2434683 Vali Loss: 0.0540485 Test Loss: 0.1607744\n",
      "Validation loss decreased (0.057806 --> 0.054049).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2027901\n",
      "\tspeed: 0.0462s/iter; left time: 416.6592s\n",
      "\titers: 200, epoch: 5 | loss: 0.3091655\n",
      "\tspeed: 0.0142s/iter; left time: 127.0882s\n",
      "\titers: 300, epoch: 5 | loss: 0.1973173\n",
      "\tspeed: 0.0143s/iter; left time: 125.8944s\n",
      "\titers: 400, epoch: 5 | loss: 0.3333797\n",
      "\tspeed: 0.0143s/iter; left time: 124.2778s\n",
      "\titers: 500, epoch: 5 | loss: 0.2070854\n",
      "\tspeed: 0.0142s/iter; left time: 122.3446s\n",
      "Epoch: 5 cost time: 8.42149043083191\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2325874 Vali Loss: 0.0519195 Test Loss: 0.1577916\n",
      "Validation loss decreased (0.054049 --> 0.051919).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2825751\n",
      "\tspeed: 0.0466s/iter; left time: 393.7059s\n",
      "\titers: 200, epoch: 6 | loss: 0.2050829\n",
      "\tspeed: 0.0141s/iter; left time: 117.9566s\n",
      "\titers: 300, epoch: 6 | loss: 0.1434190\n",
      "\tspeed: 0.0142s/iter; left time: 116.8035s\n",
      "\titers: 400, epoch: 6 | loss: 0.2691714\n",
      "\tspeed: 0.0141s/iter; left time: 115.3095s\n",
      "\titers: 500, epoch: 6 | loss: 0.1810805\n",
      "\tspeed: 0.0141s/iter; left time: 113.6816s\n",
      "Epoch: 6 cost time: 8.358530282974243\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2260295 Vali Loss: 0.0501509 Test Loss: 0.1582185\n",
      "Validation loss decreased (0.051919 --> 0.050151).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3239396\n",
      "\tspeed: 0.0480s/iter; left time: 378.5184s\n",
      "\titers: 200, epoch: 7 | loss: 0.1413668\n",
      "\tspeed: 0.0160s/iter; left time: 124.5593s\n",
      "\titers: 300, epoch: 7 | loss: 0.1768094\n",
      "\tspeed: 0.0146s/iter; left time: 112.1163s\n",
      "\titers: 400, epoch: 7 | loss: 0.3414020\n",
      "\tspeed: 0.0143s/iter; left time: 108.2872s\n",
      "\titers: 500, epoch: 7 | loss: 0.2191450\n",
      "\tspeed: 0.0143s/iter; left time: 106.8693s\n",
      "Epoch: 7 cost time: 8.821830749511719\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2243734 Vali Loss: 0.0499334 Test Loss: 0.1556317\n",
      "Validation loss decreased (0.050151 --> 0.049933).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1709580\n",
      "\tspeed: 0.0453s/iter; left time: 331.1416s\n",
      "\titers: 200, epoch: 8 | loss: 0.2004823\n",
      "\tspeed: 0.0142s/iter; left time: 102.7290s\n",
      "\titers: 300, epoch: 8 | loss: 0.2172453\n",
      "\tspeed: 0.0142s/iter; left time: 101.1824s\n",
      "\titers: 400, epoch: 8 | loss: 0.1888614\n",
      "\tspeed: 0.0142s/iter; left time: 99.6524s\n",
      "\titers: 500, epoch: 8 | loss: 0.1446288\n",
      "\tspeed: 0.0142s/iter; left time: 98.2593s\n",
      "Epoch: 8 cost time: 8.40392780303955\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2232421 Vali Loss: 0.0503342 Test Loss: 0.1561412\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2895650\n",
      "\tspeed: 0.0466s/iter; left time: 313.8570s\n",
      "\titers: 200, epoch: 9 | loss: 0.1684789\n",
      "\tspeed: 0.0142s/iter; left time: 94.5205s\n",
      "\titers: 300, epoch: 9 | loss: 0.1635505\n",
      "\tspeed: 0.0143s/iter; left time: 93.3174s\n",
      "\titers: 400, epoch: 9 | loss: 0.1515073\n",
      "\tspeed: 0.0142s/iter; left time: 91.6524s\n",
      "\titers: 500, epoch: 9 | loss: 0.2116920\n",
      "\tspeed: 0.0142s/iter; left time: 90.1513s\n",
      "Epoch: 9 cost time: 8.41294002532959\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2210441 Vali Loss: 0.0499185 Test Loss: 0.1554581\n",
      "Validation loss decreased (0.049933 --> 0.049918).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2955706\n",
      "\tspeed: 0.0450s/iter; left time: 277.8465s\n",
      "\titers: 200, epoch: 10 | loss: 0.2255763\n",
      "\tspeed: 0.0142s/iter; left time: 85.9282s\n",
      "\titers: 300, epoch: 10 | loss: 0.1939872\n",
      "\tspeed: 0.0142s/iter; left time: 84.6683s\n",
      "\titers: 400, epoch: 10 | loss: 0.2501310\n",
      "\tspeed: 0.0142s/iter; left time: 83.0876s\n",
      "\titers: 500, epoch: 10 | loss: 0.2223947\n",
      "\tspeed: 0.0141s/iter; left time: 81.5770s\n",
      "Epoch: 10 cost time: 8.3395357131958\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2197289 Vali Loss: 0.0500757 Test Loss: 0.1554331\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2479482\n",
      "\tspeed: 0.0447s/iter; left time: 250.4171s\n",
      "\titers: 200, epoch: 11 | loss: 0.1726053\n",
      "\tspeed: 0.0141s/iter; left time: 77.8081s\n",
      "\titers: 300, epoch: 11 | loss: 0.1634731\n",
      "\tspeed: 0.0141s/iter; left time: 76.3743s\n",
      "\titers: 400, epoch: 11 | loss: 0.2541782\n",
      "\tspeed: 0.0142s/iter; left time: 75.0262s\n",
      "\titers: 500, epoch: 11 | loss: 0.1446469\n",
      "\tspeed: 0.0141s/iter; left time: 73.5528s\n",
      "Epoch: 11 cost time: 8.322649240493774\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2199666 Vali Loss: 0.0500236 Test Loss: 0.1554699\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1390455\n",
      "\tspeed: 0.0449s/iter; left time: 225.8550s\n",
      "\titers: 200, epoch: 12 | loss: 0.1907740\n",
      "\tspeed: 0.0141s/iter; left time: 69.7550s\n",
      "\titers: 300, epoch: 12 | loss: 0.2275511\n",
      "\tspeed: 0.0141s/iter; left time: 68.2532s\n",
      "\titers: 400, epoch: 12 | loss: 0.1400598\n",
      "\tspeed: 0.0142s/iter; left time: 66.9738s\n",
      "\titers: 500, epoch: 12 | loss: 0.2891672\n",
      "\tspeed: 0.0142s/iter; left time: 65.8630s\n",
      "Epoch: 12 cost time: 8.354542016983032\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2222741 Vali Loss: 0.0498362 Test Loss: 0.1554103\n",
      "Validation loss decreased (0.049918 --> 0.049836).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1693378\n",
      "\tspeed: 0.0453s/iter; left time: 202.1412s\n",
      "\titers: 200, epoch: 13 | loss: 0.1841998\n",
      "\tspeed: 0.0142s/iter; left time: 62.0056s\n",
      "\titers: 300, epoch: 13 | loss: 0.2378055\n",
      "\tspeed: 0.0142s/iter; left time: 60.4549s\n",
      "\titers: 400, epoch: 13 | loss: 0.2329025\n",
      "\tspeed: 0.0142s/iter; left time: 59.0315s\n",
      "\titers: 500, epoch: 13 | loss: 0.2395816\n",
      "\tspeed: 0.0142s/iter; left time: 57.6496s\n",
      "Epoch: 13 cost time: 8.4007089138031\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2195402 Vali Loss: 0.0501228 Test Loss: 0.1554241\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1925913\n",
      "\tspeed: 0.0479s/iter; left time: 186.2290s\n",
      "\titers: 200, epoch: 14 | loss: 0.2345678\n",
      "\tspeed: 0.0142s/iter; left time: 53.7091s\n",
      "\titers: 300, epoch: 14 | loss: 0.1716644\n",
      "\tspeed: 0.0142s/iter; left time: 52.2926s\n",
      "\titers: 400, epoch: 14 | loss: 0.2786288\n",
      "\tspeed: 0.0142s/iter; left time: 50.8930s\n",
      "\titers: 500, epoch: 14 | loss: 0.2081250\n",
      "\tspeed: 0.0142s/iter; left time: 49.4987s\n",
      "Epoch: 14 cost time: 8.38364863395691\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2213041 Vali Loss: 0.0503268 Test Loss: 0.1554163\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2340338\n",
      "\tspeed: 0.0493s/iter; left time: 163.7426s\n",
      "\titers: 200, epoch: 15 | loss: 0.1625172\n",
      "\tspeed: 0.0159s/iter; left time: 51.3392s\n",
      "\titers: 300, epoch: 15 | loss: 0.2733411\n",
      "\tspeed: 0.0146s/iter; left time: 45.5025s\n",
      "\titers: 400, epoch: 15 | loss: 0.2272985\n",
      "\tspeed: 0.0141s/iter; left time: 42.6945s\n",
      "\titers: 500, epoch: 15 | loss: 0.1665039\n",
      "\tspeed: 0.0141s/iter; left time: 41.2589s\n",
      "Epoch: 15 cost time: 8.73994755744934\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2200371 Vali Loss: 0.0499670 Test Loss: 0.1554104\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15562692284584045, mae:0.2503296732902527\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4231477\n",
      "\tspeed: 0.0291s/iter; left time: 329.4019s\n",
      "\titers: 200, epoch: 1 | loss: 0.2990322\n",
      "\tspeed: 0.0169s/iter; left time: 189.7172s\n",
      "\titers: 300, epoch: 1 | loss: 0.4513031\n",
      "\tspeed: 0.0169s/iter; left time: 188.0459s\n",
      "\titers: 400, epoch: 1 | loss: 0.2904619\n",
      "\tspeed: 0.0170s/iter; left time: 186.7571s\n",
      "\titers: 500, epoch: 1 | loss: 0.2761419\n",
      "\tspeed: 0.0170s/iter; left time: 184.8584s\n",
      "Epoch: 1 cost time: 10.931333541870117\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3566688 Vali Loss: 0.0702681 Test Loss: 0.1946494\n",
      "Validation loss decreased (inf --> 0.070268).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4136405\n",
      "\tspeed: 0.0526s/iter; left time: 564.3414s\n",
      "\titers: 200, epoch: 2 | loss: 0.3237491\n",
      "\tspeed: 0.0169s/iter; left time: 180.1460s\n",
      "\titers: 300, epoch: 2 | loss: 0.3927438\n",
      "\tspeed: 0.0170s/iter; left time: 178.5594s\n",
      "\titers: 400, epoch: 2 | loss: 0.2460304\n",
      "\tspeed: 0.0169s/iter; left time: 176.3367s\n",
      "\titers: 500, epoch: 2 | loss: 0.3138577\n",
      "\tspeed: 0.0169s/iter; left time: 174.6559s\n",
      "Epoch: 2 cost time: 9.981170415878296\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3107000 Vali Loss: 0.0631987 Test Loss: 0.1842450\n",
      "Validation loss decreased (0.070268 --> 0.063199).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2380202\n",
      "\tspeed: 0.0524s/iter; left time: 532.3122s\n",
      "\titers: 200, epoch: 3 | loss: 0.2695333\n",
      "\tspeed: 0.0170s/iter; left time: 171.3926s\n",
      "\titers: 300, epoch: 3 | loss: 0.4293661\n",
      "\tspeed: 0.0170s/iter; left time: 169.1790s\n",
      "\titers: 400, epoch: 3 | loss: 0.2429115\n",
      "\tspeed: 0.0170s/iter; left time: 167.3648s\n",
      "\titers: 500, epoch: 3 | loss: 0.2631704\n",
      "\tspeed: 0.0170s/iter; left time: 165.9104s\n",
      "Epoch: 3 cost time: 10.025473594665527\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2844806 Vali Loss: 0.0596799 Test Loss: 0.1749096\n",
      "Validation loss decreased (0.063199 --> 0.059680).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3597705\n",
      "\tspeed: 0.0516s/iter; left time: 495.1564s\n",
      "\titers: 200, epoch: 4 | loss: 0.3170551\n",
      "\tspeed: 0.0170s/iter; left time: 161.4512s\n",
      "\titers: 300, epoch: 4 | loss: 0.2862722\n",
      "\tspeed: 0.0170s/iter; left time: 159.9076s\n",
      "\titers: 400, epoch: 4 | loss: 0.3588545\n",
      "\tspeed: 0.0170s/iter; left time: 158.0995s\n",
      "\titers: 500, epoch: 4 | loss: 0.2317751\n",
      "\tspeed: 0.0170s/iter; left time: 156.3508s\n",
      "Epoch: 4 cost time: 10.040975570678711\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2729959 Vali Loss: 0.0586416 Test Loss: 0.1634108\n",
      "Validation loss decreased (0.059680 --> 0.058642).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1480240\n",
      "\tspeed: 0.0527s/iter; left time: 474.9625s\n",
      "\titers: 200, epoch: 5 | loss: 0.2854924\n",
      "\tspeed: 0.0171s/iter; left time: 152.1080s\n",
      "\titers: 300, epoch: 5 | loss: 0.2601712\n",
      "\tspeed: 0.0171s/iter; left time: 150.5164s\n",
      "\titers: 400, epoch: 5 | loss: 0.2432022\n",
      "\tspeed: 0.0170s/iter; left time: 148.6426s\n",
      "\titers: 500, epoch: 5 | loss: 0.2031509\n",
      "\tspeed: 0.0170s/iter; left time: 146.7921s\n",
      "Epoch: 5 cost time: 10.021342754364014\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2617277 Vali Loss: 0.0552872 Test Loss: 0.1606847\n",
      "Validation loss decreased (0.058642 --> 0.055287).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3094109\n",
      "\tspeed: 0.0511s/iter; left time: 432.2128s\n",
      "\titers: 200, epoch: 6 | loss: 0.2309000\n",
      "\tspeed: 0.0169s/iter; left time: 141.4348s\n",
      "\titers: 300, epoch: 6 | loss: 0.2965054\n",
      "\tspeed: 0.0169s/iter; left time: 139.8543s\n",
      "\titers: 400, epoch: 6 | loss: 0.3793553\n",
      "\tspeed: 0.0169s/iter; left time: 137.9724s\n",
      "\titers: 500, epoch: 6 | loss: 0.1989893\n",
      "\tspeed: 0.0169s/iter; left time: 136.1369s\n",
      "Epoch: 6 cost time: 9.971482515335083\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2568656 Vali Loss: 0.0557706 Test Loss: 0.1589244\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1992728\n",
      "\tspeed: 0.0504s/iter; left time: 396.8560s\n",
      "\titers: 200, epoch: 7 | loss: 0.2386451\n",
      "\tspeed: 0.0170s/iter; left time: 131.9289s\n",
      "\titers: 300, epoch: 7 | loss: 0.3433176\n",
      "\tspeed: 0.0170s/iter; left time: 130.4022s\n",
      "\titers: 400, epoch: 7 | loss: 0.1856261\n",
      "\tspeed: 0.0170s/iter; left time: 128.6055s\n",
      "\titers: 500, epoch: 7 | loss: 0.2049061\n",
      "\tspeed: 0.0172s/iter; left time: 128.9989s\n",
      "Epoch: 7 cost time: 10.095287561416626\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2552189 Vali Loss: 0.0553833 Test Loss: 0.1578540\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2772514\n",
      "\tspeed: 0.0524s/iter; left time: 382.9626s\n",
      "\titers: 200, epoch: 8 | loss: 0.1852691\n",
      "\tspeed: 0.0181s/iter; left time: 130.5957s\n",
      "\titers: 300, epoch: 8 | loss: 0.2553893\n",
      "\tspeed: 0.0170s/iter; left time: 120.8605s\n",
      "\titers: 400, epoch: 8 | loss: 0.1523726\n",
      "\tspeed: 0.0170s/iter; left time: 119.0187s\n",
      "\titers: 500, epoch: 8 | loss: 0.1697097\n",
      "\tspeed: 0.0169s/iter; left time: 117.0782s\n",
      "Epoch: 8 cost time: 10.168155908584595\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2544855 Vali Loss: 0.0552160 Test Loss: 0.1583025\n",
      "Validation loss decreased (0.055287 --> 0.055216).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2468323\n",
      "\tspeed: 0.0510s/iter; left time: 343.8934s\n",
      "\titers: 200, epoch: 9 | loss: 0.1402135\n",
      "\tspeed: 0.0169s/iter; left time: 112.1503s\n",
      "\titers: 300, epoch: 9 | loss: 0.2766612\n",
      "\tspeed: 0.0169s/iter; left time: 110.5296s\n",
      "\titers: 400, epoch: 9 | loss: 0.2861530\n",
      "\tspeed: 0.0169s/iter; left time: 108.9696s\n",
      "\titers: 500, epoch: 9 | loss: 0.3543449\n",
      "\tspeed: 0.0169s/iter; left time: 107.4131s\n",
      "Epoch: 9 cost time: 9.955261945724487\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2515577 Vali Loss: 0.0554824 Test Loss: 0.1580071\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3629105\n",
      "\tspeed: 0.0526s/iter; left time: 324.6268s\n",
      "\titers: 200, epoch: 10 | loss: 0.1137268\n",
      "\tspeed: 0.0182s/iter; left time: 110.5788s\n",
      "\titers: 300, epoch: 10 | loss: 0.3536044\n",
      "\tspeed: 0.0181s/iter; left time: 108.3182s\n",
      "\titers: 400, epoch: 10 | loss: 0.3374973\n",
      "\tspeed: 0.0181s/iter; left time: 106.3650s\n",
      "\titers: 500, epoch: 10 | loss: 0.2181917\n",
      "\tspeed: 0.0181s/iter; left time: 104.5283s\n",
      "Epoch: 10 cost time: 10.669760704040527\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2517068 Vali Loss: 0.0553969 Test Loss: 0.1578377\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1954637\n",
      "\tspeed: 0.0548s/iter; left time: 307.1021s\n",
      "\titers: 200, epoch: 11 | loss: 0.2611381\n",
      "\tspeed: 0.0181s/iter; left time: 99.5500s\n",
      "\titers: 300, epoch: 11 | loss: 0.1358562\n",
      "\tspeed: 0.0181s/iter; left time: 97.7329s\n",
      "\titers: 400, epoch: 11 | loss: 0.2665876\n",
      "\tspeed: 0.0181s/iter; left time: 95.9436s\n",
      "\titers: 500, epoch: 11 | loss: 0.3374287\n",
      "\tspeed: 0.0181s/iter; left time: 94.1596s\n",
      "Epoch: 11 cost time: 10.586496114730835\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2517991 Vali Loss: 0.0553704 Test Loss: 0.1576126\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15853843092918396, mae:0.2533927857875824\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2764733\n",
      "\tspeed: 0.0300s/iter; left time: 338.7080s\n",
      "\titers: 200, epoch: 1 | loss: 0.2327543\n",
      "\tspeed: 0.0179s/iter; left time: 200.4128s\n",
      "\titers: 300, epoch: 1 | loss: 0.3357033\n",
      "\tspeed: 0.0179s/iter; left time: 198.7022s\n",
      "\titers: 400, epoch: 1 | loss: 0.3047566\n",
      "\tspeed: 0.0179s/iter; left time: 196.9367s\n",
      "\titers: 500, epoch: 1 | loss: 0.1972861\n",
      "\tspeed: 0.0179s/iter; left time: 195.2159s\n",
      "Epoch: 1 cost time: 11.45805811882019\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3256827 Vali Loss: 0.0674371 Test Loss: 0.1923324\n",
      "Validation loss decreased (inf --> 0.067437).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2472515\n",
      "\tspeed: 0.0524s/iter; left time: 561.9929s\n",
      "\titers: 200, epoch: 2 | loss: 0.1821082\n",
      "\tspeed: 0.0180s/iter; left time: 191.2611s\n",
      "\titers: 300, epoch: 2 | loss: 0.3611526\n",
      "\tspeed: 0.0180s/iter; left time: 189.5165s\n",
      "\titers: 400, epoch: 2 | loss: 0.4608039\n",
      "\tspeed: 0.0180s/iter; left time: 187.7903s\n",
      "\titers: 500, epoch: 2 | loss: 0.3331698\n",
      "\tspeed: 0.0180s/iter; left time: 185.8668s\n",
      "Epoch: 2 cost time: 10.553991556167603\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3138315 Vali Loss: 0.0646692 Test Loss: 0.1826266\n",
      "Validation loss decreased (0.067437 --> 0.064669).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3125826\n",
      "\tspeed: 0.0525s/iter; left time: 533.0140s\n",
      "\titers: 200, epoch: 3 | loss: 0.3133857\n",
      "\tspeed: 0.0180s/iter; left time: 180.6758s\n",
      "\titers: 300, epoch: 3 | loss: 0.1997578\n",
      "\tspeed: 0.0179s/iter; left time: 178.7409s\n",
      "\titers: 400, epoch: 3 | loss: 0.3287965\n",
      "\tspeed: 0.0180s/iter; left time: 177.1378s\n",
      "\titers: 500, epoch: 3 | loss: 0.1674982\n",
      "\tspeed: 0.0179s/iter; left time: 175.1309s\n",
      "Epoch: 3 cost time: 10.521435976028442\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2841374 Vali Loss: 0.0582497 Test Loss: 0.1715780\n",
      "Validation loss decreased (0.064669 --> 0.058250).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1959866\n",
      "\tspeed: 0.0544s/iter; left time: 522.0438s\n",
      "\titers: 200, epoch: 4 | loss: 0.6555514\n",
      "\tspeed: 0.0180s/iter; left time: 170.7332s\n",
      "\titers: 300, epoch: 4 | loss: 0.2739381\n",
      "\tspeed: 0.0180s/iter; left time: 168.7947s\n",
      "\titers: 400, epoch: 4 | loss: 0.1755477\n",
      "\tspeed: 0.0180s/iter; left time: 166.9206s\n",
      "\titers: 500, epoch: 4 | loss: 0.3965024\n",
      "\tspeed: 0.0180s/iter; left time: 165.1195s\n",
      "Epoch: 4 cost time: 10.541162967681885\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2689576 Vali Loss: 0.0583956 Test Loss: 0.1655976\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2582152\n",
      "\tspeed: 0.0520s/iter; left time: 469.4160s\n",
      "\titers: 200, epoch: 5 | loss: 0.3659150\n",
      "\tspeed: 0.0179s/iter; left time: 159.8561s\n",
      "\titers: 300, epoch: 5 | loss: 0.2310751\n",
      "\tspeed: 0.0179s/iter; left time: 158.1118s\n",
      "\titers: 400, epoch: 5 | loss: 0.1816646\n",
      "\tspeed: 0.0179s/iter; left time: 156.0907s\n",
      "\titers: 500, epoch: 5 | loss: 0.3596244\n",
      "\tspeed: 0.0179s/iter; left time: 154.5502s\n",
      "Epoch: 5 cost time: 10.525245189666748\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2582919 Vali Loss: 0.0567488 Test Loss: 0.1616790\n",
      "Validation loss decreased (0.058250 --> 0.056749).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2957802\n",
      "\tspeed: 0.0537s/iter; left time: 453.9291s\n",
      "\titers: 200, epoch: 6 | loss: 0.3656701\n",
      "\tspeed: 0.0180s/iter; left time: 150.1993s\n",
      "\titers: 300, epoch: 6 | loss: 0.5005087\n",
      "\tspeed: 0.0180s/iter; left time: 148.4747s\n",
      "\titers: 400, epoch: 6 | loss: 0.2646871\n",
      "\tspeed: 0.0180s/iter; left time: 146.8020s\n",
      "\titers: 500, epoch: 6 | loss: 0.2224196\n",
      "\tspeed: 0.0180s/iter; left time: 144.9934s\n",
      "Epoch: 6 cost time: 10.555970430374146\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2525463 Vali Loss: 0.0566615 Test Loss: 0.1613017\n",
      "Validation loss decreased (0.056749 --> 0.056661).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2991766\n",
      "\tspeed: 0.0526s/iter; left time: 414.9250s\n",
      "\titers: 200, epoch: 7 | loss: 0.1717590\n",
      "\tspeed: 0.0179s/iter; left time: 139.2728s\n",
      "\titers: 300, epoch: 7 | loss: 0.2409506\n",
      "\tspeed: 0.0179s/iter; left time: 137.5046s\n",
      "\titers: 400, epoch: 7 | loss: 0.3314831\n",
      "\tspeed: 0.0179s/iter; left time: 135.7222s\n",
      "\titers: 500, epoch: 7 | loss: 0.3442245\n",
      "\tspeed: 0.0179s/iter; left time: 133.9444s\n",
      "Epoch: 7 cost time: 10.513476610183716\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2513187 Vali Loss: 0.0563184 Test Loss: 0.1605487\n",
      "Validation loss decreased (0.056661 --> 0.056318).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1689795\n",
      "\tspeed: 0.0526s/iter; left time: 384.4188s\n",
      "\titers: 200, epoch: 8 | loss: 0.3971741\n",
      "\tspeed: 0.0179s/iter; left time: 129.2206s\n",
      "\titers: 300, epoch: 8 | loss: 0.3039874\n",
      "\tspeed: 0.0179s/iter; left time: 127.3856s\n",
      "\titers: 400, epoch: 8 | loss: 0.2689097\n",
      "\tspeed: 0.0179s/iter; left time: 125.4902s\n",
      "\titers: 500, epoch: 8 | loss: 0.2332681\n",
      "\tspeed: 0.0179s/iter; left time: 123.7218s\n",
      "Epoch: 8 cost time: 10.527833700180054\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2480591 Vali Loss: 0.0560904 Test Loss: 0.1599616\n",
      "Validation loss decreased (0.056318 --> 0.056090).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1714244\n",
      "\tspeed: 0.0552s/iter; left time: 371.9745s\n",
      "\titers: 200, epoch: 9 | loss: 0.2399662\n",
      "\tspeed: 0.0179s/iter; left time: 118.8583s\n",
      "\titers: 300, epoch: 9 | loss: 0.1432478\n",
      "\tspeed: 0.0179s/iter; left time: 117.1783s\n",
      "\titers: 400, epoch: 9 | loss: 0.2058955\n",
      "\tspeed: 0.0179s/iter; left time: 115.1671s\n",
      "\titers: 500, epoch: 9 | loss: 0.1150765\n",
      "\tspeed: 0.0179s/iter; left time: 113.2646s\n",
      "Epoch: 9 cost time: 10.511715412139893\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2479948 Vali Loss: 0.0558472 Test Loss: 0.1595002\n",
      "Validation loss decreased (0.056090 --> 0.055847).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3212492\n",
      "\tspeed: 0.0510s/iter; left time: 314.4371s\n",
      "\titers: 200, epoch: 10 | loss: 0.2343052\n",
      "\tspeed: 0.0163s/iter; left time: 98.7893s\n",
      "\titers: 300, epoch: 10 | loss: 0.1787103\n",
      "\tspeed: 0.0163s/iter; left time: 97.1640s\n",
      "\titers: 400, epoch: 10 | loss: 0.2474824\n",
      "\tspeed: 0.0163s/iter; left time: 95.5734s\n",
      "\titers: 500, epoch: 10 | loss: 0.1732036\n",
      "\tspeed: 0.0163s/iter; left time: 93.9896s\n",
      "Epoch: 10 cost time: 9.607587814331055\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2465933 Vali Loss: 0.0556785 Test Loss: 0.1596540\n",
      "Validation loss decreased (0.055847 --> 0.055678).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1643394\n",
      "\tspeed: 0.0499s/iter; left time: 279.5078s\n",
      "\titers: 200, epoch: 11 | loss: 0.1678609\n",
      "\tspeed: 0.0164s/iter; left time: 90.0541s\n",
      "\titers: 300, epoch: 11 | loss: 0.2018326\n",
      "\tspeed: 0.0164s/iter; left time: 88.4867s\n",
      "\titers: 400, epoch: 11 | loss: 0.2276821\n",
      "\tspeed: 0.0164s/iter; left time: 86.8054s\n",
      "\titers: 500, epoch: 11 | loss: 0.1991215\n",
      "\tspeed: 0.0164s/iter; left time: 85.1634s\n",
      "Epoch: 11 cost time: 9.657196760177612\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2476586 Vali Loss: 0.0559690 Test Loss: 0.1596856\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1573327\n",
      "\tspeed: 0.0491s/iter; left time: 246.7741s\n",
      "\titers: 200, epoch: 12 | loss: 0.2785705\n",
      "\tspeed: 0.0163s/iter; left time: 80.6146s\n",
      "\titers: 300, epoch: 12 | loss: 0.1592060\n",
      "\tspeed: 0.0163s/iter; left time: 78.9764s\n",
      "\titers: 400, epoch: 12 | loss: 0.2363529\n",
      "\tspeed: 0.0163s/iter; left time: 77.3456s\n",
      "\titers: 500, epoch: 12 | loss: 0.3511350\n",
      "\tspeed: 0.0163s/iter; left time: 75.6901s\n",
      "Epoch: 12 cost time: 9.594619274139404\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2443652 Vali Loss: 0.0558417 Test Loss: 0.1596739\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.3338568\n",
      "\tspeed: 0.0509s/iter; left time: 227.1500s\n",
      "\titers: 200, epoch: 13 | loss: 0.2935806\n",
      "\tspeed: 0.0164s/iter; left time: 71.4410s\n",
      "\titers: 300, epoch: 13 | loss: 0.2703342\n",
      "\tspeed: 0.0164s/iter; left time: 69.7927s\n",
      "\titers: 400, epoch: 13 | loss: 0.1846772\n",
      "\tspeed: 0.0164s/iter; left time: 68.1469s\n",
      "\titers: 500, epoch: 13 | loss: 0.2558920\n",
      "\tspeed: 0.0164s/iter; left time: 66.5346s\n",
      "Epoch: 13 cost time: 9.641987323760986\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2451861 Vali Loss: 0.0559950 Test Loss: 0.1596886\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15988914668560028, mae:0.255709171295166\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3528516\n",
      "\tspeed: 0.0271s/iter; left time: 306.4168s\n",
      "\titers: 200, epoch: 1 | loss: 0.3574461\n",
      "\tspeed: 0.0154s/iter; left time: 172.5331s\n",
      "\titers: 300, epoch: 1 | loss: 0.2459876\n",
      "\tspeed: 0.0154s/iter; left time: 170.6570s\n",
      "\titers: 400, epoch: 1 | loss: 0.3437310\n",
      "\tspeed: 0.0154s/iter; left time: 169.5908s\n",
      "\titers: 500, epoch: 1 | loss: 0.5735512\n",
      "\tspeed: 0.0154s/iter; left time: 168.2200s\n",
      "Epoch: 1 cost time: 10.004918813705444\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3299547 Vali Loss: 0.0726970 Test Loss: 0.1894750\n",
      "Validation loss decreased (inf --> 0.072697).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4155256\n",
      "\tspeed: 0.0514s/iter; left time: 551.1155s\n",
      "\titers: 200, epoch: 2 | loss: 0.1897063\n",
      "\tspeed: 0.0154s/iter; left time: 164.1380s\n",
      "\titers: 300, epoch: 2 | loss: 0.3088529\n",
      "\tspeed: 0.0154s/iter; left time: 162.2370s\n",
      "\titers: 400, epoch: 2 | loss: 0.3127601\n",
      "\tspeed: 0.0154s/iter; left time: 160.6653s\n",
      "\titers: 500, epoch: 2 | loss: 0.4591195\n",
      "\tspeed: 0.0154s/iter; left time: 159.0609s\n",
      "Epoch: 2 cost time: 9.084067106246948\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3048152 Vali Loss: 0.0728215 Test Loss: 0.1978776\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3362475\n",
      "\tspeed: 0.0493s/iter; left time: 501.0565s\n",
      "\titers: 200, epoch: 3 | loss: 0.1796288\n",
      "\tspeed: 0.0154s/iter; left time: 154.6287s\n",
      "\titers: 300, epoch: 3 | loss: 0.3259010\n",
      "\tspeed: 0.0154s/iter; left time: 153.3527s\n",
      "\titers: 400, epoch: 3 | loss: 0.3156268\n",
      "\tspeed: 0.0154s/iter; left time: 151.7336s\n",
      "\titers: 500, epoch: 3 | loss: 0.2664908\n",
      "\tspeed: 0.0153s/iter; left time: 149.7955s\n",
      "Epoch: 3 cost time: 9.12300157546997\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3005098 Vali Loss: 0.0633868 Test Loss: 0.1851613\n",
      "Validation loss decreased (0.072697 --> 0.063387).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2100977\n",
      "\tspeed: 0.0481s/iter; left time: 461.1915s\n",
      "\titers: 200, epoch: 4 | loss: 0.3616856\n",
      "\tspeed: 0.0155s/iter; left time: 146.6556s\n",
      "\titers: 300, epoch: 4 | loss: 0.4346203\n",
      "\tspeed: 0.0155s/iter; left time: 145.1498s\n",
      "\titers: 400, epoch: 4 | loss: 0.2474310\n",
      "\tspeed: 0.0155s/iter; left time: 143.5607s\n",
      "\titers: 500, epoch: 4 | loss: 0.3295820\n",
      "\tspeed: 0.0154s/iter; left time: 141.9613s\n",
      "Epoch: 4 cost time: 9.0999436378479\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2913144 Vali Loss: 0.0619122 Test Loss: 0.1752462\n",
      "Validation loss decreased (0.063387 --> 0.061912).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.3288084\n",
      "\tspeed: 0.0479s/iter; left time: 432.4463s\n",
      "\titers: 200, epoch: 5 | loss: 0.2195111\n",
      "\tspeed: 0.0154s/iter; left time: 137.1683s\n",
      "\titers: 300, epoch: 5 | loss: 0.2494730\n",
      "\tspeed: 0.0154s/iter; left time: 135.6983s\n",
      "\titers: 400, epoch: 5 | loss: 0.1984664\n",
      "\tspeed: 0.0154s/iter; left time: 133.8758s\n",
      "\titers: 500, epoch: 5 | loss: 0.2411065\n",
      "\tspeed: 0.0153s/iter; left time: 132.1371s\n",
      "Epoch: 5 cost time: 9.065420627593994\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2832449 Vali Loss: 0.0608572 Test Loss: 0.1716351\n",
      "Validation loss decreased (0.061912 --> 0.060857).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3289462\n",
      "\tspeed: 0.0523s/iter; left time: 441.9574s\n",
      "\titers: 200, epoch: 6 | loss: 0.2251569\n",
      "\tspeed: 0.0160s/iter; left time: 133.9290s\n",
      "\titers: 300, epoch: 6 | loss: 0.1530811\n",
      "\tspeed: 0.0154s/iter; left time: 127.1787s\n",
      "\titers: 400, epoch: 6 | loss: 0.1735361\n",
      "\tspeed: 0.0154s/iter; left time: 125.6118s\n",
      "\titers: 500, epoch: 6 | loss: 0.2480948\n",
      "\tspeed: 0.0154s/iter; left time: 123.7782s\n",
      "Epoch: 6 cost time: 9.313013553619385\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2769288 Vali Loss: 0.0607782 Test Loss: 0.1672092\n",
      "Validation loss decreased (0.060857 --> 0.060778).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2894515\n",
      "\tspeed: 0.0510s/iter; left time: 401.9395s\n",
      "\titers: 200, epoch: 7 | loss: 0.2232478\n",
      "\tspeed: 0.0154s/iter; left time: 119.4420s\n",
      "\titers: 300, epoch: 7 | loss: 0.2083797\n",
      "\tspeed: 0.0154s/iter; left time: 118.0243s\n",
      "\titers: 400, epoch: 7 | loss: 0.2925494\n",
      "\tspeed: 0.0153s/iter; left time: 116.3652s\n",
      "\titers: 500, epoch: 7 | loss: 0.2986919\n",
      "\tspeed: 0.0153s/iter; left time: 114.8158s\n",
      "Epoch: 7 cost time: 9.051892757415771\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2726304 Vali Loss: 0.0607205 Test Loss: 0.1663236\n",
      "Validation loss decreased (0.060778 --> 0.060720).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2765966\n",
      "\tspeed: 0.0475s/iter; left time: 346.9877s\n",
      "\titers: 200, epoch: 8 | loss: 0.3518615\n",
      "\tspeed: 0.0154s/iter; left time: 111.1124s\n",
      "\titers: 300, epoch: 8 | loss: 0.1765554\n",
      "\tspeed: 0.0154s/iter; left time: 109.5369s\n",
      "\titers: 400, epoch: 8 | loss: 0.2702326\n",
      "\tspeed: 0.0154s/iter; left time: 107.8100s\n",
      "\titers: 500, epoch: 8 | loss: 0.2131765\n",
      "\tspeed: 0.0154s/iter; left time: 106.1331s\n",
      "Epoch: 8 cost time: 9.078426599502563\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2694661 Vali Loss: 0.0601330 Test Loss: 0.1655186\n",
      "Validation loss decreased (0.060720 --> 0.060133).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2739267\n",
      "\tspeed: 0.0485s/iter; left time: 326.8870s\n",
      "\titers: 200, epoch: 9 | loss: 0.3057661\n",
      "\tspeed: 0.0154s/iter; left time: 102.4231s\n",
      "\titers: 300, epoch: 9 | loss: 0.2766374\n",
      "\tspeed: 0.0154s/iter; left time: 100.8908s\n",
      "\titers: 400, epoch: 9 | loss: 0.1855141\n",
      "\tspeed: 0.0154s/iter; left time: 99.3426s\n",
      "\titers: 500, epoch: 9 | loss: 0.2378395\n",
      "\tspeed: 0.0154s/iter; left time: 97.7886s\n",
      "Epoch: 9 cost time: 9.108720541000366\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2694652 Vali Loss: 0.0601589 Test Loss: 0.1652751\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2965716\n",
      "\tspeed: 0.0512s/iter; left time: 315.7919s\n",
      "\titers: 200, epoch: 10 | loss: 0.3119071\n",
      "\tspeed: 0.0156s/iter; left time: 94.7444s\n",
      "\titers: 300, epoch: 10 | loss: 0.1638429\n",
      "\tspeed: 0.0154s/iter; left time: 91.7190s\n",
      "\titers: 400, epoch: 10 | loss: 0.3317775\n",
      "\tspeed: 0.0154s/iter; left time: 90.3136s\n",
      "\titers: 500, epoch: 10 | loss: 0.5118732\n",
      "\tspeed: 0.0154s/iter; left time: 88.7347s\n",
      "Epoch: 10 cost time: 9.22945499420166\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2700157 Vali Loss: 0.0602742 Test Loss: 0.1647697\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2693446\n",
      "\tspeed: 0.0475s/iter; left time: 266.1417s\n",
      "\titers: 200, epoch: 11 | loss: 0.1961205\n",
      "\tspeed: 0.0153s/iter; left time: 84.3814s\n",
      "\titers: 300, epoch: 11 | loss: 0.2544862\n",
      "\tspeed: 0.0153s/iter; left time: 82.8842s\n",
      "\titers: 400, epoch: 11 | loss: 0.2897861\n",
      "\tspeed: 0.0153s/iter; left time: 81.3048s\n",
      "\titers: 500, epoch: 11 | loss: 0.3234709\n",
      "\tspeed: 0.0153s/iter; left time: 79.8326s\n",
      "Epoch: 11 cost time: 9.054192781448364\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2692987 Vali Loss: 0.0602649 Test Loss: 0.1648742\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16577757894992828, mae:0.26132142543792725\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2132039\n",
      "\tspeed: 0.0315s/iter; left time: 355.5984s\n",
      "\titers: 200, epoch: 1 | loss: 0.2482248\n",
      "\tspeed: 0.0213s/iter; left time: 238.2305s\n",
      "\titers: 300, epoch: 1 | loss: 0.4155103\n",
      "\tspeed: 0.0212s/iter; left time: 235.8371s\n",
      "\titers: 400, epoch: 1 | loss: 0.2788582\n",
      "\tspeed: 0.0212s/iter; left time: 233.5968s\n",
      "\titers: 500, epoch: 1 | loss: 0.2421324\n",
      "\tspeed: 0.0208s/iter; left time: 226.5883s\n",
      "Epoch: 1 cost time: 13.113003969192505\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2878056 Vali Loss: 0.0654367 Test Loss: 0.1771272\n",
      "Validation loss decreased (inf --> 0.065437).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.5202433\n",
      "\tspeed: 0.0587s/iter; left time: 629.9636s\n",
      "\titers: 200, epoch: 2 | loss: 0.2306121\n",
      "\tspeed: 0.0187s/iter; left time: 198.9606s\n",
      "\titers: 300, epoch: 2 | loss: 0.2242718\n",
      "\tspeed: 0.0187s/iter; left time: 196.6947s\n",
      "\titers: 400, epoch: 2 | loss: 0.2614449\n",
      "\tspeed: 0.0187s/iter; left time: 194.8847s\n",
      "\titers: 500, epoch: 2 | loss: 0.2152333\n",
      "\tspeed: 0.0187s/iter; left time: 192.7855s\n",
      "Epoch: 2 cost time: 10.991939306259155\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2904823 Vali Loss: 0.0628499 Test Loss: 0.1861763\n",
      "Validation loss decreased (0.065437 --> 0.062850).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2092441\n",
      "\tspeed: 0.0607s/iter; left time: 616.9825s\n",
      "\titers: 200, epoch: 3 | loss: 0.3344850\n",
      "\tspeed: 0.0214s/iter; left time: 215.5411s\n",
      "\titers: 300, epoch: 3 | loss: 0.2349855\n",
      "\tspeed: 0.0214s/iter; left time: 213.6113s\n",
      "\titers: 400, epoch: 3 | loss: 0.1700952\n",
      "\tspeed: 0.0214s/iter; left time: 211.0907s\n",
      "\titers: 500, epoch: 3 | loss: 0.2035912\n",
      "\tspeed: 0.0214s/iter; left time: 208.8304s\n",
      "Epoch: 3 cost time: 12.514484643936157\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2683181 Vali Loss: 0.0603431 Test Loss: 0.1769986\n",
      "Validation loss decreased (0.062850 --> 0.060343).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2909709\n",
      "\tspeed: 0.0611s/iter; left time: 585.6879s\n",
      "\titers: 200, epoch: 4 | loss: 0.3411694\n",
      "\tspeed: 0.0215s/iter; left time: 203.8932s\n",
      "\titers: 300, epoch: 4 | loss: 0.3257471\n",
      "\tspeed: 0.0215s/iter; left time: 202.0165s\n",
      "\titers: 400, epoch: 4 | loss: 0.1559393\n",
      "\tspeed: 0.0215s/iter; left time: 199.5007s\n",
      "\titers: 500, epoch: 4 | loss: 0.2574477\n",
      "\tspeed: 0.0215s/iter; left time: 197.2979s\n",
      "Epoch: 4 cost time: 12.547528266906738\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2628267 Vali Loss: 0.0567002 Test Loss: 0.1610373\n",
      "Validation loss decreased (0.060343 --> 0.056700).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2836421\n",
      "\tspeed: 0.0613s/iter; left time: 552.5825s\n",
      "\titers: 200, epoch: 5 | loss: 0.2339277\n",
      "\tspeed: 0.0214s/iter; left time: 190.9505s\n",
      "\titers: 300, epoch: 5 | loss: 0.2372324\n",
      "\tspeed: 0.0214s/iter; left time: 188.6712s\n",
      "\titers: 400, epoch: 5 | loss: 0.2867775\n",
      "\tspeed: 0.0214s/iter; left time: 186.5103s\n",
      "\titers: 500, epoch: 5 | loss: 0.2466409\n",
      "\tspeed: 0.0214s/iter; left time: 184.3611s\n",
      "Epoch: 5 cost time: 12.501335382461548\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2554568 Vali Loss: 0.0538172 Test Loss: 0.1566114\n",
      "Validation loss decreased (0.056700 --> 0.053817).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1382849\n",
      "\tspeed: 0.0617s/iter; left time: 521.3643s\n",
      "\titers: 200, epoch: 6 | loss: 0.2312297\n",
      "\tspeed: 0.0203s/iter; left time: 169.2861s\n",
      "\titers: 300, epoch: 6 | loss: 0.2006404\n",
      "\tspeed: 0.0215s/iter; left time: 177.2562s\n",
      "\titers: 400, epoch: 6 | loss: 0.2580297\n",
      "\tspeed: 0.0215s/iter; left time: 175.4855s\n",
      "\titers: 500, epoch: 6 | loss: 0.2003629\n",
      "\tspeed: 0.0215s/iter; left time: 173.2800s\n",
      "Epoch: 6 cost time: 12.455375671386719\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2458913 Vali Loss: 0.0522738 Test Loss: 0.1558172\n",
      "Validation loss decreased (0.053817 --> 0.052274).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1853729\n",
      "\tspeed: 0.0620s/iter; left time: 488.8613s\n",
      "\titers: 200, epoch: 7 | loss: 0.2679348\n",
      "\tspeed: 0.0187s/iter; left time: 145.2204s\n",
      "\titers: 300, epoch: 7 | loss: 0.1970098\n",
      "\tspeed: 0.0187s/iter; left time: 143.6898s\n",
      "\titers: 400, epoch: 7 | loss: 0.2792156\n",
      "\tspeed: 0.0187s/iter; left time: 141.6569s\n",
      "\titers: 500, epoch: 7 | loss: 0.3566693\n",
      "\tspeed: 0.0187s/iter; left time: 139.8331s\n",
      "Epoch: 7 cost time: 11.318219900131226\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2409843 Vali Loss: 0.0515541 Test Loss: 0.1536989\n",
      "Validation loss decreased (0.052274 --> 0.051554).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2299775\n",
      "\tspeed: 0.0565s/iter; left time: 413.3109s\n",
      "\titers: 200, epoch: 8 | loss: 0.2109481\n",
      "\tspeed: 0.0186s/iter; left time: 134.4034s\n",
      "\titers: 300, epoch: 8 | loss: 0.1976876\n",
      "\tspeed: 0.0186s/iter; left time: 132.3593s\n",
      "\titers: 400, epoch: 8 | loss: 0.3727143\n",
      "\tspeed: 0.0186s/iter; left time: 130.6040s\n",
      "\titers: 500, epoch: 8 | loss: 0.2535819\n",
      "\tspeed: 0.0186s/iter; left time: 128.6455s\n",
      "Epoch: 8 cost time: 10.908987760543823\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2374367 Vali Loss: 0.0513114 Test Loss: 0.1529048\n",
      "Validation loss decreased (0.051554 --> 0.051311).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1614419\n",
      "\tspeed: 0.0580s/iter; left time: 391.2884s\n",
      "\titers: 200, epoch: 9 | loss: 0.3201469\n",
      "\tspeed: 0.0186s/iter; left time: 123.7978s\n",
      "\titers: 300, epoch: 9 | loss: 0.1190581\n",
      "\tspeed: 0.0186s/iter; left time: 121.9433s\n",
      "\titers: 400, epoch: 9 | loss: 0.2792606\n",
      "\tspeed: 0.0186s/iter; left time: 120.0049s\n",
      "\titers: 500, epoch: 9 | loss: 0.1153495\n",
      "\tspeed: 0.0186s/iter; left time: 117.9354s\n",
      "Epoch: 9 cost time: 10.950137615203857\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2353253 Vali Loss: 0.0511915 Test Loss: 0.1528212\n",
      "Validation loss decreased (0.051311 --> 0.051191).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2203282\n",
      "\tspeed: 0.0575s/iter; left time: 354.8125s\n",
      "\titers: 200, epoch: 10 | loss: 0.2659106\n",
      "\tspeed: 0.0187s/iter; left time: 113.2582s\n",
      "\titers: 300, epoch: 10 | loss: 0.1446058\n",
      "\tspeed: 0.0187s/iter; left time: 111.4611s\n",
      "\titers: 400, epoch: 10 | loss: 0.2124428\n",
      "\tspeed: 0.0187s/iter; left time: 109.5448s\n",
      "\titers: 500, epoch: 10 | loss: 0.2000332\n",
      "\tspeed: 0.0186s/iter; left time: 107.5315s\n",
      "Epoch: 10 cost time: 10.925860404968262\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2360373 Vali Loss: 0.0511895 Test Loss: 0.1525097\n",
      "Validation loss decreased (0.051191 --> 0.051189).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1884583\n",
      "\tspeed: 0.0587s/iter; left time: 328.8971s\n",
      "\titers: 200, epoch: 11 | loss: 0.3364536\n",
      "\tspeed: 0.0209s/iter; left time: 115.0192s\n",
      "\titers: 300, epoch: 11 | loss: 0.2422140\n",
      "\tspeed: 0.0209s/iter; left time: 112.6270s\n",
      "\titers: 400, epoch: 11 | loss: 0.1363387\n",
      "\tspeed: 0.0209s/iter; left time: 110.5711s\n",
      "\titers: 500, epoch: 11 | loss: 0.3151762\n",
      "\tspeed: 0.0208s/iter; left time: 108.2375s\n",
      "Epoch: 11 cost time: 12.183650493621826\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2365175 Vali Loss: 0.0511866 Test Loss: 0.1524168\n",
      "Validation loss decreased (0.051189 --> 0.051187).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.3024474\n",
      "\tspeed: 0.0610s/iter; left time: 307.0287s\n",
      "\titers: 200, epoch: 12 | loss: 0.3122122\n",
      "\tspeed: 0.0187s/iter; left time: 92.2111s\n",
      "\titers: 300, epoch: 12 | loss: 0.1375531\n",
      "\tspeed: 0.0187s/iter; left time: 90.3159s\n",
      "\titers: 400, epoch: 12 | loss: 0.2285809\n",
      "\tspeed: 0.0187s/iter; left time: 88.6890s\n",
      "\titers: 500, epoch: 12 | loss: 0.1865078\n",
      "\tspeed: 0.0187s/iter; left time: 86.7205s\n",
      "Epoch: 12 cost time: 10.990453004837036\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2352742 Vali Loss: 0.0509963 Test Loss: 0.1524491\n",
      "Validation loss decreased (0.051187 --> 0.050996).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.3926801\n",
      "\tspeed: 0.0573s/iter; left time: 255.5781s\n",
      "\titers: 200, epoch: 13 | loss: 0.2231462\n",
      "\tspeed: 0.0187s/iter; left time: 81.5991s\n",
      "\titers: 300, epoch: 13 | loss: 0.2874231\n",
      "\tspeed: 0.0187s/iter; left time: 79.7364s\n",
      "\titers: 400, epoch: 13 | loss: 0.1770340\n",
      "\tspeed: 0.0187s/iter; left time: 77.7297s\n",
      "\titers: 500, epoch: 13 | loss: 0.2780897\n",
      "\tspeed: 0.0187s/iter; left time: 75.8323s\n",
      "Epoch: 13 cost time: 10.971324920654297\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2350211 Vali Loss: 0.0510677 Test Loss: 0.1524323\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.3067077\n",
      "\tspeed: 0.0599s/iter; left time: 233.1256s\n",
      "\titers: 200, epoch: 14 | loss: 0.1708578\n",
      "\tspeed: 0.0214s/iter; left time: 81.1304s\n",
      "\titers: 300, epoch: 14 | loss: 0.1416878\n",
      "\tspeed: 0.0214s/iter; left time: 79.0225s\n",
      "\titers: 400, epoch: 14 | loss: 0.2449211\n",
      "\tspeed: 0.0214s/iter; left time: 76.8447s\n",
      "\titers: 500, epoch: 14 | loss: 0.2723967\n",
      "\tspeed: 0.0214s/iter; left time: 74.6990s\n",
      "Epoch: 14 cost time: 12.435030698776245\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2351534 Vali Loss: 0.0510622 Test Loss: 0.1524382\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1750597\n",
      "\tspeed: 0.0631s/iter; left time: 209.4305s\n",
      "\titers: 200, epoch: 15 | loss: 0.2392311\n",
      "\tspeed: 0.0214s/iter; left time: 68.8741s\n",
      "\titers: 300, epoch: 15 | loss: 0.1884887\n",
      "\tspeed: 0.0214s/iter; left time: 66.7507s\n",
      "\titers: 400, epoch: 15 | loss: 0.2420480\n",
      "\tspeed: 0.0214s/iter; left time: 64.6097s\n",
      "\titers: 500, epoch: 15 | loss: 0.1719001\n",
      "\tspeed: 0.0214s/iter; left time: 62.4679s\n",
      "Epoch: 15 cost time: 12.455493688583374\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2347518 Vali Loss: 0.0510754 Test Loss: 0.1524345\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15269137918949127, mae:0.24526135623455048\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4440405\n",
      "\tspeed: 0.0324s/iter; left time: 366.5528s\n",
      "\titers: 200, epoch: 1 | loss: 0.2563414\n",
      "\tspeed: 0.0195s/iter; left time: 218.3404s\n",
      "\titers: 300, epoch: 1 | loss: 0.2045213\n",
      "\tspeed: 0.0184s/iter; left time: 204.2402s\n",
      "\titers: 400, epoch: 1 | loss: 0.1961407\n",
      "\tspeed: 0.0183s/iter; left time: 201.7795s\n",
      "\titers: 500, epoch: 1 | loss: 0.2555127\n",
      "\tspeed: 0.0184s/iter; left time: 200.7655s\n",
      "Epoch: 1 cost time: 12.050100803375244\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2561358 Vali Loss: 0.0489382 Test Loss: 0.1559181\n",
      "Validation loss decreased (inf --> 0.048938).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1675851\n",
      "\tspeed: 0.0573s/iter; left time: 615.2071s\n",
      "\titers: 200, epoch: 2 | loss: 0.3353744\n",
      "\tspeed: 0.0183s/iter; left time: 194.8223s\n",
      "\titers: 300, epoch: 2 | loss: 0.5020998\n",
      "\tspeed: 0.0183s/iter; left time: 192.8448s\n",
      "\titers: 400, epoch: 2 | loss: 0.2674504\n",
      "\tspeed: 0.0183s/iter; left time: 190.7290s\n",
      "\titers: 500, epoch: 2 | loss: 0.2010202\n",
      "\tspeed: 0.0183s/iter; left time: 189.2499s\n",
      "Epoch: 2 cost time: 10.750036478042603\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2872678 Vali Loss: 0.0682885 Test Loss: 0.1902094\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2125782\n",
      "\tspeed: 0.0583s/iter; left time: 592.3434s\n",
      "\titers: 200, epoch: 3 | loss: 0.2854894\n",
      "\tspeed: 0.0193s/iter; left time: 193.7875s\n",
      "\titers: 300, epoch: 3 | loss: 0.2461788\n",
      "\tspeed: 0.0184s/iter; left time: 183.5088s\n",
      "\titers: 400, epoch: 3 | loss: 0.2213762\n",
      "\tspeed: 0.0184s/iter; left time: 181.3305s\n",
      "\titers: 500, epoch: 3 | loss: 0.1950367\n",
      "\tspeed: 0.0201s/iter; left time: 196.4433s\n",
      "Epoch: 3 cost time: 11.490993738174438\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2838300 Vali Loss: 0.0612267 Test Loss: 0.1770139\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2848371\n",
      "\tspeed: 0.0619s/iter; left time: 593.8959s\n",
      "\titers: 200, epoch: 4 | loss: 0.1449135\n",
      "\tspeed: 0.0213s/iter; left time: 202.2070s\n",
      "\titers: 300, epoch: 4 | loss: 0.4600289\n",
      "\tspeed: 0.0213s/iter; left time: 199.8422s\n",
      "\titers: 400, epoch: 4 | loss: 0.1936486\n",
      "\tspeed: 0.0213s/iter; left time: 197.9495s\n",
      "\titers: 500, epoch: 4 | loss: 0.1924767\n",
      "\tspeed: 0.0213s/iter; left time: 195.5182s\n",
      "Epoch: 4 cost time: 12.445250511169434\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2692256 Vali Loss: 0.0593378 Test Loss: 0.1638209\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15614116191864014, mae:0.2531272768974304\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3439945\n",
      "\tspeed: 0.0299s/iter; left time: 337.8749s\n",
      "\titers: 200, epoch: 1 | loss: 0.2224072\n",
      "\tspeed: 0.0182s/iter; left time: 203.9254s\n",
      "\titers: 300, epoch: 1 | loss: 0.2982511\n",
      "\tspeed: 0.0182s/iter; left time: 202.1269s\n",
      "\titers: 400, epoch: 1 | loss: 0.3607741\n",
      "\tspeed: 0.0182s/iter; left time: 200.2918s\n",
      "\titers: 500, epoch: 1 | loss: 0.4477174\n",
      "\tspeed: 0.0182s/iter; left time: 198.4560s\n",
      "Epoch: 1 cost time: 11.596385955810547\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2980871 Vali Loss: 0.0661756 Test Loss: 0.1882649\n",
      "Validation loss decreased (inf --> 0.066176).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3172938\n",
      "\tspeed: 0.0587s/iter; left time: 629.5154s\n",
      "\titers: 200, epoch: 2 | loss: 0.4120645\n",
      "\tspeed: 0.0185s/iter; left time: 196.4896s\n",
      "\titers: 300, epoch: 2 | loss: 0.5728456\n",
      "\tspeed: 0.0185s/iter; left time: 194.3268s\n",
      "\titers: 400, epoch: 2 | loss: 0.2337411\n",
      "\tspeed: 0.0185s/iter; left time: 192.6667s\n",
      "\titers: 500, epoch: 2 | loss: 0.2605494\n",
      "\tspeed: 0.0184s/iter; left time: 190.4284s\n",
      "Epoch: 2 cost time: 10.830545425415039\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3149009 Vali Loss: 0.0684825 Test Loss: 0.1888304\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1760447\n",
      "\tspeed: 0.0549s/iter; left time: 557.4392s\n",
      "\titers: 200, epoch: 3 | loss: 0.3244613\n",
      "\tspeed: 0.0183s/iter; left time: 184.5583s\n",
      "\titers: 300, epoch: 3 | loss: 0.3146085\n",
      "\tspeed: 0.0183s/iter; left time: 182.6632s\n",
      "\titers: 400, epoch: 3 | loss: 0.3546068\n",
      "\tspeed: 0.0183s/iter; left time: 180.9396s\n",
      "\titers: 500, epoch: 3 | loss: 0.3117248\n",
      "\tspeed: 0.0184s/iter; left time: 179.1588s\n",
      "Epoch: 3 cost time: 10.721402168273926\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2925788 Vali Loss: 0.0609396 Test Loss: 0.1662711\n",
      "Validation loss decreased (0.066176 --> 0.060940).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2310022\n",
      "\tspeed: 0.0562s/iter; left time: 538.9507s\n",
      "\titers: 200, epoch: 4 | loss: 0.2679229\n",
      "\tspeed: 0.0184s/iter; left time: 175.1025s\n",
      "\titers: 300, epoch: 4 | loss: 0.2043655\n",
      "\tspeed: 0.0184s/iter; left time: 173.1889s\n",
      "\titers: 400, epoch: 4 | loss: 0.2407873\n",
      "\tspeed: 0.0184s/iter; left time: 171.1840s\n",
      "\titers: 500, epoch: 4 | loss: 0.1919197\n",
      "\tspeed: 0.0184s/iter; left time: 169.2577s\n",
      "Epoch: 4 cost time: 10.811872005462646\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2716240 Vali Loss: 0.0596362 Test Loss: 0.1630546\n",
      "Validation loss decreased (0.060940 --> 0.059636).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2280511\n",
      "\tspeed: 0.0584s/iter; left time: 526.4815s\n",
      "\titers: 200, epoch: 5 | loss: 0.2428658\n",
      "\tspeed: 0.0206s/iter; left time: 183.5102s\n",
      "\titers: 300, epoch: 5 | loss: 0.2224826\n",
      "\tspeed: 0.0205s/iter; left time: 181.0357s\n",
      "\titers: 400, epoch: 5 | loss: 0.2465171\n",
      "\tspeed: 0.0205s/iter; left time: 179.1803s\n",
      "\titers: 500, epoch: 5 | loss: 0.1690178\n",
      "\tspeed: 0.0205s/iter; left time: 177.0684s\n",
      "Epoch: 5 cost time: 12.011718988418579\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2609641 Vali Loss: 0.0581607 Test Loss: 0.1598530\n",
      "Validation loss decreased (0.059636 --> 0.058161).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4152997\n",
      "\tspeed: 0.0579s/iter; left time: 489.2717s\n",
      "\titers: 200, epoch: 6 | loss: 0.1902053\n",
      "\tspeed: 0.0185s/iter; left time: 154.2371s\n",
      "\titers: 300, epoch: 6 | loss: 0.4684936\n",
      "\tspeed: 0.0185s/iter; left time: 152.2812s\n",
      "\titers: 400, epoch: 6 | loss: 0.1891903\n",
      "\tspeed: 0.0184s/iter; left time: 150.3648s\n",
      "\titers: 500, epoch: 6 | loss: 0.2699345\n",
      "\tspeed: 0.0184s/iter; left time: 148.4468s\n",
      "Epoch: 6 cost time: 10.839010238647461\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2563190 Vali Loss: 0.0575713 Test Loss: 0.1564931\n",
      "Validation loss decreased (0.058161 --> 0.057571).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1933302\n",
      "\tspeed: 0.0563s/iter; left time: 443.6975s\n",
      "\titers: 200, epoch: 7 | loss: 0.2402638\n",
      "\tspeed: 0.0194s/iter; left time: 150.5706s\n",
      "\titers: 300, epoch: 7 | loss: 0.2761043\n",
      "\tspeed: 0.0205s/iter; left time: 157.5016s\n",
      "\titers: 400, epoch: 7 | loss: 0.2309266\n",
      "\tspeed: 0.0205s/iter; left time: 155.4945s\n",
      "\titers: 500, epoch: 7 | loss: 0.1958035\n",
      "\tspeed: 0.0205s/iter; left time: 153.3620s\n",
      "Epoch: 7 cost time: 11.674142599105835\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2547132 Vali Loss: 0.0579028 Test Loss: 0.1565990\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1519305\n",
      "\tspeed: 0.0563s/iter; left time: 411.3127s\n",
      "\titers: 200, epoch: 8 | loss: 0.1581205\n",
      "\tspeed: 0.0183s/iter; left time: 131.9191s\n",
      "\titers: 300, epoch: 8 | loss: 0.2114730\n",
      "\tspeed: 0.0183s/iter; left time: 129.9789s\n",
      "\titers: 400, epoch: 8 | loss: 0.2588448\n",
      "\tspeed: 0.0183s/iter; left time: 128.2440s\n",
      "\titers: 500, epoch: 8 | loss: 0.1707231\n",
      "\tspeed: 0.0183s/iter; left time: 126.3865s\n",
      "Epoch: 8 cost time: 10.69466257095337\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2521554 Vali Loss: 0.0575652 Test Loss: 0.1560879\n",
      "Validation loss decreased (0.057571 --> 0.057565).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2606703\n",
      "\tspeed: 0.0577s/iter; left time: 388.7291s\n",
      "\titers: 200, epoch: 9 | loss: 0.2921563\n",
      "\tspeed: 0.0207s/iter; left time: 137.5776s\n",
      "\titers: 300, epoch: 9 | loss: 0.2700184\n",
      "\tspeed: 0.0206s/iter; left time: 134.9063s\n",
      "\titers: 400, epoch: 9 | loss: 0.4152758\n",
      "\tspeed: 0.0206s/iter; left time: 132.4620s\n",
      "\titers: 500, epoch: 9 | loss: 0.2096159\n",
      "\tspeed: 0.0206s/iter; left time: 130.3902s\n",
      "Epoch: 9 cost time: 12.037326574325562\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2526055 Vali Loss: 0.0574870 Test Loss: 0.1556351\n",
      "Validation loss decreased (0.057565 --> 0.057487).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2231581\n",
      "\tspeed: 0.0575s/iter; left time: 354.5522s\n",
      "\titers: 200, epoch: 10 | loss: 0.2138534\n",
      "\tspeed: 0.0183s/iter; left time: 111.0832s\n",
      "\titers: 300, epoch: 10 | loss: 0.3480861\n",
      "\tspeed: 0.0183s/iter; left time: 109.3265s\n",
      "\titers: 400, epoch: 10 | loss: 0.2435866\n",
      "\tspeed: 0.0183s/iter; left time: 107.3653s\n",
      "\titers: 500, epoch: 10 | loss: 0.2342066\n",
      "\tspeed: 0.0183s/iter; left time: 105.6292s\n",
      "Epoch: 10 cost time: 10.741774797439575\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2514910 Vali Loss: 0.0572551 Test Loss: 0.1555971\n",
      "Validation loss decreased (0.057487 --> 0.057255).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2281349\n",
      "\tspeed: 0.0556s/iter; left time: 311.3301s\n",
      "\titers: 200, epoch: 11 | loss: 0.2650180\n",
      "\tspeed: 0.0183s/iter; left time: 100.5706s\n",
      "\titers: 300, epoch: 11 | loss: 0.2456894\n",
      "\tspeed: 0.0182s/iter; left time: 98.3729s\n",
      "\titers: 400, epoch: 11 | loss: 0.2605615\n",
      "\tspeed: 0.0182s/iter; left time: 96.4158s\n",
      "\titers: 500, epoch: 11 | loss: 0.1960399\n",
      "\tspeed: 0.0182s/iter; left time: 94.6521s\n",
      "Epoch: 11 cost time: 10.693013668060303\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2525947 Vali Loss: 0.0572932 Test Loss: 0.1556307\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1464265\n",
      "\tspeed: 0.0591s/iter; left time: 297.2119s\n",
      "\titers: 200, epoch: 12 | loss: 0.3264920\n",
      "\tspeed: 0.0184s/iter; left time: 90.4951s\n",
      "\titers: 300, epoch: 12 | loss: 0.2778126\n",
      "\tspeed: 0.0183s/iter; left time: 88.5766s\n",
      "\titers: 400, epoch: 12 | loss: 0.1597119\n",
      "\tspeed: 0.0183s/iter; left time: 86.5843s\n",
      "\titers: 500, epoch: 12 | loss: 0.3305517\n",
      "\tspeed: 0.0183s/iter; left time: 84.7469s\n",
      "Epoch: 12 cost time: 10.704734086990356\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2501300 Vali Loss: 0.0573288 Test Loss: 0.1556164\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2810948\n",
      "\tspeed: 0.0551s/iter; left time: 245.5792s\n",
      "\titers: 200, epoch: 13 | loss: 0.1267352\n",
      "\tspeed: 0.0183s/iter; left time: 79.9067s\n",
      "\titers: 300, epoch: 13 | loss: 0.1804745\n",
      "\tspeed: 0.0183s/iter; left time: 78.0427s\n",
      "\titers: 400, epoch: 13 | loss: 0.1703719\n",
      "\tspeed: 0.0183s/iter; left time: 76.2186s\n",
      "\titers: 500, epoch: 13 | loss: 0.1641101\n",
      "\tspeed: 0.0183s/iter; left time: 74.3771s\n",
      "Epoch: 13 cost time: 10.733674049377441\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2510339 Vali Loss: 0.0575678 Test Loss: 0.1556001\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15581247210502625, mae:0.2524816393852234\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2820024\n",
      "\tspeed: 0.0340s/iter; left time: 384.2169s\n",
      "\titers: 200, epoch: 1 | loss: 0.3506850\n",
      "\tspeed: 0.0220s/iter; left time: 246.3993s\n",
      "\titers: 300, epoch: 1 | loss: 0.3158914\n",
      "\tspeed: 0.0220s/iter; left time: 244.6439s\n",
      "\titers: 400, epoch: 1 | loss: 0.4694290\n",
      "\tspeed: 0.0220s/iter; left time: 242.3290s\n",
      "\titers: 500, epoch: 1 | loss: 0.4384179\n",
      "\tspeed: 0.0220s/iter; left time: 240.1337s\n",
      "Epoch: 1 cost time: 13.80353331565857\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3687771 Vali Loss: 0.0628052 Test Loss: 0.1830931\n",
      "Validation loss decreased (inf --> 0.062805).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3938585\n",
      "\tspeed: 0.0674s/iter; left time: 723.0322s\n",
      "\titers: 200, epoch: 2 | loss: 0.2715237\n",
      "\tspeed: 0.0220s/iter; left time: 233.8290s\n",
      "\titers: 300, epoch: 2 | loss: 0.2369220\n",
      "\tspeed: 0.0220s/iter; left time: 231.6491s\n",
      "\titers: 400, epoch: 2 | loss: 0.2923615\n",
      "\tspeed: 0.0220s/iter; left time: 229.4202s\n",
      "\titers: 500, epoch: 2 | loss: 0.2706907\n",
      "\tspeed: 0.0220s/iter; left time: 227.3420s\n",
      "Epoch: 2 cost time: 12.883723497390747\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3097597 Vali Loss: 0.0641888 Test Loss: 0.1786072\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.6045175\n",
      "\tspeed: 0.0619s/iter; left time: 629.3401s\n",
      "\titers: 200, epoch: 3 | loss: 0.1884619\n",
      "\tspeed: 0.0220s/iter; left time: 221.6047s\n",
      "\titers: 300, epoch: 3 | loss: 0.3644444\n",
      "\tspeed: 0.0220s/iter; left time: 219.3781s\n",
      "\titers: 400, epoch: 3 | loss: 0.2941984\n",
      "\tspeed: 0.0220s/iter; left time: 217.1217s\n",
      "\titers: 500, epoch: 3 | loss: 0.2039631\n",
      "\tspeed: 0.0220s/iter; left time: 214.9650s\n",
      "Epoch: 3 cost time: 12.837319135665894\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2877696 Vali Loss: 0.0594068 Test Loss: 0.1651690\n",
      "Validation loss decreased (0.062805 --> 0.059407).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2447802\n",
      "\tspeed: 0.0638s/iter; left time: 611.8430s\n",
      "\titers: 200, epoch: 4 | loss: 0.1950940\n",
      "\tspeed: 0.0221s/iter; left time: 209.3103s\n",
      "\titers: 300, epoch: 4 | loss: 0.2962869\n",
      "\tspeed: 0.0221s/iter; left time: 207.0851s\n",
      "\titers: 400, epoch: 4 | loss: 0.2045498\n",
      "\tspeed: 0.0221s/iter; left time: 204.8904s\n",
      "\titers: 500, epoch: 4 | loss: 0.3095154\n",
      "\tspeed: 0.0221s/iter; left time: 202.7547s\n",
      "Epoch: 4 cost time: 12.93319034576416\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2705669 Vali Loss: 0.0570208 Test Loss: 0.1616229\n",
      "Validation loss decreased (0.059407 --> 0.057021).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2788117\n",
      "\tspeed: 0.0640s/iter; left time: 577.4424s\n",
      "\titers: 200, epoch: 5 | loss: 0.2860644\n",
      "\tspeed: 0.0220s/iter; left time: 195.8734s\n",
      "\titers: 300, epoch: 5 | loss: 0.2103918\n",
      "\tspeed: 0.0220s/iter; left time: 193.7592s\n",
      "\titers: 400, epoch: 5 | loss: 0.2173960\n",
      "\tspeed: 0.0220s/iter; left time: 191.5213s\n",
      "\titers: 500, epoch: 5 | loss: 0.2649921\n",
      "\tspeed: 0.0220s/iter; left time: 189.4514s\n",
      "Epoch: 5 cost time: 12.819153785705566\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2607152 Vali Loss: 0.0576824 Test Loss: 0.1600697\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1931576\n",
      "\tspeed: 0.0615s/iter; left time: 519.3526s\n",
      "\titers: 200, epoch: 6 | loss: 0.3377065\n",
      "\tspeed: 0.0220s/iter; left time: 183.9307s\n",
      "\titers: 300, epoch: 6 | loss: 0.1976932\n",
      "\tspeed: 0.0220s/iter; left time: 181.7342s\n",
      "\titers: 400, epoch: 6 | loss: 0.1933704\n",
      "\tspeed: 0.0220s/iter; left time: 179.5331s\n",
      "\titers: 500, epoch: 6 | loss: 0.3341262\n",
      "\tspeed: 0.0221s/iter; left time: 177.5633s\n",
      "Epoch: 6 cost time: 12.86313247680664\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2561844 Vali Loss: 0.0569983 Test Loss: 0.1571478\n",
      "Validation loss decreased (0.057021 --> 0.056998).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3503615\n",
      "\tspeed: 0.0640s/iter; left time: 504.4689s\n",
      "\titers: 200, epoch: 7 | loss: 0.2650831\n",
      "\tspeed: 0.0220s/iter; left time: 171.1093s\n",
      "\titers: 300, epoch: 7 | loss: 0.1854428\n",
      "\tspeed: 0.0220s/iter; left time: 168.9140s\n",
      "\titers: 400, epoch: 7 | loss: 0.2032392\n",
      "\tspeed: 0.0220s/iter; left time: 166.8174s\n",
      "\titers: 500, epoch: 7 | loss: 0.2495519\n",
      "\tspeed: 0.0220s/iter; left time: 164.5604s\n",
      "Epoch: 7 cost time: 12.865455627441406\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2529105 Vali Loss: 0.0565248 Test Loss: 0.1552147\n",
      "Validation loss decreased (0.056998 --> 0.056525).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1709098\n",
      "\tspeed: 0.0649s/iter; left time: 474.6575s\n",
      "\titers: 200, epoch: 8 | loss: 0.5014006\n",
      "\tspeed: 0.0239s/iter; left time: 172.5161s\n",
      "\titers: 300, epoch: 8 | loss: 0.1560250\n",
      "\tspeed: 0.0230s/iter; left time: 163.2838s\n",
      "\titers: 400, epoch: 8 | loss: 0.2622548\n",
      "\tspeed: 0.0238s/iter; left time: 166.5255s\n",
      "\titers: 500, epoch: 8 | loss: 0.2482225\n",
      "\tspeed: 0.0239s/iter; left time: 165.3002s\n",
      "Epoch: 8 cost time: 13.822824716567993\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2526525 Vali Loss: 0.0563946 Test Loss: 0.1553344\n",
      "Validation loss decreased (0.056525 --> 0.056395).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2847161\n",
      "\tspeed: 0.0662s/iter; left time: 446.5015s\n",
      "\titers: 200, epoch: 9 | loss: 0.2171539\n",
      "\tspeed: 0.0219s/iter; left time: 145.7433s\n",
      "\titers: 300, epoch: 9 | loss: 0.3929270\n",
      "\tspeed: 0.0219s/iter; left time: 143.5175s\n",
      "\titers: 400, epoch: 9 | loss: 0.2341013\n",
      "\tspeed: 0.0219s/iter; left time: 141.3646s\n",
      "\titers: 500, epoch: 9 | loss: 0.1879824\n",
      "\tspeed: 0.0220s/iter; left time: 139.2760s\n",
      "Epoch: 9 cost time: 12.81470274925232\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2493533 Vali Loss: 0.0561724 Test Loss: 0.1555994\n",
      "Validation loss decreased (0.056395 --> 0.056172).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2559296\n",
      "\tspeed: 0.0663s/iter; left time: 409.4279s\n",
      "\titers: 200, epoch: 10 | loss: 0.2823784\n",
      "\tspeed: 0.0240s/iter; left time: 145.4614s\n",
      "\titers: 300, epoch: 10 | loss: 0.2442188\n",
      "\tspeed: 0.0227s/iter; left time: 135.7714s\n",
      "\titers: 400, epoch: 10 | loss: 0.3683121\n",
      "\tspeed: 0.0221s/iter; left time: 129.7534s\n",
      "\titers: 500, epoch: 10 | loss: 0.2910262\n",
      "\tspeed: 0.0221s/iter; left time: 127.6324s\n",
      "Epoch: 10 cost time: 13.28865933418274\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2496015 Vali Loss: 0.0566815 Test Loss: 0.1553975\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1941482\n",
      "\tspeed: 0.0636s/iter; left time: 356.4983s\n",
      "\titers: 200, epoch: 11 | loss: 0.1915396\n",
      "\tspeed: 0.0220s/iter; left time: 121.0883s\n",
      "\titers: 300, epoch: 11 | loss: 0.1241986\n",
      "\tspeed: 0.0220s/iter; left time: 118.8040s\n",
      "\titers: 400, epoch: 11 | loss: 0.1996565\n",
      "\tspeed: 0.0220s/iter; left time: 116.6044s\n",
      "\titers: 500, epoch: 11 | loss: 0.1841637\n",
      "\tspeed: 0.0220s/iter; left time: 114.4586s\n",
      "Epoch: 11 cost time: 12.907510757446289\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2511690 Vali Loss: 0.0564752 Test Loss: 0.1553328\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2536335\n",
      "\tspeed: 0.0620s/iter; left time: 311.9815s\n",
      "\titers: 200, epoch: 12 | loss: 0.2307326\n",
      "\tspeed: 0.0221s/iter; left time: 108.7479s\n",
      "\titers: 300, epoch: 12 | loss: 0.3589673\n",
      "\tspeed: 0.0220s/iter; left time: 106.1706s\n",
      "\titers: 400, epoch: 12 | loss: 0.1117583\n",
      "\tspeed: 0.0220s/iter; left time: 103.9367s\n",
      "\titers: 500, epoch: 12 | loss: 0.2942688\n",
      "\tspeed: 0.0220s/iter; left time: 101.7650s\n",
      "Epoch: 12 cost time: 12.849246740341187\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2487588 Vali Loss: 0.0564325 Test Loss: 0.1553530\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15584561228752136, mae:0.2539593577384949\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3483191\n",
      "\tspeed: 0.0352s/iter; left time: 398.1787s\n",
      "\titers: 200, epoch: 1 | loss: 0.1764646\n",
      "\tspeed: 0.0232s/iter; left time: 259.4205s\n",
      "\titers: 300, epoch: 1 | loss: 0.2731398\n",
      "\tspeed: 0.0232s/iter; left time: 257.6884s\n",
      "\titers: 400, epoch: 1 | loss: 0.3596932\n",
      "\tspeed: 0.0232s/iter; left time: 255.1898s\n",
      "\titers: 500, epoch: 1 | loss: 0.2425520\n",
      "\tspeed: 0.0232s/iter; left time: 252.3968s\n",
      "Epoch: 1 cost time: 14.481833696365356\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3932763 Vali Loss: 0.0687599 Test Loss: 0.1930391\n",
      "Validation loss decreased (inf --> 0.068760).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3420772\n",
      "\tspeed: 0.0664s/iter; left time: 712.2537s\n",
      "\titers: 200, epoch: 2 | loss: 0.2079389\n",
      "\tspeed: 0.0214s/iter; left time: 227.7822s\n",
      "\titers: 300, epoch: 2 | loss: 0.4470553\n",
      "\tspeed: 0.0214s/iter; left time: 225.5341s\n",
      "\titers: 400, epoch: 2 | loss: 0.2903394\n",
      "\tspeed: 0.0214s/iter; left time: 223.4318s\n",
      "\titers: 500, epoch: 2 | loss: 0.2193912\n",
      "\tspeed: 0.0214s/iter; left time: 221.1467s\n",
      "Epoch: 2 cost time: 12.558496952056885\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3195452 Vali Loss: 0.0677651 Test Loss: 0.1936957\n",
      "Validation loss decreased (0.068760 --> 0.067765).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2322346\n",
      "\tspeed: 0.0651s/iter; left time: 661.1284s\n",
      "\titers: 200, epoch: 3 | loss: 0.2921307\n",
      "\tspeed: 0.0233s/iter; left time: 233.9472s\n",
      "\titers: 300, epoch: 3 | loss: 0.1781242\n",
      "\tspeed: 0.0233s/iter; left time: 231.7892s\n",
      "\titers: 400, epoch: 3 | loss: 0.1827128\n",
      "\tspeed: 0.0232s/iter; left time: 229.1645s\n",
      "\titers: 500, epoch: 3 | loss: 0.2208113\n",
      "\tspeed: 0.0233s/iter; left time: 227.2506s\n",
      "Epoch: 3 cost time: 13.553138256072998\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2932154 Vali Loss: 0.0649118 Test Loss: 0.1745840\n",
      "Validation loss decreased (0.067765 --> 0.064912).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2069071\n",
      "\tspeed: 0.0638s/iter; left time: 612.1436s\n",
      "\titers: 200, epoch: 4 | loss: 0.2678215\n",
      "\tspeed: 0.0213s/iter; left time: 202.5689s\n",
      "\titers: 300, epoch: 4 | loss: 0.2591911\n",
      "\tspeed: 0.0217s/iter; left time: 203.6122s\n",
      "\titers: 400, epoch: 4 | loss: 0.3349337\n",
      "\tspeed: 0.0234s/iter; left time: 217.6259s\n",
      "\titers: 500, epoch: 4 | loss: 0.1894222\n",
      "\tspeed: 0.0234s/iter; left time: 215.2502s\n",
      "Epoch: 4 cost time: 13.065051317214966\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2769989 Vali Loss: 0.0618164 Test Loss: 0.1722599\n",
      "Validation loss decreased (0.064912 --> 0.061816).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.4016593\n",
      "\tspeed: 0.0659s/iter; left time: 594.4158s\n",
      "\titers: 200, epoch: 5 | loss: 0.1939733\n",
      "\tspeed: 0.0233s/iter; left time: 207.5360s\n",
      "\titers: 300, epoch: 5 | loss: 0.3224022\n",
      "\tspeed: 0.0232s/iter; left time: 205.0129s\n",
      "\titers: 400, epoch: 5 | loss: 0.2390254\n",
      "\tspeed: 0.0232s/iter; left time: 202.5735s\n",
      "\titers: 500, epoch: 5 | loss: 0.2287242\n",
      "\tspeed: 0.0233s/iter; left time: 201.2990s\n",
      "Epoch: 5 cost time: 13.612292289733887\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2684514 Vali Loss: 0.0615603 Test Loss: 0.1668281\n",
      "Validation loss decreased (0.061816 --> 0.061560).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2436883\n",
      "\tspeed: 0.0656s/iter; left time: 554.3725s\n",
      "\titers: 200, epoch: 6 | loss: 0.3324875\n",
      "\tspeed: 0.0214s/iter; left time: 178.6188s\n",
      "\titers: 300, epoch: 6 | loss: 0.2827042\n",
      "\tspeed: 0.0214s/iter; left time: 176.5276s\n",
      "\titers: 400, epoch: 6 | loss: 0.1656434\n",
      "\tspeed: 0.0214s/iter; left time: 174.3857s\n",
      "\titers: 500, epoch: 6 | loss: 0.1709646\n",
      "\tspeed: 0.0214s/iter; left time: 172.1588s\n",
      "Epoch: 6 cost time: 12.535096168518066\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2631734 Vali Loss: 0.0609450 Test Loss: 0.1660672\n",
      "Validation loss decreased (0.061560 --> 0.060945).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2456770\n",
      "\tspeed: 0.0622s/iter; left time: 490.0588s\n",
      "\titers: 200, epoch: 7 | loss: 0.3631036\n",
      "\tspeed: 0.0212s/iter; left time: 165.1232s\n",
      "\titers: 300, epoch: 7 | loss: 0.3073087\n",
      "\tspeed: 0.0212s/iter; left time: 162.9806s\n",
      "\titers: 400, epoch: 7 | loss: 0.1740722\n",
      "\tspeed: 0.0212s/iter; left time: 160.7741s\n",
      "\titers: 500, epoch: 7 | loss: 0.2432873\n",
      "\tspeed: 0.0212s/iter; left time: 158.7506s\n",
      "Epoch: 7 cost time: 12.424107551574707\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2611012 Vali Loss: 0.0604223 Test Loss: 0.1646789\n",
      "Validation loss decreased (0.060945 --> 0.060422).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2533799\n",
      "\tspeed: 0.0628s/iter; left time: 459.1246s\n",
      "\titers: 200, epoch: 8 | loss: 0.2918572\n",
      "\tspeed: 0.0214s/iter; left time: 154.2695s\n",
      "\titers: 300, epoch: 8 | loss: 0.2444859\n",
      "\tspeed: 0.0214s/iter; left time: 152.0478s\n",
      "\titers: 400, epoch: 8 | loss: 0.2939004\n",
      "\tspeed: 0.0214s/iter; left time: 149.8876s\n",
      "\titers: 500, epoch: 8 | loss: 0.3102149\n",
      "\tspeed: 0.0214s/iter; left time: 147.6997s\n",
      "Epoch: 8 cost time: 12.508057355880737\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2591132 Vali Loss: 0.0587517 Test Loss: 0.1642147\n",
      "Validation loss decreased (0.060422 --> 0.058752).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2027416\n",
      "\tspeed: 0.0626s/iter; left time: 422.1793s\n",
      "\titers: 200, epoch: 9 | loss: 0.3553377\n",
      "\tspeed: 0.0214s/iter; left time: 141.9750s\n",
      "\titers: 300, epoch: 9 | loss: 0.2911352\n",
      "\tspeed: 0.0214s/iter; left time: 139.8032s\n",
      "\titers: 400, epoch: 9 | loss: 0.2423984\n",
      "\tspeed: 0.0214s/iter; left time: 137.6660s\n",
      "\titers: 500, epoch: 9 | loss: 0.1473337\n",
      "\tspeed: 0.0214s/iter; left time: 135.5430s\n",
      "Epoch: 9 cost time: 12.525371074676514\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2591514 Vali Loss: 0.0583740 Test Loss: 0.1639684\n",
      "Validation loss decreased (0.058752 --> 0.058374).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1709135\n",
      "\tspeed: 0.0660s/iter; left time: 407.1380s\n",
      "\titers: 200, epoch: 10 | loss: 0.2414147\n",
      "\tspeed: 0.0216s/iter; left time: 131.3980s\n",
      "\titers: 300, epoch: 10 | loss: 0.1864398\n",
      "\tspeed: 0.0233s/iter; left time: 139.1370s\n",
      "\titers: 400, epoch: 10 | loss: 0.2411635\n",
      "\tspeed: 0.0233s/iter; left time: 136.7352s\n",
      "\titers: 500, epoch: 10 | loss: 0.3385921\n",
      "\tspeed: 0.0233s/iter; left time: 134.4784s\n",
      "Epoch: 10 cost time: 13.250301122665405\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2577685 Vali Loss: 0.0582898 Test Loss: 0.1638390\n",
      "Validation loss decreased (0.058374 --> 0.058290).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4224576\n",
      "\tspeed: 0.0650s/iter; left time: 364.1418s\n",
      "\titers: 200, epoch: 11 | loss: 0.2363219\n",
      "\tspeed: 0.0233s/iter; left time: 128.1570s\n",
      "\titers: 300, epoch: 11 | loss: 0.2084909\n",
      "\tspeed: 0.0233s/iter; left time: 125.8077s\n",
      "\titers: 400, epoch: 11 | loss: 0.1982952\n",
      "\tspeed: 0.0233s/iter; left time: 123.4977s\n",
      "\titers: 500, epoch: 11 | loss: 0.2825384\n",
      "\tspeed: 0.0233s/iter; left time: 121.1819s\n",
      "Epoch: 11 cost time: 13.607885122299194\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2576231 Vali Loss: 0.0584254 Test Loss: 0.1637842\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.3107935\n",
      "\tspeed: 0.0646s/iter; left time: 324.9062s\n",
      "\titers: 200, epoch: 12 | loss: 0.2872938\n",
      "\tspeed: 0.0214s/iter; left time: 105.4423s\n",
      "\titers: 300, epoch: 12 | loss: 0.2831005\n",
      "\tspeed: 0.0213s/iter; left time: 103.0202s\n",
      "\titers: 400, epoch: 12 | loss: 0.4618843\n",
      "\tspeed: 0.0213s/iter; left time: 100.8946s\n",
      "\titers: 500, epoch: 12 | loss: 0.1982855\n",
      "\tspeed: 0.0213s/iter; left time: 98.7551s\n",
      "Epoch: 12 cost time: 12.627089262008667\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2571457 Vali Loss: 0.0585422 Test Loss: 0.1637864\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1983078\n",
      "\tspeed: 0.0605s/iter; left time: 269.6843s\n",
      "\titers: 200, epoch: 13 | loss: 0.3412160\n",
      "\tspeed: 0.0213s/iter; left time: 92.9811s\n",
      "\titers: 300, epoch: 13 | loss: 0.3556422\n",
      "\tspeed: 0.0213s/iter; left time: 90.8928s\n",
      "\titers: 400, epoch: 13 | loss: 0.2023166\n",
      "\tspeed: 0.0213s/iter; left time: 88.7214s\n",
      "\titers: 500, epoch: 13 | loss: 0.2963874\n",
      "\tspeed: 0.0213s/iter; left time: 86.6776s\n",
      "Epoch: 13 cost time: 12.473315238952637\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2576762 Vali Loss: 0.0589034 Test Loss: 0.1637957\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16409872472286224, mae:0.2606174945831299\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3804397\n",
      "\tspeed: 0.0318s/iter; left time: 359.2997s\n",
      "\titers: 200, epoch: 1 | loss: 0.3089974\n",
      "\tspeed: 0.0201s/iter; left time: 225.1320s\n",
      "\titers: 300, epoch: 1 | loss: 0.3638180\n",
      "\tspeed: 0.0201s/iter; left time: 222.8779s\n",
      "\titers: 400, epoch: 1 | loss: 0.4387281\n",
      "\tspeed: 0.0219s/iter; left time: 240.8524s\n",
      "\titers: 500, epoch: 1 | loss: 0.3337742\n",
      "\tspeed: 0.0222s/iter; left time: 241.8229s\n",
      "Epoch: 1 cost time: 13.223371267318726\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3645407 Vali Loss: 0.0670480 Test Loss: 0.1957417\n",
      "Validation loss decreased (inf --> 0.067048).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2270536\n",
      "\tspeed: 0.0618s/iter; left time: 662.8868s\n",
      "\titers: 200, epoch: 2 | loss: 0.5691278\n",
      "\tspeed: 0.0203s/iter; left time: 216.2059s\n",
      "\titers: 300, epoch: 2 | loss: 0.2426668\n",
      "\tspeed: 0.0203s/iter; left time: 213.2584s\n",
      "\titers: 400, epoch: 2 | loss: 0.2305209\n",
      "\tspeed: 0.0203s/iter; left time: 211.4533s\n",
      "\titers: 500, epoch: 2 | loss: 0.1968035\n",
      "\tspeed: 0.0203s/iter; left time: 209.3513s\n",
      "Epoch: 2 cost time: 11.874313116073608\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3082443 Vali Loss: 0.0630910 Test Loss: 0.1835305\n",
      "Validation loss decreased (0.067048 --> 0.063091).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1988384\n",
      "\tspeed: 0.0598s/iter; left time: 607.2941s\n",
      "\titers: 200, epoch: 3 | loss: 0.2185974\n",
      "\tspeed: 0.0201s/iter; left time: 202.0593s\n",
      "\titers: 300, epoch: 3 | loss: 0.2903170\n",
      "\tspeed: 0.0201s/iter; left time: 199.7893s\n",
      "\titers: 400, epoch: 3 | loss: 0.3049778\n",
      "\tspeed: 0.0201s/iter; left time: 197.8905s\n",
      "\titers: 500, epoch: 3 | loss: 0.2555400\n",
      "\tspeed: 0.0201s/iter; left time: 195.8700s\n",
      "Epoch: 3 cost time: 11.734462976455688\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2866641 Vali Loss: 0.0612436 Test Loss: 0.1738186\n",
      "Validation loss decreased (0.063091 --> 0.061244).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1943121\n",
      "\tspeed: 0.0599s/iter; left time: 574.0318s\n",
      "\titers: 200, epoch: 4 | loss: 0.4831101\n",
      "\tspeed: 0.0202s/iter; left time: 191.6544s\n",
      "\titers: 300, epoch: 4 | loss: 0.1913214\n",
      "\tspeed: 0.0202s/iter; left time: 189.7743s\n",
      "\titers: 400, epoch: 4 | loss: 0.2798837\n",
      "\tspeed: 0.0202s/iter; left time: 187.5272s\n",
      "\titers: 500, epoch: 4 | loss: 0.2258324\n",
      "\tspeed: 0.0202s/iter; left time: 185.5867s\n",
      "Epoch: 4 cost time: 11.821897745132446\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2727965 Vali Loss: 0.0597988 Test Loss: 0.1625510\n",
      "Validation loss decreased (0.061244 --> 0.059799).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2511033\n",
      "\tspeed: 0.0605s/iter; left time: 545.6101s\n",
      "\titers: 200, epoch: 5 | loss: 0.6101279\n",
      "\tspeed: 0.0201s/iter; left time: 179.2963s\n",
      "\titers: 300, epoch: 5 | loss: 0.2532804\n",
      "\tspeed: 0.0201s/iter; left time: 177.2732s\n",
      "\titers: 400, epoch: 5 | loss: 0.2133041\n",
      "\tspeed: 0.0201s/iter; left time: 175.1340s\n",
      "\titers: 500, epoch: 5 | loss: 0.2544225\n",
      "\tspeed: 0.0201s/iter; left time: 173.1019s\n",
      "Epoch: 5 cost time: 11.743638277053833\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2646309 Vali Loss: 0.0560531 Test Loss: 0.1614465\n",
      "Validation loss decreased (0.059799 --> 0.056053).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2450040\n",
      "\tspeed: 0.0598s/iter; left time: 505.0754s\n",
      "\titers: 200, epoch: 6 | loss: 0.3549562\n",
      "\tspeed: 0.0203s/iter; left time: 169.1453s\n",
      "\titers: 300, epoch: 6 | loss: 0.2124002\n",
      "\tspeed: 0.0203s/iter; left time: 167.1340s\n",
      "\titers: 400, epoch: 6 | loss: 0.1401052\n",
      "\tspeed: 0.0202s/iter; left time: 165.0566s\n",
      "\titers: 500, epoch: 6 | loss: 0.4268948\n",
      "\tspeed: 0.0202s/iter; left time: 162.9227s\n",
      "Epoch: 6 cost time: 11.85304594039917\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2608943 Vali Loss: 0.0560516 Test Loss: 0.1611834\n",
      "Validation loss decreased (0.056053 --> 0.056052).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2776586\n",
      "\tspeed: 0.0632s/iter; left time: 498.3555s\n",
      "\titers: 200, epoch: 7 | loss: 0.2870423\n",
      "\tspeed: 0.0223s/iter; left time: 173.2533s\n",
      "\titers: 300, epoch: 7 | loss: 0.3501499\n",
      "\tspeed: 0.0223s/iter; left time: 171.0628s\n",
      "\titers: 400, epoch: 7 | loss: 0.4562049\n",
      "\tspeed: 0.0223s/iter; left time: 168.6950s\n",
      "\titers: 500, epoch: 7 | loss: 0.2012505\n",
      "\tspeed: 0.0207s/iter; left time: 154.4890s\n",
      "Epoch: 7 cost time: 12.713634967803955\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2569121 Vali Loss: 0.0560550 Test Loss: 0.1617218\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3294926\n",
      "\tspeed: 0.0581s/iter; left time: 424.9295s\n",
      "\titers: 200, epoch: 8 | loss: 0.4140672\n",
      "\tspeed: 0.0202s/iter; left time: 145.4693s\n",
      "\titers: 300, epoch: 8 | loss: 0.3153934\n",
      "\tspeed: 0.0202s/iter; left time: 143.5739s\n",
      "\titers: 400, epoch: 8 | loss: 0.1939416\n",
      "\tspeed: 0.0202s/iter; left time: 141.4371s\n",
      "\titers: 500, epoch: 8 | loss: 0.2573278\n",
      "\tspeed: 0.0202s/iter; left time: 139.2860s\n",
      "Epoch: 8 cost time: 11.763284921646118\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2557399 Vali Loss: 0.0562486 Test Loss: 0.1602705\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2525150\n",
      "\tspeed: 0.0624s/iter; left time: 420.6657s\n",
      "\titers: 200, epoch: 9 | loss: 0.2477296\n",
      "\tspeed: 0.0223s/iter; left time: 148.0132s\n",
      "\titers: 300, epoch: 9 | loss: 0.3376626\n",
      "\tspeed: 0.0222s/iter; left time: 145.4651s\n",
      "\titers: 400, epoch: 9 | loss: 0.1935522\n",
      "\tspeed: 0.0223s/iter; left time: 143.5379s\n",
      "\titers: 500, epoch: 9 | loss: 0.1374224\n",
      "\tspeed: 0.0222s/iter; left time: 141.0254s\n",
      "Epoch: 9 cost time: 12.967283725738525\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2557171 Vali Loss: 0.0556010 Test Loss: 0.1605741\n",
      "Validation loss decreased (0.056052 --> 0.055601).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2737424\n",
      "\tspeed: 0.0625s/iter; left time: 385.5241s\n",
      "\titers: 200, epoch: 10 | loss: 0.1813244\n",
      "\tspeed: 0.0202s/iter; left time: 122.6427s\n",
      "\titers: 300, epoch: 10 | loss: 0.4009261\n",
      "\tspeed: 0.0202s/iter; left time: 120.5661s\n",
      "\titers: 400, epoch: 10 | loss: 0.3548692\n",
      "\tspeed: 0.0202s/iter; left time: 118.5424s\n",
      "\titers: 500, epoch: 10 | loss: 0.2752964\n",
      "\tspeed: 0.0202s/iter; left time: 116.5716s\n",
      "Epoch: 10 cost time: 11.817394733428955\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2539065 Vali Loss: 0.0560734 Test Loss: 0.1604954\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2159983\n",
      "\tspeed: 0.0625s/iter; left time: 350.1328s\n",
      "\titers: 200, epoch: 11 | loss: 0.2328295\n",
      "\tspeed: 0.0223s/iter; left time: 122.5611s\n",
      "\titers: 300, epoch: 11 | loss: 0.1948394\n",
      "\tspeed: 0.0223s/iter; left time: 120.2153s\n",
      "\titers: 400, epoch: 11 | loss: 0.2906358\n",
      "\tspeed: 0.0222s/iter; left time: 117.8911s\n",
      "\titers: 500, epoch: 11 | loss: 0.2166879\n",
      "\tspeed: 0.0223s/iter; left time: 115.7282s\n",
      "Epoch: 11 cost time: 12.963262557983398\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2556766 Vali Loss: 0.0561369 Test Loss: 0.1604189\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2205788\n",
      "\tspeed: 0.0626s/iter; left time: 315.1270s\n",
      "\titers: 200, epoch: 12 | loss: 0.2239135\n",
      "\tspeed: 0.0202s/iter; left time: 99.6682s\n",
      "\titers: 300, epoch: 12 | loss: 0.4537135\n",
      "\tspeed: 0.0202s/iter; left time: 97.3472s\n",
      "\titers: 400, epoch: 12 | loss: 0.2867092\n",
      "\tspeed: 0.0202s/iter; left time: 95.6214s\n",
      "\titers: 500, epoch: 12 | loss: 0.3965144\n",
      "\tspeed: 0.0202s/iter; left time: 93.6129s\n",
      "Epoch: 12 cost time: 11.790961265563965\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2542800 Vali Loss: 0.0559142 Test Loss: 0.1604519\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16082032024860382, mae:0.2550908029079437\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1944437\n",
      "\tspeed: 0.0231s/iter; left time: 261.4744s\n",
      "\titers: 200, epoch: 1 | loss: 0.1642704\n",
      "\tspeed: 0.0112s/iter; left time: 124.9276s\n",
      "\titers: 300, epoch: 1 | loss: 0.2080029\n",
      "\tspeed: 0.0112s/iter; left time: 124.1263s\n",
      "\titers: 400, epoch: 1 | loss: 0.1865716\n",
      "\tspeed: 0.0111s/iter; left time: 122.5701s\n",
      "\titers: 500, epoch: 1 | loss: 0.1544829\n",
      "\tspeed: 0.0112s/iter; left time: 121.8219s\n",
      "Epoch: 1 cost time: 7.6107337474823\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1804872 Vali Loss: 0.0488531 Test Loss: 0.1410827\n",
      "Validation loss decreased (inf --> 0.048853).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1664304\n",
      "\tspeed: 0.0366s/iter; left time: 392.2806s\n",
      "\titers: 200, epoch: 2 | loss: 0.2020042\n",
      "\tspeed: 0.0099s/iter; left time: 105.1216s\n",
      "\titers: 300, epoch: 2 | loss: 0.1241654\n",
      "\tspeed: 0.0099s/iter; left time: 103.7411s\n",
      "\titers: 400, epoch: 2 | loss: 0.1342875\n",
      "\tspeed: 0.0099s/iter; left time: 102.8937s\n",
      "\titers: 500, epoch: 2 | loss: 0.1565352\n",
      "\tspeed: 0.0098s/iter; left time: 101.6498s\n",
      "Epoch: 2 cost time: 5.954463958740234\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1322044 Vali Loss: 0.0428261 Test Loss: 0.1458761\n",
      "Validation loss decreased (0.048853 --> 0.042826).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0983299\n",
      "\tspeed: 0.0366s/iter; left time: 371.4651s\n",
      "\titers: 200, epoch: 3 | loss: 0.0833045\n",
      "\tspeed: 0.0100s/iter; left time: 100.3802s\n",
      "\titers: 300, epoch: 3 | loss: 0.0775792\n",
      "\tspeed: 0.0100s/iter; left time: 99.4109s\n",
      "\titers: 400, epoch: 3 | loss: 0.1087999\n",
      "\tspeed: 0.0099s/iter; left time: 98.1120s\n",
      "\titers: 500, epoch: 3 | loss: 0.1334865\n",
      "\tspeed: 0.0100s/iter; left time: 97.1779s\n",
      "Epoch: 3 cost time: 5.99560022354126\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0894185 Vali Loss: 0.0405665 Test Loss: 0.1412212\n",
      "Validation loss decreased (0.042826 --> 0.040567).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0738157\n",
      "\tspeed: 0.0372s/iter; left time: 357.1442s\n",
      "\titers: 200, epoch: 4 | loss: 0.0421157\n",
      "\tspeed: 0.0112s/iter; left time: 106.4949s\n",
      "\titers: 300, epoch: 4 | loss: 0.0593118\n",
      "\tspeed: 0.0112s/iter; left time: 104.9668s\n",
      "\titers: 400, epoch: 4 | loss: 0.0594450\n",
      "\tspeed: 0.0112s/iter; left time: 103.7061s\n",
      "\titers: 500, epoch: 4 | loss: 0.0589572\n",
      "\tspeed: 0.0112s/iter; left time: 102.7309s\n",
      "Epoch: 4 cost time: 6.68479585647583\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0601401 Vali Loss: 0.0413942 Test Loss: 0.1406739\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0557960\n",
      "\tspeed: 0.0395s/iter; left time: 356.5184s\n",
      "\titers: 200, epoch: 5 | loss: 0.0553470\n",
      "\tspeed: 0.0112s/iter; left time: 99.5246s\n",
      "\titers: 300, epoch: 5 | loss: 0.0386101\n",
      "\tspeed: 0.0112s/iter; left time: 98.4658s\n",
      "\titers: 400, epoch: 5 | loss: 0.0521770\n",
      "\tspeed: 0.0112s/iter; left time: 97.3562s\n",
      "\titers: 500, epoch: 5 | loss: 0.0316364\n",
      "\tspeed: 0.0111s/iter; left time: 96.0765s\n",
      "Epoch: 5 cost time: 6.698359489440918\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0463121 Vali Loss: 0.0392300 Test Loss: 0.1354600\n",
      "Validation loss decreased (0.040567 --> 0.039230).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0314671\n",
      "\tspeed: 0.0386s/iter; left time: 326.1754s\n",
      "\titers: 200, epoch: 6 | loss: 0.0347356\n",
      "\tspeed: 0.0113s/iter; left time: 94.7238s\n",
      "\titers: 300, epoch: 6 | loss: 0.0368734\n",
      "\tspeed: 0.0110s/iter; left time: 90.3753s\n",
      "\titers: 400, epoch: 6 | loss: 0.0429091\n",
      "\tspeed: 0.0099s/iter; left time: 80.3310s\n",
      "\titers: 500, epoch: 6 | loss: 0.0451638\n",
      "\tspeed: 0.0098s/iter; left time: 79.1058s\n",
      "Epoch: 6 cost time: 6.311911344528198\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0389716 Vali Loss: 0.0409728 Test Loss: 0.1399496\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0390323\n",
      "\tspeed: 0.0370s/iter; left time: 291.4177s\n",
      "\titers: 200, epoch: 7 | loss: 0.0337189\n",
      "\tspeed: 0.0100s/iter; left time: 77.4670s\n",
      "\titers: 300, epoch: 7 | loss: 0.0284107\n",
      "\tspeed: 0.0100s/iter; left time: 76.4699s\n",
      "\titers: 400, epoch: 7 | loss: 0.0361575\n",
      "\tspeed: 0.0099s/iter; left time: 75.1587s\n",
      "\titers: 500, epoch: 7 | loss: 0.0334786\n",
      "\tspeed: 0.0099s/iter; left time: 74.1968s\n",
      "Epoch: 7 cost time: 5.994266986846924\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0354632 Vali Loss: 0.0407551 Test Loss: 0.1402271\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0376807\n",
      "\tspeed: 0.0366s/iter; left time: 267.8880s\n",
      "\titers: 200, epoch: 8 | loss: 0.0568481\n",
      "\tspeed: 0.0099s/iter; left time: 71.2362s\n",
      "\titers: 300, epoch: 8 | loss: 0.0314264\n",
      "\tspeed: 0.0098s/iter; left time: 69.9176s\n",
      "\titers: 400, epoch: 8 | loss: 0.0443626\n",
      "\tspeed: 0.0098s/iter; left time: 68.9238s\n",
      "\titers: 500, epoch: 8 | loss: 0.0328621\n",
      "\tspeed: 0.0098s/iter; left time: 67.9549s\n",
      "Epoch: 8 cost time: 5.872874975204468\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0336702 Vali Loss: 0.0409307 Test Loss: 0.1423887\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.135650172829628, mae:0.23488344252109528\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1940793\n",
      "\tspeed: 0.0218s/iter; left time: 246.4095s\n",
      "\titers: 200, epoch: 1 | loss: 0.1388180\n",
      "\tspeed: 0.0097s/iter; left time: 109.0732s\n",
      "\titers: 300, epoch: 1 | loss: 0.2141243\n",
      "\tspeed: 0.0098s/iter; left time: 108.3535s\n",
      "\titers: 400, epoch: 1 | loss: 0.1099621\n",
      "\tspeed: 0.0097s/iter; left time: 107.1513s\n",
      "\titers: 500, epoch: 1 | loss: 0.1165200\n",
      "\tspeed: 0.0097s/iter; left time: 106.2216s\n",
      "Epoch: 1 cost time: 6.806869268417358\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1467942 Vali Loss: 0.0391663 Test Loss: 0.1277200\n",
      "Validation loss decreased (inf --> 0.039166).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1055574\n",
      "\tspeed: 0.0356s/iter; left time: 382.4727s\n",
      "\titers: 200, epoch: 2 | loss: 0.1487699\n",
      "\tspeed: 0.0097s/iter; left time: 103.3956s\n",
      "\titers: 300, epoch: 2 | loss: 0.0745109\n",
      "\tspeed: 0.0097s/iter; left time: 102.3675s\n",
      "\titers: 400, epoch: 2 | loss: 0.0588041\n",
      "\tspeed: 0.0097s/iter; left time: 101.3768s\n",
      "\titers: 500, epoch: 2 | loss: 0.0908444\n",
      "\tspeed: 0.0097s/iter; left time: 100.4077s\n",
      "Epoch: 2 cost time: 5.8583714962005615\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1147852 Vali Loss: 0.0365444 Test Loss: 0.1178358\n",
      "Validation loss decreased (0.039166 --> 0.036544).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0494687\n",
      "\tspeed: 0.0366s/iter; left time: 371.8677s\n",
      "\titers: 200, epoch: 3 | loss: 0.0652063\n",
      "\tspeed: 0.0097s/iter; left time: 97.4264s\n",
      "\titers: 300, epoch: 3 | loss: 0.1403556\n",
      "\tspeed: 0.0097s/iter; left time: 96.4735s\n",
      "\titers: 400, epoch: 3 | loss: 0.0767204\n",
      "\tspeed: 0.0097s/iter; left time: 95.6301s\n",
      "\titers: 500, epoch: 3 | loss: 0.0613843\n",
      "\tspeed: 0.0097s/iter; left time: 94.5840s\n",
      "Epoch: 3 cost time: 5.842099666595459\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0810848 Vali Loss: 0.0354074 Test Loss: 0.1190029\n",
      "Validation loss decreased (0.036544 --> 0.035407).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0427368\n",
      "\tspeed: 0.0345s/iter; left time: 330.9102s\n",
      "\titers: 200, epoch: 4 | loss: 0.0439321\n",
      "\tspeed: 0.0097s/iter; left time: 92.0103s\n",
      "\titers: 300, epoch: 4 | loss: 0.0371526\n",
      "\tspeed: 0.0097s/iter; left time: 91.1910s\n",
      "\titers: 400, epoch: 4 | loss: 0.0446692\n",
      "\tspeed: 0.0097s/iter; left time: 90.1351s\n",
      "\titers: 500, epoch: 4 | loss: 0.0675589\n",
      "\tspeed: 0.0097s/iter; left time: 89.1297s\n",
      "Epoch: 4 cost time: 5.8341965675354\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0567281 Vali Loss: 0.0344380 Test Loss: 0.1204967\n",
      "Validation loss decreased (0.035407 --> 0.034438).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0475336\n",
      "\tspeed: 0.0370s/iter; left time: 333.5159s\n",
      "\titers: 200, epoch: 5 | loss: 0.0603414\n",
      "\tspeed: 0.0109s/iter; left time: 97.0085s\n",
      "\titers: 300, epoch: 5 | loss: 0.0557232\n",
      "\tspeed: 0.0110s/iter; left time: 96.6108s\n",
      "\titers: 400, epoch: 5 | loss: 0.0629306\n",
      "\tspeed: 0.0109s/iter; left time: 94.6372s\n",
      "\titers: 500, epoch: 5 | loss: 0.0599310\n",
      "\tspeed: 0.0104s/iter; left time: 89.6942s\n",
      "Epoch: 5 cost time: 6.390201807022095\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0443423 Vali Loss: 0.0358920 Test Loss: 0.1261836\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0302408\n",
      "\tspeed: 0.0359s/iter; left time: 303.4531s\n",
      "\titers: 200, epoch: 6 | loss: 0.0627531\n",
      "\tspeed: 0.0096s/iter; left time: 80.4266s\n",
      "\titers: 300, epoch: 6 | loss: 0.0530466\n",
      "\tspeed: 0.0096s/iter; left time: 79.3524s\n",
      "\titers: 400, epoch: 6 | loss: 0.0300475\n",
      "\tspeed: 0.0096s/iter; left time: 78.4598s\n",
      "\titers: 500, epoch: 6 | loss: 0.0230651\n",
      "\tspeed: 0.0096s/iter; left time: 77.4058s\n",
      "Epoch: 6 cost time: 5.783416032791138\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0382273 Vali Loss: 0.0365799 Test Loss: 0.1277658\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0277006\n",
      "\tspeed: 0.0374s/iter; left time: 294.8680s\n",
      "\titers: 200, epoch: 7 | loss: 0.0329772\n",
      "\tspeed: 0.0109s/iter; left time: 84.5415s\n",
      "\titers: 300, epoch: 7 | loss: 0.0540259\n",
      "\tspeed: 0.0109s/iter; left time: 83.6131s\n",
      "\titers: 400, epoch: 7 | loss: 0.0743621\n",
      "\tspeed: 0.0109s/iter; left time: 82.5463s\n",
      "\titers: 500, epoch: 7 | loss: 0.0309425\n",
      "\tspeed: 0.0109s/iter; left time: 81.3714s\n",
      "Epoch: 7 cost time: 6.532441854476929\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0351306 Vali Loss: 0.0364887 Test Loss: 0.1271494\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12067621946334839, mae:0.216682568192482\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1714890\n",
      "\tspeed: 0.0228s/iter; left time: 257.9778s\n",
      "\titers: 200, epoch: 1 | loss: 0.1268095\n",
      "\tspeed: 0.0111s/iter; left time: 124.2058s\n",
      "\titers: 300, epoch: 1 | loss: 0.1375185\n",
      "\tspeed: 0.0111s/iter; left time: 123.4492s\n",
      "\titers: 400, epoch: 1 | loss: 0.0977060\n",
      "\tspeed: 0.0111s/iter; left time: 122.1305s\n",
      "\titers: 500, epoch: 1 | loss: 0.1776273\n",
      "\tspeed: 0.0111s/iter; left time: 120.9231s\n",
      "Epoch: 1 cost time: 7.552199125289917\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1847429 Vali Loss: 0.0443673 Test Loss: 0.1467872\n",
      "Validation loss decreased (inf --> 0.044367).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1578133\n",
      "\tspeed: 0.0382s/iter; left time: 410.2405s\n",
      "\titers: 200, epoch: 2 | loss: 0.2252475\n",
      "\tspeed: 0.0113s/iter; left time: 119.6660s\n",
      "\titers: 300, epoch: 2 | loss: 0.1275061\n",
      "\tspeed: 0.0112s/iter; left time: 118.0402s\n",
      "\titers: 400, epoch: 2 | loss: 0.1975546\n",
      "\tspeed: 0.0112s/iter; left time: 117.1903s\n",
      "\titers: 500, epoch: 2 | loss: 0.1235378\n",
      "\tspeed: 0.0112s/iter; left time: 115.7574s\n",
      "Epoch: 2 cost time: 6.6874799728393555\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1313124 Vali Loss: 0.0441769 Test Loss: 0.1452591\n",
      "Validation loss decreased (0.044367 --> 0.044177).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0545728\n",
      "\tspeed: 0.0381s/iter; left time: 387.0535s\n",
      "\titers: 200, epoch: 3 | loss: 0.0621531\n",
      "\tspeed: 0.0111s/iter; left time: 111.9750s\n",
      "\titers: 300, epoch: 3 | loss: 0.0807057\n",
      "\tspeed: 0.0112s/iter; left time: 111.1513s\n",
      "\titers: 400, epoch: 3 | loss: 0.0659295\n",
      "\tspeed: 0.0112s/iter; left time: 110.2294s\n",
      "\titers: 500, epoch: 3 | loss: 0.0607469\n",
      "\tspeed: 0.0112s/iter; left time: 109.2692s\n",
      "Epoch: 3 cost time: 6.6767027378082275\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0804944 Vali Loss: 0.0399675 Test Loss: 0.1428847\n",
      "Validation loss decreased (0.044177 --> 0.039968).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0398316\n",
      "\tspeed: 0.0371s/iter; left time: 356.2886s\n",
      "\titers: 200, epoch: 4 | loss: 0.0337781\n",
      "\tspeed: 0.0109s/iter; left time: 103.5341s\n",
      "\titers: 300, epoch: 4 | loss: 0.0411565\n",
      "\tspeed: 0.0109s/iter; left time: 102.5939s\n",
      "\titers: 400, epoch: 4 | loss: 0.0658094\n",
      "\tspeed: 0.0109s/iter; left time: 101.7112s\n",
      "\titers: 500, epoch: 4 | loss: 0.0430230\n",
      "\tspeed: 0.0109s/iter; left time: 100.6365s\n",
      "Epoch: 4 cost time: 6.54393196105957\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0502238 Vali Loss: 0.0407962 Test Loss: 0.1488430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0395215\n",
      "\tspeed: 0.0358s/iter; left time: 323.0058s\n",
      "\titers: 200, epoch: 5 | loss: 0.0381135\n",
      "\tspeed: 0.0097s/iter; left time: 86.9522s\n",
      "\titers: 300, epoch: 5 | loss: 0.0297308\n",
      "\tspeed: 0.0097s/iter; left time: 85.8597s\n",
      "\titers: 400, epoch: 5 | loss: 0.0274207\n",
      "\tspeed: 0.0098s/iter; left time: 85.5152s\n",
      "\titers: 500, epoch: 5 | loss: 0.0346355\n",
      "\tspeed: 0.0098s/iter; left time: 84.7656s\n",
      "Epoch: 5 cost time: 5.883734703063965\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0374292 Vali Loss: 0.0404922 Test Loss: 0.1463897\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0315148\n",
      "\tspeed: 0.0350s/iter; left time: 295.4121s\n",
      "\titers: 200, epoch: 6 | loss: 0.0427770\n",
      "\tspeed: 0.0098s/iter; left time: 81.7480s\n",
      "\titers: 300, epoch: 6 | loss: 0.0287391\n",
      "\tspeed: 0.0098s/iter; left time: 80.5147s\n",
      "\titers: 400, epoch: 6 | loss: 0.0242794\n",
      "\tspeed: 0.0097s/iter; left time: 79.4178s\n",
      "\titers: 500, epoch: 6 | loss: 0.0340353\n",
      "\tspeed: 0.0098s/iter; left time: 78.5339s\n",
      "Epoch: 6 cost time: 5.857626438140869\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0319297 Vali Loss: 0.0407948 Test Loss: 0.1484461\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.143092542886734, mae:0.2408505529165268\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2388572\n",
      "\tspeed: 0.0232s/iter; left time: 262.5280s\n",
      "\titers: 200, epoch: 1 | loss: 0.2025720\n",
      "\tspeed: 0.0114s/iter; left time: 127.7553s\n",
      "\titers: 300, epoch: 1 | loss: 0.1443961\n",
      "\tspeed: 0.0114s/iter; left time: 126.4329s\n",
      "\titers: 400, epoch: 1 | loss: 0.0870342\n",
      "\tspeed: 0.0114s/iter; left time: 125.6276s\n",
      "\titers: 500, epoch: 1 | loss: 0.1572416\n",
      "\tspeed: 0.0115s/iter; left time: 124.8798s\n",
      "Epoch: 1 cost time: 7.7398059368133545\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2009764 Vali Loss: 0.0460886 Test Loss: 0.1417371\n",
      "Validation loss decreased (inf --> 0.046089).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1435807\n",
      "\tspeed: 0.0389s/iter; left time: 417.3806s\n",
      "\titers: 200, epoch: 2 | loss: 0.1392410\n",
      "\tspeed: 0.0115s/iter; left time: 121.8254s\n",
      "\titers: 300, epoch: 2 | loss: 0.1236219\n",
      "\tspeed: 0.0115s/iter; left time: 120.7884s\n",
      "\titers: 400, epoch: 2 | loss: 0.0819476\n",
      "\tspeed: 0.0115s/iter; left time: 119.5999s\n",
      "\titers: 500, epoch: 2 | loss: 0.1863607\n",
      "\tspeed: 0.0115s/iter; left time: 118.5341s\n",
      "Epoch: 2 cost time: 6.859601736068726\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1794212 Vali Loss: 0.0579344 Test Loss: 0.1662709\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1860282\n",
      "\tspeed: 0.0383s/iter; left time: 389.0488s\n",
      "\titers: 200, epoch: 3 | loss: 0.1703259\n",
      "\tspeed: 0.0114s/iter; left time: 114.9935s\n",
      "\titers: 300, epoch: 3 | loss: 0.1034399\n",
      "\tspeed: 0.0115s/iter; left time: 114.0635s\n",
      "\titers: 400, epoch: 3 | loss: 0.1257529\n",
      "\tspeed: 0.0114s/iter; left time: 112.7001s\n",
      "\titers: 500, epoch: 3 | loss: 0.1156912\n",
      "\tspeed: 0.0114s/iter; left time: 111.6939s\n",
      "Epoch: 3 cost time: 6.817661285400391\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1496666 Vali Loss: 0.0448545 Test Loss: 0.1353032\n",
      "Validation loss decreased (0.046089 --> 0.044854).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1272880\n",
      "\tspeed: 0.0388s/iter; left time: 371.8481s\n",
      "\titers: 200, epoch: 4 | loss: 0.1286890\n",
      "\tspeed: 0.0114s/iter; left time: 108.5797s\n",
      "\titers: 300, epoch: 4 | loss: 0.0969923\n",
      "\tspeed: 0.0114s/iter; left time: 107.2541s\n",
      "\titers: 400, epoch: 4 | loss: 0.0714660\n",
      "\tspeed: 0.0115s/iter; left time: 106.4393s\n",
      "\titers: 500, epoch: 4 | loss: 0.1516775\n",
      "\tspeed: 0.0115s/iter; left time: 105.2831s\n",
      "Epoch: 4 cost time: 6.816018581390381\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1244113 Vali Loss: 0.0458936 Test Loss: 0.1368112\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0831007\n",
      "\tspeed: 0.0374s/iter; left time: 337.2361s\n",
      "\titers: 200, epoch: 5 | loss: 0.0894234\n",
      "\tspeed: 0.0114s/iter; left time: 101.8794s\n",
      "\titers: 300, epoch: 5 | loss: 0.1539751\n",
      "\tspeed: 0.0114s/iter; left time: 100.9407s\n",
      "\titers: 400, epoch: 5 | loss: 0.0966540\n",
      "\tspeed: 0.0114s/iter; left time: 99.7683s\n",
      "\titers: 500, epoch: 5 | loss: 0.1262841\n",
      "\tspeed: 0.0114s/iter; left time: 98.4048s\n",
      "Epoch: 5 cost time: 6.79752516746521\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1050426 Vali Loss: 0.0450158 Test Loss: 0.1372498\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0814576\n",
      "\tspeed: 0.0378s/iter; left time: 319.3679s\n",
      "\titers: 200, epoch: 6 | loss: 0.0685639\n",
      "\tspeed: 0.0114s/iter; left time: 95.5603s\n",
      "\titers: 300, epoch: 6 | loss: 0.0879787\n",
      "\tspeed: 0.0114s/iter; left time: 94.3860s\n",
      "\titers: 400, epoch: 6 | loss: 0.0606897\n",
      "\tspeed: 0.0114s/iter; left time: 93.2486s\n",
      "\titers: 500, epoch: 6 | loss: 0.0625465\n",
      "\tspeed: 0.0114s/iter; left time: 92.1211s\n",
      "Epoch: 6 cost time: 6.792489767074585\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0924215 Vali Loss: 0.0437972 Test Loss: 0.1383720\n",
      "Validation loss decreased (0.044854 --> 0.043797).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0925268\n",
      "\tspeed: 0.0379s/iter; left time: 299.0015s\n",
      "\titers: 200, epoch: 7 | loss: 0.0876379\n",
      "\tspeed: 0.0114s/iter; left time: 88.6799s\n",
      "\titers: 300, epoch: 7 | loss: 0.0903184\n",
      "\tspeed: 0.0114s/iter; left time: 87.5677s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643708\n",
      "\tspeed: 0.0114s/iter; left time: 86.5562s\n",
      "\titers: 500, epoch: 7 | loss: 0.1023967\n",
      "\tspeed: 0.0114s/iter; left time: 85.2488s\n",
      "Epoch: 7 cost time: 6.780706882476807\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0852080 Vali Loss: 0.0439345 Test Loss: 0.1393003\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0709743\n",
      "\tspeed: 0.0402s/iter; left time: 294.1995s\n",
      "\titers: 200, epoch: 8 | loss: 0.0723450\n",
      "\tspeed: 0.0116s/iter; left time: 83.9817s\n",
      "\titers: 300, epoch: 8 | loss: 0.0668660\n",
      "\tspeed: 0.0115s/iter; left time: 81.5001s\n",
      "\titers: 400, epoch: 8 | loss: 0.0706788\n",
      "\tspeed: 0.0115s/iter; left time: 80.5803s\n",
      "\titers: 500, epoch: 8 | loss: 0.0804036\n",
      "\tspeed: 0.0115s/iter; left time: 79.3393s\n",
      "Epoch: 8 cost time: 6.982064962387085\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0810429 Vali Loss: 0.0440970 Test Loss: 0.1393788\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0490901\n",
      "\tspeed: 0.0402s/iter; left time: 270.9074s\n",
      "\titers: 200, epoch: 9 | loss: 0.0777733\n",
      "\tspeed: 0.0115s/iter; left time: 76.0610s\n",
      "\titers: 300, epoch: 9 | loss: 0.0971335\n",
      "\tspeed: 0.0115s/iter; left time: 75.0123s\n",
      "\titers: 400, epoch: 9 | loss: 0.0591456\n",
      "\tspeed: 0.0115s/iter; left time: 73.8601s\n",
      "\titers: 500, epoch: 9 | loss: 0.0914075\n",
      "\tspeed: 0.0114s/iter; left time: 72.5994s\n",
      "Epoch: 9 cost time: 6.808941125869751\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0786846 Vali Loss: 0.0439969 Test Loss: 0.1393926\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13855166733264923, mae:0.23596133291721344\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.0974500\n",
      "\tspeed: 0.0230s/iter; left time: 260.0262s\n",
      "\titers: 200, epoch: 1 | loss: 0.2052382\n",
      "\tspeed: 0.0110s/iter; left time: 123.6560s\n",
      "\titers: 300, epoch: 1 | loss: 0.1459617\n",
      "\tspeed: 0.0111s/iter; left time: 122.7259s\n",
      "\titers: 400, epoch: 1 | loss: 0.1657365\n",
      "\tspeed: 0.0110s/iter; left time: 121.3802s\n",
      "\titers: 500, epoch: 1 | loss: 0.1082806\n",
      "\tspeed: 0.0111s/iter; left time: 120.4589s\n",
      "Epoch: 1 cost time: 7.542965888977051\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1601642 Vali Loss: 0.0422012 Test Loss: 0.1301945\n",
      "Validation loss decreased (inf --> 0.042201).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1238574\n",
      "\tspeed: 0.0405s/iter; left time: 435.0839s\n",
      "\titers: 200, epoch: 2 | loss: 0.0918113\n",
      "\tspeed: 0.0111s/iter; left time: 117.6440s\n",
      "\titers: 300, epoch: 2 | loss: 0.1213746\n",
      "\tspeed: 0.0111s/iter; left time: 116.6608s\n",
      "\titers: 400, epoch: 2 | loss: 0.1729004\n",
      "\tspeed: 0.0111s/iter; left time: 115.4325s\n",
      "\titers: 500, epoch: 2 | loss: 0.1869538\n",
      "\tspeed: 0.0110s/iter; left time: 113.9860s\n",
      "Epoch: 2 cost time: 6.6264612674713135\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1370953 Vali Loss: 0.0412954 Test Loss: 0.1261017\n",
      "Validation loss decreased (0.042201 --> 0.041295).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1401310\n",
      "\tspeed: 0.0391s/iter; left time: 397.1015s\n",
      "\titers: 200, epoch: 3 | loss: 0.0646986\n",
      "\tspeed: 0.0111s/iter; left time: 111.4142s\n",
      "\titers: 300, epoch: 3 | loss: 0.1279343\n",
      "\tspeed: 0.0111s/iter; left time: 110.3114s\n",
      "\titers: 400, epoch: 3 | loss: 0.1239163\n",
      "\tspeed: 0.0111s/iter; left time: 109.0126s\n",
      "\titers: 500, epoch: 3 | loss: 0.1246570\n",
      "\tspeed: 0.0110s/iter; left time: 107.8185s\n",
      "Epoch: 3 cost time: 6.641491174697876\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1077242 Vali Loss: 0.0355741 Test Loss: 0.1213372\n",
      "Validation loss decreased (0.041295 --> 0.035574).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0936984\n",
      "\tspeed: 0.0394s/iter; left time: 377.5760s\n",
      "\titers: 200, epoch: 4 | loss: 0.0660693\n",
      "\tspeed: 0.0120s/iter; left time: 114.3129s\n",
      "\titers: 300, epoch: 4 | loss: 0.0422177\n",
      "\tspeed: 0.0121s/iter; left time: 113.2244s\n",
      "\titers: 400, epoch: 4 | loss: 0.1038613\n",
      "\tspeed: 0.0119s/iter; left time: 110.8176s\n",
      "\titers: 500, epoch: 4 | loss: 0.0759548\n",
      "\tspeed: 0.0110s/iter; left time: 101.1919s\n",
      "Epoch: 4 cost time: 6.985462427139282\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0860111 Vali Loss: 0.0350507 Test Loss: 0.1255417\n",
      "Validation loss decreased (0.035574 --> 0.035051).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0573087\n",
      "\tspeed: 0.0384s/iter; left time: 346.4130s\n",
      "\titers: 200, epoch: 5 | loss: 0.0602667\n",
      "\tspeed: 0.0111s/iter; left time: 98.7681s\n",
      "\titers: 300, epoch: 5 | loss: 0.0824649\n",
      "\tspeed: 0.0111s/iter; left time: 97.5708s\n",
      "\titers: 400, epoch: 5 | loss: 0.0633375\n",
      "\tspeed: 0.0111s/iter; left time: 96.4713s\n",
      "\titers: 500, epoch: 5 | loss: 0.0625824\n",
      "\tspeed: 0.0110s/iter; left time: 95.2614s\n",
      "Epoch: 5 cost time: 6.6474058628082275\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0691570 Vali Loss: 0.0333579 Test Loss: 0.1244702\n",
      "Validation loss decreased (0.035051 --> 0.033358).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0775955\n",
      "\tspeed: 0.0389s/iter; left time: 328.8064s\n",
      "\titers: 200, epoch: 6 | loss: 0.0551258\n",
      "\tspeed: 0.0110s/iter; left time: 92.2023s\n",
      "\titers: 300, epoch: 6 | loss: 0.0558651\n",
      "\tspeed: 0.0111s/iter; left time: 91.1814s\n",
      "\titers: 400, epoch: 6 | loss: 0.0444722\n",
      "\tspeed: 0.0110s/iter; left time: 89.9931s\n",
      "\titers: 500, epoch: 6 | loss: 0.0528609\n",
      "\tspeed: 0.0110s/iter; left time: 88.6404s\n",
      "Epoch: 6 cost time: 6.61210560798645\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0584916 Vali Loss: 0.0337634 Test Loss: 0.1316425\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0433577\n",
      "\tspeed: 0.0386s/iter; left time: 303.9363s\n",
      "\titers: 200, epoch: 7 | loss: 0.0621938\n",
      "\tspeed: 0.0111s/iter; left time: 86.2441s\n",
      "\titers: 300, epoch: 7 | loss: 0.0465754\n",
      "\tspeed: 0.0111s/iter; left time: 84.8773s\n",
      "\titers: 400, epoch: 7 | loss: 0.0555614\n",
      "\tspeed: 0.0110s/iter; left time: 83.7364s\n",
      "\titers: 500, epoch: 7 | loss: 0.0376010\n",
      "\tspeed: 0.0110s/iter; left time: 82.6378s\n",
      "Epoch: 7 cost time: 6.62431788444519\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0522816 Vali Loss: 0.0336066 Test Loss: 0.1316548\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0479672\n",
      "\tspeed: 0.0367s/iter; left time: 268.2666s\n",
      "\titers: 200, epoch: 8 | loss: 0.0504123\n",
      "\tspeed: 0.0110s/iter; left time: 79.4449s\n",
      "\titers: 300, epoch: 8 | loss: 0.0374279\n",
      "\tspeed: 0.0110s/iter; left time: 78.3282s\n",
      "\titers: 400, epoch: 8 | loss: 0.0494475\n",
      "\tspeed: 0.0110s/iter; left time: 77.2228s\n",
      "\titers: 500, epoch: 8 | loss: 0.0504735\n",
      "\tspeed: 0.0110s/iter; left time: 76.1324s\n",
      "Epoch: 8 cost time: 6.554762363433838\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0490019 Vali Loss: 0.0342471 Test Loss: 0.1331143\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12465690076351166, mae:0.21787424385547638\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2019889\n",
      "\tspeed: 0.0235s/iter; left time: 265.5710s\n",
      "\titers: 200, epoch: 1 | loss: 0.2130914\n",
      "\tspeed: 0.0114s/iter; left time: 127.1667s\n",
      "\titers: 300, epoch: 1 | loss: 0.1833379\n",
      "\tspeed: 0.0114s/iter; left time: 126.4591s\n",
      "\titers: 400, epoch: 1 | loss: 0.1353990\n",
      "\tspeed: 0.0114s/iter; left time: 125.0563s\n",
      "\titers: 500, epoch: 1 | loss: 0.1589779\n",
      "\tspeed: 0.0114s/iter; left time: 124.3560s\n",
      "Epoch: 1 cost time: 7.756711721420288\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1984546 Vali Loss: 0.0496909 Test Loss: 0.1584453\n",
      "Validation loss decreased (inf --> 0.049691).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1731774\n",
      "\tspeed: 0.0376s/iter; left time: 403.9704s\n",
      "\titers: 200, epoch: 2 | loss: 0.2615232\n",
      "\tspeed: 0.0103s/iter; left time: 109.0370s\n",
      "\titers: 300, epoch: 2 | loss: 0.1243211\n",
      "\tspeed: 0.0103s/iter; left time: 108.0183s\n",
      "\titers: 400, epoch: 2 | loss: 0.1666401\n",
      "\tspeed: 0.0103s/iter; left time: 107.1997s\n",
      "\titers: 500, epoch: 2 | loss: 0.1445317\n",
      "\tspeed: 0.0103s/iter; left time: 105.9342s\n",
      "Epoch: 2 cost time: 6.165400743484497\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2028869 Vali Loss: 0.0559001 Test Loss: 0.1606367\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1585160\n",
      "\tspeed: 0.0381s/iter; left time: 387.2574s\n",
      "\titers: 200, epoch: 3 | loss: 0.1345448\n",
      "\tspeed: 0.0103s/iter; left time: 103.6941s\n",
      "\titers: 300, epoch: 3 | loss: 0.1909739\n",
      "\tspeed: 0.0103s/iter; left time: 102.4446s\n",
      "\titers: 400, epoch: 3 | loss: 0.1253612\n",
      "\tspeed: 0.0103s/iter; left time: 101.3781s\n",
      "\titers: 500, epoch: 3 | loss: 0.1260650\n",
      "\tspeed: 0.0102s/iter; left time: 100.0312s\n",
      "Epoch: 3 cost time: 6.191855430603027\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1610141 Vali Loss: 0.0496181 Test Loss: 0.1464581\n",
      "Validation loss decreased (0.049691 --> 0.049618).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1535898\n",
      "\tspeed: 0.0382s/iter; left time: 366.6446s\n",
      "\titers: 200, epoch: 4 | loss: 0.1112829\n",
      "\tspeed: 0.0114s/iter; left time: 108.2287s\n",
      "\titers: 300, epoch: 4 | loss: 0.1107739\n",
      "\tspeed: 0.0107s/iter; left time: 100.6790s\n",
      "\titers: 400, epoch: 4 | loss: 0.2499674\n",
      "\tspeed: 0.0103s/iter; left time: 95.2904s\n",
      "\titers: 500, epoch: 4 | loss: 0.0918426\n",
      "\tspeed: 0.0103s/iter; left time: 94.3467s\n",
      "Epoch: 4 cost time: 6.451675891876221\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1255826 Vali Loss: 0.0450062 Test Loss: 0.1426192\n",
      "Validation loss decreased (0.049618 --> 0.045006).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1097186\n",
      "\tspeed: 0.0382s/iter; left time: 344.7734s\n",
      "\titers: 200, epoch: 5 | loss: 0.1317494\n",
      "\tspeed: 0.0114s/iter; left time: 101.5193s\n",
      "\titers: 300, epoch: 5 | loss: 0.1488588\n",
      "\tspeed: 0.0114s/iter; left time: 100.1938s\n",
      "\titers: 400, epoch: 5 | loss: 0.0857320\n",
      "\tspeed: 0.0105s/iter; left time: 91.6386s\n",
      "\titers: 500, epoch: 5 | loss: 0.1473603\n",
      "\tspeed: 0.0102s/iter; left time: 88.1586s\n",
      "Epoch: 5 cost time: 6.513279438018799\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1054959 Vali Loss: 0.0452785 Test Loss: 0.1415697\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0955923\n",
      "\tspeed: 0.0373s/iter; left time: 315.4189s\n",
      "\titers: 200, epoch: 6 | loss: 0.0796199\n",
      "\tspeed: 0.0103s/iter; left time: 86.0383s\n",
      "\titers: 300, epoch: 6 | loss: 0.0931741\n",
      "\tspeed: 0.0103s/iter; left time: 85.0698s\n",
      "\titers: 400, epoch: 6 | loss: 0.0931934\n",
      "\tspeed: 0.0103s/iter; left time: 84.1215s\n",
      "\titers: 500, epoch: 6 | loss: 0.1077420\n",
      "\tspeed: 0.0103s/iter; left time: 83.0959s\n",
      "Epoch: 6 cost time: 6.2047271728515625\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0928161 Vali Loss: 0.0452328 Test Loss: 0.1478300\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0777740\n",
      "\tspeed: 0.0393s/iter; left time: 310.0170s\n",
      "\titers: 200, epoch: 7 | loss: 0.0906318\n",
      "\tspeed: 0.0118s/iter; left time: 91.9422s\n",
      "\titers: 300, epoch: 7 | loss: 0.0649016\n",
      "\tspeed: 0.0113s/iter; left time: 86.7489s\n",
      "\titers: 400, epoch: 7 | loss: 0.0970137\n",
      "\tspeed: 0.0103s/iter; left time: 78.0975s\n",
      "\titers: 500, epoch: 7 | loss: 0.0612615\n",
      "\tspeed: 0.0103s/iter; left time: 77.1852s\n",
      "Epoch: 7 cost time: 6.5860655307769775\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0846187 Vali Loss: 0.0453729 Test Loss: 0.1449361\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1428108662366867, mae:0.2442036122083664\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1978606\n",
      "\tspeed: 0.0253s/iter; left time: 285.7327s\n",
      "\titers: 200, epoch: 1 | loss: 0.1406488\n",
      "\tspeed: 0.0138s/iter; left time: 154.9034s\n",
      "\titers: 300, epoch: 1 | loss: 0.2377709\n",
      "\tspeed: 0.0138s/iter; left time: 153.1342s\n",
      "\titers: 400, epoch: 1 | loss: 0.1688309\n",
      "\tspeed: 0.0138s/iter; left time: 152.0688s\n",
      "\titers: 500, epoch: 1 | loss: 0.1429378\n",
      "\tspeed: 0.0138s/iter; left time: 150.6334s\n",
      "Epoch: 1 cost time: 9.077648401260376\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1824579 Vali Loss: 0.0461201 Test Loss: 0.1406951\n",
      "Validation loss decreased (inf --> 0.046120).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1808321\n",
      "\tspeed: 0.0451s/iter; left time: 484.2704s\n",
      "\titers: 200, epoch: 2 | loss: 0.1237853\n",
      "\tspeed: 0.0137s/iter; left time: 145.3898s\n",
      "\titers: 300, epoch: 2 | loss: 0.1161452\n",
      "\tspeed: 0.0137s/iter; left time: 143.9057s\n",
      "\titers: 400, epoch: 2 | loss: 0.0831951\n",
      "\tspeed: 0.0137s/iter; left time: 142.7784s\n",
      "\titers: 500, epoch: 2 | loss: 0.1708268\n",
      "\tspeed: 0.0137s/iter; left time: 141.0289s\n",
      "Epoch: 2 cost time: 8.10493278503418\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1403602 Vali Loss: 0.0479635 Test Loss: 0.1585671\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1754857\n",
      "\tspeed: 0.0470s/iter; left time: 477.9896s\n",
      "\titers: 200, epoch: 3 | loss: 0.0950794\n",
      "\tspeed: 0.0138s/iter; left time: 138.7746s\n",
      "\titers: 300, epoch: 3 | loss: 0.0790503\n",
      "\tspeed: 0.0138s/iter; left time: 137.2874s\n",
      "\titers: 400, epoch: 3 | loss: 0.0907891\n",
      "\tspeed: 0.0137s/iter; left time: 135.3569s\n",
      "\titers: 500, epoch: 3 | loss: 0.0887034\n",
      "\tspeed: 0.0137s/iter; left time: 133.9684s\n",
      "Epoch: 3 cost time: 8.151356220245361\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1063732 Vali Loss: 0.0409150 Test Loss: 0.1384491\n",
      "Validation loss decreased (0.046120 --> 0.040915).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0839169\n",
      "\tspeed: 0.0471s/iter; left time: 452.1925s\n",
      "\titers: 200, epoch: 4 | loss: 0.0629344\n",
      "\tspeed: 0.0138s/iter; left time: 130.5748s\n",
      "\titers: 300, epoch: 4 | loss: 0.0953755\n",
      "\tspeed: 0.0138s/iter; left time: 129.1842s\n",
      "\titers: 400, epoch: 4 | loss: 0.0753769\n",
      "\tspeed: 0.0137s/iter; left time: 127.7102s\n",
      "\titers: 500, epoch: 4 | loss: 0.0546866\n",
      "\tspeed: 0.0137s/iter; left time: 126.2286s\n",
      "Epoch: 4 cost time: 8.154263496398926\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0716987 Vali Loss: 0.0403713 Test Loss: 0.1423966\n",
      "Validation loss decreased (0.040915 --> 0.040371).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0682062\n",
      "\tspeed: 0.0444s/iter; left time: 400.7152s\n",
      "\titers: 200, epoch: 5 | loss: 0.0543592\n",
      "\tspeed: 0.0136s/iter; left time: 121.4990s\n",
      "\titers: 300, epoch: 5 | loss: 0.0492750\n",
      "\tspeed: 0.0136s/iter; left time: 120.4039s\n",
      "\titers: 400, epoch: 5 | loss: 0.0439108\n",
      "\tspeed: 0.0136s/iter; left time: 119.0160s\n",
      "\titers: 500, epoch: 5 | loss: 0.0325127\n",
      "\tspeed: 0.0136s/iter; left time: 117.6236s\n",
      "Epoch: 5 cost time: 8.04454231262207\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0540604 Vali Loss: 0.0405654 Test Loss: 0.1445131\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0450938\n",
      "\tspeed: 0.0449s/iter; left time: 379.1597s\n",
      "\titers: 200, epoch: 6 | loss: 0.0751078\n",
      "\tspeed: 0.0138s/iter; left time: 115.2269s\n",
      "\titers: 300, epoch: 6 | loss: 0.0407169\n",
      "\tspeed: 0.0138s/iter; left time: 113.8489s\n",
      "\titers: 400, epoch: 6 | loss: 0.0473953\n",
      "\tspeed: 0.0138s/iter; left time: 112.4643s\n",
      "\titers: 500, epoch: 6 | loss: 0.0438990\n",
      "\tspeed: 0.0138s/iter; left time: 111.1997s\n",
      "Epoch: 6 cost time: 8.113397359848022\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0449369 Vali Loss: 0.0414444 Test Loss: 0.1482243\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0427545\n",
      "\tspeed: 0.0439s/iter; left time: 346.1075s\n",
      "\titers: 200, epoch: 7 | loss: 0.0429754\n",
      "\tspeed: 0.0137s/iter; left time: 106.5255s\n",
      "\titers: 300, epoch: 7 | loss: 0.0315846\n",
      "\tspeed: 0.0137s/iter; left time: 105.3638s\n",
      "\titers: 400, epoch: 7 | loss: 0.0442837\n",
      "\tspeed: 0.0138s/iter; left time: 104.2444s\n",
      "\titers: 500, epoch: 7 | loss: 0.0533348\n",
      "\tspeed: 0.0137s/iter; left time: 102.8119s\n",
      "Epoch: 7 cost time: 8.11225938796997\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0403531 Vali Loss: 0.0424153 Test Loss: 0.1492263\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1425885260105133, mae:0.24156534671783447\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2268765\n",
      "\tspeed: 0.0249s/iter; left time: 281.9181s\n",
      "\titers: 200, epoch: 1 | loss: 0.1184546\n",
      "\tspeed: 0.0134s/iter; left time: 150.1864s\n",
      "\titers: 300, epoch: 1 | loss: 0.1239855\n",
      "\tspeed: 0.0134s/iter; left time: 148.5352s\n",
      "\titers: 400, epoch: 1 | loss: 0.1680717\n",
      "\tspeed: 0.0134s/iter; left time: 147.5342s\n",
      "\titers: 500, epoch: 1 | loss: 0.1292608\n",
      "\tspeed: 0.0134s/iter; left time: 146.0721s\n",
      "Epoch: 1 cost time: 8.840498685836792\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1520934 Vali Loss: 0.0372690 Test Loss: 0.1163540\n",
      "Validation loss decreased (inf --> 0.037269).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.0825776\n",
      "\tspeed: 0.0467s/iter; left time: 501.6026s\n",
      "\titers: 200, epoch: 2 | loss: 0.2555671\n",
      "\tspeed: 0.0135s/iter; left time: 143.7085s\n",
      "\titers: 300, epoch: 2 | loss: 0.1489368\n",
      "\tspeed: 0.0135s/iter; left time: 142.5570s\n",
      "\titers: 400, epoch: 2 | loss: 0.0923580\n",
      "\tspeed: 0.0135s/iter; left time: 141.0398s\n",
      "\titers: 500, epoch: 2 | loss: 0.0957127\n",
      "\tspeed: 0.0135s/iter; left time: 139.9604s\n",
      "Epoch: 2 cost time: 8.00999116897583\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1173969 Vali Loss: 0.0392548 Test Loss: 0.1194504\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1095200\n",
      "\tspeed: 0.0449s/iter; left time: 455.8432s\n",
      "\titers: 200, epoch: 3 | loss: 0.0508968\n",
      "\tspeed: 0.0135s/iter; left time: 135.6097s\n",
      "\titers: 300, epoch: 3 | loss: 0.0897601\n",
      "\tspeed: 0.0135s/iter; left time: 134.3686s\n",
      "\titers: 400, epoch: 3 | loss: 0.0699842\n",
      "\tspeed: 0.0135s/iter; left time: 132.8286s\n",
      "\titers: 500, epoch: 3 | loss: 0.0651250\n",
      "\tspeed: 0.0135s/iter; left time: 131.4421s\n",
      "Epoch: 3 cost time: 7.990720272064209\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0834549 Vali Loss: 0.0350599 Test Loss: 0.1154133\n",
      "Validation loss decreased (0.037269 --> 0.035060).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0440067\n",
      "\tspeed: 0.0445s/iter; left time: 426.9734s\n",
      "\titers: 200, epoch: 4 | loss: 0.0632474\n",
      "\tspeed: 0.0135s/iter; left time: 128.4234s\n",
      "\titers: 300, epoch: 4 | loss: 0.0656902\n",
      "\tspeed: 0.0135s/iter; left time: 127.0102s\n",
      "\titers: 400, epoch: 4 | loss: 0.0422224\n",
      "\tspeed: 0.0135s/iter; left time: 125.1020s\n",
      "\titers: 500, epoch: 4 | loss: 0.0366644\n",
      "\tspeed: 0.0135s/iter; left time: 123.7260s\n",
      "Epoch: 4 cost time: 7.982506990432739\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0564874 Vali Loss: 0.0340818 Test Loss: 0.1165414\n",
      "Validation loss decreased (0.035060 --> 0.034082).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0329393\n",
      "\tspeed: 0.0442s/iter; left time: 398.2907s\n",
      "\titers: 200, epoch: 5 | loss: 0.0378728\n",
      "\tspeed: 0.0135s/iter; left time: 120.5737s\n",
      "\titers: 300, epoch: 5 | loss: 0.0312857\n",
      "\tspeed: 0.0135s/iter; left time: 119.2375s\n",
      "\titers: 400, epoch: 5 | loss: 0.0266005\n",
      "\tspeed: 0.0135s/iter; left time: 117.8995s\n",
      "\titers: 500, epoch: 5 | loss: 0.0470453\n",
      "\tspeed: 0.0135s/iter; left time: 116.3442s\n",
      "Epoch: 5 cost time: 7.980235576629639\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0417427 Vali Loss: 0.0357250 Test Loss: 0.1175037\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0335595\n",
      "\tspeed: 0.0436s/iter; left time: 368.5394s\n",
      "\titers: 200, epoch: 6 | loss: 0.0529354\n",
      "\tspeed: 0.0135s/iter; left time: 112.5754s\n",
      "\titers: 300, epoch: 6 | loss: 0.0342036\n",
      "\tspeed: 0.0135s/iter; left time: 111.3704s\n",
      "\titers: 400, epoch: 6 | loss: 0.0658579\n",
      "\tspeed: 0.0135s/iter; left time: 110.2865s\n",
      "\titers: 500, epoch: 6 | loss: 0.0367821\n",
      "\tspeed: 0.0135s/iter; left time: 108.8250s\n",
      "Epoch: 6 cost time: 7.967366695404053\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0348177 Vali Loss: 0.0363659 Test Loss: 0.1205297\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0248893\n",
      "\tspeed: 0.0440s/iter; left time: 346.5777s\n",
      "\titers: 200, epoch: 7 | loss: 0.0280079\n",
      "\tspeed: 0.0134s/iter; left time: 104.3460s\n",
      "\titers: 300, epoch: 7 | loss: 0.0273204\n",
      "\tspeed: 0.0134s/iter; left time: 103.1093s\n",
      "\titers: 400, epoch: 7 | loss: 0.0482202\n",
      "\tspeed: 0.0134s/iter; left time: 101.7093s\n",
      "\titers: 500, epoch: 7 | loss: 0.0353472\n",
      "\tspeed: 0.0134s/iter; left time: 100.4292s\n",
      "Epoch: 7 cost time: 7.914416313171387\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0313630 Vali Loss: 0.0362975 Test Loss: 0.1222203\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11669544130563736, mae:0.2160900980234146\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1069971\n",
      "\tspeed: 0.0272s/iter; left time: 307.2920s\n",
      "\titers: 200, epoch: 1 | loss: 0.2587161\n",
      "\tspeed: 0.0152s/iter; left time: 169.8205s\n",
      "\titers: 300, epoch: 1 | loss: 0.2063680\n",
      "\tspeed: 0.0151s/iter; left time: 167.7873s\n",
      "\titers: 400, epoch: 1 | loss: 0.2087482\n",
      "\tspeed: 0.0151s/iter; left time: 166.3578s\n",
      "\titers: 500, epoch: 1 | loss: 0.2114511\n",
      "\tspeed: 0.0136s/iter; left time: 147.8987s\n",
      "Epoch: 1 cost time: 9.616235733032227\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1899124 Vali Loss: 0.0503171 Test Loss: 0.1435419\n",
      "Validation loss decreased (inf --> 0.050317).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1118510\n",
      "\tspeed: 0.0464s/iter; left time: 498.0457s\n",
      "\titers: 200, epoch: 2 | loss: 0.1333888\n",
      "\tspeed: 0.0148s/iter; left time: 157.3386s\n",
      "\titers: 300, epoch: 2 | loss: 0.0712786\n",
      "\tspeed: 0.0151s/iter; left time: 158.7619s\n",
      "\titers: 400, epoch: 2 | loss: 0.0988185\n",
      "\tspeed: 0.0146s/iter; left time: 152.0484s\n",
      "\titers: 500, epoch: 2 | loss: 0.1156343\n",
      "\tspeed: 0.0134s/iter; left time: 138.6206s\n",
      "Epoch: 2 cost time: 8.523264169692993\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1346016 Vali Loss: 0.0433708 Test Loss: 0.1410017\n",
      "Validation loss decreased (0.050317 --> 0.043371).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0975002\n",
      "\tspeed: 0.0452s/iter; left time: 459.6507s\n",
      "\titers: 200, epoch: 3 | loss: 0.1073149\n",
      "\tspeed: 0.0135s/iter; left time: 135.6554s\n",
      "\titers: 300, epoch: 3 | loss: 0.0861029\n",
      "\tspeed: 0.0135s/iter; left time: 134.2873s\n",
      "\titers: 400, epoch: 3 | loss: 0.0734350\n",
      "\tspeed: 0.0135s/iter; left time: 133.5314s\n",
      "\titers: 500, epoch: 3 | loss: 0.0743892\n",
      "\tspeed: 0.0135s/iter; left time: 131.3241s\n",
      "Epoch: 3 cost time: 7.998064756393433\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0793540 Vali Loss: 0.0424974 Test Loss: 0.1376587\n",
      "Validation loss decreased (0.043371 --> 0.042497).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0444206\n",
      "\tspeed: 0.0455s/iter; left time: 436.2949s\n",
      "\titers: 200, epoch: 4 | loss: 0.0393224\n",
      "\tspeed: 0.0134s/iter; left time: 127.3100s\n",
      "\titers: 300, epoch: 4 | loss: 0.0441419\n",
      "\tspeed: 0.0134s/iter; left time: 125.6971s\n",
      "\titers: 400, epoch: 4 | loss: 0.0433377\n",
      "\tspeed: 0.0134s/iter; left time: 124.7684s\n",
      "\titers: 500, epoch: 4 | loss: 0.0273443\n",
      "\tspeed: 0.0134s/iter; left time: 123.1587s\n",
      "Epoch: 4 cost time: 7.925135612487793\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0454741 Vali Loss: 0.0429966 Test Loss: 0.1436175\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0456207\n",
      "\tspeed: 0.0441s/iter; left time: 397.5183s\n",
      "\titers: 200, epoch: 5 | loss: 0.0484772\n",
      "\tspeed: 0.0136s/iter; left time: 121.0091s\n",
      "\titers: 300, epoch: 5 | loss: 0.0314100\n",
      "\tspeed: 0.0136s/iter; left time: 119.6222s\n",
      "\titers: 400, epoch: 5 | loss: 0.0519954\n",
      "\tspeed: 0.0136s/iter; left time: 118.5510s\n",
      "\titers: 500, epoch: 5 | loss: 0.0293121\n",
      "\tspeed: 0.0136s/iter; left time: 116.9425s\n",
      "Epoch: 5 cost time: 8.026621341705322\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0318291 Vali Loss: 0.0437204 Test Loss: 0.1449698\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0287018\n",
      "\tspeed: 0.0447s/iter; left time: 377.8267s\n",
      "\titers: 200, epoch: 6 | loss: 0.0228253\n",
      "\tspeed: 0.0135s/iter; left time: 112.5578s\n",
      "\titers: 300, epoch: 6 | loss: 0.0222545\n",
      "\tspeed: 0.0134s/iter; left time: 110.8749s\n",
      "\titers: 400, epoch: 6 | loss: 0.0427278\n",
      "\tspeed: 0.0134s/iter; left time: 109.5982s\n",
      "\titers: 500, epoch: 6 | loss: 0.0231968\n",
      "\tspeed: 0.0134s/iter; left time: 108.1321s\n",
      "Epoch: 6 cost time: 7.968699932098389\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0262436 Vali Loss: 0.0438920 Test Loss: 0.1476728\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13781242072582245, mae:0.24142491817474365\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1986155\n",
      "\tspeed: 0.0298s/iter; left time: 337.2337s\n",
      "\titers: 200, epoch: 1 | loss: 0.1691805\n",
      "\tspeed: 0.0177s/iter; left time: 198.4888s\n",
      "\titers: 300, epoch: 1 | loss: 0.2655474\n",
      "\tspeed: 0.0178s/iter; left time: 197.2031s\n",
      "\titers: 400, epoch: 1 | loss: 0.1940346\n",
      "\tspeed: 0.0177s/iter; left time: 194.9184s\n",
      "\titers: 500, epoch: 1 | loss: 0.1390540\n",
      "\tspeed: 0.0178s/iter; left time: 194.2342s\n",
      "Epoch: 1 cost time: 11.380595207214355\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1975997 Vali Loss: 0.0505389 Test Loss: 0.1476364\n",
      "Validation loss decreased (inf --> 0.050539).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2220080\n",
      "\tspeed: 0.0535s/iter; left time: 574.2126s\n",
      "\titers: 200, epoch: 2 | loss: 0.2834747\n",
      "\tspeed: 0.0178s/iter; left time: 189.5842s\n",
      "\titers: 300, epoch: 2 | loss: 0.3231107\n",
      "\tspeed: 0.0178s/iter; left time: 187.1038s\n",
      "\titers: 400, epoch: 2 | loss: 0.2974539\n",
      "\tspeed: 0.0178s/iter; left time: 185.3687s\n",
      "\titers: 500, epoch: 2 | loss: 0.2700153\n",
      "\tspeed: 0.0169s/iter; left time: 174.5312s\n",
      "Epoch: 2 cost time: 10.287063837051392\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2516783 Vali Loss: 0.0667279 Test Loss: 0.1820506\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1836330\n",
      "\tspeed: 0.0493s/iter; left time: 501.1702s\n",
      "\titers: 200, epoch: 3 | loss: 0.2148254\n",
      "\tspeed: 0.0163s/iter; left time: 164.2010s\n",
      "\titers: 300, epoch: 3 | loss: 0.3247426\n",
      "\tspeed: 0.0163s/iter; left time: 162.5045s\n",
      "\titers: 400, epoch: 3 | loss: 0.1836453\n",
      "\tspeed: 0.0163s/iter; left time: 160.8882s\n",
      "\titers: 500, epoch: 3 | loss: 0.1678694\n",
      "\tspeed: 0.0163s/iter; left time: 159.1637s\n",
      "Epoch: 3 cost time: 9.621311902999878\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2291430 Vali Loss: 0.0621862 Test Loss: 0.1663595\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2186916\n",
      "\tspeed: 0.0497s/iter; left time: 476.9400s\n",
      "\titers: 200, epoch: 4 | loss: 0.1787783\n",
      "\tspeed: 0.0172s/iter; left time: 163.4362s\n",
      "\titers: 300, epoch: 4 | loss: 0.1733173\n",
      "\tspeed: 0.0172s/iter; left time: 161.6590s\n",
      "\titers: 400, epoch: 4 | loss: 0.1957897\n",
      "\tspeed: 0.0172s/iter; left time: 159.9852s\n",
      "\titers: 500, epoch: 4 | loss: 0.1448365\n",
      "\tspeed: 0.0172s/iter; left time: 158.2155s\n",
      "Epoch: 4 cost time: 10.082873582839966\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1949281 Vali Loss: 0.0626483 Test Loss: 0.1643371\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14789031445980072, mae:0.24401988089084625\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1967783\n",
      "\tspeed: 0.0294s/iter; left time: 331.7051s\n",
      "\titers: 200, epoch: 1 | loss: 0.0812769\n",
      "\tspeed: 0.0172s/iter; left time: 192.7487s\n",
      "\titers: 300, epoch: 1 | loss: 0.2097766\n",
      "\tspeed: 0.0172s/iter; left time: 191.4158s\n",
      "\titers: 400, epoch: 1 | loss: 0.1258427\n",
      "\tspeed: 0.0173s/iter; left time: 189.8722s\n",
      "\titers: 500, epoch: 1 | loss: 0.0920935\n",
      "\tspeed: 0.0172s/iter; left time: 188.0108s\n",
      "Epoch: 1 cost time: 11.08919358253479\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1694094 Vali Loss: 0.0414194 Test Loss: 0.1318534\n",
      "Validation loss decreased (inf --> 0.041419).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1300433\n",
      "\tspeed: 0.0505s/iter; left time: 541.4993s\n",
      "\titers: 200, epoch: 2 | loss: 0.1766018\n",
      "\tspeed: 0.0158s/iter; left time: 168.4416s\n",
      "\titers: 300, epoch: 2 | loss: 0.1776148\n",
      "\tspeed: 0.0158s/iter; left time: 166.8579s\n",
      "\titers: 400, epoch: 2 | loss: 0.6121952\n",
      "\tspeed: 0.0158s/iter; left time: 165.2048s\n",
      "\titers: 500, epoch: 2 | loss: 0.2668276\n",
      "\tspeed: 0.0158s/iter; left time: 163.7222s\n",
      "Epoch: 2 cost time: 9.364811897277832\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2193054 Vali Loss: 0.0827282 Test Loss: 0.2201046\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2272870\n",
      "\tspeed: 0.0496s/iter; left time: 503.6867s\n",
      "\titers: 200, epoch: 3 | loss: 0.2946670\n",
      "\tspeed: 0.0158s/iter; left time: 159.2937s\n",
      "\titers: 300, epoch: 3 | loss: 0.1645177\n",
      "\tspeed: 0.0159s/iter; left time: 157.9017s\n",
      "\titers: 400, epoch: 3 | loss: 0.2846656\n",
      "\tspeed: 0.0158s/iter; left time: 156.1253s\n",
      "\titers: 500, epoch: 3 | loss: 0.1483205\n",
      "\tspeed: 0.0158s/iter; left time: 154.5222s\n",
      "Epoch: 3 cost time: 9.359254837036133\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2507399 Vali Loss: 0.0618857 Test Loss: 0.1737114\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1715246\n",
      "\tspeed: 0.0485s/iter; left time: 465.4196s\n",
      "\titers: 200, epoch: 4 | loss: 0.3059893\n",
      "\tspeed: 0.0159s/iter; left time: 150.9421s\n",
      "\titers: 300, epoch: 4 | loss: 0.2712645\n",
      "\tspeed: 0.0159s/iter; left time: 149.2174s\n",
      "\titers: 400, epoch: 4 | loss: 0.1277343\n",
      "\tspeed: 0.0159s/iter; left time: 147.7398s\n",
      "\titers: 500, epoch: 4 | loss: 0.2868744\n",
      "\tspeed: 0.0159s/iter; left time: 146.0389s\n",
      "Epoch: 4 cost time: 9.354865312576294\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2091032 Vali Loss: 0.0661804 Test Loss: 0.1758471\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1320623755455017, mae:0.23152689635753632\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1846964\n",
      "\tspeed: 0.0283s/iter; left time: 320.3807s\n",
      "\titers: 200, epoch: 1 | loss: 0.1534819\n",
      "\tspeed: 0.0151s/iter; left time: 169.2495s\n",
      "\titers: 300, epoch: 1 | loss: 0.1390241\n",
      "\tspeed: 0.0147s/iter; left time: 162.9257s\n",
      "\titers: 400, epoch: 1 | loss: 0.1953725\n",
      "\tspeed: 0.0147s/iter; left time: 162.0118s\n",
      "\titers: 500, epoch: 1 | loss: 0.3213199\n",
      "\tspeed: 0.0147s/iter; left time: 160.4910s\n",
      "Epoch: 1 cost time: 9.843920707702637\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2022955 Vali Loss: 0.0596252 Test Loss: 0.1810754\n",
      "Validation loss decreased (inf --> 0.059625).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3014162\n",
      "\tspeed: 0.0475s/iter; left time: 509.3662s\n",
      "\titers: 200, epoch: 2 | loss: 0.1825785\n",
      "\tspeed: 0.0147s/iter; left time: 156.4320s\n",
      "\titers: 300, epoch: 2 | loss: 0.2656878\n",
      "\tspeed: 0.0147s/iter; left time: 154.7906s\n",
      "\titers: 400, epoch: 2 | loss: 0.2396755\n",
      "\tspeed: 0.0147s/iter; left time: 153.4916s\n",
      "\titers: 500, epoch: 2 | loss: 0.3102220\n",
      "\tspeed: 0.0147s/iter; left time: 151.9770s\n",
      "Epoch: 2 cost time: 8.708213567733765\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2341447 Vali Loss: 0.0562900 Test Loss: 0.1692586\n",
      "Validation loss decreased (0.059625 --> 0.056290).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2520309\n",
      "\tspeed: 0.0473s/iter; left time: 480.5975s\n",
      "\titers: 200, epoch: 3 | loss: 0.1126558\n",
      "\tspeed: 0.0147s/iter; left time: 147.8968s\n",
      "\titers: 300, epoch: 3 | loss: 0.2455842\n",
      "\tspeed: 0.0147s/iter; left time: 146.5743s\n",
      "\titers: 400, epoch: 3 | loss: 0.1633121\n",
      "\tspeed: 0.0147s/iter; left time: 144.9726s\n",
      "\titers: 500, epoch: 3 | loss: 0.1875406\n",
      "\tspeed: 0.0147s/iter; left time: 143.5934s\n",
      "Epoch: 3 cost time: 8.705339908599854\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1976280 Vali Loss: 0.0501224 Test Loss: 0.1469226\n",
      "Validation loss decreased (0.056290 --> 0.050122).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1423798\n",
      "\tspeed: 0.0476s/iter; left time: 456.3403s\n",
      "\titers: 200, epoch: 4 | loss: 0.1833380\n",
      "\tspeed: 0.0148s/iter; left time: 140.1485s\n",
      "\titers: 300, epoch: 4 | loss: 0.1767692\n",
      "\tspeed: 0.0168s/iter; left time: 157.9111s\n",
      "\titers: 400, epoch: 4 | loss: 0.1392000\n",
      "\tspeed: 0.0168s/iter; left time: 156.4180s\n",
      "\titers: 500, epoch: 4 | loss: 0.1467425\n",
      "\tspeed: 0.0168s/iter; left time: 154.6078s\n",
      "Epoch: 4 cost time: 9.466946840286255\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1538924 Vali Loss: 0.0479066 Test Loss: 0.1444010\n",
      "Validation loss decreased (0.050122 --> 0.047907).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0917977\n",
      "\tspeed: 0.0510s/iter; left time: 460.5205s\n",
      "\titers: 200, epoch: 5 | loss: 0.1277048\n",
      "\tspeed: 0.0168s/iter; left time: 149.4561s\n",
      "\titers: 300, epoch: 5 | loss: 0.1231754\n",
      "\tspeed: 0.0167s/iter; left time: 147.7252s\n",
      "\titers: 400, epoch: 5 | loss: 0.0834559\n",
      "\tspeed: 0.0167s/iter; left time: 146.0354s\n",
      "\titers: 500, epoch: 5 | loss: 0.1487637\n",
      "\tspeed: 0.0167s/iter; left time: 144.4001s\n",
      "Epoch: 5 cost time: 9.830368280410767\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1337128 Vali Loss: 0.0466142 Test Loss: 0.1435483\n",
      "Validation loss decreased (0.047907 --> 0.046614).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1269798\n",
      "\tspeed: 0.0508s/iter; left time: 429.0487s\n",
      "\titers: 200, epoch: 6 | loss: 0.1186455\n",
      "\tspeed: 0.0168s/iter; left time: 140.4555s\n",
      "\titers: 300, epoch: 6 | loss: 0.0984075\n",
      "\tspeed: 0.0168s/iter; left time: 138.7475s\n",
      "\titers: 400, epoch: 6 | loss: 0.0881625\n",
      "\tspeed: 0.0168s/iter; left time: 137.0360s\n",
      "\titers: 500, epoch: 6 | loss: 0.1166409\n",
      "\tspeed: 0.0168s/iter; left time: 135.2798s\n",
      "Epoch: 6 cost time: 9.90358567237854\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1221631 Vali Loss: 0.0458232 Test Loss: 0.1406332\n",
      "Validation loss decreased (0.046614 --> 0.045823).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1413343\n",
      "\tspeed: 0.0513s/iter; left time: 404.1826s\n",
      "\titers: 200, epoch: 7 | loss: 0.0839411\n",
      "\tspeed: 0.0169s/iter; left time: 131.2013s\n",
      "\titers: 300, epoch: 7 | loss: 0.1261159\n",
      "\tspeed: 0.0169s/iter; left time: 129.4941s\n",
      "\titers: 400, epoch: 7 | loss: 0.0824899\n",
      "\tspeed: 0.0169s/iter; left time: 127.8621s\n",
      "\titers: 500, epoch: 7 | loss: 0.1464380\n",
      "\tspeed: 0.0169s/iter; left time: 126.1481s\n",
      "Epoch: 7 cost time: 9.931205034255981\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1152028 Vali Loss: 0.0455213 Test Loss: 0.1399625\n",
      "Validation loss decreased (0.045823 --> 0.045521).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0983356\n",
      "\tspeed: 0.0509s/iter; left time: 372.2029s\n",
      "\titers: 200, epoch: 8 | loss: 0.1660734\n",
      "\tspeed: 0.0168s/iter; left time: 121.0700s\n",
      "\titers: 300, epoch: 8 | loss: 0.0982681\n",
      "\tspeed: 0.0168s/iter; left time: 119.5100s\n",
      "\titers: 400, epoch: 8 | loss: 0.1109750\n",
      "\tspeed: 0.0168s/iter; left time: 117.7867s\n",
      "\titers: 500, epoch: 8 | loss: 0.0811666\n",
      "\tspeed: 0.0168s/iter; left time: 116.2020s\n",
      "Epoch: 8 cost time: 9.916582107543945\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1107535 Vali Loss: 0.0446388 Test Loss: 0.1394486\n",
      "Validation loss decreased (0.045521 --> 0.044639).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0843178\n",
      "\tspeed: 0.0506s/iter; left time: 340.8785s\n",
      "\titers: 200, epoch: 9 | loss: 0.0957296\n",
      "\tspeed: 0.0168s/iter; left time: 111.3935s\n",
      "\titers: 300, epoch: 9 | loss: 0.0958310\n",
      "\tspeed: 0.0168s/iter; left time: 109.6103s\n",
      "\titers: 400, epoch: 9 | loss: 0.0913657\n",
      "\tspeed: 0.0167s/iter; left time: 107.5606s\n",
      "\titers: 500, epoch: 9 | loss: 0.1282018\n",
      "\tspeed: 0.0147s/iter; left time: 93.0923s\n",
      "Epoch: 9 cost time: 9.51785922050476\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1083913 Vali Loss: 0.0448575 Test Loss: 0.1397041\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1445305\n",
      "\tspeed: 0.0475s/iter; left time: 293.2807s\n",
      "\titers: 200, epoch: 10 | loss: 0.1169368\n",
      "\tspeed: 0.0163s/iter; left time: 98.8758s\n",
      "\titers: 300, epoch: 10 | loss: 0.0864425\n",
      "\tspeed: 0.0146s/iter; left time: 87.2130s\n",
      "\titers: 400, epoch: 10 | loss: 0.1313425\n",
      "\tspeed: 0.0146s/iter; left time: 85.7297s\n",
      "\titers: 500, epoch: 10 | loss: 0.1608877\n",
      "\tspeed: 0.0146s/iter; left time: 84.1472s\n",
      "Epoch: 10 cost time: 8.754920244216919\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1071333 Vali Loss: 0.0449487 Test Loss: 0.1401038\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1228852\n",
      "\tspeed: 0.0477s/iter; left time: 267.1378s\n",
      "\titers: 200, epoch: 11 | loss: 0.0852268\n",
      "\tspeed: 0.0162s/iter; left time: 89.2020s\n",
      "\titers: 300, epoch: 11 | loss: 0.0976405\n",
      "\tspeed: 0.0162s/iter; left time: 87.5708s\n",
      "\titers: 400, epoch: 11 | loss: 0.1279954\n",
      "\tspeed: 0.0162s/iter; left time: 85.9068s\n",
      "\titers: 500, epoch: 11 | loss: 0.1509469\n",
      "\tspeed: 0.0162s/iter; left time: 84.2788s\n",
      "Epoch: 11 cost time: 9.552314281463623\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1064138 Vali Loss: 0.0450237 Test Loss: 0.1398980\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13961860537528992, mae:0.2440221607685089\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1424365\n",
      "\tspeed: 0.0289s/iter; left time: 326.5482s\n",
      "\titers: 200, epoch: 1 | loss: 0.1840478\n",
      "\tspeed: 0.0175s/iter; left time: 195.5497s\n",
      "\titers: 300, epoch: 1 | loss: 0.2390462\n",
      "\tspeed: 0.0174s/iter; left time: 193.5633s\n",
      "\titers: 400, epoch: 1 | loss: 0.1257376\n",
      "\tspeed: 0.0175s/iter; left time: 191.9688s\n",
      "\titers: 500, epoch: 1 | loss: 0.1427562\n",
      "\tspeed: 0.0175s/iter; left time: 190.3794s\n",
      "Epoch: 1 cost time: 11.140733003616333\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1878913 Vali Loss: 0.0468647 Test Loss: 0.1412132\n",
      "Validation loss decreased (inf --> 0.046865).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2610076\n",
      "\tspeed: 0.0546s/iter; left time: 585.6054s\n",
      "\titers: 200, epoch: 2 | loss: 0.1107580\n",
      "\tspeed: 0.0176s/iter; left time: 186.6025s\n",
      "\titers: 300, epoch: 2 | loss: 0.1036385\n",
      "\tspeed: 0.0175s/iter; left time: 184.7845s\n",
      "\titers: 400, epoch: 2 | loss: 0.1070925\n",
      "\tspeed: 0.0176s/iter; left time: 183.2917s\n",
      "\titers: 500, epoch: 2 | loss: 0.1219436\n",
      "\tspeed: 0.0175s/iter; left time: 181.2733s\n",
      "Epoch: 2 cost time: 10.303776502609253\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1391301 Vali Loss: 0.0438123 Test Loss: 0.1367771\n",
      "Validation loss decreased (0.046865 --> 0.043812).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0830769\n",
      "\tspeed: 0.0544s/iter; left time: 552.5159s\n",
      "\titers: 200, epoch: 3 | loss: 0.0651449\n",
      "\tspeed: 0.0176s/iter; left time: 176.7942s\n",
      "\titers: 300, epoch: 3 | loss: 0.0848362\n",
      "\tspeed: 0.0175s/iter; left time: 174.5447s\n",
      "\titers: 400, epoch: 3 | loss: 0.0720099\n",
      "\tspeed: 0.0175s/iter; left time: 172.8727s\n",
      "\titers: 500, epoch: 3 | loss: 0.0553302\n",
      "\tspeed: 0.0176s/iter; left time: 171.5943s\n",
      "Epoch: 3 cost time: 10.311809062957764\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0927919 Vali Loss: 0.0406958 Test Loss: 0.1434521\n",
      "Validation loss decreased (0.043812 --> 0.040696).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0647112\n",
      "\tspeed: 0.0549s/iter; left time: 526.1696s\n",
      "\titers: 200, epoch: 4 | loss: 0.0716239\n",
      "\tspeed: 0.0175s/iter; left time: 165.6594s\n",
      "\titers: 300, epoch: 4 | loss: 0.0661048\n",
      "\tspeed: 0.0175s/iter; left time: 163.8924s\n",
      "\titers: 400, epoch: 4 | loss: 0.0359052\n",
      "\tspeed: 0.0174s/iter; left time: 162.0842s\n",
      "\titers: 500, epoch: 4 | loss: 0.0596870\n",
      "\tspeed: 0.0174s/iter; left time: 160.0197s\n",
      "Epoch: 4 cost time: 10.249898672103882\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0583398 Vali Loss: 0.0424002 Test Loss: 0.1405669\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0493537\n",
      "\tspeed: 0.0571s/iter; left time: 515.0347s\n",
      "\titers: 200, epoch: 5 | loss: 0.0440921\n",
      "\tspeed: 0.0197s/iter; left time: 175.6686s\n",
      "\titers: 300, epoch: 5 | loss: 0.0384487\n",
      "\tspeed: 0.0196s/iter; left time: 172.8455s\n",
      "\titers: 400, epoch: 5 | loss: 0.0319154\n",
      "\tspeed: 0.0196s/iter; left time: 170.7646s\n",
      "\titers: 500, epoch: 5 | loss: 0.0568911\n",
      "\tspeed: 0.0182s/iter; left time: 156.7535s\n",
      "Epoch: 5 cost time: 11.169706344604492\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0422725 Vali Loss: 0.0417912 Test Loss: 0.1395553\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0277847\n",
      "\tspeed: 0.0531s/iter; left time: 448.6668s\n",
      "\titers: 200, epoch: 6 | loss: 0.0349614\n",
      "\tspeed: 0.0175s/iter; left time: 145.8119s\n",
      "\titers: 300, epoch: 6 | loss: 0.0368796\n",
      "\tspeed: 0.0175s/iter; left time: 144.1512s\n",
      "\titers: 400, epoch: 6 | loss: 0.0483236\n",
      "\tspeed: 0.0174s/iter; left time: 142.1653s\n",
      "\titers: 500, epoch: 6 | loss: 0.0325147\n",
      "\tspeed: 0.0174s/iter; left time: 140.3215s\n",
      "Epoch: 6 cost time: 10.206709861755371\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0343979 Vali Loss: 0.0434447 Test Loss: 0.1454348\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14360164105892181, mae:0.24396653473377228\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3013273\n",
      "\tspeed: 0.0291s/iter; left time: 328.5063s\n",
      "\titers: 200, epoch: 1 | loss: 0.1627599\n",
      "\tspeed: 0.0172s/iter; left time: 192.2547s\n",
      "\titers: 300, epoch: 1 | loss: 0.1636733\n",
      "\tspeed: 0.0172s/iter; left time: 190.9079s\n",
      "\titers: 400, epoch: 1 | loss: 0.1275357\n",
      "\tspeed: 0.0172s/iter; left time: 189.5296s\n",
      "\titers: 500, epoch: 1 | loss: 0.1798341\n",
      "\tspeed: 0.0172s/iter; left time: 187.5996s\n",
      "Epoch: 1 cost time: 11.046336889266968\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1556122 Vali Loss: 0.0425841 Test Loss: 0.1366059\n",
      "Validation loss decreased (inf --> 0.042584).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1080536\n",
      "\tspeed: 0.0546s/iter; left time: 586.2126s\n",
      "\titers: 200, epoch: 2 | loss: 0.1725593\n",
      "\tspeed: 0.0172s/iter; left time: 182.3649s\n",
      "\titers: 300, epoch: 2 | loss: 0.1873654\n",
      "\tspeed: 0.0171s/iter; left time: 180.2322s\n",
      "\titers: 400, epoch: 2 | loss: 0.1795883\n",
      "\tspeed: 0.0171s/iter; left time: 178.7338s\n",
      "\titers: 500, epoch: 2 | loss: 0.1130972\n",
      "\tspeed: 0.0171s/iter; left time: 177.1338s\n",
      "Epoch: 2 cost time: 10.090810060501099\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1257417 Vali Loss: 0.0436001 Test Loss: 0.1318278\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0872726\n",
      "\tspeed: 0.0560s/iter; left time: 569.3212s\n",
      "\titers: 200, epoch: 3 | loss: 0.0909489\n",
      "\tspeed: 0.0193s/iter; left time: 193.7334s\n",
      "\titers: 300, epoch: 3 | loss: 0.1098259\n",
      "\tspeed: 0.0192s/iter; left time: 191.1282s\n",
      "\titers: 400, epoch: 3 | loss: 0.0617533\n",
      "\tspeed: 0.0193s/iter; left time: 189.8892s\n",
      "\titers: 500, epoch: 3 | loss: 0.0701032\n",
      "\tspeed: 0.0192s/iter; left time: 187.8730s\n",
      "Epoch: 3 cost time: 11.23965311050415\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0934521 Vali Loss: 0.0351826 Test Loss: 0.1203531\n",
      "Validation loss decreased (0.042584 --> 0.035183).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0649448\n",
      "\tspeed: 0.0575s/iter; left time: 551.1784s\n",
      "\titers: 200, epoch: 4 | loss: 0.0549003\n",
      "\tspeed: 0.0192s/iter; left time: 182.3173s\n",
      "\titers: 300, epoch: 4 | loss: 0.0712037\n",
      "\tspeed: 0.0193s/iter; left time: 180.9028s\n",
      "\titers: 400, epoch: 4 | loss: 0.0707857\n",
      "\tspeed: 0.0192s/iter; left time: 178.4177s\n",
      "\titers: 500, epoch: 4 | loss: 0.0471515\n",
      "\tspeed: 0.0192s/iter; left time: 176.9246s\n",
      "Epoch: 4 cost time: 11.285292625427246\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0624750 Vali Loss: 0.0335348 Test Loss: 0.1206524\n",
      "Validation loss decreased (0.035183 --> 0.033535).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0518756\n",
      "\tspeed: 0.0558s/iter; left time: 503.4326s\n",
      "\titers: 200, epoch: 5 | loss: 0.0462244\n",
      "\tspeed: 0.0173s/iter; left time: 154.2958s\n",
      "\titers: 300, epoch: 5 | loss: 0.0481729\n",
      "\tspeed: 0.0173s/iter; left time: 152.3836s\n",
      "\titers: 400, epoch: 5 | loss: 0.0416372\n",
      "\tspeed: 0.0173s/iter; left time: 150.6579s\n",
      "\titers: 500, epoch: 5 | loss: 0.0582077\n",
      "\tspeed: 0.0173s/iter; left time: 149.0286s\n",
      "Epoch: 5 cost time: 10.178947448730469\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0454446 Vali Loss: 0.0346301 Test Loss: 0.1202264\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0512787\n",
      "\tspeed: 0.0546s/iter; left time: 461.8402s\n",
      "\titers: 200, epoch: 6 | loss: 0.0321706\n",
      "\tspeed: 0.0171s/iter; left time: 142.9764s\n",
      "\titers: 300, epoch: 6 | loss: 0.0402475\n",
      "\tspeed: 0.0172s/iter; left time: 141.5435s\n",
      "\titers: 400, epoch: 6 | loss: 0.0389622\n",
      "\tspeed: 0.0171s/iter; left time: 139.7400s\n",
      "\titers: 500, epoch: 6 | loss: 0.0262756\n",
      "\tspeed: 0.0171s/iter; left time: 137.8613s\n",
      "Epoch: 6 cost time: 10.045132637023926\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0372056 Vali Loss: 0.0347524 Test Loss: 0.1218830\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0415928\n",
      "\tspeed: 0.0575s/iter; left time: 453.1981s\n",
      "\titers: 200, epoch: 7 | loss: 0.0410634\n",
      "\tspeed: 0.0172s/iter; left time: 133.7896s\n",
      "\titers: 300, epoch: 7 | loss: 0.0310007\n",
      "\tspeed: 0.0172s/iter; left time: 132.0680s\n",
      "\titers: 400, epoch: 7 | loss: 0.0346273\n",
      "\tspeed: 0.0172s/iter; left time: 130.3357s\n",
      "\titers: 500, epoch: 7 | loss: 0.0262612\n",
      "\tspeed: 0.0172s/iter; left time: 128.5735s\n",
      "Epoch: 7 cost time: 10.129865646362305\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0329937 Vali Loss: 0.0353262 Test Loss: 0.1226116\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12082037329673767, mae:0.21284867823123932\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2177885\n",
      "\tspeed: 0.0290s/iter; left time: 327.6979s\n",
      "\titers: 200, epoch: 1 | loss: 0.1740898\n",
      "\tspeed: 0.0174s/iter; left time: 194.5910s\n",
      "\titers: 300, epoch: 1 | loss: 0.2318988\n",
      "\tspeed: 0.0174s/iter; left time: 192.6594s\n",
      "\titers: 400, epoch: 1 | loss: 0.1778409\n",
      "\tspeed: 0.0174s/iter; left time: 190.9553s\n",
      "\titers: 500, epoch: 1 | loss: 0.2104654\n",
      "\tspeed: 0.0174s/iter; left time: 189.2533s\n",
      "Epoch: 1 cost time: 11.110748052597046\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1894304 Vali Loss: 0.0450520 Test Loss: 0.1393649\n",
      "Validation loss decreased (inf --> 0.045052).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2218399\n",
      "\tspeed: 0.0543s/iter; left time: 583.1696s\n",
      "\titers: 200, epoch: 2 | loss: 0.2031770\n",
      "\tspeed: 0.0171s/iter; left time: 181.7134s\n",
      "\titers: 300, epoch: 2 | loss: 0.2917352\n",
      "\tspeed: 0.0171s/iter; left time: 180.0839s\n",
      "\titers: 400, epoch: 2 | loss: 0.1115078\n",
      "\tspeed: 0.0171s/iter; left time: 178.4058s\n",
      "\titers: 500, epoch: 2 | loss: 0.0887230\n",
      "\tspeed: 0.0171s/iter; left time: 176.8382s\n",
      "Epoch: 2 cost time: 10.023196935653687\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1400354 Vali Loss: 0.0453597 Test Loss: 0.1389147\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0976101\n",
      "\tspeed: 0.0550s/iter; left time: 559.3455s\n",
      "\titers: 200, epoch: 3 | loss: 0.0790695\n",
      "\tspeed: 0.0174s/iter; left time: 174.9114s\n",
      "\titers: 300, epoch: 3 | loss: 0.0755557\n",
      "\tspeed: 0.0172s/iter; left time: 171.6344s\n",
      "\titers: 400, epoch: 3 | loss: 0.0531154\n",
      "\tspeed: 0.0172s/iter; left time: 169.2931s\n",
      "\titers: 500, epoch: 3 | loss: 0.0669597\n",
      "\tspeed: 0.0172s/iter; left time: 167.7346s\n",
      "Epoch: 3 cost time: 10.144858837127686\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0817688 Vali Loss: 0.0419895 Test Loss: 0.1375994\n",
      "Validation loss decreased (0.045052 --> 0.041990).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0431753\n",
      "\tspeed: 0.0539s/iter; left time: 517.0085s\n",
      "\titers: 200, epoch: 4 | loss: 0.0574122\n",
      "\tspeed: 0.0173s/iter; left time: 164.1965s\n",
      "\titers: 300, epoch: 4 | loss: 0.0522037\n",
      "\tspeed: 0.0173s/iter; left time: 162.5197s\n",
      "\titers: 400, epoch: 4 | loss: 0.0496517\n",
      "\tspeed: 0.0173s/iter; left time: 160.8846s\n",
      "\titers: 500, epoch: 4 | loss: 0.0428037\n",
      "\tspeed: 0.0173s/iter; left time: 159.0288s\n",
      "Epoch: 4 cost time: 10.139140367507935\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0467967 Vali Loss: 0.0405782 Test Loss: 0.1369759\n",
      "Validation loss decreased (0.041990 --> 0.040578).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0329710\n",
      "\tspeed: 0.0540s/iter; left time: 486.9106s\n",
      "\titers: 200, epoch: 5 | loss: 0.0373534\n",
      "\tspeed: 0.0172s/iter; left time: 153.6014s\n",
      "\titers: 300, epoch: 5 | loss: 0.0254652\n",
      "\tspeed: 0.0172s/iter; left time: 151.8373s\n",
      "\titers: 400, epoch: 5 | loss: 0.0320666\n",
      "\tspeed: 0.0172s/iter; left time: 149.9077s\n",
      "\titers: 500, epoch: 5 | loss: 0.0248615\n",
      "\tspeed: 0.0172s/iter; left time: 148.4310s\n",
      "Epoch: 5 cost time: 10.109692811965942\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0323576 Vali Loss: 0.0408936 Test Loss: 0.1396430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0270176\n",
      "\tspeed: 0.0534s/iter; left time: 451.1302s\n",
      "\titers: 200, epoch: 6 | loss: 0.0235493\n",
      "\tspeed: 0.0172s/iter; left time: 143.7567s\n",
      "\titers: 300, epoch: 6 | loss: 0.0301357\n",
      "\tspeed: 0.0172s/iter; left time: 141.7224s\n",
      "\titers: 400, epoch: 6 | loss: 0.0253702\n",
      "\tspeed: 0.0172s/iter; left time: 139.8391s\n",
      "\titers: 500, epoch: 6 | loss: 0.0251508\n",
      "\tspeed: 0.0171s/iter; left time: 138.0633s\n",
      "Epoch: 6 cost time: 10.041589498519897\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0259659 Vali Loss: 0.0409562 Test Loss: 0.1410119\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0220625\n",
      "\tspeed: 0.0545s/iter; left time: 429.1885s\n",
      "\titers: 200, epoch: 7 | loss: 0.0194717\n",
      "\tspeed: 0.0193s/iter; left time: 150.1440s\n",
      "\titers: 300, epoch: 7 | loss: 0.0231270\n",
      "\tspeed: 0.0193s/iter; left time: 148.1972s\n",
      "\titers: 400, epoch: 7 | loss: 0.0367678\n",
      "\tspeed: 0.0193s/iter; left time: 146.1753s\n",
      "\titers: 500, epoch: 7 | loss: 0.0201000\n",
      "\tspeed: 0.0193s/iter; left time: 144.2452s\n",
      "Epoch: 7 cost time: 11.273231506347656\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0229394 Vali Loss: 0.0413790 Test Loss: 0.1412562\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13710694015026093, mae:0.2393602728843689\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1982598\n",
      "\tspeed: 0.0333s/iter; left time: 375.8195s\n",
      "\titers: 200, epoch: 1 | loss: 0.1722663\n",
      "\tspeed: 0.0212s/iter; left time: 237.9283s\n",
      "\titers: 300, epoch: 1 | loss: 0.2368596\n",
      "\tspeed: 0.0213s/iter; left time: 236.2168s\n",
      "\titers: 400, epoch: 1 | loss: 0.2784756\n",
      "\tspeed: 0.0213s/iter; left time: 233.8824s\n",
      "\titers: 500, epoch: 1 | loss: 0.2624359\n",
      "\tspeed: 0.0212s/iter; left time: 231.3761s\n",
      "Epoch: 1 cost time: 13.371165752410889\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2215046 Vali Loss: 0.0689446 Test Loss: 0.2047751\n",
      "Validation loss decreased (inf --> 0.068945).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3135594\n",
      "\tspeed: 0.0617s/iter; left time: 662.4307s\n",
      "\titers: 200, epoch: 2 | loss: 0.2461600\n",
      "\tspeed: 0.0213s/iter; left time: 225.9902s\n",
      "\titers: 300, epoch: 2 | loss: 0.1640294\n",
      "\tspeed: 0.0213s/iter; left time: 224.0331s\n",
      "\titers: 400, epoch: 2 | loss: 0.2334460\n",
      "\tspeed: 0.0213s/iter; left time: 222.0858s\n",
      "\titers: 500, epoch: 2 | loss: 0.1787507\n",
      "\tspeed: 0.0213s/iter; left time: 219.6911s\n",
      "Epoch: 2 cost time: 12.439074993133545\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2409333 Vali Loss: 0.0579337 Test Loss: 0.1650611\n",
      "Validation loss decreased (0.068945 --> 0.057934).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3445709\n",
      "\tspeed: 0.0618s/iter; left time: 628.2179s\n",
      "\titers: 200, epoch: 3 | loss: 0.1344387\n",
      "\tspeed: 0.0213s/iter; left time: 214.7213s\n",
      "\titers: 300, epoch: 3 | loss: 0.1870665\n",
      "\tspeed: 0.0213s/iter; left time: 212.5403s\n",
      "\titers: 400, epoch: 3 | loss: 0.1502970\n",
      "\tspeed: 0.0213s/iter; left time: 210.5204s\n",
      "\titers: 500, epoch: 3 | loss: 0.1235990\n",
      "\tspeed: 0.0213s/iter; left time: 208.1586s\n",
      "Epoch: 3 cost time: 12.477965831756592\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1725269 Vali Loss: 0.0474227 Test Loss: 0.1467401\n",
      "Validation loss decreased (0.057934 --> 0.047423).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1178306\n",
      "\tspeed: 0.0622s/iter; left time: 596.3877s\n",
      "\titers: 200, epoch: 4 | loss: 0.1037992\n",
      "\tspeed: 0.0213s/iter; left time: 202.0233s\n",
      "\titers: 300, epoch: 4 | loss: 0.1877244\n",
      "\tspeed: 0.0213s/iter; left time: 199.8193s\n",
      "\titers: 400, epoch: 4 | loss: 0.1446683\n",
      "\tspeed: 0.0213s/iter; left time: 197.8748s\n",
      "\titers: 500, epoch: 4 | loss: 0.1847490\n",
      "\tspeed: 0.0213s/iter; left time: 195.6460s\n",
      "Epoch: 4 cost time: 12.469421148300171\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1489754 Vali Loss: 0.0459652 Test Loss: 0.1424586\n",
      "Validation loss decreased (0.047423 --> 0.045965).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1684721\n",
      "\tspeed: 0.0630s/iter; left time: 568.3546s\n",
      "\titers: 200, epoch: 5 | loss: 0.2174387\n",
      "\tspeed: 0.0214s/iter; left time: 190.7905s\n",
      "\titers: 300, epoch: 5 | loss: 0.1022878\n",
      "\tspeed: 0.0214s/iter; left time: 188.7972s\n",
      "\titers: 400, epoch: 5 | loss: 0.1460482\n",
      "\tspeed: 0.0214s/iter; left time: 186.3669s\n",
      "\titers: 500, epoch: 5 | loss: 0.1213383\n",
      "\tspeed: 0.0214s/iter; left time: 184.2538s\n",
      "Epoch: 5 cost time: 12.491177558898926\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1338308 Vali Loss: 0.0453069 Test Loss: 0.1409828\n",
      "Validation loss decreased (0.045965 --> 0.045307).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1173596\n",
      "\tspeed: 0.0638s/iter; left time: 539.1788s\n",
      "\titers: 200, epoch: 6 | loss: 0.1838098\n",
      "\tspeed: 0.0214s/iter; left time: 178.8740s\n",
      "\titers: 300, epoch: 6 | loss: 0.1131455\n",
      "\tspeed: 0.0214s/iter; left time: 176.7189s\n",
      "\titers: 400, epoch: 6 | loss: 0.0865408\n",
      "\tspeed: 0.0214s/iter; left time: 174.7005s\n",
      "\titers: 500, epoch: 6 | loss: 0.1666597\n",
      "\tspeed: 0.0214s/iter; left time: 172.3473s\n",
      "Epoch: 6 cost time: 12.521835088729858\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1246720 Vali Loss: 0.0451699 Test Loss: 0.1443853\n",
      "Validation loss decreased (0.045307 --> 0.045170).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1408811\n",
      "\tspeed: 0.0615s/iter; left time: 484.6871s\n",
      "\titers: 200, epoch: 7 | loss: 0.1244936\n",
      "\tspeed: 0.0213s/iter; left time: 165.8438s\n",
      "\titers: 300, epoch: 7 | loss: 0.1025622\n",
      "\tspeed: 0.0213s/iter; left time: 163.4796s\n",
      "\titers: 400, epoch: 7 | loss: 0.1524026\n",
      "\tspeed: 0.0213s/iter; left time: 161.3538s\n",
      "\titers: 500, epoch: 7 | loss: 0.1064116\n",
      "\tspeed: 0.0213s/iter; left time: 159.1942s\n",
      "Epoch: 7 cost time: 12.468711614608765\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1183561 Vali Loss: 0.0456586 Test Loss: 0.1454745\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1128823\n",
      "\tspeed: 0.0610s/iter; left time: 445.6537s\n",
      "\titers: 200, epoch: 8 | loss: 0.1403389\n",
      "\tspeed: 0.0213s/iter; left time: 153.3452s\n",
      "\titers: 300, epoch: 8 | loss: 0.0930431\n",
      "\tspeed: 0.0213s/iter; left time: 151.2527s\n",
      "\titers: 400, epoch: 8 | loss: 0.0984493\n",
      "\tspeed: 0.0213s/iter; left time: 149.1102s\n",
      "\titers: 500, epoch: 8 | loss: 0.0983002\n",
      "\tspeed: 0.0212s/iter; left time: 146.8477s\n",
      "Epoch: 8 cost time: 12.431091547012329\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1149548 Vali Loss: 0.0459172 Test Loss: 0.1456573\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1132303\n",
      "\tspeed: 0.0607s/iter; left time: 408.8713s\n",
      "\titers: 200, epoch: 9 | loss: 0.1282349\n",
      "\tspeed: 0.0213s/iter; left time: 141.4783s\n",
      "\titers: 300, epoch: 9 | loss: 0.1326363\n",
      "\tspeed: 0.0213s/iter; left time: 139.4767s\n",
      "\titers: 400, epoch: 9 | loss: 0.0945258\n",
      "\tspeed: 0.0213s/iter; left time: 137.1670s\n",
      "\titers: 500, epoch: 9 | loss: 0.1243167\n",
      "\tspeed: 0.0213s/iter; left time: 135.0652s\n",
      "Epoch: 9 cost time: 12.424091100692749\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1132426 Vali Loss: 0.0457194 Test Loss: 0.1453414\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14455336332321167, mae:0.23860737681388855\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1611827\n",
      "\tspeed: 0.0325s/iter; left time: 367.4625s\n",
      "\titers: 200, epoch: 1 | loss: 0.0880886\n",
      "\tspeed: 0.0207s/iter; left time: 231.5773s\n",
      "\titers: 300, epoch: 1 | loss: 0.1551008\n",
      "\tspeed: 0.0207s/iter; left time: 229.3053s\n",
      "\titers: 400, epoch: 1 | loss: 0.1174971\n",
      "\tspeed: 0.0207s/iter; left time: 227.2855s\n",
      "\titers: 500, epoch: 1 | loss: 0.1570380\n",
      "\tspeed: 0.0207s/iter; left time: 226.1514s\n",
      "Epoch: 1 cost time: 13.026667833328247\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1819967 Vali Loss: 0.0417195 Test Loss: 0.1302326\n",
      "Validation loss decreased (inf --> 0.041720).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2005797\n",
      "\tspeed: 0.0611s/iter; left time: 655.3795s\n",
      "\titers: 200, epoch: 2 | loss: 0.1317277\n",
      "\tspeed: 0.0208s/iter; left time: 220.7380s\n",
      "\titers: 300, epoch: 2 | loss: 0.2992740\n",
      "\tspeed: 0.0207s/iter; left time: 218.3242s\n",
      "\titers: 400, epoch: 2 | loss: 0.2557242\n",
      "\tspeed: 0.0207s/iter; left time: 216.0288s\n",
      "\titers: 500, epoch: 2 | loss: 0.1844640\n",
      "\tspeed: 0.0207s/iter; left time: 214.1210s\n",
      "Epoch: 2 cost time: 12.156408309936523\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2159472 Vali Loss: 0.0691888 Test Loss: 0.2015038\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1917845\n",
      "\tspeed: 0.0627s/iter; left time: 637.4545s\n",
      "\titers: 200, epoch: 3 | loss: 0.1939210\n",
      "\tspeed: 0.0206s/iter; left time: 207.0162s\n",
      "\titers: 300, epoch: 3 | loss: 0.1543978\n",
      "\tspeed: 0.0206s/iter; left time: 204.8683s\n",
      "\titers: 400, epoch: 3 | loss: 0.1578405\n",
      "\tspeed: 0.0206s/iter; left time: 202.7113s\n",
      "\titers: 500, epoch: 3 | loss: 0.1485855\n",
      "\tspeed: 0.0206s/iter; left time: 200.8089s\n",
      "Epoch: 3 cost time: 11.988124370574951\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2254620 Vali Loss: 0.0597600 Test Loss: 0.1643411\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1238583\n",
      "\tspeed: 0.0605s/iter; left time: 580.5920s\n",
      "\titers: 200, epoch: 4 | loss: 0.1780287\n",
      "\tspeed: 0.0207s/iter; left time: 196.3723s\n",
      "\titers: 300, epoch: 4 | loss: 0.1466343\n",
      "\tspeed: 0.0214s/iter; left time: 200.9289s\n",
      "\titers: 400, epoch: 4 | loss: 0.2122630\n",
      "\tspeed: 0.0224s/iter; left time: 208.1672s\n",
      "\titers: 500, epoch: 4 | loss: 0.1696354\n",
      "\tspeed: 0.0224s/iter; left time: 205.8220s\n",
      "Epoch: 4 cost time: 12.623074531555176\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1947377 Vali Loss: 0.0546327 Test Loss: 0.1529316\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13044115900993347, mae:0.23281407356262207\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2434345\n",
      "\tspeed: 0.0307s/iter; left time: 346.6333s\n",
      "\titers: 200, epoch: 1 | loss: 0.1531063\n",
      "\tspeed: 0.0191s/iter; left time: 214.0166s\n",
      "\titers: 300, epoch: 1 | loss: 0.2530334\n",
      "\tspeed: 0.0191s/iter; left time: 211.9900s\n",
      "\titers: 400, epoch: 1 | loss: 0.2281839\n",
      "\tspeed: 0.0191s/iter; left time: 210.6530s\n",
      "\titers: 500, epoch: 1 | loss: 0.2358771\n",
      "\tspeed: 0.0191s/iter; left time: 208.3127s\n",
      "Epoch: 1 cost time: 12.103790521621704\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2170983 Vali Loss: 0.0559171 Test Loss: 0.1756478\n",
      "Validation loss decreased (inf --> 0.055917).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1163736\n",
      "\tspeed: 0.0583s/iter; left time: 625.7049s\n",
      "\titers: 200, epoch: 2 | loss: 0.4850007\n",
      "\tspeed: 0.0191s/iter; left time: 202.7783s\n",
      "\titers: 300, epoch: 2 | loss: 0.2708116\n",
      "\tspeed: 0.0191s/iter; left time: 200.8953s\n",
      "\titers: 400, epoch: 2 | loss: 0.1452087\n",
      "\tspeed: 0.0191s/iter; left time: 198.8336s\n",
      "\titers: 500, epoch: 2 | loss: 0.1954087\n",
      "\tspeed: 0.0191s/iter; left time: 196.8897s\n",
      "Epoch: 2 cost time: 11.180995225906372\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2521133 Vali Loss: 0.0596861 Test Loss: 0.1777243\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1769210\n",
      "\tspeed: 0.0575s/iter; left time: 584.1062s\n",
      "\titers: 200, epoch: 3 | loss: 0.1579297\n",
      "\tspeed: 0.0190s/iter; left time: 191.6298s\n",
      "\titers: 300, epoch: 3 | loss: 0.2444570\n",
      "\tspeed: 0.0190s/iter; left time: 189.6373s\n",
      "\titers: 400, epoch: 3 | loss: 0.1686455\n",
      "\tspeed: 0.0190s/iter; left time: 187.7558s\n",
      "\titers: 500, epoch: 3 | loss: 0.2149357\n",
      "\tspeed: 0.0190s/iter; left time: 185.5773s\n",
      "Epoch: 3 cost time: 11.10656476020813\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2108717 Vali Loss: 0.0600512 Test Loss: 0.1710273\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1616878\n",
      "\tspeed: 0.0560s/iter; left time: 536.7146s\n",
      "\titers: 200, epoch: 4 | loss: 0.2954238\n",
      "\tspeed: 0.0191s/iter; left time: 181.7203s\n",
      "\titers: 300, epoch: 4 | loss: 0.1252136\n",
      "\tspeed: 0.0191s/iter; left time: 179.6168s\n",
      "\titers: 400, epoch: 4 | loss: 0.1872400\n",
      "\tspeed: 0.0191s/iter; left time: 177.3646s\n",
      "\titers: 500, epoch: 4 | loss: 0.1753382\n",
      "\tspeed: 0.0191s/iter; left time: 175.9725s\n",
      "Epoch: 4 cost time: 11.189905166625977\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1836865 Vali Loss: 0.0583106 Test Loss: 0.1626222\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17591072618961334, mae:0.27432191371917725\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1875591\n",
      "\tspeed: 0.0221s/iter; left time: 249.9177s\n",
      "\titers: 200, epoch: 1 | loss: 0.1685902\n",
      "\tspeed: 0.0105s/iter; left time: 117.6419s\n",
      "\titers: 300, epoch: 1 | loss: 0.2426253\n",
      "\tspeed: 0.0105s/iter; left time: 116.8394s\n",
      "\titers: 400, epoch: 1 | loss: 0.2055227\n",
      "\tspeed: 0.0105s/iter; left time: 115.5104s\n",
      "\titers: 500, epoch: 1 | loss: 0.1781254\n",
      "\tspeed: 0.0105s/iter; left time: 114.6237s\n",
      "Epoch: 1 cost time: 7.197439908981323\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2129296 Vali Loss: 0.0461688 Test Loss: 0.1384224\n",
      "Validation loss decreased (inf --> 0.046169).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2534581\n",
      "\tspeed: 0.0376s/iter; left time: 403.4313s\n",
      "\titers: 200, epoch: 2 | loss: 0.2095187\n",
      "\tspeed: 0.0119s/iter; left time: 126.6564s\n",
      "\titers: 300, epoch: 2 | loss: 0.1648936\n",
      "\tspeed: 0.0119s/iter; left time: 124.9280s\n",
      "\titers: 400, epoch: 2 | loss: 0.1565098\n",
      "\tspeed: 0.0112s/iter; left time: 116.5862s\n",
      "\titers: 500, epoch: 2 | loss: 0.1552203\n",
      "\tspeed: 0.0106s/iter; left time: 109.2313s\n",
      "Epoch: 2 cost time: 6.780377149581909\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1706752 Vali Loss: 0.0441416 Test Loss: 0.1367652\n",
      "Validation loss decreased (0.046169 --> 0.044142).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1195780\n",
      "\tspeed: 0.0369s/iter; left time: 375.0335s\n",
      "\titers: 200, epoch: 3 | loss: 0.1630878\n",
      "\tspeed: 0.0105s/iter; left time: 105.9302s\n",
      "\titers: 300, epoch: 3 | loss: 0.1316595\n",
      "\tspeed: 0.0106s/iter; left time: 105.1866s\n",
      "\titers: 400, epoch: 3 | loss: 0.1441677\n",
      "\tspeed: 0.0106s/iter; left time: 104.3031s\n",
      "\titers: 500, epoch: 3 | loss: 0.1832184\n",
      "\tspeed: 0.0105s/iter; left time: 102.5106s\n",
      "Epoch: 3 cost time: 6.313525915145874\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1405180 Vali Loss: 0.0383746 Test Loss: 0.1244795\n",
      "Validation loss decreased (0.044142 --> 0.038375).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1192791\n",
      "\tspeed: 0.0363s/iter; left time: 348.5465s\n",
      "\titers: 200, epoch: 4 | loss: 0.0985092\n",
      "\tspeed: 0.0105s/iter; left time: 99.9385s\n",
      "\titers: 300, epoch: 4 | loss: 0.1505321\n",
      "\tspeed: 0.0105s/iter; left time: 99.0608s\n",
      "\titers: 400, epoch: 4 | loss: 0.1254500\n",
      "\tspeed: 0.0105s/iter; left time: 97.5366s\n",
      "\titers: 500, epoch: 4 | loss: 0.1224056\n",
      "\tspeed: 0.0105s/iter; left time: 96.4599s\n",
      "Epoch: 4 cost time: 6.2951741218566895\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1215223 Vali Loss: 0.0378357 Test Loss: 0.1281882\n",
      "Validation loss decreased (0.038375 --> 0.037836).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1180496\n",
      "\tspeed: 0.0370s/iter; left time: 334.1209s\n",
      "\titers: 200, epoch: 5 | loss: 0.1264121\n",
      "\tspeed: 0.0105s/iter; left time: 94.0389s\n",
      "\titers: 300, epoch: 5 | loss: 0.0851760\n",
      "\tspeed: 0.0105s/iter; left time: 92.6939s\n",
      "\titers: 400, epoch: 5 | loss: 0.1103657\n",
      "\tspeed: 0.0105s/iter; left time: 91.7686s\n",
      "\titers: 500, epoch: 5 | loss: 0.1040657\n",
      "\tspeed: 0.0105s/iter; left time: 90.8803s\n",
      "Epoch: 5 cost time: 6.315140962600708\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1098918 Vali Loss: 0.0366639 Test Loss: 0.1306551\n",
      "Validation loss decreased (0.037836 --> 0.036664).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0982700\n",
      "\tspeed: 0.0360s/iter; left time: 303.9795s\n",
      "\titers: 200, epoch: 6 | loss: 0.1011713\n",
      "\tspeed: 0.0105s/iter; left time: 87.3942s\n",
      "\titers: 300, epoch: 6 | loss: 0.1064663\n",
      "\tspeed: 0.0105s/iter; left time: 86.5547s\n",
      "\titers: 400, epoch: 6 | loss: 0.1059360\n",
      "\tspeed: 0.0105s/iter; left time: 85.5736s\n",
      "\titers: 500, epoch: 6 | loss: 0.1032415\n",
      "\tspeed: 0.0105s/iter; left time: 84.3984s\n",
      "Epoch: 6 cost time: 6.278334140777588\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1038395 Vali Loss: 0.0361007 Test Loss: 0.1349440\n",
      "Validation loss decreased (0.036664 --> 0.036101).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1149267\n",
      "\tspeed: 0.0364s/iter; left time: 286.7510s\n",
      "\titers: 200, epoch: 7 | loss: 0.1476670\n",
      "\tspeed: 0.0105s/iter; left time: 81.8972s\n",
      "\titers: 300, epoch: 7 | loss: 0.0790868\n",
      "\tspeed: 0.0105s/iter; left time: 81.0239s\n",
      "\titers: 400, epoch: 7 | loss: 0.0798678\n",
      "\tspeed: 0.0105s/iter; left time: 79.8805s\n",
      "\titers: 500, epoch: 7 | loss: 0.1045000\n",
      "\tspeed: 0.0105s/iter; left time: 78.5853s\n",
      "Epoch: 7 cost time: 6.285093784332275\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1021100 Vali Loss: 0.0367107 Test Loss: 0.1336253\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0942366\n",
      "\tspeed: 0.0368s/iter; left time: 269.1128s\n",
      "\titers: 200, epoch: 8 | loss: 0.0988189\n",
      "\tspeed: 0.0105s/iter; left time: 76.0700s\n",
      "\titers: 300, epoch: 8 | loss: 0.1260353\n",
      "\tspeed: 0.0106s/iter; left time: 75.0479s\n",
      "\titers: 400, epoch: 8 | loss: 0.1617311\n",
      "\tspeed: 0.0106s/iter; left time: 74.0137s\n",
      "\titers: 500, epoch: 8 | loss: 0.0782998\n",
      "\tspeed: 0.0105s/iter; left time: 72.8809s\n",
      "Epoch: 8 cost time: 6.32673454284668\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0995351 Vali Loss: 0.0362384 Test Loss: 0.1333975\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1689786\n",
      "\tspeed: 0.0373s/iter; left time: 251.7276s\n",
      "\titers: 200, epoch: 9 | loss: 0.0877496\n",
      "\tspeed: 0.0105s/iter; left time: 69.8341s\n",
      "\titers: 300, epoch: 9 | loss: 0.0617598\n",
      "\tspeed: 0.0105s/iter; left time: 68.4649s\n",
      "\titers: 400, epoch: 9 | loss: 0.0963161\n",
      "\tspeed: 0.0105s/iter; left time: 67.4641s\n",
      "\titers: 500, epoch: 9 | loss: 0.0842207\n",
      "\tspeed: 0.0105s/iter; left time: 66.6468s\n",
      "Epoch: 9 cost time: 6.359121322631836\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0986069 Vali Loss: 0.0366251 Test Loss: 0.1347573\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13512693345546722, mae:0.23280654847621918\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2258161\n",
      "\tspeed: 0.0220s/iter; left time: 248.3529s\n",
      "\titers: 200, epoch: 1 | loss: 0.1745539\n",
      "\tspeed: 0.0103s/iter; left time: 115.4333s\n",
      "\titers: 300, epoch: 1 | loss: 0.2272285\n",
      "\tspeed: 0.0103s/iter; left time: 114.8108s\n",
      "\titers: 400, epoch: 1 | loss: 0.1798825\n",
      "\tspeed: 0.0103s/iter; left time: 113.3805s\n",
      "\titers: 500, epoch: 1 | loss: 0.1475461\n",
      "\tspeed: 0.0103s/iter; left time: 112.5698s\n",
      "Epoch: 1 cost time: 7.100583553314209\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1818829 Vali Loss: 0.0384824 Test Loss: 0.1245723\n",
      "Validation loss decreased (inf --> 0.038482).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1269545\n",
      "\tspeed: 0.0360s/iter; left time: 386.5524s\n",
      "\titers: 200, epoch: 2 | loss: 0.1704410\n",
      "\tspeed: 0.0103s/iter; left time: 109.4899s\n",
      "\titers: 300, epoch: 2 | loss: 0.1202096\n",
      "\tspeed: 0.0103s/iter; left time: 108.6442s\n",
      "\titers: 400, epoch: 2 | loss: 0.1311851\n",
      "\tspeed: 0.0103s/iter; left time: 107.2272s\n",
      "\titers: 500, epoch: 2 | loss: 0.1385412\n",
      "\tspeed: 0.0103s/iter; left time: 106.1888s\n",
      "Epoch: 2 cost time: 6.184586763381958\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1543717 Vali Loss: 0.0365131 Test Loss: 0.1147417\n",
      "Validation loss decreased (0.038482 --> 0.036513).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0716504\n",
      "\tspeed: 0.0367s/iter; left time: 372.6639s\n",
      "\titers: 200, epoch: 3 | loss: 0.1160610\n",
      "\tspeed: 0.0103s/iter; left time: 103.2498s\n",
      "\titers: 300, epoch: 3 | loss: 0.2342663\n",
      "\tspeed: 0.0103s/iter; left time: 102.1293s\n",
      "\titers: 400, epoch: 3 | loss: 0.1401396\n",
      "\tspeed: 0.0102s/iter; left time: 100.9683s\n",
      "\titers: 500, epoch: 3 | loss: 0.0908380\n",
      "\tspeed: 0.0102s/iter; left time: 99.8081s\n",
      "Epoch: 3 cost time: 6.12970757484436\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1285174 Vali Loss: 0.0326620 Test Loss: 0.1128577\n",
      "Validation loss decreased (0.036513 --> 0.032662).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1006823\n",
      "\tspeed: 0.0351s/iter; left time: 336.6025s\n",
      "\titers: 200, epoch: 4 | loss: 0.0886865\n",
      "\tspeed: 0.0103s/iter; left time: 97.3534s\n",
      "\titers: 300, epoch: 4 | loss: 0.0701043\n",
      "\tspeed: 0.0103s/iter; left time: 96.3149s\n",
      "\titers: 400, epoch: 4 | loss: 0.0819835\n",
      "\tspeed: 0.0102s/iter; left time: 95.1125s\n",
      "\titers: 500, epoch: 4 | loss: 0.1388223\n",
      "\tspeed: 0.0102s/iter; left time: 94.0243s\n",
      "Epoch: 4 cost time: 6.1052374839782715\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1119747 Vali Loss: 0.0325327 Test Loss: 0.1104302\n",
      "Validation loss decreased (0.032662 --> 0.032533).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0873269\n",
      "\tspeed: 0.0363s/iter; left time: 327.5036s\n",
      "\titers: 200, epoch: 5 | loss: 0.1224051\n",
      "\tspeed: 0.0103s/iter; left time: 91.5876s\n",
      "\titers: 300, epoch: 5 | loss: 0.0985460\n",
      "\tspeed: 0.0103s/iter; left time: 90.4590s\n",
      "\titers: 400, epoch: 5 | loss: 0.1158594\n",
      "\tspeed: 0.0102s/iter; left time: 89.2218s\n",
      "\titers: 500, epoch: 5 | loss: 0.0986964\n",
      "\tspeed: 0.0102s/iter; left time: 88.2755s\n",
      "Epoch: 5 cost time: 6.128857851028442\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1033130 Vali Loss: 0.0320356 Test Loss: 0.1119445\n",
      "Validation loss decreased (0.032533 --> 0.032036).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0776036\n",
      "\tspeed: 0.0366s/iter; left time: 309.4629s\n",
      "\titers: 200, epoch: 6 | loss: 0.1331430\n",
      "\tspeed: 0.0104s/iter; left time: 86.6411s\n",
      "\titers: 300, epoch: 6 | loss: 0.1295920\n",
      "\tspeed: 0.0112s/iter; left time: 92.8054s\n",
      "\titers: 400, epoch: 6 | loss: 0.0783443\n",
      "\tspeed: 0.0102s/iter; left time: 83.3327s\n",
      "\titers: 500, epoch: 6 | loss: 0.0676099\n",
      "\tspeed: 0.0102s/iter; left time: 82.2825s\n",
      "Epoch: 6 cost time: 6.211097240447998\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0982573 Vali Loss: 0.0314279 Test Loss: 0.1120153\n",
      "Validation loss decreased (0.032036 --> 0.031428).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0900342\n",
      "\tspeed: 0.0351s/iter; left time: 276.8070s\n",
      "\titers: 200, epoch: 7 | loss: 0.1217557\n",
      "\tspeed: 0.0103s/iter; left time: 80.1405s\n",
      "\titers: 300, epoch: 7 | loss: 0.1556553\n",
      "\tspeed: 0.0103s/iter; left time: 78.9689s\n",
      "\titers: 400, epoch: 7 | loss: 0.1425365\n",
      "\tspeed: 0.0103s/iter; left time: 78.0974s\n",
      "\titers: 500, epoch: 7 | loss: 0.0993003\n",
      "\tspeed: 0.0103s/iter; left time: 76.9627s\n",
      "Epoch: 7 cost time: 6.151717662811279\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0967970 Vali Loss: 0.0316864 Test Loss: 0.1134103\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0859262\n",
      "\tspeed: 0.0356s/iter; left time: 260.1996s\n",
      "\titers: 200, epoch: 8 | loss: 0.0744396\n",
      "\tspeed: 0.0103s/iter; left time: 73.9457s\n",
      "\titers: 300, epoch: 8 | loss: 0.0877784\n",
      "\tspeed: 0.0103s/iter; left time: 72.8882s\n",
      "\titers: 400, epoch: 8 | loss: 0.0967927\n",
      "\tspeed: 0.0103s/iter; left time: 71.9169s\n",
      "\titers: 500, epoch: 8 | loss: 0.0578659\n",
      "\tspeed: 0.0103s/iter; left time: 70.9353s\n",
      "Epoch: 8 cost time: 6.130937576293945\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0951235 Vali Loss: 0.0315706 Test Loss: 0.1138512\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0745744\n",
      "\tspeed: 0.0354s/iter; left time: 238.4671s\n",
      "\titers: 200, epoch: 9 | loss: 0.1081465\n",
      "\tspeed: 0.0102s/iter; left time: 67.7870s\n",
      "\titers: 300, epoch: 9 | loss: 0.0850372\n",
      "\tspeed: 0.0102s/iter; left time: 66.7216s\n",
      "\titers: 400, epoch: 9 | loss: 0.1032244\n",
      "\tspeed: 0.0102s/iter; left time: 65.6682s\n",
      "\titers: 500, epoch: 9 | loss: 0.0823691\n",
      "\tspeed: 0.0102s/iter; left time: 64.6464s\n",
      "Epoch: 9 cost time: 6.100155830383301\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0938647 Vali Loss: 0.0316149 Test Loss: 0.1139777\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11217200011014938, mae:0.20515543222427368\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2126052\n",
      "\tspeed: 0.0223s/iter; left time: 251.6548s\n",
      "\titers: 200, epoch: 1 | loss: 0.1374715\n",
      "\tspeed: 0.0104s/iter; left time: 116.8625s\n",
      "\titers: 300, epoch: 1 | loss: 0.1647153\n",
      "\tspeed: 0.0104s/iter; left time: 115.3999s\n",
      "\titers: 400, epoch: 1 | loss: 0.1097612\n",
      "\tspeed: 0.0104s/iter; left time: 114.4619s\n",
      "\titers: 500, epoch: 1 | loss: 0.2070301\n",
      "\tspeed: 0.0105s/iter; left time: 113.9347s\n",
      "Epoch: 1 cost time: 7.176550388336182\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2159055 Vali Loss: 0.0433398 Test Loss: 0.1446968\n",
      "Validation loss decreased (inf --> 0.043340).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1930389\n",
      "\tspeed: 0.0364s/iter; left time: 391.0698s\n",
      "\titers: 200, epoch: 2 | loss: 0.2482810\n",
      "\tspeed: 0.0104s/iter; left time: 110.2828s\n",
      "\titers: 300, epoch: 2 | loss: 0.1856881\n",
      "\tspeed: 0.0103s/iter; left time: 108.1565s\n",
      "\titers: 400, epoch: 2 | loss: 0.2534928\n",
      "\tspeed: 0.0103s/iter; left time: 107.7451s\n",
      "\titers: 500, epoch: 2 | loss: 0.1951734\n",
      "\tspeed: 0.0103s/iter; left time: 106.3653s\n",
      "Epoch: 2 cost time: 6.189270257949829\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1715210 Vali Loss: 0.0442632 Test Loss: 0.1414142\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0967074\n",
      "\tspeed: 0.0382s/iter; left time: 388.2735s\n",
      "\titers: 200, epoch: 3 | loss: 0.1278625\n",
      "\tspeed: 0.0104s/iter; left time: 104.2288s\n",
      "\titers: 300, epoch: 3 | loss: 0.1800192\n",
      "\tspeed: 0.0104s/iter; left time: 103.4230s\n",
      "\titers: 400, epoch: 3 | loss: 0.1153174\n",
      "\tspeed: 0.0104s/iter; left time: 102.7234s\n",
      "\titers: 500, epoch: 3 | loss: 0.0864907\n",
      "\tspeed: 0.0104s/iter; left time: 101.3632s\n",
      "Epoch: 3 cost time: 6.175130605697632\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1365773 Vali Loss: 0.0402464 Test Loss: 0.1316162\n",
      "Validation loss decreased (0.043340 --> 0.040246).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0689162\n",
      "\tspeed: 0.0359s/iter; left time: 344.5712s\n",
      "\titers: 200, epoch: 4 | loss: 0.0810238\n",
      "\tspeed: 0.0104s/iter; left time: 98.3994s\n",
      "\titers: 300, epoch: 4 | loss: 0.0791520\n",
      "\tspeed: 0.0104s/iter; left time: 97.3982s\n",
      "\titers: 400, epoch: 4 | loss: 0.1398570\n",
      "\tspeed: 0.0104s/iter; left time: 96.5331s\n",
      "\titers: 500, epoch: 4 | loss: 0.0956015\n",
      "\tspeed: 0.0104s/iter; left time: 95.6847s\n",
      "Epoch: 4 cost time: 6.223194122314453\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1136333 Vali Loss: 0.0403347 Test Loss: 0.1356540\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1071966\n",
      "\tspeed: 0.0356s/iter; left time: 320.8776s\n",
      "\titers: 200, epoch: 5 | loss: 0.0704997\n",
      "\tspeed: 0.0103s/iter; left time: 91.9499s\n",
      "\titers: 300, epoch: 5 | loss: 0.0892456\n",
      "\tspeed: 0.0103s/iter; left time: 91.0571s\n",
      "\titers: 400, epoch: 5 | loss: 0.1097680\n",
      "\tspeed: 0.0103s/iter; left time: 90.0486s\n",
      "\titers: 500, epoch: 5 | loss: 0.1410007\n",
      "\tspeed: 0.0103s/iter; left time: 88.7351s\n",
      "Epoch: 5 cost time: 6.139986276626587\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1049160 Vali Loss: 0.0393753 Test Loss: 0.1345298\n",
      "Validation loss decreased (0.040246 --> 0.039375).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0837624\n",
      "\tspeed: 0.0372s/iter; left time: 314.5553s\n",
      "\titers: 200, epoch: 6 | loss: 0.0961889\n",
      "\tspeed: 0.0104s/iter; left time: 87.0009s\n",
      "\titers: 300, epoch: 6 | loss: 0.1093323\n",
      "\tspeed: 0.0104s/iter; left time: 85.8993s\n",
      "\titers: 400, epoch: 6 | loss: 0.0809935\n",
      "\tspeed: 0.0104s/iter; left time: 84.8550s\n",
      "\titers: 500, epoch: 6 | loss: 0.1131487\n",
      "\tspeed: 0.0104s/iter; left time: 83.8092s\n",
      "Epoch: 6 cost time: 6.2379679679870605\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0989067 Vali Loss: 0.0389056 Test Loss: 0.1327591\n",
      "Validation loss decreased (0.039375 --> 0.038906).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0908248\n",
      "\tspeed: 0.0364s/iter; left time: 287.0306s\n",
      "\titers: 200, epoch: 7 | loss: 0.0762231\n",
      "\tspeed: 0.0103s/iter; left time: 80.0835s\n",
      "\titers: 300, epoch: 7 | loss: 0.0909373\n",
      "\tspeed: 0.0103s/iter; left time: 79.0223s\n",
      "\titers: 400, epoch: 7 | loss: 0.0681608\n",
      "\tspeed: 0.0103s/iter; left time: 78.1128s\n",
      "\titers: 500, epoch: 7 | loss: 0.0986060\n",
      "\tspeed: 0.0103s/iter; left time: 77.0998s\n",
      "Epoch: 7 cost time: 6.1860833168029785\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0962614 Vali Loss: 0.0390874 Test Loss: 0.1329040\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0775798\n",
      "\tspeed: 0.0383s/iter; left time: 279.9255s\n",
      "\titers: 200, epoch: 8 | loss: 0.1064886\n",
      "\tspeed: 0.0117s/iter; left time: 84.0222s\n",
      "\titers: 300, epoch: 8 | loss: 0.0902569\n",
      "\tspeed: 0.0117s/iter; left time: 82.9349s\n",
      "\titers: 400, epoch: 8 | loss: 0.0937475\n",
      "\tspeed: 0.0117s/iter; left time: 81.7754s\n",
      "\titers: 500, epoch: 8 | loss: 0.0717437\n",
      "\tspeed: 0.0117s/iter; left time: 80.5864s\n",
      "Epoch: 8 cost time: 6.922955751419067\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0940405 Vali Loss: 0.0391326 Test Loss: 0.1337127\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0720170\n",
      "\tspeed: 0.0388s/iter; left time: 261.3828s\n",
      "\titers: 200, epoch: 9 | loss: 0.1180055\n",
      "\tspeed: 0.0117s/iter; left time: 77.8109s\n",
      "\titers: 300, epoch: 9 | loss: 0.0763217\n",
      "\tspeed: 0.0117s/iter; left time: 76.7547s\n",
      "\titers: 400, epoch: 9 | loss: 0.0791094\n",
      "\tspeed: 0.0117s/iter; left time: 75.0807s\n",
      "\titers: 500, epoch: 9 | loss: 0.0849110\n",
      "\tspeed: 0.0117s/iter; left time: 73.9299s\n",
      "Epoch: 9 cost time: 6.989758491516113\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0928958 Vali Loss: 0.0394123 Test Loss: 0.1342105\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13295453786849976, mae:0.23362568020820618\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2748502\n",
      "\tspeed: 0.0250s/iter; left time: 282.0247s\n",
      "\titers: 200, epoch: 1 | loss: 0.2084380\n",
      "\tspeed: 0.0129s/iter; left time: 144.8314s\n",
      "\titers: 300, epoch: 1 | loss: 0.2147193\n",
      "\tspeed: 0.0129s/iter; left time: 143.1690s\n",
      "\titers: 400, epoch: 1 | loss: 0.1102636\n",
      "\tspeed: 0.0129s/iter; left time: 142.2694s\n",
      "\titers: 500, epoch: 1 | loss: 0.2449139\n",
      "\tspeed: 0.0129s/iter; left time: 140.5849s\n",
      "Epoch: 1 cost time: 8.621979236602783\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2353376 Vali Loss: 0.0468720 Test Loss: 0.1393561\n",
      "Validation loss decreased (inf --> 0.046872).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1956535\n",
      "\tspeed: 0.0409s/iter; left time: 438.4906s\n",
      "\titers: 200, epoch: 2 | loss: 0.1641064\n",
      "\tspeed: 0.0128s/iter; left time: 135.8292s\n",
      "\titers: 300, epoch: 2 | loss: 0.1314635\n",
      "\tspeed: 0.0128s/iter; left time: 134.7155s\n",
      "\titers: 400, epoch: 2 | loss: 0.1056886\n",
      "\tspeed: 0.0128s/iter; left time: 133.3553s\n",
      "\titers: 500, epoch: 2 | loss: 0.1730652\n",
      "\tspeed: 0.0128s/iter; left time: 132.1067s\n",
      "Epoch: 2 cost time: 7.605746507644653\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1995923 Vali Loss: 0.0450163 Test Loss: 0.1359542\n",
      "Validation loss decreased (0.046872 --> 0.045016).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1934972\n",
      "\tspeed: 0.0429s/iter; left time: 436.2680s\n",
      "\titers: 200, epoch: 3 | loss: 0.2169630\n",
      "\tspeed: 0.0128s/iter; left time: 128.3886s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213193\n",
      "\tspeed: 0.0128s/iter; left time: 127.1780s\n",
      "\titers: 400, epoch: 3 | loss: 0.1498200\n",
      "\tspeed: 0.0119s/iter; left time: 117.4192s\n",
      "\titers: 500, epoch: 3 | loss: 0.1662777\n",
      "\tspeed: 0.0119s/iter; left time: 116.0396s\n",
      "Epoch: 3 cost time: 7.360851764678955\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1741398 Vali Loss: 0.0422012 Test Loss: 0.1378659\n",
      "Validation loss decreased (0.045016 --> 0.042201).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1304682\n",
      "\tspeed: 0.0400s/iter; left time: 383.2883s\n",
      "\titers: 200, epoch: 4 | loss: 0.1854112\n",
      "\tspeed: 0.0118s/iter; left time: 112.1237s\n",
      "\titers: 300, epoch: 4 | loss: 0.1072130\n",
      "\tspeed: 0.0118s/iter; left time: 110.8831s\n",
      "\titers: 400, epoch: 4 | loss: 0.0934103\n",
      "\tspeed: 0.0118s/iter; left time: 109.6963s\n",
      "\titers: 500, epoch: 4 | loss: 0.1440407\n",
      "\tspeed: 0.0118s/iter; left time: 108.6186s\n",
      "Epoch: 4 cost time: 7.048134803771973\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1554324 Vali Loss: 0.0426190 Test Loss: 0.1270937\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1229717\n",
      "\tspeed: 0.0396s/iter; left time: 356.8603s\n",
      "\titers: 200, epoch: 5 | loss: 0.1247959\n",
      "\tspeed: 0.0119s/iter; left time: 105.9764s\n",
      "\titers: 300, epoch: 5 | loss: 0.1960773\n",
      "\tspeed: 0.0119s/iter; left time: 104.7680s\n",
      "\titers: 400, epoch: 5 | loss: 0.1387040\n",
      "\tspeed: 0.0119s/iter; left time: 103.4973s\n",
      "\titers: 500, epoch: 5 | loss: 0.1279780\n",
      "\tspeed: 0.0119s/iter; left time: 102.2995s\n",
      "Epoch: 5 cost time: 7.094903469085693\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1429132 Vali Loss: 0.0403843 Test Loss: 0.1261380\n",
      "Validation loss decreased (0.042201 --> 0.040384).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1534443\n",
      "\tspeed: 0.0394s/iter; left time: 332.8328s\n",
      "\titers: 200, epoch: 6 | loss: 0.0734791\n",
      "\tspeed: 0.0118s/iter; left time: 98.5572s\n",
      "\titers: 300, epoch: 6 | loss: 0.1539678\n",
      "\tspeed: 0.0118s/iter; left time: 97.4459s\n",
      "\titers: 400, epoch: 6 | loss: 0.1341914\n",
      "\tspeed: 0.0118s/iter; left time: 96.2642s\n",
      "\titers: 500, epoch: 6 | loss: 0.1050217\n",
      "\tspeed: 0.0118s/iter; left time: 95.0709s\n",
      "Epoch: 6 cost time: 7.05040168762207\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1340782 Vali Loss: 0.0394809 Test Loss: 0.1283349\n",
      "Validation loss decreased (0.040384 --> 0.039481).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1350735\n",
      "\tspeed: 0.0420s/iter; left time: 331.3949s\n",
      "\titers: 200, epoch: 7 | loss: 0.1328423\n",
      "\tspeed: 0.0129s/iter; left time: 100.5190s\n",
      "\titers: 300, epoch: 7 | loss: 0.1311206\n",
      "\tspeed: 0.0130s/iter; left time: 99.6439s\n",
      "\titers: 400, epoch: 7 | loss: 0.0903304\n",
      "\tspeed: 0.0129s/iter; left time: 98.1416s\n",
      "\titers: 500, epoch: 7 | loss: 0.1177615\n",
      "\tspeed: 0.0129s/iter; left time: 96.6849s\n",
      "Epoch: 7 cost time: 7.73091721534729\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1292391 Vali Loss: 0.0397825 Test Loss: 0.1285097\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0874919\n",
      "\tspeed: 0.0389s/iter; left time: 284.3073s\n",
      "\titers: 200, epoch: 8 | loss: 0.1236357\n",
      "\tspeed: 0.0119s/iter; left time: 85.5454s\n",
      "\titers: 300, epoch: 8 | loss: 0.1306811\n",
      "\tspeed: 0.0119s/iter; left time: 84.4534s\n",
      "\titers: 400, epoch: 8 | loss: 0.1075007\n",
      "\tspeed: 0.0118s/iter; left time: 83.0557s\n",
      "\titers: 500, epoch: 8 | loss: 0.1073734\n",
      "\tspeed: 0.0119s/iter; left time: 81.9038s\n",
      "Epoch: 8 cost time: 7.046395540237427\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1282189 Vali Loss: 0.0393154 Test Loss: 0.1281180\n",
      "Validation loss decreased (0.039481 --> 0.039315).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1067264\n",
      "\tspeed: 0.0401s/iter; left time: 270.3638s\n",
      "\titers: 200, epoch: 9 | loss: 0.1032476\n",
      "\tspeed: 0.0118s/iter; left time: 78.5624s\n",
      "\titers: 300, epoch: 9 | loss: 0.1405332\n",
      "\tspeed: 0.0118s/iter; left time: 77.2721s\n",
      "\titers: 400, epoch: 9 | loss: 0.0929061\n",
      "\tspeed: 0.0118s/iter; left time: 76.2846s\n",
      "\titers: 500, epoch: 9 | loss: 0.1268479\n",
      "\tspeed: 0.0118s/iter; left time: 74.9248s\n",
      "Epoch: 9 cost time: 7.063356399536133\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1259667 Vali Loss: 0.0393005 Test Loss: 0.1285784\n",
      "Validation loss decreased (0.039315 --> 0.039301).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1167146\n",
      "\tspeed: 0.0403s/iter; left time: 248.9301s\n",
      "\titers: 200, epoch: 10 | loss: 0.1376054\n",
      "\tspeed: 0.0119s/iter; left time: 72.1268s\n",
      "\titers: 300, epoch: 10 | loss: 0.1336108\n",
      "\tspeed: 0.0119s/iter; left time: 70.8325s\n",
      "\titers: 400, epoch: 10 | loss: 0.1067017\n",
      "\tspeed: 0.0118s/iter; left time: 69.4733s\n",
      "\titers: 500, epoch: 10 | loss: 0.1816334\n",
      "\tspeed: 0.0119s/iter; left time: 68.4239s\n",
      "Epoch: 10 cost time: 7.088929891586304\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1260375 Vali Loss: 0.0393978 Test Loss: 0.1289789\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1116521\n",
      "\tspeed: 0.0389s/iter; left time: 218.0111s\n",
      "\titers: 200, epoch: 11 | loss: 0.1775654\n",
      "\tspeed: 0.0118s/iter; left time: 65.0262s\n",
      "\titers: 300, epoch: 11 | loss: 0.1322004\n",
      "\tspeed: 0.0118s/iter; left time: 63.9814s\n",
      "\titers: 400, epoch: 11 | loss: 0.1197584\n",
      "\tspeed: 0.0118s/iter; left time: 62.6531s\n",
      "\titers: 500, epoch: 11 | loss: 0.1856377\n",
      "\tspeed: 0.0118s/iter; left time: 61.4374s\n",
      "Epoch: 11 cost time: 7.078430414199829\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1244359 Vali Loss: 0.0394877 Test Loss: 0.1293370\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.0919617\n",
      "\tspeed: 0.0398s/iter; left time: 200.0002s\n",
      "\titers: 200, epoch: 12 | loss: 0.0905452\n",
      "\tspeed: 0.0119s/iter; left time: 58.7055s\n",
      "\titers: 300, epoch: 12 | loss: 0.1048527\n",
      "\tspeed: 0.0119s/iter; left time: 57.4251s\n",
      "\titers: 400, epoch: 12 | loss: 0.1121643\n",
      "\tspeed: 0.0119s/iter; left time: 56.2956s\n",
      "\titers: 500, epoch: 12 | loss: 0.1147581\n",
      "\tspeed: 0.0119s/iter; left time: 55.0511s\n",
      "Epoch: 12 cost time: 7.108351230621338\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1251730 Vali Loss: 0.0395623 Test Loss: 0.1292676\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12871778011322021, mae:0.22809365391731262\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1300711\n",
      "\tspeed: 0.0234s/iter; left time: 263.9984s\n",
      "\titers: 200, epoch: 1 | loss: 0.3094113\n",
      "\tspeed: 0.0115s/iter; left time: 128.4862s\n",
      "\titers: 300, epoch: 1 | loss: 0.1772321\n",
      "\tspeed: 0.0115s/iter; left time: 127.5790s\n",
      "\titers: 400, epoch: 1 | loss: 0.1927970\n",
      "\tspeed: 0.0115s/iter; left time: 126.1777s\n",
      "\titers: 500, epoch: 1 | loss: 0.1227823\n",
      "\tspeed: 0.0115s/iter; left time: 125.0530s\n",
      "Epoch: 1 cost time: 7.78114914894104\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1955146 Vali Loss: 0.0420059 Test Loss: 0.1293657\n",
      "Validation loss decreased (inf --> 0.042006).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1480882\n",
      "\tspeed: 0.0406s/iter; left time: 435.4715s\n",
      "\titers: 200, epoch: 2 | loss: 0.1168805\n",
      "\tspeed: 0.0115s/iter; left time: 121.7834s\n",
      "\titers: 300, epoch: 2 | loss: 0.1539083\n",
      "\tspeed: 0.0115s/iter; left time: 120.9135s\n",
      "\titers: 400, epoch: 2 | loss: 0.2182992\n",
      "\tspeed: 0.0115s/iter; left time: 119.4797s\n",
      "\titers: 500, epoch: 2 | loss: 0.6056554\n",
      "\tspeed: 0.0115s/iter; left time: 118.4963s\n",
      "Epoch: 2 cost time: 6.940418243408203\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1892001 Vali Loss: 0.0559730 Test Loss: 0.1621517\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2122157\n",
      "\tspeed: 0.0394s/iter; left time: 400.6823s\n",
      "\titers: 200, epoch: 3 | loss: 0.0970625\n",
      "\tspeed: 0.0116s/iter; left time: 116.3204s\n",
      "\titers: 300, epoch: 3 | loss: 0.1917423\n",
      "\tspeed: 0.0116s/iter; left time: 115.0567s\n",
      "\titers: 400, epoch: 3 | loss: 0.1587247\n",
      "\tspeed: 0.0115s/iter; left time: 113.6300s\n",
      "\titers: 500, epoch: 3 | loss: 0.2420566\n",
      "\tspeed: 0.0116s/iter; left time: 112.7672s\n",
      "Epoch: 3 cost time: 6.915986061096191\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1813456 Vali Loss: 0.0372267 Test Loss: 0.1250522\n",
      "Validation loss decreased (0.042006 --> 0.037227).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1855680\n",
      "\tspeed: 0.0404s/iter; left time: 387.5487s\n",
      "\titers: 200, epoch: 4 | loss: 0.0917098\n",
      "\tspeed: 0.0115s/iter; left time: 108.9737s\n",
      "\titers: 300, epoch: 4 | loss: 0.0880305\n",
      "\tspeed: 0.0115s/iter; left time: 107.8915s\n",
      "\titers: 400, epoch: 4 | loss: 0.1410631\n",
      "\tspeed: 0.0115s/iter; left time: 106.5433s\n",
      "\titers: 500, epoch: 4 | loss: 0.1505861\n",
      "\tspeed: 0.0115s/iter; left time: 105.3892s\n",
      "Epoch: 4 cost time: 6.873798131942749\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1574655 Vali Loss: 0.0376252 Test Loss: 0.1238940\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1433762\n",
      "\tspeed: 0.0389s/iter; left time: 350.7341s\n",
      "\titers: 200, epoch: 5 | loss: 0.1362560\n",
      "\tspeed: 0.0115s/iter; left time: 102.2550s\n",
      "\titers: 300, epoch: 5 | loss: 0.2190448\n",
      "\tspeed: 0.0115s/iter; left time: 101.1944s\n",
      "\titers: 400, epoch: 5 | loss: 0.1679020\n",
      "\tspeed: 0.0115s/iter; left time: 99.9493s\n",
      "\titers: 500, epoch: 5 | loss: 0.1148061\n",
      "\tspeed: 0.0115s/iter; left time: 98.8807s\n",
      "Epoch: 5 cost time: 6.848667860031128\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1475436 Vali Loss: 0.0346353 Test Loss: 0.1169233\n",
      "Validation loss decreased (0.037227 --> 0.034635).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1732468\n",
      "\tspeed: 0.0389s/iter; left time: 329.0539s\n",
      "\titers: 200, epoch: 6 | loss: 0.1484320\n",
      "\tspeed: 0.0115s/iter; left time: 95.9279s\n",
      "\titers: 300, epoch: 6 | loss: 0.0845025\n",
      "\tspeed: 0.0115s/iter; left time: 95.0278s\n",
      "\titers: 400, epoch: 6 | loss: 0.0988227\n",
      "\tspeed: 0.0115s/iter; left time: 93.8036s\n",
      "\titers: 500, epoch: 6 | loss: 0.1221070\n",
      "\tspeed: 0.0115s/iter; left time: 92.6768s\n",
      "Epoch: 6 cost time: 6.874903917312622\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1396978 Vali Loss: 0.0338739 Test Loss: 0.1158155\n",
      "Validation loss decreased (0.034635 --> 0.033874).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1004824\n",
      "\tspeed: 0.0392s/iter; left time: 308.5634s\n",
      "\titers: 200, epoch: 7 | loss: 0.2261177\n",
      "\tspeed: 0.0115s/iter; left time: 89.3396s\n",
      "\titers: 300, epoch: 7 | loss: 0.1112074\n",
      "\tspeed: 0.0115s/iter; left time: 88.4728s\n",
      "\titers: 400, epoch: 7 | loss: 0.1229349\n",
      "\tspeed: 0.0115s/iter; left time: 87.2963s\n",
      "\titers: 500, epoch: 7 | loss: 0.1122292\n",
      "\tspeed: 0.0115s/iter; left time: 85.8461s\n",
      "Epoch: 7 cost time: 6.882357358932495\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1377403 Vali Loss: 0.0337382 Test Loss: 0.1156678\n",
      "Validation loss decreased (0.033874 --> 0.033738).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1037824\n",
      "\tspeed: 0.0398s/iter; left time: 291.2491s\n",
      "\titers: 200, epoch: 8 | loss: 0.1524660\n",
      "\tspeed: 0.0115s/iter; left time: 82.7212s\n",
      "\titers: 300, epoch: 8 | loss: 0.1183857\n",
      "\tspeed: 0.0115s/iter; left time: 81.6051s\n",
      "\titers: 400, epoch: 8 | loss: 0.1234620\n",
      "\tspeed: 0.0115s/iter; left time: 80.3759s\n",
      "\titers: 500, epoch: 8 | loss: 0.1097233\n",
      "\tspeed: 0.0115s/iter; left time: 79.2446s\n",
      "Epoch: 8 cost time: 6.867618083953857\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1357162 Vali Loss: 0.0336374 Test Loss: 0.1146246\n",
      "Validation loss decreased (0.033738 --> 0.033637).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1797483\n",
      "\tspeed: 0.0393s/iter; left time: 265.1488s\n",
      "\titers: 200, epoch: 9 | loss: 0.0966282\n",
      "\tspeed: 0.0115s/iter; left time: 76.5781s\n",
      "\titers: 300, epoch: 9 | loss: 0.1592433\n",
      "\tspeed: 0.0115s/iter; left time: 75.4252s\n",
      "\titers: 400, epoch: 9 | loss: 0.1242249\n",
      "\tspeed: 0.0115s/iter; left time: 74.2696s\n",
      "\titers: 500, epoch: 9 | loss: 0.2368679\n",
      "\tspeed: 0.0125s/iter; left time: 79.3197s\n",
      "Epoch: 9 cost time: 7.062002420425415\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1345700 Vali Loss: 0.0333215 Test Loss: 0.1144174\n",
      "Validation loss decreased (0.033637 --> 0.033322).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1224813\n",
      "\tspeed: 0.0399s/iter; left time: 246.4031s\n",
      "\titers: 200, epoch: 10 | loss: 0.0959411\n",
      "\tspeed: 0.0115s/iter; left time: 69.8941s\n",
      "\titers: 300, epoch: 10 | loss: 0.1260364\n",
      "\tspeed: 0.0115s/iter; left time: 68.5740s\n",
      "\titers: 400, epoch: 10 | loss: 0.1690525\n",
      "\tspeed: 0.0115s/iter; left time: 67.4265s\n",
      "\titers: 500, epoch: 10 | loss: 0.1556181\n",
      "\tspeed: 0.0115s/iter; left time: 66.3613s\n",
      "Epoch: 10 cost time: 6.85985541343689\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1346377 Vali Loss: 0.0333686 Test Loss: 0.1149346\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1144761\n",
      "\tspeed: 0.0396s/iter; left time: 221.5317s\n",
      "\titers: 200, epoch: 11 | loss: 0.1198494\n",
      "\tspeed: 0.0115s/iter; left time: 63.1675s\n",
      "\titers: 300, epoch: 11 | loss: 0.1082775\n",
      "\tspeed: 0.0115s/iter; left time: 62.0109s\n",
      "\titers: 400, epoch: 11 | loss: 0.1738293\n",
      "\tspeed: 0.0115s/iter; left time: 60.9828s\n",
      "\titers: 500, epoch: 11 | loss: 0.1408115\n",
      "\tspeed: 0.0115s/iter; left time: 59.8145s\n",
      "Epoch: 11 cost time: 6.8390891551971436\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1339782 Vali Loss: 0.0333226 Test Loss: 0.1150690\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1342557\n",
      "\tspeed: 0.0379s/iter; left time: 190.8716s\n",
      "\titers: 200, epoch: 12 | loss: 0.1216956\n",
      "\tspeed: 0.0115s/iter; left time: 56.8086s\n",
      "\titers: 300, epoch: 12 | loss: 0.1211447\n",
      "\tspeed: 0.0115s/iter; left time: 55.7113s\n",
      "\titers: 400, epoch: 12 | loss: 0.1809108\n",
      "\tspeed: 0.0115s/iter; left time: 54.5715s\n",
      "\titers: 500, epoch: 12 | loss: 0.1421872\n",
      "\tspeed: 0.0115s/iter; left time: 53.4115s\n",
      "Epoch: 12 cost time: 6.833071231842041\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1330528 Vali Loss: 0.0335861 Test Loss: 0.1150666\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11457356065511703, mae:0.2082524299621582\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2563054\n",
      "\tspeed: 0.0240s/iter; left time: 271.2517s\n",
      "\titers: 200, epoch: 1 | loss: 0.3060882\n",
      "\tspeed: 0.0120s/iter; left time: 134.8536s\n",
      "\titers: 300, epoch: 1 | loss: 0.2427146\n",
      "\tspeed: 0.0120s/iter; left time: 133.2927s\n",
      "\titers: 400, epoch: 1 | loss: 0.1704900\n",
      "\tspeed: 0.0120s/iter; left time: 132.2514s\n",
      "\titers: 500, epoch: 1 | loss: 0.1884707\n",
      "\tspeed: 0.0121s/iter; left time: 131.8408s\n",
      "Epoch: 1 cost time: 8.108466386795044\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2254649 Vali Loss: 0.0460251 Test Loss: 0.1410485\n",
      "Validation loss decreased (inf --> 0.046025).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2278082\n",
      "\tspeed: 0.0399s/iter; left time: 428.1738s\n",
      "\titers: 200, epoch: 2 | loss: 0.2863010\n",
      "\tspeed: 0.0108s/iter; left time: 115.1900s\n",
      "\titers: 300, epoch: 2 | loss: 0.1574631\n",
      "\tspeed: 0.0109s/iter; left time: 114.4583s\n",
      "\titers: 400, epoch: 2 | loss: 0.1781293\n",
      "\tspeed: 0.0108s/iter; left time: 113.0331s\n",
      "\titers: 500, epoch: 2 | loss: 0.3219774\n",
      "\tspeed: 0.0109s/iter; left time: 112.2817s\n",
      "Epoch: 2 cost time: 6.550750732421875\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2344164 Vali Loss: 0.0522266 Test Loss: 0.1590629\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1617962\n",
      "\tspeed: 0.0368s/iter; left time: 373.6391s\n",
      "\titers: 200, epoch: 3 | loss: 0.1521518\n",
      "\tspeed: 0.0109s/iter; left time: 109.2225s\n",
      "\titers: 300, epoch: 3 | loss: 0.2234536\n",
      "\tspeed: 0.0109s/iter; left time: 108.1599s\n",
      "\titers: 400, epoch: 3 | loss: 0.1817826\n",
      "\tspeed: 0.0109s/iter; left time: 107.2112s\n",
      "\titers: 500, epoch: 3 | loss: 0.2091747\n",
      "\tspeed: 0.0108s/iter; left time: 105.8488s\n",
      "Epoch: 3 cost time: 6.480379581451416\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2069548 Vali Loss: 0.0490118 Test Loss: 0.1398464\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2131798\n",
      "\tspeed: 0.0382s/iter; left time: 366.3586s\n",
      "\titers: 200, epoch: 4 | loss: 0.1151125\n",
      "\tspeed: 0.0109s/iter; left time: 103.3951s\n",
      "\titers: 300, epoch: 4 | loss: 0.1875534\n",
      "\tspeed: 0.0109s/iter; left time: 102.1971s\n",
      "\titers: 400, epoch: 4 | loss: 0.2876613\n",
      "\tspeed: 0.0109s/iter; left time: 101.1657s\n",
      "\titers: 500, epoch: 4 | loss: 0.1665280\n",
      "\tspeed: 0.0109s/iter; left time: 100.2599s\n",
      "Epoch: 4 cost time: 6.482029676437378\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1805104 Vali Loss: 0.0436446 Test Loss: 0.1359658\n",
      "Validation loss decreased (0.046025 --> 0.043645).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1752025\n",
      "\tspeed: 0.0381s/iter; left time: 343.6311s\n",
      "\titers: 200, epoch: 5 | loss: 0.1700835\n",
      "\tspeed: 0.0109s/iter; left time: 97.3192s\n",
      "\titers: 300, epoch: 5 | loss: 0.1537457\n",
      "\tspeed: 0.0109s/iter; left time: 96.3739s\n",
      "\titers: 400, epoch: 5 | loss: 0.1103722\n",
      "\tspeed: 0.0109s/iter; left time: 95.1316s\n",
      "\titers: 500, epoch: 5 | loss: 0.2354374\n",
      "\tspeed: 0.0109s/iter; left time: 94.0187s\n",
      "Epoch: 5 cost time: 6.546094179153442\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1616194 Vali Loss: 0.0424842 Test Loss: 0.1335908\n",
      "Validation loss decreased (0.043645 --> 0.042484).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1213849\n",
      "\tspeed: 0.0414s/iter; left time: 350.0067s\n",
      "\titers: 200, epoch: 6 | loss: 0.1398307\n",
      "\tspeed: 0.0121s/iter; left time: 100.8311s\n",
      "\titers: 300, epoch: 6 | loss: 0.1669592\n",
      "\tspeed: 0.0121s/iter; left time: 99.4822s\n",
      "\titers: 400, epoch: 6 | loss: 0.1325021\n",
      "\tspeed: 0.0118s/iter; left time: 95.9943s\n",
      "\titers: 500, epoch: 6 | loss: 0.1601461\n",
      "\tspeed: 0.0109s/iter; left time: 87.5859s\n",
      "Epoch: 6 cost time: 6.957869291305542\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1514458 Vali Loss: 0.0423038 Test Loss: 0.1384318\n",
      "Validation loss decreased (0.042484 --> 0.042304).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1293175\n",
      "\tspeed: 0.0383s/iter; left time: 301.8842s\n",
      "\titers: 200, epoch: 7 | loss: 0.1011876\n",
      "\tspeed: 0.0109s/iter; left time: 85.0009s\n",
      "\titers: 300, epoch: 7 | loss: 0.0974072\n",
      "\tspeed: 0.0109s/iter; left time: 83.8142s\n",
      "\titers: 400, epoch: 7 | loss: 0.1650061\n",
      "\tspeed: 0.0109s/iter; left time: 82.6012s\n",
      "\titers: 500, epoch: 7 | loss: 0.1089765\n",
      "\tspeed: 0.0109s/iter; left time: 81.4498s\n",
      "Epoch: 7 cost time: 6.5218329429626465\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1461564 Vali Loss: 0.0408922 Test Loss: 0.1339641\n",
      "Validation loss decreased (0.042304 --> 0.040892).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1181434\n",
      "\tspeed: 0.0383s/iter; left time: 280.1453s\n",
      "\titers: 200, epoch: 8 | loss: 0.1858453\n",
      "\tspeed: 0.0109s/iter; left time: 78.7396s\n",
      "\titers: 300, epoch: 8 | loss: 0.1627850\n",
      "\tspeed: 0.0109s/iter; left time: 77.4486s\n",
      "\titers: 400, epoch: 8 | loss: 0.1537389\n",
      "\tspeed: 0.0109s/iter; left time: 76.2331s\n",
      "\titers: 500, epoch: 8 | loss: 0.1267983\n",
      "\tspeed: 0.0109s/iter; left time: 75.3266s\n",
      "Epoch: 8 cost time: 6.5266852378845215\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1429851 Vali Loss: 0.0401977 Test Loss: 0.1338758\n",
      "Validation loss decreased (0.040892 --> 0.040198).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1155324\n",
      "\tspeed: 0.0390s/iter; left time: 263.0395s\n",
      "\titers: 200, epoch: 9 | loss: 0.1059210\n",
      "\tspeed: 0.0109s/iter; left time: 72.5817s\n",
      "\titers: 300, epoch: 9 | loss: 0.1898012\n",
      "\tspeed: 0.0109s/iter; left time: 71.2687s\n",
      "\titers: 400, epoch: 9 | loss: 0.1801835\n",
      "\tspeed: 0.0109s/iter; left time: 70.0957s\n",
      "\titers: 500, epoch: 9 | loss: 0.0898061\n",
      "\tspeed: 0.0109s/iter; left time: 69.0010s\n",
      "Epoch: 9 cost time: 6.52003812789917\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1410465 Vali Loss: 0.0404282 Test Loss: 0.1336343\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1446062\n",
      "\tspeed: 0.0396s/iter; left time: 244.2844s\n",
      "\titers: 200, epoch: 10 | loss: 0.0783195\n",
      "\tspeed: 0.0115s/iter; left time: 69.9240s\n",
      "\titers: 300, epoch: 10 | loss: 0.1531917\n",
      "\tspeed: 0.0110s/iter; left time: 65.4016s\n",
      "\titers: 400, epoch: 10 | loss: 0.1077531\n",
      "\tspeed: 0.0109s/iter; left time: 64.1907s\n",
      "\titers: 500, epoch: 10 | loss: 0.1898513\n",
      "\tspeed: 0.0109s/iter; left time: 63.1706s\n",
      "Epoch: 10 cost time: 6.706769704818726\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1413932 Vali Loss: 0.0402531 Test Loss: 0.1331305\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0896061\n",
      "\tspeed: 0.0369s/iter; left time: 206.7232s\n",
      "\titers: 200, epoch: 11 | loss: 0.1391569\n",
      "\tspeed: 0.0108s/iter; left time: 59.5227s\n",
      "\titers: 300, epoch: 11 | loss: 0.1467878\n",
      "\tspeed: 0.0108s/iter; left time: 58.5297s\n",
      "\titers: 400, epoch: 11 | loss: 0.1015909\n",
      "\tspeed: 0.0108s/iter; left time: 57.4081s\n",
      "\titers: 500, epoch: 11 | loss: 0.1169378\n",
      "\tspeed: 0.0108s/iter; left time: 56.3121s\n",
      "Epoch: 11 cost time: 6.487159013748169\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1408087 Vali Loss: 0.0402770 Test Loss: 0.1330332\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13402952253818512, mae:0.23121598362922668\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1887528\n",
      "\tspeed: 0.0263s/iter; left time: 297.2034s\n",
      "\titers: 200, epoch: 1 | loss: 0.1745297\n",
      "\tspeed: 0.0146s/iter; left time: 163.3285s\n",
      "\titers: 300, epoch: 1 | loss: 0.2686448\n",
      "\tspeed: 0.0146s/iter; left time: 161.6412s\n",
      "\titers: 400, epoch: 1 | loss: 0.1905873\n",
      "\tspeed: 0.0146s/iter; left time: 160.4710s\n",
      "\titers: 500, epoch: 1 | loss: 0.1562005\n",
      "\tspeed: 0.0146s/iter; left time: 159.3693s\n",
      "Epoch: 1 cost time: 9.539918422698975\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2121014 Vali Loss: 0.0457071 Test Loss: 0.1369302\n",
      "Validation loss decreased (inf --> 0.045707).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2004817\n",
      "\tspeed: 0.0471s/iter; left time: 505.8297s\n",
      "\titers: 200, epoch: 2 | loss: 0.1453238\n",
      "\tspeed: 0.0146s/iter; left time: 155.4346s\n",
      "\titers: 300, epoch: 2 | loss: 0.1475892\n",
      "\tspeed: 0.0146s/iter; left time: 153.9990s\n",
      "\titers: 400, epoch: 2 | loss: 0.0873680\n",
      "\tspeed: 0.0146s/iter; left time: 152.6279s\n",
      "\titers: 500, epoch: 2 | loss: 0.1953811\n",
      "\tspeed: 0.0146s/iter; left time: 150.9691s\n",
      "Epoch: 2 cost time: 8.656659603118896\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1697742 Vali Loss: 0.0448095 Test Loss: 0.1532207\n",
      "Validation loss decreased (0.045707 --> 0.044810).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2172320\n",
      "\tspeed: 0.0470s/iter; left time: 477.7769s\n",
      "\titers: 200, epoch: 3 | loss: 0.1722004\n",
      "\tspeed: 0.0147s/iter; left time: 147.5230s\n",
      "\titers: 300, epoch: 3 | loss: 0.0974896\n",
      "\tspeed: 0.0147s/iter; left time: 146.1692s\n",
      "\titers: 400, epoch: 3 | loss: 0.1056065\n",
      "\tspeed: 0.0147s/iter; left time: 144.8839s\n",
      "\titers: 500, epoch: 3 | loss: 0.0902604\n",
      "\tspeed: 0.0147s/iter; left time: 143.1429s\n",
      "Epoch: 3 cost time: 8.687320947647095\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1363809 Vali Loss: 0.0369532 Test Loss: 0.1324762\n",
      "Validation loss decreased (0.044810 --> 0.036953).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1073803\n",
      "\tspeed: 0.0485s/iter; left time: 465.3882s\n",
      "\titers: 200, epoch: 4 | loss: 0.0982647\n",
      "\tspeed: 0.0165s/iter; left time: 156.1273s\n",
      "\titers: 300, epoch: 4 | loss: 0.1236442\n",
      "\tspeed: 0.0156s/iter; left time: 146.8999s\n",
      "\titers: 400, epoch: 4 | loss: 0.1236129\n",
      "\tspeed: 0.0146s/iter; left time: 135.8473s\n",
      "\titers: 500, epoch: 4 | loss: 0.0928323\n",
      "\tspeed: 0.0146s/iter; left time: 134.4445s\n",
      "Epoch: 4 cost time: 9.113266229629517\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1142296 Vali Loss: 0.0382205 Test Loss: 0.1314452\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1057231\n",
      "\tspeed: 0.0459s/iter; left time: 413.7922s\n",
      "\titers: 200, epoch: 5 | loss: 0.0959686\n",
      "\tspeed: 0.0147s/iter; left time: 131.5545s\n",
      "\titers: 300, epoch: 5 | loss: 0.1053961\n",
      "\tspeed: 0.0147s/iter; left time: 129.9197s\n",
      "\titers: 400, epoch: 5 | loss: 0.0760272\n",
      "\tspeed: 0.0148s/iter; left time: 128.7153s\n",
      "\titers: 500, epoch: 5 | loss: 0.0985045\n",
      "\tspeed: 0.0147s/iter; left time: 127.0876s\n",
      "Epoch: 5 cost time: 8.672228813171387\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1028115 Vali Loss: 0.0374991 Test Loss: 0.1352442\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0769622\n",
      "\tspeed: 0.0462s/iter; left time: 390.0554s\n",
      "\titers: 200, epoch: 6 | loss: 0.1111268\n",
      "\tspeed: 0.0147s/iter; left time: 122.5007s\n",
      "\titers: 300, epoch: 6 | loss: 0.0763125\n",
      "\tspeed: 0.0147s/iter; left time: 120.9743s\n",
      "\titers: 400, epoch: 6 | loss: 0.0836025\n",
      "\tspeed: 0.0146s/iter; left time: 119.3091s\n",
      "\titers: 500, epoch: 6 | loss: 0.0896173\n",
      "\tspeed: 0.0146s/iter; left time: 117.8496s\n",
      "Epoch: 6 cost time: 8.62747836112976\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0954362 Vali Loss: 0.0381863 Test Loss: 0.1341667\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1326751559972763, mae:0.23089633882045746\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2571278\n",
      "\tspeed: 0.0281s/iter; left time: 317.5207s\n",
      "\titers: 200, epoch: 1 | loss: 0.1795674\n",
      "\tspeed: 0.0164s/iter; left time: 183.2689s\n",
      "\titers: 300, epoch: 1 | loss: 0.1446693\n",
      "\tspeed: 0.0164s/iter; left time: 181.8129s\n",
      "\titers: 400, epoch: 1 | loss: 0.2394119\n",
      "\tspeed: 0.0164s/iter; left time: 180.0496s\n",
      "\titers: 500, epoch: 1 | loss: 0.1704829\n",
      "\tspeed: 0.0164s/iter; left time: 178.7969s\n",
      "Epoch: 1 cost time: 10.55946159362793\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1852389 Vali Loss: 0.0390686 Test Loss: 0.1200361\n",
      "Validation loss decreased (inf --> 0.039069).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1506557\n",
      "\tspeed: 0.0507s/iter; left time: 543.6751s\n",
      "\titers: 200, epoch: 2 | loss: 0.2355483\n",
      "\tspeed: 0.0164s/iter; left time: 174.5275s\n",
      "\titers: 300, epoch: 2 | loss: 0.1674860\n",
      "\tspeed: 0.0165s/iter; left time: 173.3000s\n",
      "\titers: 400, epoch: 2 | loss: 0.1498764\n",
      "\tspeed: 0.0164s/iter; left time: 171.5250s\n",
      "\titers: 500, epoch: 2 | loss: 0.1291158\n",
      "\tspeed: 0.0164s/iter; left time: 169.5408s\n",
      "Epoch: 2 cost time: 9.670292377471924\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1558052 Vali Loss: 0.0364324 Test Loss: 0.1229202\n",
      "Validation loss decreased (0.039069 --> 0.036432).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1597399\n",
      "\tspeed: 0.0506s/iter; left time: 514.5095s\n",
      "\titers: 200, epoch: 3 | loss: 0.0761815\n",
      "\tspeed: 0.0165s/iter; left time: 166.0624s\n",
      "\titers: 300, epoch: 3 | loss: 0.1524725\n",
      "\tspeed: 0.0165s/iter; left time: 164.1581s\n",
      "\titers: 400, epoch: 3 | loss: 0.1095894\n",
      "\tspeed: 0.0164s/iter; left time: 161.9155s\n",
      "\titers: 500, epoch: 3 | loss: 0.1125388\n",
      "\tspeed: 0.0164s/iter; left time: 160.3990s\n",
      "Epoch: 3 cost time: 9.6886625289917\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1273141 Vali Loss: 0.0354726 Test Loss: 0.1159626\n",
      "Validation loss decreased (0.036432 --> 0.035473).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1125436\n",
      "\tspeed: 0.0519s/iter; left time: 498.1597s\n",
      "\titers: 200, epoch: 4 | loss: 0.1218719\n",
      "\tspeed: 0.0166s/iter; left time: 157.0870s\n",
      "\titers: 300, epoch: 4 | loss: 0.1170438\n",
      "\tspeed: 0.0165s/iter; left time: 155.3044s\n",
      "\titers: 400, epoch: 4 | loss: 0.0681171\n",
      "\tspeed: 0.0165s/iter; left time: 153.5092s\n",
      "\titers: 500, epoch: 4 | loss: 0.0770603\n",
      "\tspeed: 0.0165s/iter; left time: 151.5792s\n",
      "Epoch: 4 cost time: 9.74596357345581\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1111331 Vali Loss: 0.0332167 Test Loss: 0.1165192\n",
      "Validation loss decreased (0.035473 --> 0.033217).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0582938\n",
      "\tspeed: 0.0487s/iter; left time: 438.9356s\n",
      "\titers: 200, epoch: 5 | loss: 0.0808735\n",
      "\tspeed: 0.0143s/iter; left time: 127.3641s\n",
      "\titers: 300, epoch: 5 | loss: 0.1255110\n",
      "\tspeed: 0.0143s/iter; left time: 126.1013s\n",
      "\titers: 400, epoch: 5 | loss: 0.0846190\n",
      "\tspeed: 0.0143s/iter; left time: 124.6877s\n",
      "\titers: 500, epoch: 5 | loss: 0.1239723\n",
      "\tspeed: 0.0142s/iter; left time: 122.7187s\n",
      "Epoch: 5 cost time: 8.521896362304688\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0998496 Vali Loss: 0.0335846 Test Loss: 0.1124407\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0829497\n",
      "\tspeed: 0.0476s/iter; left time: 402.4453s\n",
      "\titers: 200, epoch: 6 | loss: 0.1824692\n",
      "\tspeed: 0.0143s/iter; left time: 119.5344s\n",
      "\titers: 300, epoch: 6 | loss: 0.1384225\n",
      "\tspeed: 0.0143s/iter; left time: 118.3309s\n",
      "\titers: 400, epoch: 6 | loss: 0.1213723\n",
      "\tspeed: 0.0143s/iter; left time: 116.5914s\n",
      "\titers: 500, epoch: 6 | loss: 0.1057120\n",
      "\tspeed: 0.0143s/iter; left time: 115.1506s\n",
      "Epoch: 6 cost time: 8.489892482757568\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0947269 Vali Loss: 0.0343582 Test Loss: 0.1162853\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0796793\n",
      "\tspeed: 0.0461s/iter; left time: 363.2980s\n",
      "\titers: 200, epoch: 7 | loss: 0.1019718\n",
      "\tspeed: 0.0143s/iter; left time: 111.2174s\n",
      "\titers: 300, epoch: 7 | loss: 0.1382055\n",
      "\tspeed: 0.0162s/iter; left time: 124.3300s\n",
      "\titers: 400, epoch: 7 | loss: 0.1120844\n",
      "\tspeed: 0.0164s/iter; left time: 124.6451s\n",
      "\titers: 500, epoch: 7 | loss: 0.1253088\n",
      "\tspeed: 0.0165s/iter; left time: 123.2756s\n",
      "Epoch: 7 cost time: 9.208753824234009\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0917960 Vali Loss: 0.0335369 Test Loss: 0.1151851\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11667660623788834, mae:0.2073177695274353\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1167674\n",
      "\tspeed: 0.0279s/iter; left time: 314.7835s\n",
      "\titers: 200, epoch: 1 | loss: 0.2955054\n",
      "\tspeed: 0.0161s/iter; left time: 180.4379s\n",
      "\titers: 300, epoch: 1 | loss: 0.2616664\n",
      "\tspeed: 0.0161s/iter; left time: 178.8650s\n",
      "\titers: 400, epoch: 1 | loss: 0.2577615\n",
      "\tspeed: 0.0162s/iter; left time: 177.7492s\n",
      "\titers: 500, epoch: 1 | loss: 0.2450613\n",
      "\tspeed: 0.0161s/iter; left time: 176.0122s\n",
      "Epoch: 1 cost time: 10.439857006072998\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2198592 Vali Loss: 0.0458057 Test Loss: 0.1364354\n",
      "Validation loss decreased (inf --> 0.045806).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1457091\n",
      "\tspeed: 0.0496s/iter; left time: 532.6805s\n",
      "\titers: 200, epoch: 2 | loss: 0.1517224\n",
      "\tspeed: 0.0165s/iter; left time: 175.6399s\n",
      "\titers: 300, epoch: 2 | loss: 0.1078367\n",
      "\tspeed: 0.0165s/iter; left time: 173.9293s\n",
      "\titers: 400, epoch: 2 | loss: 0.1300821\n",
      "\tspeed: 0.0165s/iter; left time: 172.1770s\n",
      "\titers: 500, epoch: 2 | loss: 0.1867114\n",
      "\tspeed: 0.0165s/iter; left time: 170.4399s\n",
      "Epoch: 2 cost time: 9.697509527206421\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1734601 Vali Loss: 0.0465346 Test Loss: 0.1419535\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1603560\n",
      "\tspeed: 0.0503s/iter; left time: 511.1666s\n",
      "\titers: 200, epoch: 3 | loss: 0.1496567\n",
      "\tspeed: 0.0143s/iter; left time: 143.5708s\n",
      "\titers: 300, epoch: 3 | loss: 0.1131109\n",
      "\tspeed: 0.0142s/iter; left time: 141.8560s\n",
      "\titers: 400, epoch: 3 | loss: 0.1033980\n",
      "\tspeed: 0.0142s/iter; left time: 140.4463s\n",
      "\titers: 500, epoch: 3 | loss: 0.1396991\n",
      "\tspeed: 0.0142s/iter; left time: 139.0420s\n",
      "Epoch: 3 cost time: 8.481893301010132\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1330556 Vali Loss: 0.0399300 Test Loss: 0.1306822\n",
      "Validation loss decreased (0.045806 --> 0.039930).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1255874\n",
      "\tspeed: 0.0453s/iter; left time: 434.4615s\n",
      "\titers: 200, epoch: 4 | loss: 0.0922611\n",
      "\tspeed: 0.0143s/iter; left time: 135.9469s\n",
      "\titers: 300, epoch: 4 | loss: 0.1204252\n",
      "\tspeed: 0.0143s/iter; left time: 134.3561s\n",
      "\titers: 400, epoch: 4 | loss: 0.0807351\n",
      "\tspeed: 0.0143s/iter; left time: 133.1779s\n",
      "\titers: 500, epoch: 4 | loss: 0.0890441\n",
      "\tspeed: 0.0143s/iter; left time: 131.7439s\n",
      "Epoch: 4 cost time: 8.457921504974365\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1093582 Vali Loss: 0.0405465 Test Loss: 0.1377843\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1012835\n",
      "\tspeed: 0.0464s/iter; left time: 418.8518s\n",
      "\titers: 200, epoch: 5 | loss: 0.1233751\n",
      "\tspeed: 0.0143s/iter; left time: 127.3590s\n",
      "\titers: 300, epoch: 5 | loss: 0.0911026\n",
      "\tspeed: 0.0143s/iter; left time: 125.7420s\n",
      "\titers: 400, epoch: 5 | loss: 0.1299900\n",
      "\tspeed: 0.0144s/iter; left time: 125.7435s\n",
      "\titers: 500, epoch: 5 | loss: 0.0800860\n",
      "\tspeed: 0.0164s/iter; left time: 141.4061s\n",
      "Epoch: 5 cost time: 8.777215719223022\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0979579 Vali Loss: 0.0395616 Test Loss: 0.1374321\n",
      "Validation loss decreased (0.039930 --> 0.039562).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1015463\n",
      "\tspeed: 0.0497s/iter; left time: 420.0439s\n",
      "\titers: 200, epoch: 6 | loss: 0.0948733\n",
      "\tspeed: 0.0162s/iter; left time: 134.9666s\n",
      "\titers: 300, epoch: 6 | loss: 0.0730019\n",
      "\tspeed: 0.0161s/iter; left time: 133.1401s\n",
      "\titers: 400, epoch: 6 | loss: 0.1324817\n",
      "\tspeed: 0.0161s/iter; left time: 131.6265s\n",
      "\titers: 500, epoch: 6 | loss: 0.0903305\n",
      "\tspeed: 0.0164s/iter; left time: 131.8234s\n",
      "Epoch: 6 cost time: 9.5691397190094\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0920441 Vali Loss: 0.0392771 Test Loss: 0.1379205\n",
      "Validation loss decreased (0.039562 --> 0.039277).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1402810\n",
      "\tspeed: 0.0509s/iter; left time: 401.2862s\n",
      "\titers: 200, epoch: 7 | loss: 0.0584416\n",
      "\tspeed: 0.0165s/iter; left time: 128.0932s\n",
      "\titers: 300, epoch: 7 | loss: 0.0678023\n",
      "\tspeed: 0.0165s/iter; left time: 126.4261s\n",
      "\titers: 400, epoch: 7 | loss: 0.1021162\n",
      "\tspeed: 0.0165s/iter; left time: 124.7919s\n",
      "\titers: 500, epoch: 7 | loss: 0.0978183\n",
      "\tspeed: 0.0165s/iter; left time: 123.1373s\n",
      "Epoch: 7 cost time: 9.691774606704712\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0904201 Vali Loss: 0.0401387 Test Loss: 0.1376490\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0721863\n",
      "\tspeed: 0.0510s/iter; left time: 373.1123s\n",
      "\titers: 200, epoch: 8 | loss: 0.0762913\n",
      "\tspeed: 0.0165s/iter; left time: 118.8422s\n",
      "\titers: 300, epoch: 8 | loss: 0.0771560\n",
      "\tspeed: 0.0165s/iter; left time: 117.2646s\n",
      "\titers: 400, epoch: 8 | loss: 0.0905376\n",
      "\tspeed: 0.0165s/iter; left time: 115.6008s\n",
      "\titers: 500, epoch: 8 | loss: 0.0657823\n",
      "\tspeed: 0.0165s/iter; left time: 113.7504s\n",
      "Epoch: 8 cost time: 9.672219276428223\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0885404 Vali Loss: 0.0401814 Test Loss: 0.1384605\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1196062\n",
      "\tspeed: 0.0506s/iter; left time: 340.8979s\n",
      "\titers: 200, epoch: 9 | loss: 0.0744827\n",
      "\tspeed: 0.0161s/iter; left time: 107.0134s\n",
      "\titers: 300, epoch: 9 | loss: 0.0610878\n",
      "\tspeed: 0.0161s/iter; left time: 105.4108s\n",
      "\titers: 400, epoch: 9 | loss: 0.0683761\n",
      "\tspeed: 0.0161s/iter; left time: 103.8454s\n",
      "\titers: 500, epoch: 9 | loss: 0.0821749\n",
      "\tspeed: 0.0161s/iter; left time: 102.3545s\n",
      "Epoch: 9 cost time: 9.46431303024292\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0874677 Vali Loss: 0.0394991 Test Loss: 0.1376500\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1380898505449295, mae:0.2346818745136261\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2557442\n",
      "\tspeed: 0.0290s/iter; left time: 327.5302s\n",
      "\titers: 200, epoch: 1 | loss: 0.2155398\n",
      "\tspeed: 0.0169s/iter; left time: 189.4642s\n",
      "\titers: 300, epoch: 1 | loss: 0.2420019\n",
      "\tspeed: 0.0169s/iter; left time: 187.5468s\n",
      "\titers: 400, epoch: 1 | loss: 0.2215629\n",
      "\tspeed: 0.0170s/iter; left time: 186.6285s\n",
      "\titers: 500, epoch: 1 | loss: 0.1729687\n",
      "\tspeed: 0.0169s/iter; left time: 184.5321s\n",
      "Epoch: 1 cost time: 10.907882690429688\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2311400 Vali Loss: 0.0515341 Test Loss: 0.1655550\n",
      "Validation loss decreased (inf --> 0.051534).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2657224\n",
      "\tspeed: 0.0536s/iter; left time: 574.9421s\n",
      "\titers: 200, epoch: 2 | loss: 0.2257499\n",
      "\tspeed: 0.0169s/iter; left time: 179.6952s\n",
      "\titers: 300, epoch: 2 | loss: 0.3500930\n",
      "\tspeed: 0.0169s/iter; left time: 177.8023s\n",
      "\titers: 400, epoch: 2 | loss: 0.2537546\n",
      "\tspeed: 0.0169s/iter; left time: 176.2156s\n",
      "\titers: 500, epoch: 2 | loss: 0.2537118\n",
      "\tspeed: 0.0169s/iter; left time: 174.3403s\n",
      "Epoch: 2 cost time: 9.977526426315308\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2654837 Vali Loss: 0.0593745 Test Loss: 0.1681601\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2091331\n",
      "\tspeed: 0.0506s/iter; left time: 514.6213s\n",
      "\titers: 200, epoch: 3 | loss: 0.2144510\n",
      "\tspeed: 0.0169s/iter; left time: 170.0023s\n",
      "\titers: 300, epoch: 3 | loss: 0.3650085\n",
      "\tspeed: 0.0169s/iter; left time: 168.2903s\n",
      "\titers: 400, epoch: 3 | loss: 0.2017132\n",
      "\tspeed: 0.0169s/iter; left time: 166.4618s\n",
      "\titers: 500, epoch: 3 | loss: 0.1605293\n",
      "\tspeed: 0.0169s/iter; left time: 164.7076s\n",
      "Epoch: 3 cost time: 9.947157621383667\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2349367 Vali Loss: 0.0586023 Test Loss: 0.1637388\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1823092\n",
      "\tspeed: 0.0510s/iter; left time: 488.7811s\n",
      "\titers: 200, epoch: 4 | loss: 0.1900171\n",
      "\tspeed: 0.0169s/iter; left time: 160.7976s\n",
      "\titers: 300, epoch: 4 | loss: 0.2166175\n",
      "\tspeed: 0.0169s/iter; left time: 159.1708s\n",
      "\titers: 400, epoch: 4 | loss: 0.2219737\n",
      "\tspeed: 0.0169s/iter; left time: 157.2187s\n",
      "\titers: 500, epoch: 4 | loss: 0.2073140\n",
      "\tspeed: 0.0169s/iter; left time: 155.5867s\n",
      "Epoch: 4 cost time: 10.01744294166565\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2113202 Vali Loss: 0.0572437 Test Loss: 0.1619374\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16583923995494843, mae:0.2654508650302887\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1760582\n",
      "\tspeed: 0.0282s/iter; left time: 319.1292s\n",
      "\titers: 200, epoch: 1 | loss: 0.1070482\n",
      "\tspeed: 0.0164s/iter; left time: 183.5074s\n",
      "\titers: 300, epoch: 1 | loss: 0.2112269\n",
      "\tspeed: 0.0164s/iter; left time: 181.6434s\n",
      "\titers: 400, epoch: 1 | loss: 0.1858908\n",
      "\tspeed: 0.0164s/iter; left time: 180.0344s\n",
      "\titers: 500, epoch: 1 | loss: 0.1099274\n",
      "\tspeed: 0.0164s/iter; left time: 178.4558s\n",
      "Epoch: 1 cost time: 10.563828468322754\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2009359 Vali Loss: 0.0405242 Test Loss: 0.1315651\n",
      "Validation loss decreased (inf --> 0.040524).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2052581\n",
      "\tspeed: 0.0514s/iter; left time: 551.9993s\n",
      "\titers: 200, epoch: 2 | loss: 0.1780758\n",
      "\tspeed: 0.0163s/iter; left time: 173.2695s\n",
      "\titers: 300, epoch: 2 | loss: 0.3228746\n",
      "\tspeed: 0.0163s/iter; left time: 171.4590s\n",
      "\titers: 400, epoch: 2 | loss: 0.3696241\n",
      "\tspeed: 0.0163s/iter; left time: 169.9328s\n",
      "\titers: 500, epoch: 2 | loss: 0.2204932\n",
      "\tspeed: 0.0163s/iter; left time: 168.5390s\n",
      "Epoch: 2 cost time: 9.633000373840332\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2766150 Vali Loss: 0.0629898 Test Loss: 0.1795397\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2809814\n",
      "\tspeed: 0.0507s/iter; left time: 514.6575s\n",
      "\titers: 200, epoch: 3 | loss: 0.2335446\n",
      "\tspeed: 0.0164s/iter; left time: 164.9578s\n",
      "\titers: 300, epoch: 3 | loss: 0.1699868\n",
      "\tspeed: 0.0164s/iter; left time: 163.0030s\n",
      "\titers: 400, epoch: 3 | loss: 0.2662534\n",
      "\tspeed: 0.0163s/iter; left time: 160.9774s\n",
      "\titers: 500, epoch: 3 | loss: 0.1413514\n",
      "\tspeed: 0.0163s/iter; left time: 159.3634s\n",
      "Epoch: 3 cost time: 9.607754468917847\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2469185 Vali Loss: 0.0556733 Test Loss: 0.1584507\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1767578\n",
      "\tspeed: 0.0494s/iter; left time: 474.1015s\n",
      "\titers: 200, epoch: 4 | loss: 0.4568399\n",
      "\tspeed: 0.0163s/iter; left time: 154.8789s\n",
      "\titers: 300, epoch: 4 | loss: 0.2135920\n",
      "\tspeed: 0.0163s/iter; left time: 153.0892s\n",
      "\titers: 400, epoch: 4 | loss: 0.1967251\n",
      "\tspeed: 0.0163s/iter; left time: 151.5247s\n",
      "\titers: 500, epoch: 4 | loss: 0.3326260\n",
      "\tspeed: 0.0163s/iter; left time: 149.8523s\n",
      "Epoch: 4 cost time: 9.562763214111328\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2256985 Vali Loss: 0.0552150 Test Loss: 0.1522728\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13178963959217072, mae:0.22601796686649323\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2132356\n",
      "\tspeed: 0.0291s/iter; left time: 328.5212s\n",
      "\titers: 200, epoch: 1 | loss: 0.2197873\n",
      "\tspeed: 0.0170s/iter; left time: 190.0380s\n",
      "\titers: 300, epoch: 1 | loss: 0.1616414\n",
      "\tspeed: 0.0169s/iter; left time: 187.9722s\n",
      "\titers: 400, epoch: 1 | loss: 0.2144824\n",
      "\tspeed: 0.0170s/iter; left time: 186.9730s\n",
      "\titers: 500, epoch: 1 | loss: 0.3870519\n",
      "\tspeed: 0.0170s/iter; left time: 185.1918s\n",
      "Epoch: 1 cost time: 10.933105707168579\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2323277 Vali Loss: 0.0471214 Test Loss: 0.1549581\n",
      "Validation loss decreased (inf --> 0.047121).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2906809\n",
      "\tspeed: 0.0494s/iter; left time: 530.0561s\n",
      "\titers: 200, epoch: 2 | loss: 0.1560972\n",
      "\tspeed: 0.0154s/iter; left time: 163.5415s\n",
      "\titers: 300, epoch: 2 | loss: 0.3482560\n",
      "\tspeed: 0.0154s/iter; left time: 161.8756s\n",
      "\titers: 400, epoch: 2 | loss: 0.2620814\n",
      "\tspeed: 0.0154s/iter; left time: 160.2618s\n",
      "\titers: 500, epoch: 2 | loss: 0.3732356\n",
      "\tspeed: 0.0154s/iter; left time: 158.7466s\n",
      "Epoch: 2 cost time: 9.073968172073364\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2595969 Vali Loss: 0.0632168 Test Loss: 0.1850292\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2194456\n",
      "\tspeed: 0.0489s/iter; left time: 496.8409s\n",
      "\titers: 200, epoch: 3 | loss: 0.1570438\n",
      "\tspeed: 0.0154s/iter; left time: 154.8359s\n",
      "\titers: 300, epoch: 3 | loss: 0.4517294\n",
      "\tspeed: 0.0154s/iter; left time: 152.9200s\n",
      "\titers: 400, epoch: 3 | loss: 0.2757720\n",
      "\tspeed: 0.0154s/iter; left time: 151.3872s\n",
      "\titers: 500, epoch: 3 | loss: 0.2946413\n",
      "\tspeed: 0.0153s/iter; left time: 149.8064s\n",
      "Epoch: 3 cost time: 9.043947219848633\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2807978 Vali Loss: 0.0621568 Test Loss: 0.1712141\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2104102\n",
      "\tspeed: 0.0471s/iter; left time: 451.4336s\n",
      "\titers: 200, epoch: 4 | loss: 0.3161986\n",
      "\tspeed: 0.0154s/iter; left time: 146.3697s\n",
      "\titers: 300, epoch: 4 | loss: 0.3023233\n",
      "\tspeed: 0.0154s/iter; left time: 144.7628s\n",
      "\titers: 400, epoch: 4 | loss: 0.1917839\n",
      "\tspeed: 0.0154s/iter; left time: 143.2259s\n",
      "\titers: 500, epoch: 4 | loss: 0.2984703\n",
      "\tspeed: 0.0155s/iter; left time: 142.0339s\n",
      "Epoch: 4 cost time: 9.067560911178589\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2446291 Vali Loss: 0.0604046 Test Loss: 0.1626612\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15516559779644012, mae:0.24410508573055267\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1601215\n",
      "\tspeed: 0.0326s/iter; left time: 368.7966s\n",
      "\titers: 200, epoch: 1 | loss: 0.2679668\n",
      "\tspeed: 0.0207s/iter; left time: 232.2152s\n",
      "\titers: 300, epoch: 1 | loss: 0.3529364\n",
      "\tspeed: 0.0208s/iter; left time: 231.3258s\n",
      "\titers: 400, epoch: 1 | loss: 0.1937822\n",
      "\tspeed: 0.0208s/iter; left time: 228.7796s\n",
      "\titers: 500, epoch: 1 | loss: 0.1617287\n",
      "\tspeed: 0.0189s/iter; left time: 206.2345s\n",
      "Epoch: 1 cost time: 12.748009204864502\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2158539 Vali Loss: 0.0480296 Test Loss: 0.1407557\n",
      "Validation loss decreased (inf --> 0.048030).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3049391\n",
      "\tspeed: 0.0565s/iter; left time: 606.1212s\n",
      "\titers: 200, epoch: 2 | loss: 0.1487861\n",
      "\tspeed: 0.0186s/iter; left time: 197.2501s\n",
      "\titers: 300, epoch: 2 | loss: 0.1564662\n",
      "\tspeed: 0.0185s/iter; left time: 194.7709s\n",
      "\titers: 400, epoch: 2 | loss: 0.1327024\n",
      "\tspeed: 0.0185s/iter; left time: 192.9607s\n",
      "\titers: 500, epoch: 2 | loss: 0.1627572\n",
      "\tspeed: 0.0185s/iter; left time: 190.9105s\n",
      "Epoch: 2 cost time: 10.878407716751099\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1726822 Vali Loss: 0.0468829 Test Loss: 0.1471639\n",
      "Validation loss decreased (0.048030 --> 0.046883).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1179763\n",
      "\tspeed: 0.0560s/iter; left time: 569.0533s\n",
      "\titers: 200, epoch: 3 | loss: 0.1013117\n",
      "\tspeed: 0.0185s/iter; left time: 186.2095s\n",
      "\titers: 300, epoch: 3 | loss: 0.1304374\n",
      "\tspeed: 0.0185s/iter; left time: 184.2240s\n",
      "\titers: 400, epoch: 3 | loss: 0.1004486\n",
      "\tspeed: 0.0185s/iter; left time: 182.4233s\n",
      "\titers: 500, epoch: 3 | loss: 0.0968303\n",
      "\tspeed: 0.0185s/iter; left time: 180.4950s\n",
      "Epoch: 3 cost time: 10.858896732330322\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1370727 Vali Loss: 0.0421463 Test Loss: 0.1415687\n",
      "Validation loss decreased (0.046883 --> 0.042146).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1208971\n",
      "\tspeed: 0.0565s/iter; left time: 541.4935s\n",
      "\titers: 200, epoch: 4 | loss: 0.1769108\n",
      "\tspeed: 0.0186s/iter; left time: 176.2411s\n",
      "\titers: 300, epoch: 4 | loss: 0.1254952\n",
      "\tspeed: 0.0186s/iter; left time: 174.3231s\n",
      "\titers: 400, epoch: 4 | loss: 0.0638355\n",
      "\tspeed: 0.0186s/iter; left time: 172.4253s\n",
      "\titers: 500, epoch: 4 | loss: 0.1190091\n",
      "\tspeed: 0.0186s/iter; left time: 170.6103s\n",
      "Epoch: 4 cost time: 10.921736717224121\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1120965 Vali Loss: 0.0393526 Test Loss: 0.1339446\n",
      "Validation loss decreased (0.042146 --> 0.039353).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1096135\n",
      "\tspeed: 0.0580s/iter; left time: 523.5647s\n",
      "\titers: 200, epoch: 5 | loss: 0.1083995\n",
      "\tspeed: 0.0186s/iter; left time: 165.8449s\n",
      "\titers: 300, epoch: 5 | loss: 0.1029091\n",
      "\tspeed: 0.0186s/iter; left time: 163.9718s\n",
      "\titers: 400, epoch: 5 | loss: 0.0780801\n",
      "\tspeed: 0.0186s/iter; left time: 162.0541s\n",
      "\titers: 500, epoch: 5 | loss: 0.0935497\n",
      "\tspeed: 0.0186s/iter; left time: 160.1094s\n",
      "Epoch: 5 cost time: 10.891927242279053\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0984703 Vali Loss: 0.0383031 Test Loss: 0.1369257\n",
      "Validation loss decreased (0.039353 --> 0.038303).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0612615\n",
      "\tspeed: 0.0592s/iter; left time: 500.5425s\n",
      "\titers: 200, epoch: 6 | loss: 0.0969343\n",
      "\tspeed: 0.0186s/iter; left time: 155.4497s\n",
      "\titers: 300, epoch: 6 | loss: 0.0809905\n",
      "\tspeed: 0.0186s/iter; left time: 153.3469s\n",
      "\titers: 400, epoch: 6 | loss: 0.1350560\n",
      "\tspeed: 0.0186s/iter; left time: 151.5881s\n",
      "\titers: 500, epoch: 6 | loss: 0.0834133\n",
      "\tspeed: 0.0186s/iter; left time: 149.6903s\n",
      "Epoch: 6 cost time: 10.91262936592102\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0927784 Vali Loss: 0.0386098 Test Loss: 0.1366741\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0775667\n",
      "\tspeed: 0.0574s/iter; left time: 452.0619s\n",
      "\titers: 200, epoch: 7 | loss: 0.0709670\n",
      "\tspeed: 0.0186s/iter; left time: 144.3497s\n",
      "\titers: 300, epoch: 7 | loss: 0.0941858\n",
      "\tspeed: 0.0186s/iter; left time: 142.6106s\n",
      "\titers: 400, epoch: 7 | loss: 0.0993690\n",
      "\tspeed: 0.0185s/iter; left time: 140.6024s\n",
      "\titers: 500, epoch: 7 | loss: 0.1505994\n",
      "\tspeed: 0.0186s/iter; left time: 138.8158s\n",
      "Epoch: 7 cost time: 10.853829383850098\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0896315 Vali Loss: 0.0384480 Test Loss: 0.1356376\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0899162\n",
      "\tspeed: 0.0590s/iter; left time: 431.0294s\n",
      "\titers: 200, epoch: 8 | loss: 0.0799357\n",
      "\tspeed: 0.0186s/iter; left time: 134.2789s\n",
      "\titers: 300, epoch: 8 | loss: 0.0750812\n",
      "\tspeed: 0.0186s/iter; left time: 132.3336s\n",
      "\titers: 400, epoch: 8 | loss: 0.1160712\n",
      "\tspeed: 0.0186s/iter; left time: 130.2671s\n",
      "\titers: 500, epoch: 8 | loss: 0.0674134\n",
      "\tspeed: 0.0186s/iter; left time: 128.5223s\n",
      "Epoch: 8 cost time: 10.906166315078735\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0882210 Vali Loss: 0.0388550 Test Loss: 0.1363302\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13711073994636536, mae:0.23102308809757233\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3742481\n",
      "\tspeed: 0.0300s/iter; left time: 338.4662s\n",
      "\titers: 200, epoch: 1 | loss: 0.1943786\n",
      "\tspeed: 0.0183s/iter; left time: 204.9473s\n",
      "\titers: 300, epoch: 1 | loss: 0.1684622\n",
      "\tspeed: 0.0183s/iter; left time: 203.1267s\n",
      "\titers: 400, epoch: 1 | loss: 0.1639359\n",
      "\tspeed: 0.0183s/iter; left time: 201.5907s\n",
      "\titers: 500, epoch: 1 | loss: 0.1772284\n",
      "\tspeed: 0.0183s/iter; left time: 199.7066s\n",
      "Epoch: 1 cost time: 11.654429912567139\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1876974 Vali Loss: 0.0407946 Test Loss: 0.1276755\n",
      "Validation loss decreased (inf --> 0.040795).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1380969\n",
      "\tspeed: 0.0576s/iter; left time: 618.3683s\n",
      "\titers: 200, epoch: 2 | loss: 0.2264183\n",
      "\tspeed: 0.0183s/iter; left time: 194.0638s\n",
      "\titers: 300, epoch: 2 | loss: 0.2026057\n",
      "\tspeed: 0.0182s/iter; left time: 191.3649s\n",
      "\titers: 400, epoch: 2 | loss: 0.2185692\n",
      "\tspeed: 0.0182s/iter; left time: 189.4262s\n",
      "\titers: 500, epoch: 2 | loss: 0.1197596\n",
      "\tspeed: 0.0181s/iter; left time: 187.3410s\n",
      "Epoch: 2 cost time: 10.869986772537231\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1580204 Vali Loss: 0.0412605 Test Loss: 0.1215693\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0871293\n",
      "\tspeed: 0.0559s/iter; left time: 568.1813s\n",
      "\titers: 200, epoch: 3 | loss: 0.1306126\n",
      "\tspeed: 0.0182s/iter; left time: 183.4527s\n",
      "\titers: 300, epoch: 3 | loss: 0.1405563\n",
      "\tspeed: 0.0182s/iter; left time: 181.0525s\n",
      "\titers: 400, epoch: 3 | loss: 0.0955729\n",
      "\tspeed: 0.0182s/iter; left time: 179.4952s\n",
      "\titers: 500, epoch: 3 | loss: 0.1021546\n",
      "\tspeed: 0.0183s/iter; left time: 178.2491s\n",
      "Epoch: 3 cost time: 10.645654201507568\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1316289 Vali Loss: 0.0364323 Test Loss: 0.1165626\n",
      "Validation loss decreased (0.040795 --> 0.036432).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1236294\n",
      "\tspeed: 0.0592s/iter; left time: 568.1350s\n",
      "\titers: 200, epoch: 4 | loss: 0.0730140\n",
      "\tspeed: 0.0183s/iter; left time: 173.3349s\n",
      "\titers: 300, epoch: 4 | loss: 0.1182805\n",
      "\tspeed: 0.0183s/iter; left time: 171.8951s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956322\n",
      "\tspeed: 0.0183s/iter; left time: 169.9181s\n",
      "\titers: 500, epoch: 4 | loss: 0.0843183\n",
      "\tspeed: 0.0183s/iter; left time: 168.0088s\n",
      "Epoch: 4 cost time: 10.923299789428711\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1119665 Vali Loss: 0.0349799 Test Loss: 0.1142847\n",
      "Validation loss decreased (0.036432 --> 0.034980).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0831127\n",
      "\tspeed: 0.0551s/iter; left time: 497.0798s\n",
      "\titers: 200, epoch: 5 | loss: 0.0967013\n",
      "\tspeed: 0.0182s/iter; left time: 162.0117s\n",
      "\titers: 300, epoch: 5 | loss: 0.0894479\n",
      "\tspeed: 0.0182s/iter; left time: 160.1126s\n",
      "\titers: 400, epoch: 5 | loss: 0.0886109\n",
      "\tspeed: 0.0182s/iter; left time: 158.3169s\n",
      "\titers: 500, epoch: 5 | loss: 0.0990796\n",
      "\tspeed: 0.0182s/iter; left time: 156.5169s\n",
      "Epoch: 5 cost time: 10.637493133544922\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0984995 Vali Loss: 0.0343408 Test Loss: 0.1158925\n",
      "Validation loss decreased (0.034980 --> 0.034341).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1077913\n",
      "\tspeed: 0.0547s/iter; left time: 462.3854s\n",
      "\titers: 200, epoch: 6 | loss: 0.0714055\n",
      "\tspeed: 0.0182s/iter; left time: 152.3727s\n",
      "\titers: 300, epoch: 6 | loss: 0.1109908\n",
      "\tspeed: 0.0182s/iter; left time: 150.2115s\n",
      "\titers: 400, epoch: 6 | loss: 0.0810228\n",
      "\tspeed: 0.0182s/iter; left time: 148.1323s\n",
      "\titers: 500, epoch: 6 | loss: 0.0629373\n",
      "\tspeed: 0.0182s/iter; left time: 146.6088s\n",
      "Epoch: 6 cost time: 10.667121410369873\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0925016 Vali Loss: 0.0345410 Test Loss: 0.1169708\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0780927\n",
      "\tspeed: 0.0578s/iter; left time: 455.4917s\n",
      "\titers: 200, epoch: 7 | loss: 0.1124896\n",
      "\tspeed: 0.0201s/iter; left time: 156.3754s\n",
      "\titers: 300, epoch: 7 | loss: 0.0596608\n",
      "\tspeed: 0.0201s/iter; left time: 154.3685s\n",
      "\titers: 400, epoch: 7 | loss: 0.1044410\n",
      "\tspeed: 0.0203s/iter; left time: 154.0588s\n",
      "\titers: 500, epoch: 7 | loss: 0.0788359\n",
      "\tspeed: 0.0206s/iter; left time: 154.1953s\n",
      "Epoch: 7 cost time: 11.935520887374878\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0886925 Vali Loss: 0.0339803 Test Loss: 0.1164838\n",
      "Validation loss decreased (0.034341 --> 0.033980).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0700560\n",
      "\tspeed: 0.0586s/iter; left time: 428.6341s\n",
      "\titers: 200, epoch: 8 | loss: 0.0891792\n",
      "\tspeed: 0.0183s/iter; left time: 132.2131s\n",
      "\titers: 300, epoch: 8 | loss: 0.0794379\n",
      "\tspeed: 0.0183s/iter; left time: 129.8353s\n",
      "\titers: 400, epoch: 8 | loss: 0.0846472\n",
      "\tspeed: 0.0183s/iter; left time: 128.3187s\n",
      "\titers: 500, epoch: 8 | loss: 0.1003609\n",
      "\tspeed: 0.0183s/iter; left time: 126.1492s\n",
      "Epoch: 8 cost time: 10.77053689956665\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0865921 Vali Loss: 0.0339551 Test Loss: 0.1166554\n",
      "Validation loss decreased (0.033980 --> 0.033955).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0524424\n",
      "\tspeed: 0.0560s/iter; left time: 377.4951s\n",
      "\titers: 200, epoch: 9 | loss: 0.0777009\n",
      "\tspeed: 0.0183s/iter; left time: 121.4497s\n",
      "\titers: 300, epoch: 9 | loss: 0.0828364\n",
      "\tspeed: 0.0182s/iter; left time: 119.2764s\n",
      "\titers: 400, epoch: 9 | loss: 0.0816082\n",
      "\tspeed: 0.0183s/iter; left time: 117.5559s\n",
      "\titers: 500, epoch: 9 | loss: 0.0778431\n",
      "\tspeed: 0.0182s/iter; left time: 115.2711s\n",
      "Epoch: 9 cost time: 10.75480055809021\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0863239 Vali Loss: 0.0340197 Test Loss: 0.1164078\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.0703812\n",
      "\tspeed: 0.0561s/iter; left time: 346.1502s\n",
      "\titers: 200, epoch: 10 | loss: 0.0600727\n",
      "\tspeed: 0.0183s/iter; left time: 111.0397s\n",
      "\titers: 300, epoch: 10 | loss: 0.1222803\n",
      "\tspeed: 0.0183s/iter; left time: 109.0325s\n",
      "\titers: 400, epoch: 10 | loss: 0.0940767\n",
      "\tspeed: 0.0183s/iter; left time: 107.1917s\n",
      "\titers: 500, epoch: 10 | loss: 0.0836515\n",
      "\tspeed: 0.0183s/iter; left time: 105.3606s\n",
      "Epoch: 10 cost time: 10.703775882720947\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0867250 Vali Loss: 0.0338425 Test Loss: 0.1164990\n",
      "Validation loss decreased (0.033955 --> 0.033843).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0886704\n",
      "\tspeed: 0.0548s/iter; left time: 307.0764s\n",
      "\titers: 200, epoch: 11 | loss: 0.0949598\n",
      "\tspeed: 0.0182s/iter; left time: 100.2290s\n",
      "\titers: 300, epoch: 11 | loss: 0.0609087\n",
      "\tspeed: 0.0182s/iter; left time: 98.2393s\n",
      "\titers: 400, epoch: 11 | loss: 0.1235714\n",
      "\tspeed: 0.0181s/iter; left time: 95.9603s\n",
      "\titers: 500, epoch: 11 | loss: 0.1064821\n",
      "\tspeed: 0.0181s/iter; left time: 94.0461s\n",
      "Epoch: 11 cost time: 10.615653038024902\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0859769 Vali Loss: 0.0337616 Test Loss: 0.1164080\n",
      "Validation loss decreased (0.033843 --> 0.033762).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1195657\n",
      "\tspeed: 0.0554s/iter; left time: 278.8327s\n",
      "\titers: 200, epoch: 12 | loss: 0.0771970\n",
      "\tspeed: 0.0183s/iter; left time: 90.3983s\n",
      "\titers: 300, epoch: 12 | loss: 0.0692294\n",
      "\tspeed: 0.0183s/iter; left time: 88.4228s\n",
      "\titers: 400, epoch: 12 | loss: 0.0837279\n",
      "\tspeed: 0.0183s/iter; left time: 86.6285s\n",
      "\titers: 500, epoch: 12 | loss: 0.0855555\n",
      "\tspeed: 0.0183s/iter; left time: 84.7565s\n",
      "Epoch: 12 cost time: 10.755096435546875\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0858482 Vali Loss: 0.0338301 Test Loss: 0.1163966\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.0759818\n",
      "\tspeed: 0.0555s/iter; left time: 247.7515s\n",
      "\titers: 200, epoch: 13 | loss: 0.0878199\n",
      "\tspeed: 0.0182s/iter; left time: 79.4178s\n",
      "\titers: 300, epoch: 13 | loss: 0.0670783\n",
      "\tspeed: 0.0182s/iter; left time: 77.5186s\n",
      "\titers: 400, epoch: 13 | loss: 0.1085088\n",
      "\tspeed: 0.0182s/iter; left time: 75.9087s\n",
      "\titers: 500, epoch: 13 | loss: 0.1070844\n",
      "\tspeed: 0.0183s/iter; left time: 74.3236s\n",
      "Epoch: 13 cost time: 10.710075855255127\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0867677 Vali Loss: 0.0338175 Test Loss: 0.1164519\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.0902542\n",
      "\tspeed: 0.0566s/iter; left time: 220.4247s\n",
      "\titers: 200, epoch: 14 | loss: 0.0839087\n",
      "\tspeed: 0.0182s/iter; left time: 68.8691s\n",
      "\titers: 300, epoch: 14 | loss: 0.0895629\n",
      "\tspeed: 0.0182s/iter; left time: 67.1696s\n",
      "\titers: 400, epoch: 14 | loss: 0.0804068\n",
      "\tspeed: 0.0182s/iter; left time: 65.2275s\n",
      "\titers: 500, epoch: 14 | loss: 0.0657523\n",
      "\tspeed: 0.0182s/iter; left time: 63.5315s\n",
      "Epoch: 14 cost time: 10.66966700553894\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.0862531 Vali Loss: 0.0338502 Test Loss: 0.1164589\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11655744165182114, mae:0.20830540359020233\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2465455\n",
      "\tspeed: 0.0326s/iter; left time: 368.4679s\n",
      "\titers: 200, epoch: 1 | loss: 0.1742174\n",
      "\tspeed: 0.0205s/iter; left time: 229.2769s\n",
      "\titers: 300, epoch: 1 | loss: 0.2763492\n",
      "\tspeed: 0.0205s/iter; left time: 227.9470s\n",
      "\titers: 400, epoch: 1 | loss: 0.2178470\n",
      "\tspeed: 0.0205s/iter; left time: 225.4617s\n",
      "\titers: 500, epoch: 1 | loss: 0.2626901\n",
      "\tspeed: 0.0205s/iter; left time: 223.0637s\n",
      "Epoch: 1 cost time: 12.944639682769775\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2196197 Vali Loss: 0.0462815 Test Loss: 0.1330467\n",
      "Validation loss decreased (inf --> 0.046282).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2293462\n",
      "\tspeed: 0.0575s/iter; left time: 616.5602s\n",
      "\titers: 200, epoch: 2 | loss: 0.1865129\n",
      "\tspeed: 0.0183s/iter; left time: 194.1273s\n",
      "\titers: 300, epoch: 2 | loss: 0.3665048\n",
      "\tspeed: 0.0183s/iter; left time: 192.3267s\n",
      "\titers: 400, epoch: 2 | loss: 0.1814012\n",
      "\tspeed: 0.0182s/iter; left time: 190.2675s\n",
      "\titers: 500, epoch: 2 | loss: 0.1074840\n",
      "\tspeed: 0.0183s/iter; left time: 188.5660s\n",
      "Epoch: 2 cost time: 10.730437994003296\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1843974 Vali Loss: 0.0468489 Test Loss: 0.1434171\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1040126\n",
      "\tspeed: 0.0553s/iter; left time: 561.8836s\n",
      "\titers: 200, epoch: 3 | loss: 0.1511192\n",
      "\tspeed: 0.0183s/iter; left time: 184.1739s\n",
      "\titers: 300, epoch: 3 | loss: 0.1757332\n",
      "\tspeed: 0.0183s/iter; left time: 182.1727s\n",
      "\titers: 400, epoch: 3 | loss: 0.1398604\n",
      "\tspeed: 0.0183s/iter; left time: 180.2853s\n",
      "\titers: 500, epoch: 3 | loss: 0.1326717\n",
      "\tspeed: 0.0183s/iter; left time: 178.4930s\n",
      "Epoch: 3 cost time: 10.707965850830078\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1427732 Vali Loss: 0.0422266 Test Loss: 0.1323110\n",
      "Validation loss decreased (0.046282 --> 0.042227).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0990643\n",
      "\tspeed: 0.0594s/iter; left time: 569.5374s\n",
      "\titers: 200, epoch: 4 | loss: 0.1284356\n",
      "\tspeed: 0.0210s/iter; left time: 199.7499s\n",
      "\titers: 300, epoch: 4 | loss: 0.1189185\n",
      "\tspeed: 0.0210s/iter; left time: 197.5868s\n",
      "\titers: 400, epoch: 4 | loss: 0.1148597\n",
      "\tspeed: 0.0210s/iter; left time: 195.4381s\n",
      "\titers: 500, epoch: 4 | loss: 0.0875525\n",
      "\tspeed: 0.0210s/iter; left time: 193.4475s\n",
      "Epoch: 4 cost time: 12.279780387878418\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1152169 Vali Loss: 0.0403422 Test Loss: 0.1315215\n",
      "Validation loss decreased (0.042227 --> 0.040342).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0930144\n",
      "\tspeed: 0.0605s/iter; left time: 545.3261s\n",
      "\titers: 200, epoch: 5 | loss: 0.1091283\n",
      "\tspeed: 0.0211s/iter; left time: 187.8322s\n",
      "\titers: 300, epoch: 5 | loss: 0.0675936\n",
      "\tspeed: 0.0205s/iter; left time: 181.2205s\n",
      "\titers: 400, epoch: 5 | loss: 0.0938081\n",
      "\tspeed: 0.0205s/iter; left time: 178.8645s\n",
      "\titers: 500, epoch: 5 | loss: 0.0558497\n",
      "\tspeed: 0.0205s/iter; left time: 177.0765s\n",
      "Epoch: 5 cost time: 12.120334386825562\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1007919 Vali Loss: 0.0410147 Test Loss: 0.1349144\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1119724\n",
      "\tspeed: 0.0571s/iter; left time: 482.4909s\n",
      "\titers: 200, epoch: 6 | loss: 0.0632188\n",
      "\tspeed: 0.0184s/iter; left time: 153.5081s\n",
      "\titers: 300, epoch: 6 | loss: 0.1177115\n",
      "\tspeed: 0.0183s/iter; left time: 151.3013s\n",
      "\titers: 400, epoch: 6 | loss: 0.0846691\n",
      "\tspeed: 0.0184s/iter; left time: 149.5839s\n",
      "\titers: 500, epoch: 6 | loss: 0.0950823\n",
      "\tspeed: 0.0183s/iter; left time: 147.6371s\n",
      "Epoch: 6 cost time: 10.737748384475708\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0944268 Vali Loss: 0.0401300 Test Loss: 0.1352709\n",
      "Validation loss decreased (0.040342 --> 0.040130).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0799287\n",
      "\tspeed: 0.0553s/iter; left time: 436.0057s\n",
      "\titers: 200, epoch: 7 | loss: 0.0747626\n",
      "\tspeed: 0.0182s/iter; left time: 141.9756s\n",
      "\titers: 300, epoch: 7 | loss: 0.1034898\n",
      "\tspeed: 0.0183s/iter; left time: 140.2060s\n",
      "\titers: 400, epoch: 7 | loss: 0.0954349\n",
      "\tspeed: 0.0182s/iter; left time: 138.0006s\n",
      "\titers: 500, epoch: 7 | loss: 0.0578673\n",
      "\tspeed: 0.0182s/iter; left time: 136.4238s\n",
      "Epoch: 7 cost time: 10.665987730026245\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0923948 Vali Loss: 0.0397917 Test Loss: 0.1350439\n",
      "Validation loss decreased (0.040130 --> 0.039792).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0772300\n",
      "\tspeed: 0.0549s/iter; left time: 401.6541s\n",
      "\titers: 200, epoch: 8 | loss: 0.0666272\n",
      "\tspeed: 0.0182s/iter; left time: 131.5598s\n",
      "\titers: 300, epoch: 8 | loss: 0.0906120\n",
      "\tspeed: 0.0182s/iter; left time: 129.6600s\n",
      "\titers: 400, epoch: 8 | loss: 0.1087127\n",
      "\tspeed: 0.0182s/iter; left time: 127.7714s\n",
      "\titers: 500, epoch: 8 | loss: 0.0791122\n",
      "\tspeed: 0.0182s/iter; left time: 125.8702s\n",
      "Epoch: 8 cost time: 10.6782808303833\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0900067 Vali Loss: 0.0397980 Test Loss: 0.1351777\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0902390\n",
      "\tspeed: 0.0553s/iter; left time: 372.9460s\n",
      "\titers: 200, epoch: 9 | loss: 0.1244761\n",
      "\tspeed: 0.0182s/iter; left time: 121.0643s\n",
      "\titers: 300, epoch: 9 | loss: 0.1000864\n",
      "\tspeed: 0.0182s/iter; left time: 119.2494s\n",
      "\titers: 400, epoch: 9 | loss: 0.1546550\n",
      "\tspeed: 0.0182s/iter; left time: 117.4516s\n",
      "\titers: 500, epoch: 9 | loss: 0.0599694\n",
      "\tspeed: 0.0197s/iter; left time: 124.6313s\n",
      "Epoch: 9 cost time: 10.979910135269165\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0894911 Vali Loss: 0.0397211 Test Loss: 0.1346833\n",
      "Validation loss decreased (0.039792 --> 0.039721).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.0847079\n",
      "\tspeed: 0.0574s/iter; left time: 353.9544s\n",
      "\titers: 200, epoch: 10 | loss: 0.0722764\n",
      "\tspeed: 0.0182s/iter; left time: 110.6480s\n",
      "\titers: 300, epoch: 10 | loss: 0.0958148\n",
      "\tspeed: 0.0182s/iter; left time: 108.8058s\n",
      "\titers: 400, epoch: 10 | loss: 0.0959902\n",
      "\tspeed: 0.0182s/iter; left time: 107.0102s\n",
      "\titers: 500, epoch: 10 | loss: 0.0901856\n",
      "\tspeed: 0.0182s/iter; left time: 105.1598s\n",
      "Epoch: 10 cost time: 10.683642148971558\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0890617 Vali Loss: 0.0394520 Test Loss: 0.1355194\n",
      "Validation loss decreased (0.039721 --> 0.039452).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0883052\n",
      "\tspeed: 0.0557s/iter; left time: 312.0704s\n",
      "\titers: 200, epoch: 11 | loss: 0.0981652\n",
      "\tspeed: 0.0182s/iter; left time: 100.1286s\n",
      "\titers: 300, epoch: 11 | loss: 0.0668945\n",
      "\tspeed: 0.0182s/iter; left time: 98.3439s\n",
      "\titers: 400, epoch: 11 | loss: 0.1120059\n",
      "\tspeed: 0.0182s/iter; left time: 96.5336s\n",
      "\titers: 500, epoch: 11 | loss: 0.0745076\n",
      "\tspeed: 0.0182s/iter; left time: 94.7069s\n",
      "Epoch: 11 cost time: 10.654800176620483\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0892274 Vali Loss: 0.0395513 Test Loss: 0.1355649\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.0855307\n",
      "\tspeed: 0.0569s/iter; left time: 286.3790s\n",
      "\titers: 200, epoch: 12 | loss: 0.1050664\n",
      "\tspeed: 0.0183s/iter; left time: 90.1093s\n",
      "\titers: 300, epoch: 12 | loss: 0.0800592\n",
      "\tspeed: 0.0183s/iter; left time: 88.2083s\n",
      "\titers: 400, epoch: 12 | loss: 0.0910716\n",
      "\tspeed: 0.0183s/iter; left time: 86.3553s\n",
      "\titers: 500, epoch: 12 | loss: 0.0960518\n",
      "\tspeed: 0.0182s/iter; left time: 84.4977s\n",
      "Epoch: 12 cost time: 10.6627357006073\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0887536 Vali Loss: 0.0395426 Test Loss: 0.1356473\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.0758656\n",
      "\tspeed: 0.0552s/iter; left time: 246.0907s\n",
      "\titers: 200, epoch: 13 | loss: 0.0575939\n",
      "\tspeed: 0.0183s/iter; left time: 79.8694s\n",
      "\titers: 300, epoch: 13 | loss: 0.0811126\n",
      "\tspeed: 0.0183s/iter; left time: 78.0446s\n",
      "\titers: 400, epoch: 13 | loss: 0.0636982\n",
      "\tspeed: 0.0183s/iter; left time: 76.1973s\n",
      "\titers: 500, epoch: 13 | loss: 0.0718686\n",
      "\tspeed: 0.0183s/iter; left time: 74.3373s\n",
      "Epoch: 13 cost time: 10.700054168701172\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0880837 Vali Loss: 0.0396148 Test Loss: 0.1357707\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1356518715620041, mae:0.23420971632003784\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2044321\n",
      "\tspeed: 0.0351s/iter; left time: 396.3600s\n",
      "\titers: 200, epoch: 1 | loss: 0.1907370\n",
      "\tspeed: 0.0235s/iter; left time: 262.7351s\n",
      "\titers: 300, epoch: 1 | loss: 0.2209329\n",
      "\tspeed: 0.0234s/iter; left time: 260.1607s\n",
      "\titers: 400, epoch: 1 | loss: 0.3088663\n",
      "\tspeed: 0.0234s/iter; left time: 257.9625s\n",
      "\titers: 500, epoch: 1 | loss: 0.3752808\n",
      "\tspeed: 0.0235s/iter; left time: 256.1545s\n",
      "Epoch: 1 cost time: 14.595470428466797\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2527241 Vali Loss: 0.0623574 Test Loss: 0.1859929\n",
      "Validation loss decreased (inf --> 0.062357).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3072649\n",
      "\tspeed: 0.0680s/iter; left time: 729.8418s\n",
      "\titers: 200, epoch: 2 | loss: 0.2502496\n",
      "\tspeed: 0.0241s/iter; left time: 255.7045s\n",
      "\titers: 300, epoch: 2 | loss: 0.1918114\n",
      "\tspeed: 0.0223s/iter; left time: 234.7080s\n",
      "\titers: 400, epoch: 2 | loss: 0.3376081\n",
      "\tspeed: 0.0222s/iter; left time: 231.2941s\n",
      "\titers: 500, epoch: 2 | loss: 0.2363885\n",
      "\tspeed: 0.0221s/iter; left time: 228.7366s\n",
      "Epoch: 2 cost time: 13.353620529174805\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2683277 Vali Loss: 0.0661320 Test Loss: 0.1890021\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4923572\n",
      "\tspeed: 0.0646s/iter; left time: 656.4924s\n",
      "\titers: 200, epoch: 3 | loss: 0.1829364\n",
      "\tspeed: 0.0229s/iter; left time: 230.3467s\n",
      "\titers: 300, epoch: 3 | loss: 0.2508573\n",
      "\tspeed: 0.0222s/iter; left time: 220.8551s\n",
      "\titers: 400, epoch: 3 | loss: 0.2537628\n",
      "\tspeed: 0.0240s/iter; left time: 237.0696s\n",
      "\titers: 500, epoch: 3 | loss: 0.1679934\n",
      "\tspeed: 0.0241s/iter; left time: 234.8501s\n",
      "Epoch: 3 cost time: 13.700138807296753\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2455218 Vali Loss: 0.0570564 Test Loss: 0.1525956\n",
      "Validation loss decreased (0.062357 --> 0.057056).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1775687\n",
      "\tspeed: 0.0679s/iter; left time: 650.9413s\n",
      "\titers: 200, epoch: 4 | loss: 0.1459089\n",
      "\tspeed: 0.0223s/iter; left time: 211.4759s\n",
      "\titers: 300, epoch: 4 | loss: 0.2227755\n",
      "\tspeed: 0.0222s/iter; left time: 208.6792s\n",
      "\titers: 400, epoch: 4 | loss: 0.1869705\n",
      "\tspeed: 0.0222s/iter; left time: 206.6815s\n",
      "\titers: 500, epoch: 4 | loss: 0.2770719\n",
      "\tspeed: 0.0222s/iter; left time: 204.2582s\n",
      "Epoch: 4 cost time: 13.015325784683228\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2214344 Vali Loss: 0.0565339 Test Loss: 0.1514897\n",
      "Validation loss decreased (0.057056 --> 0.056534).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2272343\n",
      "\tspeed: 0.0679s/iter; left time: 612.2571s\n",
      "\titers: 200, epoch: 5 | loss: 0.1697168\n",
      "\tspeed: 0.0222s/iter; left time: 197.8636s\n",
      "\titers: 300, epoch: 5 | loss: 0.1575013\n",
      "\tspeed: 0.0221s/iter; left time: 195.3350s\n",
      "\titers: 400, epoch: 5 | loss: 0.2007736\n",
      "\tspeed: 0.0221s/iter; left time: 193.0461s\n",
      "\titers: 500, epoch: 5 | loss: 0.2119050\n",
      "\tspeed: 0.0222s/iter; left time: 191.2140s\n",
      "Epoch: 5 cost time: 12.948059320449829\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2082672 Vali Loss: 0.0547199 Test Loss: 0.1453940\n",
      "Validation loss decreased (0.056534 --> 0.054720).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1648124\n",
      "\tspeed: 0.0636s/iter; left time: 537.8939s\n",
      "\titers: 200, epoch: 6 | loss: 0.3070776\n",
      "\tspeed: 0.0222s/iter; left time: 185.3955s\n",
      "\titers: 300, epoch: 6 | loss: 0.1929084\n",
      "\tspeed: 0.0222s/iter; left time: 182.9105s\n",
      "\titers: 400, epoch: 6 | loss: 0.1536869\n",
      "\tspeed: 0.0222s/iter; left time: 180.8464s\n",
      "\titers: 500, epoch: 6 | loss: 0.2804589\n",
      "\tspeed: 0.0222s/iter; left time: 178.4071s\n",
      "Epoch: 6 cost time: 12.973344326019287\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2023327 Vali Loss: 0.0543583 Test Loss: 0.1462831\n",
      "Validation loss decreased (0.054720 --> 0.054358).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2472715\n",
      "\tspeed: 0.0646s/iter; left time: 509.2803s\n",
      "\titers: 200, epoch: 7 | loss: 0.1912365\n",
      "\tspeed: 0.0221s/iter; left time: 171.6663s\n",
      "\titers: 300, epoch: 7 | loss: 0.1569330\n",
      "\tspeed: 0.0220s/iter; left time: 168.9871s\n",
      "\titers: 400, epoch: 7 | loss: 0.1843344\n",
      "\tspeed: 0.0220s/iter; left time: 166.8118s\n",
      "\titers: 500, epoch: 7 | loss: 0.2040082\n",
      "\tspeed: 0.0220s/iter; left time: 164.9129s\n",
      "Epoch: 7 cost time: 12.888179063796997\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1976341 Vali Loss: 0.0550024 Test Loss: 0.1458890\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1246326\n",
      "\tspeed: 0.0637s/iter; left time: 465.5030s\n",
      "\titers: 200, epoch: 8 | loss: 0.4012249\n",
      "\tspeed: 0.0221s/iter; left time: 159.2290s\n",
      "\titers: 300, epoch: 8 | loss: 0.1279386\n",
      "\tspeed: 0.0221s/iter; left time: 157.0776s\n",
      "\titers: 400, epoch: 8 | loss: 0.1646966\n",
      "\tspeed: 0.0221s/iter; left time: 154.7708s\n",
      "\titers: 500, epoch: 8 | loss: 0.1723369\n",
      "\tspeed: 0.0221s/iter; left time: 152.4794s\n",
      "Epoch: 8 cost time: 12.874224662780762\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1953455 Vali Loss: 0.0543657 Test Loss: 0.1452569\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1745945\n",
      "\tspeed: 0.0621s/iter; left time: 418.5233s\n",
      "\titers: 200, epoch: 9 | loss: 0.1344898\n",
      "\tspeed: 0.0223s/iter; left time: 147.9939s\n",
      "\titers: 300, epoch: 9 | loss: 0.2274153\n",
      "\tspeed: 0.0223s/iter; left time: 145.6682s\n",
      "\titers: 400, epoch: 9 | loss: 0.1888725\n",
      "\tspeed: 0.0223s/iter; left time: 143.3633s\n",
      "\titers: 500, epoch: 9 | loss: 0.1694360\n",
      "\tspeed: 0.0223s/iter; left time: 141.1703s\n",
      "Epoch: 9 cost time: 12.962656736373901\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1939532 Vali Loss: 0.0545372 Test Loss: 0.1449776\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14646118879318237, mae:0.24671439826488495\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2160419\n",
      "\tspeed: 0.0334s/iter; left time: 377.2716s\n",
      "\titers: 200, epoch: 1 | loss: 0.1162213\n",
      "\tspeed: 0.0214s/iter; left time: 239.3620s\n",
      "\titers: 300, epoch: 1 | loss: 0.1844167\n",
      "\tspeed: 0.0214s/iter; left time: 237.3244s\n",
      "\titers: 400, epoch: 1 | loss: 0.1534699\n",
      "\tspeed: 0.0214s/iter; left time: 235.3872s\n",
      "\titers: 500, epoch: 1 | loss: 0.1646366\n",
      "\tspeed: 0.0214s/iter; left time: 233.2835s\n",
      "Epoch: 1 cost time: 13.438807249069214\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2125935 Vali Loss: 0.0405905 Test Loss: 0.1299499\n",
      "Validation loss decreased (inf --> 0.040591).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2203218\n",
      "\tspeed: 0.0635s/iter; left time: 681.2834s\n",
      "\titers: 200, epoch: 2 | loss: 0.2456415\n",
      "\tspeed: 0.0213s/iter; left time: 226.2601s\n",
      "\titers: 300, epoch: 2 | loss: 0.5099041\n",
      "\tspeed: 0.0213s/iter; left time: 224.0897s\n",
      "\titers: 400, epoch: 2 | loss: 0.2630272\n",
      "\tspeed: 0.0213s/iter; left time: 221.9274s\n",
      "\titers: 500, epoch: 2 | loss: 0.2143973\n",
      "\tspeed: 0.0213s/iter; left time: 219.9411s\n",
      "Epoch: 2 cost time: 12.452905416488647\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2649165 Vali Loss: 0.0637003 Test Loss: 0.1830065\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2386678\n",
      "\tspeed: 0.0610s/iter; left time: 619.4159s\n",
      "\titers: 200, epoch: 3 | loss: 0.2139221\n",
      "\tspeed: 0.0213s/iter; left time: 214.0259s\n",
      "\titers: 300, epoch: 3 | loss: 0.1634926\n",
      "\tspeed: 0.0213s/iter; left time: 211.7607s\n",
      "\titers: 400, epoch: 3 | loss: 0.1910556\n",
      "\tspeed: 0.0213s/iter; left time: 209.8187s\n",
      "\titers: 500, epoch: 3 | loss: 0.2340789\n",
      "\tspeed: 0.0213s/iter; left time: 207.5382s\n",
      "Epoch: 3 cost time: 12.428721189498901\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2446887 Vali Loss: 0.0576729 Test Loss: 0.1636531\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1887436\n",
      "\tspeed: 0.0653s/iter; left time: 626.5549s\n",
      "\titers: 200, epoch: 4 | loss: 0.2005028\n",
      "\tspeed: 0.0213s/iter; left time: 201.7892s\n",
      "\titers: 300, epoch: 4 | loss: 0.2122133\n",
      "\tspeed: 0.0213s/iter; left time: 199.5749s\n",
      "\titers: 400, epoch: 4 | loss: 0.2909198\n",
      "\tspeed: 0.0212s/iter; left time: 197.3781s\n",
      "\titers: 500, epoch: 4 | loss: 0.1725634\n",
      "\tspeed: 0.0212s/iter; left time: 195.2769s\n",
      "Epoch: 4 cost time: 12.389708518981934\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2258179 Vali Loss: 0.0539668 Test Loss: 0.1522814\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13016191124916077, mae:0.22739572823047638\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2422605\n",
      "\tspeed: 0.0342s/iter; left time: 386.1397s\n",
      "\titers: 200, epoch: 1 | loss: 0.1829510\n",
      "\tspeed: 0.0221s/iter; left time: 247.4088s\n",
      "\titers: 300, epoch: 1 | loss: 0.2373776\n",
      "\tspeed: 0.0208s/iter; left time: 230.4634s\n",
      "\titers: 400, epoch: 1 | loss: 0.3229593\n",
      "\tspeed: 0.0200s/iter; left time: 220.3603s\n",
      "\titers: 500, epoch: 1 | loss: 0.4051868\n",
      "\tspeed: 0.0201s/iter; left time: 218.7473s\n",
      "Epoch: 1 cost time: 13.250053882598877\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2792140 Vali Loss: 0.0759798 Test Loss: 0.2162345\n",
      "Validation loss decreased (inf --> 0.075980).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2276985\n",
      "\tspeed: 0.0609s/iter; left time: 653.5203s\n",
      "\titers: 200, epoch: 2 | loss: 0.4476156\n",
      "\tspeed: 0.0201s/iter; left time: 214.0455s\n",
      "\titers: 300, epoch: 2 | loss: 0.2548041\n",
      "\tspeed: 0.0201s/iter; left time: 211.6087s\n",
      "\titers: 400, epoch: 2 | loss: 0.1816117\n",
      "\tspeed: 0.0201s/iter; left time: 209.3931s\n",
      "\titers: 500, epoch: 2 | loss: 0.1685787\n",
      "\tspeed: 0.0201s/iter; left time: 207.2812s\n",
      "Epoch: 2 cost time: 11.767088651657104\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2823152 Vali Loss: 0.0623125 Test Loss: 0.1725155\n",
      "Validation loss decreased (0.075980 --> 0.062312).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1956048\n",
      "\tspeed: 0.0635s/iter; left time: 645.3933s\n",
      "\titers: 200, epoch: 3 | loss: 0.1986732\n",
      "\tspeed: 0.0222s/iter; left time: 223.3168s\n",
      "\titers: 300, epoch: 3 | loss: 0.2300066\n",
      "\tspeed: 0.0222s/iter; left time: 221.1003s\n",
      "\titers: 400, epoch: 3 | loss: 0.2127498\n",
      "\tspeed: 0.0213s/iter; left time: 209.9440s\n",
      "\titers: 500, epoch: 3 | loss: 0.2054931\n",
      "\tspeed: 0.0230s/iter; left time: 224.0892s\n",
      "Epoch: 3 cost time: 13.011893272399902\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2416417 Vali Loss: 0.0628371 Test Loss: 0.1696558\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1648703\n",
      "\tspeed: 0.0653s/iter; left time: 626.5349s\n",
      "\titers: 200, epoch: 4 | loss: 0.4601944\n",
      "\tspeed: 0.0231s/iter; left time: 219.3850s\n",
      "\titers: 300, epoch: 4 | loss: 0.1544110\n",
      "\tspeed: 0.0231s/iter; left time: 216.8437s\n",
      "\titers: 400, epoch: 4 | loss: 0.2180265\n",
      "\tspeed: 0.0231s/iter; left time: 214.5748s\n",
      "\titers: 500, epoch: 4 | loss: 0.1886454\n",
      "\tspeed: 0.0231s/iter; left time: 212.2176s\n",
      "Epoch: 4 cost time: 13.492231607437134\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2286725 Vali Loss: 0.0597612 Test Loss: 0.1620128\n",
      "Validation loss decreased (0.062312 --> 0.059761).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1822467\n",
      "\tspeed: 0.0664s/iter; left time: 598.7082s\n",
      "\titers: 200, epoch: 5 | loss: 0.2959248\n",
      "\tspeed: 0.0231s/iter; left time: 206.2911s\n",
      "\titers: 300, epoch: 5 | loss: 0.1727977\n",
      "\tspeed: 0.0231s/iter; left time: 204.0070s\n",
      "\titers: 400, epoch: 5 | loss: 0.1904349\n",
      "\tspeed: 0.0231s/iter; left time: 201.5835s\n",
      "\titers: 500, epoch: 5 | loss: 0.2039821\n",
      "\tspeed: 0.0231s/iter; left time: 199.2021s\n",
      "Epoch: 5 cost time: 13.505336999893188\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2188623 Vali Loss: 0.0574483 Test Loss: 0.1568991\n",
      "Validation loss decreased (0.059761 --> 0.057448).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2260060\n",
      "\tspeed: 0.0651s/iter; left time: 549.9143s\n",
      "\titers: 200, epoch: 6 | loss: 0.2270817\n",
      "\tspeed: 0.0215s/iter; left time: 179.4801s\n",
      "\titers: 300, epoch: 6 | loss: 0.2142158\n",
      "\tspeed: 0.0200s/iter; left time: 164.8903s\n",
      "\titers: 400, epoch: 6 | loss: 0.1900075\n",
      "\tspeed: 0.0200s/iter; left time: 162.7933s\n",
      "\titers: 500, epoch: 6 | loss: 0.2526564\n",
      "\tspeed: 0.0200s/iter; left time: 160.7901s\n",
      "Epoch: 6 cost time: 12.16146731376648\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2127382 Vali Loss: 0.0570407 Test Loss: 0.1548477\n",
      "Validation loss decreased (0.057448 --> 0.057041).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1727831\n",
      "\tspeed: 0.0600s/iter; left time: 472.9050s\n",
      "\titers: 200, epoch: 7 | loss: 0.2374889\n",
      "\tspeed: 0.0201s/iter; left time: 156.0978s\n",
      "\titers: 300, epoch: 7 | loss: 0.2728970\n",
      "\tspeed: 0.0201s/iter; left time: 154.0411s\n",
      "\titers: 400, epoch: 7 | loss: 0.2354636\n",
      "\tspeed: 0.0200s/iter; left time: 151.8934s\n",
      "\titers: 500, epoch: 7 | loss: 0.1577698\n",
      "\tspeed: 0.0200s/iter; left time: 149.9351s\n",
      "Epoch: 7 cost time: 11.75057315826416\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2092387 Vali Loss: 0.0582214 Test Loss: 0.1559023\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2333621\n",
      "\tspeed: 0.0586s/iter; left time: 428.7831s\n",
      "\titers: 200, epoch: 8 | loss: 0.3190812\n",
      "\tspeed: 0.0200s/iter; left time: 144.1964s\n",
      "\titers: 300, epoch: 8 | loss: 0.2758017\n",
      "\tspeed: 0.0200s/iter; left time: 142.2776s\n",
      "\titers: 400, epoch: 8 | loss: 0.1584807\n",
      "\tspeed: 0.0200s/iter; left time: 140.2715s\n",
      "\titers: 500, epoch: 8 | loss: 0.2110446\n",
      "\tspeed: 0.0200s/iter; left time: 138.2698s\n",
      "Epoch: 8 cost time: 11.691559791564941\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2084264 Vali Loss: 0.0572500 Test Loss: 0.1541620\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1923905\n",
      "\tspeed: 0.0582s/iter; left time: 392.1285s\n",
      "\titers: 200, epoch: 9 | loss: 0.1711386\n",
      "\tspeed: 0.0200s/iter; left time: 132.7664s\n",
      "\titers: 300, epoch: 9 | loss: 0.2769994\n",
      "\tspeed: 0.0200s/iter; left time: 130.7975s\n",
      "\titers: 400, epoch: 9 | loss: 0.1681122\n",
      "\tspeed: 0.0200s/iter; left time: 128.7135s\n",
      "\titers: 500, epoch: 9 | loss: 0.1237359\n",
      "\tspeed: 0.0200s/iter; left time: 126.7180s\n",
      "Epoch: 9 cost time: 11.685113191604614\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2062991 Vali Loss: 0.0573906 Test Loss: 0.1547263\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1550510972738266, mae:0.25246042013168335\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1968861\n",
      "\tspeed: 0.0220s/iter; left time: 248.6442s\n",
      "\titers: 200, epoch: 1 | loss: 0.2074055\n",
      "\tspeed: 0.0104s/iter; left time: 116.4362s\n",
      "\titers: 300, epoch: 1 | loss: 0.2668394\n",
      "\tspeed: 0.0104s/iter; left time: 115.6557s\n",
      "\titers: 400, epoch: 1 | loss: 0.2510618\n",
      "\tspeed: 0.0104s/iter; left time: 114.3724s\n",
      "\titers: 500, epoch: 1 | loss: 0.1887305\n",
      "\tspeed: 0.0104s/iter; left time: 113.4432s\n",
      "Epoch: 1 cost time: 7.139196157455444\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2436748 Vali Loss: 0.0457816 Test Loss: 0.1451397\n",
      "Validation loss decreased (inf --> 0.045782).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3275155\n",
      "\tspeed: 0.0367s/iter; left time: 394.3263s\n",
      "\titers: 200, epoch: 2 | loss: 0.1967727\n",
      "\tspeed: 0.0105s/iter; left time: 111.4504s\n",
      "\titers: 300, epoch: 2 | loss: 0.1991920\n",
      "\tspeed: 0.0104s/iter; left time: 109.9714s\n",
      "\titers: 400, epoch: 2 | loss: 0.1718873\n",
      "\tspeed: 0.0105s/iter; left time: 109.3439s\n",
      "\titers: 500, epoch: 2 | loss: 0.2023968\n",
      "\tspeed: 0.0104s/iter; left time: 107.8413s\n",
      "Epoch: 2 cost time: 6.264881372451782\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2031243 Vali Loss: 0.0430505 Test Loss: 0.1352728\n",
      "Validation loss decreased (0.045782 --> 0.043050).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1456728\n",
      "\tspeed: 0.0373s/iter; left time: 378.5472s\n",
      "\titers: 200, epoch: 3 | loss: 0.2073755\n",
      "\tspeed: 0.0105s/iter; left time: 105.8082s\n",
      "\titers: 300, epoch: 3 | loss: 0.1619648\n",
      "\tspeed: 0.0105s/iter; left time: 104.5371s\n",
      "\titers: 400, epoch: 3 | loss: 0.1799439\n",
      "\tspeed: 0.0105s/iter; left time: 103.3451s\n",
      "\titers: 500, epoch: 3 | loss: 0.2282066\n",
      "\tspeed: 0.0105s/iter; left time: 102.1840s\n",
      "Epoch: 3 cost time: 6.277078151702881\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1790444 Vali Loss: 0.0378854 Test Loss: 0.1217924\n",
      "Validation loss decreased (0.043050 --> 0.037885).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1916265\n",
      "\tspeed: 0.0364s/iter; left time: 349.0827s\n",
      "\titers: 200, epoch: 4 | loss: 0.0987980\n",
      "\tspeed: 0.0105s/iter; left time: 99.2243s\n",
      "\titers: 300, epoch: 4 | loss: 0.1622420\n",
      "\tspeed: 0.0105s/iter; left time: 98.4214s\n",
      "\titers: 400, epoch: 4 | loss: 0.1659676\n",
      "\tspeed: 0.0105s/iter; left time: 97.2350s\n",
      "\titers: 500, epoch: 4 | loss: 0.1543243\n",
      "\tspeed: 0.0104s/iter; left time: 95.9147s\n",
      "Epoch: 4 cost time: 6.255189657211304\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1644141 Vali Loss: 0.0373587 Test Loss: 0.1246603\n",
      "Validation loss decreased (0.037885 --> 0.037359).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1373155\n",
      "\tspeed: 0.0367s/iter; left time: 331.0457s\n",
      "\titers: 200, epoch: 5 | loss: 0.1544386\n",
      "\tspeed: 0.0105s/iter; left time: 93.6975s\n",
      "\titers: 300, epoch: 5 | loss: 0.1008306\n",
      "\tspeed: 0.0105s/iter; left time: 92.4322s\n",
      "\titers: 400, epoch: 5 | loss: 0.1563086\n",
      "\tspeed: 0.0105s/iter; left time: 91.8086s\n",
      "\titers: 500, epoch: 5 | loss: 0.1853543\n",
      "\tspeed: 0.0105s/iter; left time: 90.6076s\n",
      "Epoch: 5 cost time: 6.301286220550537\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1551671 Vali Loss: 0.0357990 Test Loss: 0.1252222\n",
      "Validation loss decreased (0.037359 --> 0.035799).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1380461\n",
      "\tspeed: 0.0366s/iter; left time: 309.5365s\n",
      "\titers: 200, epoch: 6 | loss: 0.1179587\n",
      "\tspeed: 0.0105s/iter; left time: 87.4125s\n",
      "\titers: 300, epoch: 6 | loss: 0.1492406\n",
      "\tspeed: 0.0105s/iter; left time: 86.3636s\n",
      "\titers: 400, epoch: 6 | loss: 0.1771906\n",
      "\tspeed: 0.0104s/iter; left time: 84.9770s\n",
      "\titers: 500, epoch: 6 | loss: 0.1527208\n",
      "\tspeed: 0.0104s/iter; left time: 84.1017s\n",
      "Epoch: 6 cost time: 6.266862154006958\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1508814 Vali Loss: 0.0361586 Test Loss: 0.1267179\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1690600\n",
      "\tspeed: 0.0361s/iter; left time: 284.5939s\n",
      "\titers: 200, epoch: 7 | loss: 0.2181375\n",
      "\tspeed: 0.0104s/iter; left time: 81.1958s\n",
      "\titers: 300, epoch: 7 | loss: 0.1134887\n",
      "\tspeed: 0.0105s/iter; left time: 80.3908s\n",
      "\titers: 400, epoch: 7 | loss: 0.1205991\n",
      "\tspeed: 0.0105s/iter; left time: 79.5613s\n",
      "\titers: 500, epoch: 7 | loss: 0.1330346\n",
      "\tspeed: 0.0105s/iter; left time: 78.3963s\n",
      "Epoch: 7 cost time: 6.2279441356658936\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1488097 Vali Loss: 0.0360066 Test Loss: 0.1266913\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1359771\n",
      "\tspeed: 0.0359s/iter; left time: 262.3075s\n",
      "\titers: 200, epoch: 8 | loss: 0.1457441\n",
      "\tspeed: 0.0104s/iter; left time: 74.8312s\n",
      "\titers: 300, epoch: 8 | loss: 0.1882059\n",
      "\tspeed: 0.0104s/iter; left time: 74.1547s\n",
      "\titers: 400, epoch: 8 | loss: 0.2122235\n",
      "\tspeed: 0.0104s/iter; left time: 73.1073s\n",
      "\titers: 500, epoch: 8 | loss: 0.0939350\n",
      "\tspeed: 0.0104s/iter; left time: 72.2057s\n",
      "Epoch: 8 cost time: 6.21821928024292\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1469399 Vali Loss: 0.0357397 Test Loss: 0.1275342\n",
      "Validation loss decreased (0.035799 --> 0.035740).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2393412\n",
      "\tspeed: 0.0362s/iter; left time: 243.8935s\n",
      "\titers: 200, epoch: 9 | loss: 0.1287752\n",
      "\tspeed: 0.0105s/iter; left time: 69.8832s\n",
      "\titers: 300, epoch: 9 | loss: 0.0813232\n",
      "\tspeed: 0.0105s/iter; left time: 68.9384s\n",
      "\titers: 400, epoch: 9 | loss: 0.1237925\n",
      "\tspeed: 0.0105s/iter; left time: 67.9483s\n",
      "\titers: 500, epoch: 9 | loss: 0.1202961\n",
      "\tspeed: 0.0105s/iter; left time: 66.7545s\n",
      "Epoch: 9 cost time: 6.273076772689819\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1460428 Vali Loss: 0.0362367 Test Loss: 0.1276402\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1430824\n",
      "\tspeed: 0.0357s/iter; left time: 220.4056s\n",
      "\titers: 200, epoch: 10 | loss: 0.1595148\n",
      "\tspeed: 0.0104s/iter; left time: 63.2166s\n",
      "\titers: 300, epoch: 10 | loss: 0.1019429\n",
      "\tspeed: 0.0104s/iter; left time: 62.3782s\n",
      "\titers: 400, epoch: 10 | loss: 0.1213916\n",
      "\tspeed: 0.0104s/iter; left time: 61.3275s\n",
      "\titers: 500, epoch: 10 | loss: 0.2282617\n",
      "\tspeed: 0.0104s/iter; left time: 60.1035s\n",
      "Epoch: 10 cost time: 6.216574192047119\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1460244 Vali Loss: 0.0359605 Test Loss: 0.1277746\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1593354\n",
      "\tspeed: 0.0359s/iter; left time: 200.9906s\n",
      "\titers: 200, epoch: 11 | loss: 0.1470495\n",
      "\tspeed: 0.0105s/iter; left time: 57.8994s\n",
      "\titers: 300, epoch: 11 | loss: 0.1202945\n",
      "\tspeed: 0.0105s/iter; left time: 56.8529s\n",
      "\titers: 400, epoch: 11 | loss: 0.2002390\n",
      "\tspeed: 0.0105s/iter; left time: 55.8429s\n",
      "\titers: 500, epoch: 11 | loss: 0.1476796\n",
      "\tspeed: 0.0105s/iter; left time: 54.7325s\n",
      "Epoch: 11 cost time: 6.274201393127441\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1469775 Vali Loss: 0.0358628 Test Loss: 0.1277538\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1277076005935669, mae:0.22084355354309082\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2401721\n",
      "\tspeed: 0.0222s/iter; left time: 250.9339s\n",
      "\titers: 200, epoch: 1 | loss: 0.2424997\n",
      "\tspeed: 0.0102s/iter; left time: 114.7494s\n",
      "\titers: 300, epoch: 1 | loss: 0.3234658\n",
      "\tspeed: 0.0102s/iter; left time: 113.5263s\n",
      "\titers: 400, epoch: 1 | loss: 0.1961393\n",
      "\tspeed: 0.0104s/iter; left time: 113.8627s\n",
      "\titers: 500, epoch: 1 | loss: 0.1873902\n",
      "\tspeed: 0.0104s/iter; left time: 112.9115s\n",
      "Epoch: 1 cost time: 7.111665725708008\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2151568 Vali Loss: 0.0361725 Test Loss: 0.1191960\n",
      "Validation loss decreased (inf --> 0.036173).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1610263\n",
      "\tspeed: 0.0375s/iter; left time: 402.3924s\n",
      "\titers: 200, epoch: 2 | loss: 0.2114646\n",
      "\tspeed: 0.0103s/iter; left time: 109.7171s\n",
      "\titers: 300, epoch: 2 | loss: 0.1568907\n",
      "\tspeed: 0.0103s/iter; left time: 108.8679s\n",
      "\titers: 400, epoch: 2 | loss: 0.1601709\n",
      "\tspeed: 0.0103s/iter; left time: 107.5710s\n",
      "\titers: 500, epoch: 2 | loss: 0.1835256\n",
      "\tspeed: 0.0103s/iter; left time: 106.5332s\n",
      "Epoch: 2 cost time: 6.191450119018555\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1871854 Vali Loss: 0.0372932 Test Loss: 0.1156548\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0921855\n",
      "\tspeed: 0.0376s/iter; left time: 382.5584s\n",
      "\titers: 200, epoch: 3 | loss: 0.1439721\n",
      "\tspeed: 0.0103s/iter; left time: 103.4726s\n",
      "\titers: 300, epoch: 3 | loss: 0.2918147\n",
      "\tspeed: 0.0103s/iter; left time: 102.4046s\n",
      "\titers: 400, epoch: 3 | loss: 0.1909704\n",
      "\tspeed: 0.0102s/iter; left time: 100.9514s\n",
      "\titers: 500, epoch: 3 | loss: 0.1056564\n",
      "\tspeed: 0.0102s/iter; left time: 99.9389s\n",
      "Epoch: 3 cost time: 6.168665409088135\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1657281 Vali Loss: 0.0321570 Test Loss: 0.1056891\n",
      "Validation loss decreased (0.036173 --> 0.032157).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1245359\n",
      "\tspeed: 0.0369s/iter; left time: 353.6536s\n",
      "\titers: 200, epoch: 4 | loss: 0.1304310\n",
      "\tspeed: 0.0103s/iter; left time: 97.9310s\n",
      "\titers: 300, epoch: 4 | loss: 0.0928682\n",
      "\tspeed: 0.0103s/iter; left time: 96.7324s\n",
      "\titers: 400, epoch: 4 | loss: 0.1208657\n",
      "\tspeed: 0.0103s/iter; left time: 96.1318s\n",
      "\titers: 500, epoch: 4 | loss: 0.1988564\n",
      "\tspeed: 0.0103s/iter; left time: 95.0536s\n",
      "Epoch: 4 cost time: 6.179898977279663\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1537255 Vali Loss: 0.0323878 Test Loss: 0.1043980\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1135346\n",
      "\tspeed: 0.0377s/iter; left time: 340.5209s\n",
      "\titers: 200, epoch: 5 | loss: 0.1933572\n",
      "\tspeed: 0.0102s/iter; left time: 90.7927s\n",
      "\titers: 300, epoch: 5 | loss: 0.1349498\n",
      "\tspeed: 0.0102s/iter; left time: 90.0604s\n",
      "\titers: 400, epoch: 5 | loss: 0.1440819\n",
      "\tspeed: 0.0102s/iter; left time: 89.0702s\n",
      "\titers: 500, epoch: 5 | loss: 0.1472165\n",
      "\tspeed: 0.0102s/iter; left time: 87.8494s\n",
      "Epoch: 5 cost time: 6.0816144943237305\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1476107 Vali Loss: 0.0316899 Test Loss: 0.1041419\n",
      "Validation loss decreased (0.032157 --> 0.031690).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1084595\n",
      "\tspeed: 0.0377s/iter; left time: 318.6251s\n",
      "\titers: 200, epoch: 6 | loss: 0.1837830\n",
      "\tspeed: 0.0103s/iter; left time: 86.1645s\n",
      "\titers: 300, epoch: 6 | loss: 0.1503467\n",
      "\tspeed: 0.0103s/iter; left time: 84.8464s\n",
      "\titers: 400, epoch: 6 | loss: 0.1284380\n",
      "\tspeed: 0.0103s/iter; left time: 84.0316s\n",
      "\titers: 500, epoch: 6 | loss: 0.1001743\n",
      "\tspeed: 0.0103s/iter; left time: 83.0424s\n",
      "Epoch: 6 cost time: 6.173323154449463\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1432669 Vali Loss: 0.0317257 Test Loss: 0.1050080\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1394527\n",
      "\tspeed: 0.0358s/iter; left time: 282.4948s\n",
      "\titers: 200, epoch: 7 | loss: 0.1708174\n",
      "\tspeed: 0.0103s/iter; left time: 79.9726s\n",
      "\titers: 300, epoch: 7 | loss: 0.2254554\n",
      "\tspeed: 0.0103s/iter; left time: 78.7379s\n",
      "\titers: 400, epoch: 7 | loss: 0.1847659\n",
      "\tspeed: 0.0102s/iter; left time: 77.6779s\n",
      "\titers: 500, epoch: 7 | loss: 0.1539829\n",
      "\tspeed: 0.0103s/iter; left time: 76.7163s\n",
      "Epoch: 7 cost time: 6.109286785125732\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1421129 Vali Loss: 0.0313872 Test Loss: 0.1043943\n",
      "Validation loss decreased (0.031690 --> 0.031387).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1260215\n",
      "\tspeed: 0.0360s/iter; left time: 262.8820s\n",
      "\titers: 200, epoch: 8 | loss: 0.0947148\n",
      "\tspeed: 0.0102s/iter; left time: 73.6803s\n",
      "\titers: 300, epoch: 8 | loss: 0.1159244\n",
      "\tspeed: 0.0102s/iter; left time: 72.6632s\n",
      "\titers: 400, epoch: 8 | loss: 0.1393307\n",
      "\tspeed: 0.0102s/iter; left time: 71.5623s\n",
      "\titers: 500, epoch: 8 | loss: 0.0789673\n",
      "\tspeed: 0.0102s/iter; left time: 70.5412s\n",
      "Epoch: 8 cost time: 6.099022388458252\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1416707 Vali Loss: 0.0313756 Test Loss: 0.1045958\n",
      "Validation loss decreased (0.031387 --> 0.031376).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1174904\n",
      "\tspeed: 0.0370s/iter; left time: 249.7343s\n",
      "\titers: 200, epoch: 9 | loss: 0.1468606\n",
      "\tspeed: 0.0103s/iter; left time: 68.3706s\n",
      "\titers: 300, epoch: 9 | loss: 0.1139230\n",
      "\tspeed: 0.0103s/iter; left time: 67.2973s\n",
      "\titers: 400, epoch: 9 | loss: 0.1485003\n",
      "\tspeed: 0.0103s/iter; left time: 66.1347s\n",
      "\titers: 500, epoch: 9 | loss: 0.1246711\n",
      "\tspeed: 0.0103s/iter; left time: 65.1528s\n",
      "Epoch: 9 cost time: 6.183746576309204\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1395523 Vali Loss: 0.0313584 Test Loss: 0.1043669\n",
      "Validation loss decreased (0.031376 --> 0.031358).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1400148\n",
      "\tspeed: 0.0373s/iter; left time: 230.0236s\n",
      "\titers: 200, epoch: 10 | loss: 0.1162626\n",
      "\tspeed: 0.0103s/iter; left time: 62.3444s\n",
      "\titers: 300, epoch: 10 | loss: 0.1198061\n",
      "\tspeed: 0.0103s/iter; left time: 61.3171s\n",
      "\titers: 400, epoch: 10 | loss: 0.1005436\n",
      "\tspeed: 0.0103s/iter; left time: 60.3748s\n",
      "\titers: 500, epoch: 10 | loss: 0.1011312\n",
      "\tspeed: 0.0103s/iter; left time: 59.3672s\n",
      "Epoch: 10 cost time: 6.18208646774292\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1411722 Vali Loss: 0.0313830 Test Loss: 0.1042795\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1380363\n",
      "\tspeed: 0.0381s/iter; left time: 213.4410s\n",
      "\titers: 200, epoch: 11 | loss: 0.1670581\n",
      "\tspeed: 0.0115s/iter; left time: 63.2654s\n",
      "\titers: 300, epoch: 11 | loss: 0.1588926\n",
      "\tspeed: 0.0115s/iter; left time: 62.1442s\n",
      "\titers: 400, epoch: 11 | loss: 0.1483600\n",
      "\tspeed: 0.0115s/iter; left time: 60.9803s\n",
      "\titers: 500, epoch: 11 | loss: 0.1227315\n",
      "\tspeed: 0.0115s/iter; left time: 59.8063s\n",
      "Epoch: 11 cost time: 6.826860189437866\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1392189 Vali Loss: 0.0312490 Test Loss: 0.1042706\n",
      "Validation loss decreased (0.031358 --> 0.031249).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1175639\n",
      "\tspeed: 0.0372s/iter; left time: 187.1549s\n",
      "\titers: 200, epoch: 12 | loss: 0.1478161\n",
      "\tspeed: 0.0103s/iter; left time: 50.7960s\n",
      "\titers: 300, epoch: 12 | loss: 0.1135787\n",
      "\tspeed: 0.0103s/iter; left time: 49.7659s\n",
      "\titers: 400, epoch: 12 | loss: 0.1800499\n",
      "\tspeed: 0.0103s/iter; left time: 48.7404s\n",
      "\titers: 500, epoch: 12 | loss: 0.1252864\n",
      "\tspeed: 0.0103s/iter; left time: 47.7032s\n",
      "Epoch: 12 cost time: 6.177725315093994\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1398091 Vali Loss: 0.0313090 Test Loss: 0.1042849\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2295697\n",
      "\tspeed: 0.0386s/iter; left time: 172.3644s\n",
      "\titers: 200, epoch: 13 | loss: 0.1836542\n",
      "\tspeed: 0.0103s/iter; left time: 44.8757s\n",
      "\titers: 300, epoch: 13 | loss: 0.1145843\n",
      "\tspeed: 0.0103s/iter; left time: 43.8102s\n",
      "\titers: 400, epoch: 13 | loss: 0.1641874\n",
      "\tspeed: 0.0103s/iter; left time: 42.7934s\n",
      "\titers: 500, epoch: 13 | loss: 0.1717600\n",
      "\tspeed: 0.0103s/iter; left time: 41.7468s\n",
      "Epoch: 13 cost time: 6.339035272598267\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1398790 Vali Loss: 0.0312312 Test Loss: 0.1042820\n",
      "Validation loss decreased (0.031249 --> 0.031231).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1311188\n",
      "\tspeed: 0.0373s/iter; left time: 145.2934s\n",
      "\titers: 200, epoch: 14 | loss: 0.1039703\n",
      "\tspeed: 0.0115s/iter; left time: 43.6807s\n",
      "\titers: 300, epoch: 14 | loss: 0.1311249\n",
      "\tspeed: 0.0115s/iter; left time: 42.4972s\n",
      "\titers: 400, epoch: 14 | loss: 0.1338442\n",
      "\tspeed: 0.0115s/iter; left time: 41.3874s\n",
      "\titers: 500, epoch: 14 | loss: 0.1419042\n",
      "\tspeed: 0.0113s/iter; left time: 39.5983s\n",
      "Epoch: 14 cost time: 6.7413740158081055\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1398163 Vali Loss: 0.0312761 Test Loss: 0.1042791\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.0882273\n",
      "\tspeed: 0.0363s/iter; left time: 120.4990s\n",
      "\titers: 200, epoch: 15 | loss: 0.1478414\n",
      "\tspeed: 0.0115s/iter; left time: 37.0665s\n",
      "\titers: 300, epoch: 15 | loss: 0.1274982\n",
      "\tspeed: 0.0115s/iter; left time: 35.8818s\n",
      "\titers: 400, epoch: 15 | loss: 0.1245105\n",
      "\tspeed: 0.0115s/iter; left time: 34.7536s\n",
      "\titers: 500, epoch: 15 | loss: 0.1308725\n",
      "\tspeed: 0.0115s/iter; left time: 33.5911s\n",
      "Epoch: 15 cost time: 6.722913980484009\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1395391 Vali Loss: 0.0312022 Test Loss: 0.1042811\n",
      "Validation loss decreased (0.031231 --> 0.031202).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.2173437\n",
      "\tspeed: 0.0366s/iter; left time: 100.7277s\n",
      "\titers: 200, epoch: 16 | loss: 0.1206843\n",
      "\tspeed: 0.0103s/iter; left time: 27.2208s\n",
      "\titers: 300, epoch: 16 | loss: 0.1893770\n",
      "\tspeed: 0.0103s/iter; left time: 26.2214s\n",
      "\titers: 400, epoch: 16 | loss: 0.1291029\n",
      "\tspeed: 0.0103s/iter; left time: 25.2024s\n",
      "\titers: 500, epoch: 16 | loss: 0.2286672\n",
      "\tspeed: 0.0103s/iter; left time: 24.1903s\n",
      "Epoch: 16 cost time: 6.146344900131226\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1400929 Vali Loss: 0.0311803 Test Loss: 0.1042822\n",
      "Validation loss decreased (0.031202 --> 0.031180).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1043478\n",
      "\tspeed: 0.0377s/iter; left time: 82.2921s\n",
      "\titers: 200, epoch: 17 | loss: 0.1241303\n",
      "\tspeed: 0.0103s/iter; left time: 21.4295s\n",
      "\titers: 300, epoch: 17 | loss: 0.1408147\n",
      "\tspeed: 0.0103s/iter; left time: 20.3696s\n",
      "\titers: 400, epoch: 17 | loss: 0.1139410\n",
      "\tspeed: 0.0103s/iter; left time: 19.3410s\n",
      "\titers: 500, epoch: 17 | loss: 0.0959115\n",
      "\tspeed: 0.0103s/iter; left time: 18.3677s\n",
      "Epoch: 17 cost time: 6.185192346572876\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1397438 Vali Loss: 0.0313544 Test Loss: 0.1042828\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1547170\n",
      "\tspeed: 0.0372s/iter; left time: 59.9610s\n",
      "\titers: 200, epoch: 18 | loss: 0.1407096\n",
      "\tspeed: 0.0102s/iter; left time: 15.4362s\n",
      "\titers: 300, epoch: 18 | loss: 0.1202417\n",
      "\tspeed: 0.0102s/iter; left time: 14.3901s\n",
      "\titers: 400, epoch: 18 | loss: 0.2134871\n",
      "\tspeed: 0.0102s/iter; left time: 13.4085s\n",
      "\titers: 500, epoch: 18 | loss: 0.1240891\n",
      "\tspeed: 0.0102s/iter; left time: 12.3694s\n",
      "Epoch: 18 cost time: 6.133153438568115\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.1396246 Vali Loss: 0.0312675 Test Loss: 0.1042829\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.814697265625e-09\n",
      "\titers: 100, epoch: 19 | loss: 0.1086819\n",
      "\tspeed: 0.0368s/iter; left time: 38.3038s\n",
      "\titers: 200, epoch: 19 | loss: 0.1497446\n",
      "\tspeed: 0.0103s/iter; left time: 9.6512s\n",
      "\titers: 300, epoch: 19 | loss: 0.1158870\n",
      "\tspeed: 0.0103s/iter; left time: 8.6439s\n",
      "\titers: 400, epoch: 19 | loss: 0.1645359\n",
      "\tspeed: 0.0103s/iter; left time: 7.6160s\n",
      "\titers: 500, epoch: 19 | loss: 0.1304467\n",
      "\tspeed: 0.0103s/iter; left time: 6.5916s\n",
      "Epoch: 19 cost time: 6.174314498901367\n",
      "Epoch: 19, Steps: 570 | Train Loss: 0.1395322 Vali Loss: 0.0313423 Test Loss: 0.1042826\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1044272854924202, mae:0.19677524268627167\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2458244\n",
      "\tspeed: 0.0234s/iter; left time: 264.5711s\n",
      "\titers: 200, epoch: 1 | loss: 0.1580599\n",
      "\tspeed: 0.0116s/iter; left time: 129.8436s\n",
      "\titers: 300, epoch: 1 | loss: 0.1607421\n",
      "\tspeed: 0.0116s/iter; left time: 128.4187s\n",
      "\titers: 400, epoch: 1 | loss: 0.1947172\n",
      "\tspeed: 0.0116s/iter; left time: 127.3042s\n",
      "\titers: 500, epoch: 1 | loss: 0.2462651\n",
      "\tspeed: 0.0104s/iter; left time: 113.1814s\n",
      "Epoch: 1 cost time: 7.621551513671875\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2496192 Vali Loss: 0.0448467 Test Loss: 0.1367395\n",
      "Validation loss decreased (inf --> 0.044847).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2090575\n",
      "\tspeed: 0.0369s/iter; left time: 396.5018s\n",
      "\titers: 200, epoch: 2 | loss: 0.2855562\n",
      "\tspeed: 0.0103s/iter; left time: 109.4078s\n",
      "\titers: 300, epoch: 2 | loss: 0.1796440\n",
      "\tspeed: 0.0103s/iter; left time: 108.6123s\n",
      "\titers: 400, epoch: 2 | loss: 0.3255115\n",
      "\tspeed: 0.0103s/iter; left time: 107.5033s\n",
      "\titers: 500, epoch: 2 | loss: 0.2540377\n",
      "\tspeed: 0.0103s/iter; left time: 106.5572s\n",
      "Epoch: 2 cost time: 6.188753128051758\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2090411 Vali Loss: 0.0425315 Test Loss: 0.1379918\n",
      "Validation loss decreased (0.044847 --> 0.042531).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1236370\n",
      "\tspeed: 0.0360s/iter; left time: 365.7224s\n",
      "\titers: 200, epoch: 3 | loss: 0.1724854\n",
      "\tspeed: 0.0104s/iter; left time: 104.1735s\n",
      "\titers: 300, epoch: 3 | loss: 0.2191168\n",
      "\tspeed: 0.0103s/iter; left time: 102.9300s\n",
      "\titers: 400, epoch: 3 | loss: 0.1433497\n",
      "\tspeed: 0.0103s/iter; left time: 101.8978s\n",
      "\titers: 500, epoch: 3 | loss: 0.1347058\n",
      "\tspeed: 0.0104s/iter; left time: 101.1216s\n",
      "Epoch: 3 cost time: 6.207587242126465\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1822572 Vali Loss: 0.0381314 Test Loss: 0.1287609\n",
      "Validation loss decreased (0.042531 --> 0.038131).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0993757\n",
      "\tspeed: 0.0376s/iter; left time: 360.9101s\n",
      "\titers: 200, epoch: 4 | loss: 0.1030221\n",
      "\tspeed: 0.0116s/iter; left time: 110.1914s\n",
      "\titers: 300, epoch: 4 | loss: 0.1255772\n",
      "\tspeed: 0.0116s/iter; left time: 109.3039s\n",
      "\titers: 400, epoch: 4 | loss: 0.1874091\n",
      "\tspeed: 0.0110s/iter; left time: 102.1351s\n",
      "\titers: 500, epoch: 4 | loss: 0.1362787\n",
      "\tspeed: 0.0103s/iter; left time: 94.9677s\n",
      "Epoch: 4 cost time: 6.634077548980713\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1654112 Vali Loss: 0.0390523 Test Loss: 0.1291916\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1629340\n",
      "\tspeed: 0.0373s/iter; left time: 336.8116s\n",
      "\titers: 200, epoch: 5 | loss: 0.1039051\n",
      "\tspeed: 0.0103s/iter; left time: 91.5099s\n",
      "\titers: 300, epoch: 5 | loss: 0.1660782\n",
      "\tspeed: 0.0103s/iter; left time: 90.7682s\n",
      "\titers: 400, epoch: 5 | loss: 0.1498400\n",
      "\tspeed: 0.0103s/iter; left time: 89.9875s\n",
      "\titers: 500, epoch: 5 | loss: 0.2172457\n",
      "\tspeed: 0.0103s/iter; left time: 88.8171s\n",
      "Epoch: 5 cost time: 6.178137302398682\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1577477 Vali Loss: 0.0375868 Test Loss: 0.1292249\n",
      "Validation loss decreased (0.038131 --> 0.037587).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1206211\n",
      "\tspeed: 0.0366s/iter; left time: 309.0165s\n",
      "\titers: 200, epoch: 6 | loss: 0.1372995\n",
      "\tspeed: 0.0103s/iter; left time: 85.9285s\n",
      "\titers: 300, epoch: 6 | loss: 0.1432529\n",
      "\tspeed: 0.0103s/iter; left time: 84.8213s\n",
      "\titers: 400, epoch: 6 | loss: 0.1195650\n",
      "\tspeed: 0.0103s/iter; left time: 83.5553s\n",
      "\titers: 500, epoch: 6 | loss: 0.1516001\n",
      "\tspeed: 0.0102s/iter; left time: 82.4709s\n",
      "Epoch: 6 cost time: 6.145037651062012\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1522886 Vali Loss: 0.0371695 Test Loss: 0.1265572\n",
      "Validation loss decreased (0.037587 --> 0.037169).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1134902\n",
      "\tspeed: 0.0370s/iter; left time: 291.3178s\n",
      "\titers: 200, epoch: 7 | loss: 0.1240653\n",
      "\tspeed: 0.0103s/iter; left time: 80.2969s\n",
      "\titers: 300, epoch: 7 | loss: 0.1308912\n",
      "\tspeed: 0.0103s/iter; left time: 79.4308s\n",
      "\titers: 400, epoch: 7 | loss: 0.1020128\n",
      "\tspeed: 0.0103s/iter; left time: 78.3971s\n",
      "\titers: 500, epoch: 7 | loss: 0.1568721\n",
      "\tspeed: 0.0103s/iter; left time: 77.3759s\n",
      "Epoch: 7 cost time: 6.2012505531311035\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1493799 Vali Loss: 0.0368687 Test Loss: 0.1266420\n",
      "Validation loss decreased (0.037169 --> 0.036869).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1191704\n",
      "\tspeed: 0.0368s/iter; left time: 269.3812s\n",
      "\titers: 200, epoch: 8 | loss: 0.1438675\n",
      "\tspeed: 0.0104s/iter; left time: 74.8224s\n",
      "\titers: 300, epoch: 8 | loss: 0.1819857\n",
      "\tspeed: 0.0103s/iter; left time: 73.4478s\n",
      "\titers: 400, epoch: 8 | loss: 0.1481585\n",
      "\tspeed: 0.0103s/iter; left time: 72.3627s\n",
      "\titers: 500, epoch: 8 | loss: 0.1317616\n",
      "\tspeed: 0.0103s/iter; left time: 71.3211s\n",
      "Epoch: 8 cost time: 6.296011447906494\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1479293 Vali Loss: 0.0369548 Test Loss: 0.1264471\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1167659\n",
      "\tspeed: 0.0357s/iter; left time: 240.8677s\n",
      "\titers: 200, epoch: 9 | loss: 0.1701135\n",
      "\tspeed: 0.0102s/iter; left time: 67.8566s\n",
      "\titers: 300, epoch: 9 | loss: 0.1141123\n",
      "\tspeed: 0.0102s/iter; left time: 66.8312s\n",
      "\titers: 400, epoch: 9 | loss: 0.1052207\n",
      "\tspeed: 0.0102s/iter; left time: 65.8699s\n",
      "\titers: 500, epoch: 9 | loss: 0.1552950\n",
      "\tspeed: 0.0102s/iter; left time: 64.8239s\n",
      "Epoch: 9 cost time: 6.0986902713775635\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1471686 Vali Loss: 0.0371080 Test Loss: 0.1267642\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1323274\n",
      "\tspeed: 0.0351s/iter; left time: 216.7596s\n",
      "\titers: 200, epoch: 10 | loss: 0.2113402\n",
      "\tspeed: 0.0102s/iter; left time: 62.1104s\n",
      "\titers: 300, epoch: 10 | loss: 0.2047139\n",
      "\tspeed: 0.0102s/iter; left time: 61.2013s\n",
      "\titers: 400, epoch: 10 | loss: 0.1622301\n",
      "\tspeed: 0.0102s/iter; left time: 60.0442s\n",
      "\titers: 500, epoch: 10 | loss: 0.1510198\n",
      "\tspeed: 0.0102s/iter; left time: 59.1331s\n",
      "Epoch: 10 cost time: 6.110207557678223\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1486454 Vali Loss: 0.0370910 Test Loss: 0.1269608\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12679612636566162, mae:0.22187824547290802\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3012963\n",
      "\tspeed: 0.0237s/iter; left time: 267.4723s\n",
      "\titers: 200, epoch: 1 | loss: 0.2677741\n",
      "\tspeed: 0.0119s/iter; left time: 133.1371s\n",
      "\titers: 300, epoch: 1 | loss: 0.1750219\n",
      "\tspeed: 0.0119s/iter; left time: 132.0829s\n",
      "\titers: 400, epoch: 1 | loss: 0.1297865\n",
      "\tspeed: 0.0119s/iter; left time: 130.8549s\n",
      "\titers: 500, epoch: 1 | loss: 0.2133226\n",
      "\tspeed: 0.0119s/iter; left time: 129.6455s\n",
      "Epoch: 1 cost time: 8.0092453956604\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2574803 Vali Loss: 0.0458579 Test Loss: 0.1352860\n",
      "Validation loss decreased (inf --> 0.045858).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1982875\n",
      "\tspeed: 0.0406s/iter; left time: 435.6817s\n",
      "\titers: 200, epoch: 2 | loss: 0.1828269\n",
      "\tspeed: 0.0129s/iter; left time: 137.2458s\n",
      "\titers: 300, epoch: 2 | loss: 0.2175472\n",
      "\tspeed: 0.0129s/iter; left time: 136.0538s\n",
      "\titers: 400, epoch: 2 | loss: 0.1286643\n",
      "\tspeed: 0.0130s/iter; left time: 135.4027s\n",
      "\titers: 500, epoch: 2 | loss: 0.2117110\n",
      "\tspeed: 0.0129s/iter; left time: 133.4766s\n",
      "Epoch: 2 cost time: 7.696255207061768\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2258754 Vali Loss: 0.0447057 Test Loss: 0.1362181\n",
      "Validation loss decreased (0.045858 --> 0.044706).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2756763\n",
      "\tspeed: 0.0405s/iter; left time: 411.5977s\n",
      "\titers: 200, epoch: 3 | loss: 0.2064056\n",
      "\tspeed: 0.0118s/iter; left time: 119.1245s\n",
      "\titers: 300, epoch: 3 | loss: 0.1095314\n",
      "\tspeed: 0.0118s/iter; left time: 117.7830s\n",
      "\titers: 400, epoch: 3 | loss: 0.2068678\n",
      "\tspeed: 0.0118s/iter; left time: 116.8032s\n",
      "\titers: 500, epoch: 3 | loss: 0.2011012\n",
      "\tspeed: 0.0118s/iter; left time: 115.4100s\n",
      "Epoch: 3 cost time: 7.065578937530518\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2037236 Vali Loss: 0.0462443 Test Loss: 0.1363997\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1503361\n",
      "\tspeed: 0.0390s/iter; left time: 374.1239s\n",
      "\titers: 200, epoch: 4 | loss: 0.2408571\n",
      "\tspeed: 0.0118s/iter; left time: 112.4363s\n",
      "\titers: 300, epoch: 4 | loss: 0.1057899\n",
      "\tspeed: 0.0119s/iter; left time: 111.3303s\n",
      "\titers: 400, epoch: 4 | loss: 0.1237857\n",
      "\tspeed: 0.0119s/iter; left time: 110.2046s\n",
      "\titers: 500, epoch: 4 | loss: 0.1619836\n",
      "\tspeed: 0.0119s/iter; left time: 109.0803s\n",
      "Epoch: 4 cost time: 7.051420450210571\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1848698 Vali Loss: 0.0414682 Test Loss: 0.1244780\n",
      "Validation loss decreased (0.044706 --> 0.041468).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1547678\n",
      "\tspeed: 0.0398s/iter; left time: 358.7752s\n",
      "\titers: 200, epoch: 5 | loss: 0.1490418\n",
      "\tspeed: 0.0118s/iter; left time: 105.3372s\n",
      "\titers: 300, epoch: 5 | loss: 0.2137638\n",
      "\tspeed: 0.0118s/iter; left time: 104.3182s\n",
      "\titers: 400, epoch: 5 | loss: 0.1746981\n",
      "\tspeed: 0.0118s/iter; left time: 103.0409s\n",
      "\titers: 500, epoch: 5 | loss: 0.1618993\n",
      "\tspeed: 0.0118s/iter; left time: 101.8522s\n",
      "Epoch: 5 cost time: 7.022732734680176\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1739019 Vali Loss: 0.0393822 Test Loss: 0.1224011\n",
      "Validation loss decreased (0.041468 --> 0.039382).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1413294\n",
      "\tspeed: 0.0402s/iter; left time: 339.3079s\n",
      "\titers: 200, epoch: 6 | loss: 0.0935599\n",
      "\tspeed: 0.0119s/iter; left time: 99.0620s\n",
      "\titers: 300, epoch: 6 | loss: 0.2263413\n",
      "\tspeed: 0.0119s/iter; left time: 98.1030s\n",
      "\titers: 400, epoch: 6 | loss: 0.1823796\n",
      "\tspeed: 0.0119s/iter; left time: 96.8652s\n",
      "\titers: 500, epoch: 6 | loss: 0.1201744\n",
      "\tspeed: 0.0119s/iter; left time: 95.4829s\n",
      "Epoch: 6 cost time: 7.102685451507568\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1673662 Vali Loss: 0.0386579 Test Loss: 0.1205866\n",
      "Validation loss decreased (0.039382 --> 0.038658).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1570462\n",
      "\tspeed: 0.0410s/iter; left time: 323.0567s\n",
      "\titers: 200, epoch: 7 | loss: 0.1613402\n",
      "\tspeed: 0.0118s/iter; left time: 91.8850s\n",
      "\titers: 300, epoch: 7 | loss: 0.1535271\n",
      "\tspeed: 0.0118s/iter; left time: 90.8250s\n",
      "\titers: 400, epoch: 7 | loss: 0.1166422\n",
      "\tspeed: 0.0118s/iter; left time: 89.7576s\n",
      "\titers: 500, epoch: 7 | loss: 0.1554347\n",
      "\tspeed: 0.0118s/iter; left time: 88.3485s\n",
      "Epoch: 7 cost time: 7.073322772979736\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1631525 Vali Loss: 0.0386661 Test Loss: 0.1209979\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0983596\n",
      "\tspeed: 0.0392s/iter; left time: 286.4206s\n",
      "\titers: 200, epoch: 8 | loss: 0.1509856\n",
      "\tspeed: 0.0119s/iter; left time: 85.6167s\n",
      "\titers: 300, epoch: 8 | loss: 0.1772835\n",
      "\tspeed: 0.0119s/iter; left time: 84.3531s\n",
      "\titers: 400, epoch: 8 | loss: 0.1319452\n",
      "\tspeed: 0.0119s/iter; left time: 83.1004s\n",
      "\titers: 500, epoch: 8 | loss: 0.1221954\n",
      "\tspeed: 0.0118s/iter; left time: 81.8383s\n",
      "Epoch: 8 cost time: 7.090970516204834\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1622830 Vali Loss: 0.0382901 Test Loss: 0.1199921\n",
      "Validation loss decreased (0.038658 --> 0.038290).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1114881\n",
      "\tspeed: 0.0423s/iter; left time: 284.9647s\n",
      "\titers: 200, epoch: 9 | loss: 0.1543981\n",
      "\tspeed: 0.0119s/iter; left time: 78.8466s\n",
      "\titers: 300, epoch: 9 | loss: 0.1817339\n",
      "\tspeed: 0.0118s/iter; left time: 77.4210s\n",
      "\titers: 400, epoch: 9 | loss: 0.1271408\n",
      "\tspeed: 0.0118s/iter; left time: 76.1382s\n",
      "\titers: 500, epoch: 9 | loss: 0.1836076\n",
      "\tspeed: 0.0118s/iter; left time: 75.1362s\n",
      "Epoch: 9 cost time: 7.080049276351929\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1617940 Vali Loss: 0.0380872 Test Loss: 0.1199515\n",
      "Validation loss decreased (0.038290 --> 0.038087).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1570071\n",
      "\tspeed: 0.0399s/iter; left time: 245.9682s\n",
      "\titers: 200, epoch: 10 | loss: 0.1784642\n",
      "\tspeed: 0.0119s/iter; left time: 72.0660s\n",
      "\titers: 300, epoch: 10 | loss: 0.1366245\n",
      "\tspeed: 0.0119s/iter; left time: 71.1253s\n",
      "\titers: 400, epoch: 10 | loss: 0.1409702\n",
      "\tspeed: 0.0119s/iter; left time: 69.7149s\n",
      "\titers: 500, epoch: 10 | loss: 0.1993697\n",
      "\tspeed: 0.0119s/iter; left time: 68.4102s\n",
      "Epoch: 10 cost time: 7.097779750823975\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1603146 Vali Loss: 0.0381728 Test Loss: 0.1202390\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1282211\n",
      "\tspeed: 0.0387s/iter; left time: 216.9835s\n",
      "\titers: 200, epoch: 11 | loss: 0.2455364\n",
      "\tspeed: 0.0118s/iter; left time: 65.0306s\n",
      "\titers: 300, epoch: 11 | loss: 0.1538334\n",
      "\tspeed: 0.0119s/iter; left time: 64.0049s\n",
      "\titers: 400, epoch: 11 | loss: 0.1733185\n",
      "\tspeed: 0.0118s/iter; left time: 62.6635s\n",
      "\titers: 500, epoch: 11 | loss: 0.2067696\n",
      "\tspeed: 0.0118s/iter; left time: 61.4506s\n",
      "Epoch: 11 cost time: 7.041460037231445\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1596519 Vali Loss: 0.0382238 Test Loss: 0.1203443\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1390537\n",
      "\tspeed: 0.0388s/iter; left time: 195.2956s\n",
      "\titers: 200, epoch: 12 | loss: 0.1245567\n",
      "\tspeed: 0.0118s/iter; left time: 58.2939s\n",
      "\titers: 300, epoch: 12 | loss: 0.1152004\n",
      "\tspeed: 0.0118s/iter; left time: 57.0879s\n",
      "\titers: 400, epoch: 12 | loss: 0.1446932\n",
      "\tspeed: 0.0118s/iter; left time: 55.9214s\n",
      "\titers: 500, epoch: 12 | loss: 0.1685894\n",
      "\tspeed: 0.0118s/iter; left time: 54.7230s\n",
      "Epoch: 12 cost time: 7.076025485992432\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1602082 Vali Loss: 0.0383167 Test Loss: 0.1203712\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12011267244815826, mae:0.2147633582353592\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1519469\n",
      "\tspeed: 0.0232s/iter; left time: 262.7179s\n",
      "\titers: 200, epoch: 1 | loss: 0.3518038\n",
      "\tspeed: 0.0115s/iter; left time: 129.0832s\n",
      "\titers: 300, epoch: 1 | loss: 0.1749141\n",
      "\tspeed: 0.0115s/iter; left time: 128.0576s\n",
      "\titers: 400, epoch: 1 | loss: 0.2966926\n",
      "\tspeed: 0.0115s/iter; left time: 126.7798s\n",
      "\titers: 500, epoch: 1 | loss: 0.1596343\n",
      "\tspeed: 0.0116s/iter; left time: 125.9697s\n",
      "Epoch: 1 cost time: 7.802969694137573\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2292013 Vali Loss: 0.0412115 Test Loss: 0.1274067\n",
      "Validation loss decreased (inf --> 0.041211).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1706311\n",
      "\tspeed: 0.0409s/iter; left time: 438.5258s\n",
      "\titers: 200, epoch: 2 | loss: 0.1401508\n",
      "\tspeed: 0.0125s/iter; left time: 133.2498s\n",
      "\titers: 300, epoch: 2 | loss: 0.1847728\n",
      "\tspeed: 0.0126s/iter; left time: 132.3281s\n",
      "\titers: 400, epoch: 2 | loss: 0.2843027\n",
      "\tspeed: 0.0126s/iter; left time: 131.0124s\n",
      "\titers: 500, epoch: 2 | loss: 0.2906266\n",
      "\tspeed: 0.0125s/iter; left time: 129.4183s\n",
      "Epoch: 2 cost time: 7.450432062149048\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2022052 Vali Loss: 0.0417904 Test Loss: 0.1300673\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1933681\n",
      "\tspeed: 0.0386s/iter; left time: 392.0206s\n",
      "\titers: 200, epoch: 3 | loss: 0.0985367\n",
      "\tspeed: 0.0115s/iter; left time: 115.5206s\n",
      "\titers: 300, epoch: 3 | loss: 0.1996156\n",
      "\tspeed: 0.0115s/iter; left time: 114.7074s\n",
      "\titers: 400, epoch: 3 | loss: 0.1665078\n",
      "\tspeed: 0.0115s/iter; left time: 113.2855s\n",
      "\titers: 500, epoch: 3 | loss: 0.2089575\n",
      "\tspeed: 0.0115s/iter; left time: 112.1688s\n",
      "Epoch: 3 cost time: 6.838870048522949\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1803595 Vali Loss: 0.0353008 Test Loss: 0.1156385\n",
      "Validation loss decreased (0.041211 --> 0.035301).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1596104\n",
      "\tspeed: 0.0385s/iter; left time: 369.2806s\n",
      "\titers: 200, epoch: 4 | loss: 0.1095146\n",
      "\tspeed: 0.0115s/iter; left time: 108.9106s\n",
      "\titers: 300, epoch: 4 | loss: 0.0954783\n",
      "\tspeed: 0.0115s/iter; left time: 107.8193s\n",
      "\titers: 400, epoch: 4 | loss: 0.1333924\n",
      "\tspeed: 0.0115s/iter; left time: 106.6323s\n",
      "\titers: 500, epoch: 4 | loss: 0.1625584\n",
      "\tspeed: 0.0115s/iter; left time: 105.5658s\n",
      "Epoch: 4 cost time: 6.852359294891357\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1650652 Vali Loss: 0.0338985 Test Loss: 0.1111508\n",
      "Validation loss decreased (0.035301 --> 0.033899).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1666282\n",
      "\tspeed: 0.0387s/iter; left time: 349.0557s\n",
      "\titers: 200, epoch: 5 | loss: 0.1077879\n",
      "\tspeed: 0.0115s/iter; left time: 102.2619s\n",
      "\titers: 300, epoch: 5 | loss: 0.2390414\n",
      "\tspeed: 0.0115s/iter; left time: 101.1646s\n",
      "\titers: 400, epoch: 5 | loss: 0.1366075\n",
      "\tspeed: 0.0114s/iter; left time: 99.7815s\n",
      "\titers: 500, epoch: 5 | loss: 0.1202177\n",
      "\tspeed: 0.0115s/iter; left time: 98.8638s\n",
      "Epoch: 5 cost time: 6.862772464752197\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1558943 Vali Loss: 0.0339489 Test Loss: 0.1121132\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1796040\n",
      "\tspeed: 0.0383s/iter; left time: 324.0646s\n",
      "\titers: 200, epoch: 6 | loss: 0.1611378\n",
      "\tspeed: 0.0115s/iter; left time: 95.6558s\n",
      "\titers: 300, epoch: 6 | loss: 0.1117627\n",
      "\tspeed: 0.0115s/iter; left time: 94.4940s\n",
      "\titers: 400, epoch: 6 | loss: 0.1110432\n",
      "\tspeed: 0.0115s/iter; left time: 93.3620s\n",
      "\titers: 500, epoch: 6 | loss: 0.1269647\n",
      "\tspeed: 0.0114s/iter; left time: 92.0792s\n",
      "Epoch: 6 cost time: 6.835015058517456\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1512940 Vali Loss: 0.0335551 Test Loss: 0.1112797\n",
      "Validation loss decreased (0.033899 --> 0.033555).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1086403\n",
      "\tspeed: 0.0386s/iter; left time: 304.5983s\n",
      "\titers: 200, epoch: 7 | loss: 0.1870593\n",
      "\tspeed: 0.0115s/iter; left time: 89.3011s\n",
      "\titers: 300, epoch: 7 | loss: 0.0947685\n",
      "\tspeed: 0.0115s/iter; left time: 88.0247s\n",
      "\titers: 400, epoch: 7 | loss: 0.1145303\n",
      "\tspeed: 0.0115s/iter; left time: 86.9123s\n",
      "\titers: 500, epoch: 7 | loss: 0.1414022\n",
      "\tspeed: 0.0114s/iter; left time: 85.6565s\n",
      "Epoch: 7 cost time: 6.820448875427246\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1482953 Vali Loss: 0.0331711 Test Loss: 0.1111351\n",
      "Validation loss decreased (0.033555 --> 0.033171).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1101499\n",
      "\tspeed: 0.0384s/iter; left time: 281.0833s\n",
      "\titers: 200, epoch: 8 | loss: 0.1743105\n",
      "\tspeed: 0.0115s/iter; left time: 82.7960s\n",
      "\titers: 300, epoch: 8 | loss: 0.1020674\n",
      "\tspeed: 0.0115s/iter; left time: 81.4645s\n",
      "\titers: 400, epoch: 8 | loss: 0.1296077\n",
      "\tspeed: 0.0115s/iter; left time: 80.3095s\n",
      "\titers: 500, epoch: 8 | loss: 0.1146621\n",
      "\tspeed: 0.0115s/iter; left time: 79.1481s\n",
      "Epoch: 8 cost time: 6.863987684249878\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1471755 Vali Loss: 0.0333272 Test Loss: 0.1113495\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2292286\n",
      "\tspeed: 0.0409s/iter; left time: 275.9357s\n",
      "\titers: 200, epoch: 9 | loss: 0.1055641\n",
      "\tspeed: 0.0125s/iter; left time: 82.8937s\n",
      "\titers: 300, epoch: 9 | loss: 0.1617718\n",
      "\tspeed: 0.0115s/iter; left time: 75.1375s\n",
      "\titers: 400, epoch: 9 | loss: 0.1685493\n",
      "\tspeed: 0.0115s/iter; left time: 73.9814s\n",
      "\titers: 500, epoch: 9 | loss: 0.2229301\n",
      "\tspeed: 0.0115s/iter; left time: 72.8627s\n",
      "Epoch: 9 cost time: 7.041830778121948\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1456881 Vali Loss: 0.0330472 Test Loss: 0.1112786\n",
      "Validation loss decreased (0.033171 --> 0.033047).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1415712\n",
      "\tspeed: 0.0382s/iter; left time: 235.8174s\n",
      "\titers: 200, epoch: 10 | loss: 0.1169592\n",
      "\tspeed: 0.0114s/iter; left time: 69.4243s\n",
      "\titers: 300, epoch: 10 | loss: 0.1292818\n",
      "\tspeed: 0.0114s/iter; left time: 68.2733s\n",
      "\titers: 400, epoch: 10 | loss: 0.1493463\n",
      "\tspeed: 0.0114s/iter; left time: 67.1245s\n",
      "\titers: 500, epoch: 10 | loss: 0.1628127\n",
      "\tspeed: 0.0114s/iter; left time: 65.9772s\n",
      "Epoch: 10 cost time: 6.833430290222168\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1465767 Vali Loss: 0.0331408 Test Loss: 0.1112651\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1133842\n",
      "\tspeed: 0.0409s/iter; left time: 228.9959s\n",
      "\titers: 200, epoch: 11 | loss: 0.1636380\n",
      "\tspeed: 0.0125s/iter; left time: 69.0043s\n",
      "\titers: 300, epoch: 11 | loss: 0.1353102\n",
      "\tspeed: 0.0125s/iter; left time: 67.7424s\n",
      "\titers: 400, epoch: 11 | loss: 0.1801481\n",
      "\tspeed: 0.0121s/iter; left time: 64.0825s\n",
      "\titers: 500, epoch: 11 | loss: 0.2217366\n",
      "\tspeed: 0.0115s/iter; left time: 59.7744s\n",
      "Epoch: 11 cost time: 7.320434808731079\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1468188 Vali Loss: 0.0330274 Test Loss: 0.1112319\n",
      "Validation loss decreased (0.033047 --> 0.033027).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1327773\n",
      "\tspeed: 0.0386s/iter; left time: 194.3912s\n",
      "\titers: 200, epoch: 12 | loss: 0.1372503\n",
      "\tspeed: 0.0115s/iter; left time: 56.9289s\n",
      "\titers: 300, epoch: 12 | loss: 0.1172687\n",
      "\tspeed: 0.0116s/iter; left time: 55.8018s\n",
      "\titers: 400, epoch: 12 | loss: 0.2106243\n",
      "\tspeed: 0.0115s/iter; left time: 54.4962s\n",
      "\titers: 500, epoch: 12 | loss: 0.1307931\n",
      "\tspeed: 0.0115s/iter; left time: 53.4584s\n",
      "Epoch: 12 cost time: 6.9001405239105225\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1459731 Vali Loss: 0.0332833 Test Loss: 0.1112486\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2057044\n",
      "\tspeed: 0.0390s/iter; left time: 173.9394s\n",
      "\titers: 200, epoch: 13 | loss: 0.1851881\n",
      "\tspeed: 0.0114s/iter; left time: 49.8009s\n",
      "\titers: 300, epoch: 13 | loss: 0.1428877\n",
      "\tspeed: 0.0114s/iter; left time: 48.6942s\n",
      "\titers: 400, epoch: 13 | loss: 0.1263070\n",
      "\tspeed: 0.0114s/iter; left time: 47.5724s\n",
      "\titers: 500, epoch: 13 | loss: 0.1877145\n",
      "\tspeed: 0.0114s/iter; left time: 46.4549s\n",
      "Epoch: 13 cost time: 6.861037492752075\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1447158 Vali Loss: 0.0331614 Test Loss: 0.1112201\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1153721\n",
      "\tspeed: 0.0384s/iter; left time: 149.3471s\n",
      "\titers: 200, epoch: 14 | loss: 0.2060436\n",
      "\tspeed: 0.0115s/iter; left time: 43.5317s\n",
      "\titers: 300, epoch: 14 | loss: 0.1331390\n",
      "\tspeed: 0.0115s/iter; left time: 42.3854s\n",
      "\titers: 400, epoch: 14 | loss: 0.1472175\n",
      "\tspeed: 0.0115s/iter; left time: 41.2497s\n",
      "\titers: 500, epoch: 14 | loss: 0.2155699\n",
      "\tspeed: 0.0115s/iter; left time: 40.0517s\n",
      "Epoch: 14 cost time: 6.807992219924927\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1452995 Vali Loss: 0.0329629 Test Loss: 0.1111845\n",
      "Validation loss decreased (0.033027 --> 0.032963).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1701639\n",
      "\tspeed: 0.0380s/iter; left time: 126.2597s\n",
      "\titers: 200, epoch: 15 | loss: 0.1698841\n",
      "\tspeed: 0.0115s/iter; left time: 36.9983s\n",
      "\titers: 300, epoch: 15 | loss: 0.1489871\n",
      "\tspeed: 0.0115s/iter; left time: 35.8038s\n",
      "\titers: 400, epoch: 15 | loss: 0.1076717\n",
      "\tspeed: 0.0115s/iter; left time: 34.6979s\n",
      "\titers: 500, epoch: 15 | loss: 0.1245630\n",
      "\tspeed: 0.0115s/iter; left time: 33.5554s\n",
      "Epoch: 15 cost time: 6.866876602172852\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1444354 Vali Loss: 0.0330157 Test Loss: 0.1111896\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1521598\n",
      "\tspeed: 0.0395s/iter; left time: 108.7100s\n",
      "\titers: 200, epoch: 16 | loss: 0.1556416\n",
      "\tspeed: 0.0126s/iter; left time: 33.3007s\n",
      "\titers: 300, epoch: 16 | loss: 0.1187774\n",
      "\tspeed: 0.0126s/iter; left time: 32.1358s\n",
      "\titers: 400, epoch: 16 | loss: 0.0956007\n",
      "\tspeed: 0.0125s/iter; left time: 30.7205s\n",
      "\titers: 500, epoch: 16 | loss: 0.1450910\n",
      "\tspeed: 0.0126s/iter; left time: 29.5255s\n",
      "Epoch: 16 cost time: 7.435526609420776\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1461396 Vali Loss: 0.0330571 Test Loss: 0.1111890\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1512654\n",
      "\tspeed: 0.0392s/iter; left time: 85.3996s\n",
      "\titers: 200, epoch: 17 | loss: 0.0914336\n",
      "\tspeed: 0.0115s/iter; left time: 23.9116s\n",
      "\titers: 300, epoch: 17 | loss: 0.1645135\n",
      "\tspeed: 0.0115s/iter; left time: 22.7586s\n",
      "\titers: 400, epoch: 17 | loss: 0.0635268\n",
      "\tspeed: 0.0115s/iter; left time: 21.5647s\n",
      "\titers: 500, epoch: 17 | loss: 0.1457991\n",
      "\tspeed: 0.0115s/iter; left time: 20.4822s\n",
      "Epoch: 17 cost time: 6.829618453979492\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1452971 Vali Loss: 0.0331200 Test Loss: 0.1111891\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11135132610797882, mae:0.20238979160785675\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3023773\n",
      "\tspeed: 0.0239s/iter; left time: 269.7294s\n",
      "\titers: 200, epoch: 1 | loss: 0.3520382\n",
      "\tspeed: 0.0120s/iter; left time: 134.0128s\n",
      "\titers: 300, epoch: 1 | loss: 0.3291507\n",
      "\tspeed: 0.0120s/iter; left time: 133.2174s\n",
      "\titers: 400, epoch: 1 | loss: 0.2533366\n",
      "\tspeed: 0.0120s/iter; left time: 131.7074s\n",
      "\titers: 500, epoch: 1 | loss: 0.2025783\n",
      "\tspeed: 0.0120s/iter; left time: 130.5160s\n",
      "Epoch: 1 cost time: 8.070539474487305\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2592653 Vali Loss: 0.0470340 Test Loss: 0.1491200\n",
      "Validation loss decreased (inf --> 0.047034).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2725223\n",
      "\tspeed: 0.0401s/iter; left time: 430.1632s\n",
      "\titers: 200, epoch: 2 | loss: 0.3464039\n",
      "\tspeed: 0.0120s/iter; left time: 127.8107s\n",
      "\titers: 300, epoch: 2 | loss: 0.1615402\n",
      "\tspeed: 0.0121s/iter; left time: 126.9154s\n",
      "\titers: 400, epoch: 2 | loss: 0.1890907\n",
      "\tspeed: 0.0120s/iter; left time: 125.3905s\n",
      "\titers: 500, epoch: 2 | loss: 0.1945120\n",
      "\tspeed: 0.0120s/iter; left time: 124.4133s\n",
      "Epoch: 2 cost time: 7.176655054092407\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2294851 Vali Loss: 0.0506599 Test Loss: 0.1529285\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1668201\n",
      "\tspeed: 0.0375s/iter; left time: 381.1757s\n",
      "\titers: 200, epoch: 3 | loss: 0.1748754\n",
      "\tspeed: 0.0109s/iter; left time: 109.3894s\n",
      "\titers: 300, epoch: 3 | loss: 0.2789643\n",
      "\tspeed: 0.0109s/iter; left time: 108.5200s\n",
      "\titers: 400, epoch: 3 | loss: 0.1677472\n",
      "\tspeed: 0.0109s/iter; left time: 107.1105s\n",
      "\titers: 500, epoch: 3 | loss: 0.2055227\n",
      "\tspeed: 0.0109s/iter; left time: 106.0200s\n",
      "Epoch: 3 cost time: 6.488773584365845\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2052950 Vali Loss: 0.0422072 Test Loss: 0.1373033\n",
      "Validation loss decreased (0.047034 --> 0.042207).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1818763\n",
      "\tspeed: 0.0385s/iter; left time: 369.3323s\n",
      "\titers: 200, epoch: 4 | loss: 0.1296567\n",
      "\tspeed: 0.0108s/iter; left time: 102.7657s\n",
      "\titers: 300, epoch: 4 | loss: 0.1650539\n",
      "\tspeed: 0.0108s/iter; left time: 101.8453s\n",
      "\titers: 400, epoch: 4 | loss: 0.3428080\n",
      "\tspeed: 0.0108s/iter; left time: 100.7331s\n",
      "\titers: 500, epoch: 4 | loss: 0.1801892\n",
      "\tspeed: 0.0108s/iter; left time: 99.5861s\n",
      "Epoch: 4 cost time: 6.589330434799194\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1827566 Vali Loss: 0.0414400 Test Loss: 0.1408591\n",
      "Validation loss decreased (0.042207 --> 0.041440).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1668240\n",
      "\tspeed: 0.0397s/iter; left time: 358.1591s\n",
      "\titers: 200, epoch: 5 | loss: 0.1677529\n",
      "\tspeed: 0.0121s/iter; left time: 107.6264s\n",
      "\titers: 300, epoch: 5 | loss: 0.1709006\n",
      "\tspeed: 0.0121s/iter; left time: 106.5135s\n",
      "\titers: 400, epoch: 5 | loss: 0.1128015\n",
      "\tspeed: 0.0120s/iter; left time: 104.3695s\n",
      "\titers: 500, epoch: 5 | loss: 0.2561677\n",
      "\tspeed: 0.0109s/iter; left time: 93.6740s\n",
      "Epoch: 5 cost time: 6.996452808380127\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1705200 Vali Loss: 0.0390431 Test Loss: 0.1338034\n",
      "Validation loss decreased (0.041440 --> 0.039043).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1403041\n",
      "\tspeed: 0.0370s/iter; left time: 312.8802s\n",
      "\titers: 200, epoch: 6 | loss: 0.1482296\n",
      "\tspeed: 0.0109s/iter; left time: 90.6728s\n",
      "\titers: 300, epoch: 6 | loss: 0.1887355\n",
      "\tspeed: 0.0109s/iter; left time: 89.6775s\n",
      "\titers: 400, epoch: 6 | loss: 0.1394947\n",
      "\tspeed: 0.0109s/iter; left time: 88.5978s\n",
      "\titers: 500, epoch: 6 | loss: 0.1561777\n",
      "\tspeed: 0.0109s/iter; left time: 87.5674s\n",
      "Epoch: 6 cost time: 6.5009605884552\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1623241 Vali Loss: 0.0388792 Test Loss: 0.1365796\n",
      "Validation loss decreased (0.039043 --> 0.038879).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1332874\n",
      "\tspeed: 0.0377s/iter; left time: 297.0912s\n",
      "\titers: 200, epoch: 7 | loss: 0.1300848\n",
      "\tspeed: 0.0108s/iter; left time: 84.3664s\n",
      "\titers: 300, epoch: 7 | loss: 0.1099305\n",
      "\tspeed: 0.0108s/iter; left time: 83.3069s\n",
      "\titers: 400, epoch: 7 | loss: 0.1499352\n",
      "\tspeed: 0.0109s/iter; left time: 82.4273s\n",
      "\titers: 500, epoch: 7 | loss: 0.1161368\n",
      "\tspeed: 0.0109s/iter; left time: 81.2645s\n",
      "Epoch: 7 cost time: 6.517701625823975\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1599118 Vali Loss: 0.0385099 Test Loss: 0.1332457\n",
      "Validation loss decreased (0.038879 --> 0.038510).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1220800\n",
      "\tspeed: 0.0377s/iter; left time: 275.7029s\n",
      "\titers: 200, epoch: 8 | loss: 0.1919292\n",
      "\tspeed: 0.0109s/iter; left time: 78.4994s\n",
      "\titers: 300, epoch: 8 | loss: 0.2086399\n",
      "\tspeed: 0.0109s/iter; left time: 77.5096s\n",
      "\titers: 400, epoch: 8 | loss: 0.1443205\n",
      "\tspeed: 0.0109s/iter; left time: 76.1924s\n",
      "\titers: 500, epoch: 8 | loss: 0.0975310\n",
      "\tspeed: 0.0109s/iter; left time: 75.2352s\n",
      "Epoch: 8 cost time: 6.518476247787476\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1577346 Vali Loss: 0.0380855 Test Loss: 0.1331222\n",
      "Validation loss decreased (0.038510 --> 0.038086).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1655697\n",
      "\tspeed: 0.0395s/iter; left time: 266.5882s\n",
      "\titers: 200, epoch: 9 | loss: 0.1161146\n",
      "\tspeed: 0.0108s/iter; left time: 71.8879s\n",
      "\titers: 300, epoch: 9 | loss: 0.2196243\n",
      "\tspeed: 0.0108s/iter; left time: 70.8243s\n",
      "\titers: 400, epoch: 9 | loss: 0.2073046\n",
      "\tspeed: 0.0108s/iter; left time: 69.7758s\n",
      "\titers: 500, epoch: 9 | loss: 0.1035316\n",
      "\tspeed: 0.0108s/iter; left time: 68.7417s\n",
      "Epoch: 9 cost time: 6.471206426620483\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1557757 Vali Loss: 0.0381641 Test Loss: 0.1334061\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1572970\n",
      "\tspeed: 0.0375s/iter; left time: 231.1410s\n",
      "\titers: 200, epoch: 10 | loss: 0.0930295\n",
      "\tspeed: 0.0108s/iter; left time: 65.8239s\n",
      "\titers: 300, epoch: 10 | loss: 0.1732631\n",
      "\tspeed: 0.0108s/iter; left time: 64.7342s\n",
      "\titers: 400, epoch: 10 | loss: 0.1366965\n",
      "\tspeed: 0.0109s/iter; left time: 63.7232s\n",
      "\titers: 500, epoch: 10 | loss: 0.1793620\n",
      "\tspeed: 0.0108s/iter; left time: 62.5391s\n",
      "Epoch: 10 cost time: 6.481356382369995\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1560762 Vali Loss: 0.0381785 Test Loss: 0.1330903\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1224089\n",
      "\tspeed: 0.0363s/iter; left time: 203.2608s\n",
      "\titers: 200, epoch: 11 | loss: 0.1424767\n",
      "\tspeed: 0.0108s/iter; left time: 59.5787s\n",
      "\titers: 300, epoch: 11 | loss: 0.1435373\n",
      "\tspeed: 0.0108s/iter; left time: 58.4753s\n",
      "\titers: 400, epoch: 11 | loss: 0.1171310\n",
      "\tspeed: 0.0108s/iter; left time: 57.3919s\n",
      "\titers: 500, epoch: 11 | loss: 0.1325386\n",
      "\tspeed: 0.0108s/iter; left time: 56.2977s\n",
      "Epoch: 11 cost time: 6.450270891189575\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1562124 Vali Loss: 0.0382650 Test Loss: 0.1332655\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13330425322055817, mae:0.2314433604478836\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2088144\n",
      "\tspeed: 0.0283s/iter; left time: 319.4645s\n",
      "\titers: 200, epoch: 1 | loss: 0.1824291\n",
      "\tspeed: 0.0168s/iter; left time: 187.8583s\n",
      "\titers: 300, epoch: 1 | loss: 0.2788930\n",
      "\tspeed: 0.0168s/iter; left time: 186.1548s\n",
      "\titers: 400, epoch: 1 | loss: 0.2357036\n",
      "\tspeed: 0.0168s/iter; left time: 184.7473s\n",
      "\titers: 500, epoch: 1 | loss: 0.1783668\n",
      "\tspeed: 0.0152s/iter; left time: 166.1172s\n",
      "Epoch: 1 cost time: 10.448149919509888\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2419916 Vali Loss: 0.0462555 Test Loss: 0.1280871\n",
      "Validation loss decreased (inf --> 0.046256).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3006781\n",
      "\tspeed: 0.0473s/iter; left time: 507.6545s\n",
      "\titers: 200, epoch: 2 | loss: 0.1948502\n",
      "\tspeed: 0.0148s/iter; left time: 157.4958s\n",
      "\titers: 300, epoch: 2 | loss: 0.1645432\n",
      "\tspeed: 0.0148s/iter; left time: 155.6360s\n",
      "\titers: 400, epoch: 2 | loss: 0.1193968\n",
      "\tspeed: 0.0148s/iter; left time: 153.9883s\n",
      "\titers: 500, epoch: 2 | loss: 0.2239374\n",
      "\tspeed: 0.0147s/iter; left time: 152.2037s\n",
      "Epoch: 2 cost time: 8.752765655517578\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2050041 Vali Loss: 0.0459917 Test Loss: 0.1426752\n",
      "Validation loss decreased (0.046256 --> 0.045992).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3307565\n",
      "\tspeed: 0.0477s/iter; left time: 484.5046s\n",
      "\titers: 200, epoch: 3 | loss: 0.1560543\n",
      "\tspeed: 0.0146s/iter; left time: 147.3427s\n",
      "\titers: 300, epoch: 3 | loss: 0.0987948\n",
      "\tspeed: 0.0146s/iter; left time: 145.7704s\n",
      "\titers: 400, epoch: 3 | loss: 0.1366630\n",
      "\tspeed: 0.0146s/iter; left time: 143.8832s\n",
      "\titers: 500, epoch: 3 | loss: 0.1182564\n",
      "\tspeed: 0.0146s/iter; left time: 142.4587s\n",
      "Epoch: 3 cost time: 8.631799221038818\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1765109 Vali Loss: 0.0391808 Test Loss: 0.1271644\n",
      "Validation loss decreased (0.045992 --> 0.039181).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1397561\n",
      "\tspeed: 0.0485s/iter; left time: 464.9884s\n",
      "\titers: 200, epoch: 4 | loss: 0.1401083\n",
      "\tspeed: 0.0164s/iter; left time: 155.6732s\n",
      "\titers: 300, epoch: 4 | loss: 0.1984763\n",
      "\tspeed: 0.0164s/iter; left time: 154.1655s\n",
      "\titers: 400, epoch: 4 | loss: 0.1807085\n",
      "\tspeed: 0.0164s/iter; left time: 152.2945s\n",
      "\titers: 500, epoch: 4 | loss: 0.1222808\n",
      "\tspeed: 0.0164s/iter; left time: 150.4234s\n",
      "Epoch: 4 cost time: 9.630901575088501\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1574438 Vali Loss: 0.0374154 Test Loss: 0.1241968\n",
      "Validation loss decreased (0.039181 --> 0.037415).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1441234\n",
      "\tspeed: 0.0486s/iter; left time: 438.4507s\n",
      "\titers: 200, epoch: 5 | loss: 0.1478885\n",
      "\tspeed: 0.0146s/iter; left time: 130.1964s\n",
      "\titers: 300, epoch: 5 | loss: 0.1427533\n",
      "\tspeed: 0.0146s/iter; left time: 128.8453s\n",
      "\titers: 400, epoch: 5 | loss: 0.1568673\n",
      "\tspeed: 0.0146s/iter; left time: 127.1852s\n",
      "\titers: 500, epoch: 5 | loss: 0.1767756\n",
      "\tspeed: 0.0146s/iter; left time: 125.9823s\n",
      "Epoch: 5 cost time: 8.611501455307007\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1490328 Vali Loss: 0.0381137 Test Loss: 0.1268485\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1252706\n",
      "\tspeed: 0.0461s/iter; left time: 389.7548s\n",
      "\titers: 200, epoch: 6 | loss: 0.1413923\n",
      "\tspeed: 0.0147s/iter; left time: 122.7905s\n",
      "\titers: 300, epoch: 6 | loss: 0.1185930\n",
      "\tspeed: 0.0147s/iter; left time: 121.3692s\n",
      "\titers: 400, epoch: 6 | loss: 0.1176067\n",
      "\tspeed: 0.0147s/iter; left time: 119.6733s\n",
      "\titers: 500, epoch: 6 | loss: 0.1488429\n",
      "\tspeed: 0.0147s/iter; left time: 118.2620s\n",
      "Epoch: 6 cost time: 8.65410828590393\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1430332 Vali Loss: 0.0376692 Test Loss: 0.1223561\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1383386\n",
      "\tspeed: 0.0471s/iter; left time: 370.9412s\n",
      "\titers: 200, epoch: 7 | loss: 0.1056294\n",
      "\tspeed: 0.0147s/iter; left time: 114.5564s\n",
      "\titers: 300, epoch: 7 | loss: 0.1030869\n",
      "\tspeed: 0.0147s/iter; left time: 113.0121s\n",
      "\titers: 400, epoch: 7 | loss: 0.1361030\n",
      "\tspeed: 0.0147s/iter; left time: 111.4874s\n",
      "\titers: 500, epoch: 7 | loss: 0.2070615\n",
      "\tspeed: 0.0147s/iter; left time: 110.0587s\n",
      "Epoch: 7 cost time: 8.693596363067627\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1416667 Vali Loss: 0.0376299 Test Loss: 0.1246513\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12436190247535706, mae:0.21817733347415924\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3041493\n",
      "\tspeed: 0.0261s/iter; left time: 295.4402s\n",
      "\titers: 200, epoch: 1 | loss: 0.2585823\n",
      "\tspeed: 0.0143s/iter; left time: 160.7123s\n",
      "\titers: 300, epoch: 1 | loss: 0.1520086\n",
      "\tspeed: 0.0143s/iter; left time: 158.8655s\n",
      "\titers: 400, epoch: 1 | loss: 0.3295628\n",
      "\tspeed: 0.0143s/iter; left time: 157.5945s\n",
      "\titers: 500, epoch: 1 | loss: 0.1836459\n",
      "\tspeed: 0.0143s/iter; left time: 156.2134s\n",
      "Epoch: 1 cost time: 9.396763563156128\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2175849 Vali Loss: 0.0411615 Test Loss: 0.1243817\n",
      "Validation loss decreased (inf --> 0.041161).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1491904\n",
      "\tspeed: 0.0455s/iter; left time: 487.8898s\n",
      "\titers: 200, epoch: 2 | loss: 0.2791035\n",
      "\tspeed: 0.0142s/iter; left time: 151.4740s\n",
      "\titers: 300, epoch: 2 | loss: 0.1883979\n",
      "\tspeed: 0.0142s/iter; left time: 149.8137s\n",
      "\titers: 400, epoch: 2 | loss: 0.1699372\n",
      "\tspeed: 0.0142s/iter; left time: 148.4570s\n",
      "\titers: 500, epoch: 2 | loss: 0.1642059\n",
      "\tspeed: 0.0142s/iter; left time: 147.0015s\n",
      "Epoch: 2 cost time: 8.428610801696777\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1894586 Vali Loss: 0.0383928 Test Loss: 0.1226561\n",
      "Validation loss decreased (0.041161 --> 0.038393).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1771815\n",
      "\tspeed: 0.0470s/iter; left time: 477.2518s\n",
      "\titers: 200, epoch: 3 | loss: 0.0839687\n",
      "\tspeed: 0.0143s/iter; left time: 143.7776s\n",
      "\titers: 300, epoch: 3 | loss: 0.2152462\n",
      "\tspeed: 0.0143s/iter; left time: 142.3437s\n",
      "\titers: 400, epoch: 3 | loss: 0.1249974\n",
      "\tspeed: 0.0143s/iter; left time: 141.0179s\n",
      "\titers: 500, epoch: 3 | loss: 0.1507803\n",
      "\tspeed: 0.0143s/iter; left time: 139.6530s\n",
      "Epoch: 3 cost time: 8.47367000579834\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1661950 Vali Loss: 0.0359099 Test Loss: 0.1177011\n",
      "Validation loss decreased (0.038393 --> 0.035910).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1583731\n",
      "\tspeed: 0.0473s/iter; left time: 453.2171s\n",
      "\titers: 200, epoch: 4 | loss: 0.1549593\n",
      "\tspeed: 0.0143s/iter; left time: 135.5319s\n",
      "\titers: 300, epoch: 4 | loss: 0.1846991\n",
      "\tspeed: 0.0143s/iter; left time: 134.1701s\n",
      "\titers: 400, epoch: 4 | loss: 0.0967964\n",
      "\tspeed: 0.0143s/iter; left time: 132.7119s\n",
      "\titers: 500, epoch: 4 | loss: 0.1095245\n",
      "\tspeed: 0.0143s/iter; left time: 131.0345s\n",
      "Epoch: 4 cost time: 8.403527021408081\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1546359 Vali Loss: 0.0341666 Test Loss: 0.1127359\n",
      "Validation loss decreased (0.035910 --> 0.034167).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0815493\n",
      "\tspeed: 0.0452s/iter; left time: 408.0879s\n",
      "\titers: 200, epoch: 5 | loss: 0.1315108\n",
      "\tspeed: 0.0143s/iter; left time: 127.5060s\n",
      "\titers: 300, epoch: 5 | loss: 0.1942120\n",
      "\tspeed: 0.0143s/iter; left time: 126.2658s\n",
      "\titers: 400, epoch: 5 | loss: 0.1044161\n",
      "\tspeed: 0.0143s/iter; left time: 124.4282s\n",
      "\titers: 500, epoch: 5 | loss: 0.1637261\n",
      "\tspeed: 0.0142s/iter; left time: 122.6538s\n",
      "Epoch: 5 cost time: 8.435765027999878\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1448837 Vali Loss: 0.0341896 Test Loss: 0.1115746\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1139430\n",
      "\tspeed: 0.0473s/iter; left time: 399.4775s\n",
      "\titers: 200, epoch: 6 | loss: 0.2346027\n",
      "\tspeed: 0.0142s/iter; left time: 118.8620s\n",
      "\titers: 300, epoch: 6 | loss: 0.2354135\n",
      "\tspeed: 0.0142s/iter; left time: 117.5292s\n",
      "\titers: 400, epoch: 6 | loss: 0.1607544\n",
      "\tspeed: 0.0142s/iter; left time: 116.0735s\n",
      "\titers: 500, epoch: 6 | loss: 0.1464290\n",
      "\tspeed: 0.0142s/iter; left time: 114.6019s\n",
      "Epoch: 6 cost time: 8.41111969947815\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1417928 Vali Loss: 0.0337849 Test Loss: 0.1103700\n",
      "Validation loss decreased (0.034167 --> 0.033785).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1030021\n",
      "\tspeed: 0.0476s/iter; left time: 374.8027s\n",
      "\titers: 200, epoch: 7 | loss: 0.1366124\n",
      "\tspeed: 0.0143s/iter; left time: 111.2241s\n",
      "\titers: 300, epoch: 7 | loss: 0.2279001\n",
      "\tspeed: 0.0142s/iter; left time: 109.4388s\n",
      "\titers: 400, epoch: 7 | loss: 0.1248613\n",
      "\tspeed: 0.0143s/iter; left time: 108.3294s\n",
      "\titers: 500, epoch: 7 | loss: 0.1861413\n",
      "\tspeed: 0.0143s/iter; left time: 106.8846s\n",
      "Epoch: 7 cost time: 8.456965684890747\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1395018 Vali Loss: 0.0332101 Test Loss: 0.1089533\n",
      "Validation loss decreased (0.033785 --> 0.033210).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0943482\n",
      "\tspeed: 0.0454s/iter; left time: 331.9538s\n",
      "\titers: 200, epoch: 8 | loss: 0.1349510\n",
      "\tspeed: 0.0142s/iter; left time: 102.5177s\n",
      "\titers: 300, epoch: 8 | loss: 0.2093175\n",
      "\tspeed: 0.0142s/iter; left time: 101.1528s\n",
      "\titers: 400, epoch: 8 | loss: 0.1370911\n",
      "\tspeed: 0.0142s/iter; left time: 99.6604s\n",
      "\titers: 500, epoch: 8 | loss: 0.1521060\n",
      "\tspeed: 0.0142s/iter; left time: 98.3548s\n",
      "Epoch: 8 cost time: 8.404860973358154\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1379513 Vali Loss: 0.0333427 Test Loss: 0.1091910\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0990435\n",
      "\tspeed: 0.0464s/iter; left time: 312.7577s\n",
      "\titers: 200, epoch: 9 | loss: 0.1075702\n",
      "\tspeed: 0.0143s/iter; left time: 95.1387s\n",
      "\titers: 300, epoch: 9 | loss: 0.1758587\n",
      "\tspeed: 0.0143s/iter; left time: 93.2660s\n",
      "\titers: 400, epoch: 9 | loss: 0.1283492\n",
      "\tspeed: 0.0143s/iter; left time: 91.9989s\n",
      "\titers: 500, epoch: 9 | loss: 0.1375320\n",
      "\tspeed: 0.0143s/iter; left time: 90.5853s\n",
      "Epoch: 9 cost time: 8.421502828598022\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1371401 Vali Loss: 0.0331055 Test Loss: 0.1088744\n",
      "Validation loss decreased (0.033210 --> 0.033106).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1440584\n",
      "\tspeed: 0.0476s/iter; left time: 293.7555s\n",
      "\titers: 200, epoch: 10 | loss: 0.1124659\n",
      "\tspeed: 0.0143s/iter; left time: 86.6358s\n",
      "\titers: 300, epoch: 10 | loss: 0.1372264\n",
      "\tspeed: 0.0143s/iter; left time: 85.1483s\n",
      "\titers: 400, epoch: 10 | loss: 0.1360071\n",
      "\tspeed: 0.0143s/iter; left time: 83.6964s\n",
      "\titers: 500, epoch: 10 | loss: 0.1742094\n",
      "\tspeed: 0.0143s/iter; left time: 82.2898s\n",
      "Epoch: 10 cost time: 8.435937404632568\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1371056 Vali Loss: 0.0330677 Test Loss: 0.1088897\n",
      "Validation loss decreased (0.033106 --> 0.033068).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2021715\n",
      "\tspeed: 0.0482s/iter; left time: 269.8538s\n",
      "\titers: 200, epoch: 11 | loss: 0.1225517\n",
      "\tspeed: 0.0143s/iter; left time: 78.8695s\n",
      "\titers: 300, epoch: 11 | loss: 0.1704631\n",
      "\tspeed: 0.0143s/iter; left time: 77.4690s\n",
      "\titers: 400, epoch: 11 | loss: 0.0939213\n",
      "\tspeed: 0.0143s/iter; left time: 75.8953s\n",
      "\titers: 500, epoch: 11 | loss: 0.2140114\n",
      "\tspeed: 0.0143s/iter; left time: 74.3988s\n",
      "Epoch: 11 cost time: 8.50165843963623\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1350687 Vali Loss: 0.0329884 Test Loss: 0.1088659\n",
      "Validation loss decreased (0.033068 --> 0.032988).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1625562\n",
      "\tspeed: 0.0461s/iter; left time: 231.7295s\n",
      "\titers: 200, epoch: 12 | loss: 0.1708721\n",
      "\tspeed: 0.0143s/iter; left time: 70.4337s\n",
      "\titers: 300, epoch: 12 | loss: 0.1148917\n",
      "\tspeed: 0.0156s/iter; left time: 75.2812s\n",
      "\titers: 400, epoch: 12 | loss: 0.1476015\n",
      "\tspeed: 0.0160s/iter; left time: 75.8584s\n",
      "\titers: 500, epoch: 12 | loss: 0.1615509\n",
      "\tspeed: 0.0160s/iter; left time: 74.0879s\n",
      "Epoch: 12 cost time: 9.063811302185059\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1370205 Vali Loss: 0.0331084 Test Loss: 0.1088943\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1212869\n",
      "\tspeed: 0.0476s/iter; left time: 212.2523s\n",
      "\titers: 200, epoch: 13 | loss: 0.0968870\n",
      "\tspeed: 0.0143s/iter; left time: 62.5469s\n",
      "\titers: 300, epoch: 13 | loss: 0.1208057\n",
      "\tspeed: 0.0144s/iter; left time: 61.2581s\n",
      "\titers: 400, epoch: 13 | loss: 0.0852482\n",
      "\tspeed: 0.0144s/iter; left time: 59.7450s\n",
      "\titers: 500, epoch: 13 | loss: 0.1432980\n",
      "\tspeed: 0.0144s/iter; left time: 58.3959s\n",
      "Epoch: 13 cost time: 8.45564579963684\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1366182 Vali Loss: 0.0329717 Test Loss: 0.1089000\n",
      "Validation loss decreased (0.032988 --> 0.032972).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1207893\n",
      "\tspeed: 0.0462s/iter; left time: 179.8247s\n",
      "\titers: 200, epoch: 14 | loss: 0.2127361\n",
      "\tspeed: 0.0143s/iter; left time: 54.0978s\n",
      "\titers: 300, epoch: 14 | loss: 0.0937397\n",
      "\tspeed: 0.0143s/iter; left time: 52.6638s\n",
      "\titers: 400, epoch: 14 | loss: 0.0882648\n",
      "\tspeed: 0.0143s/iter; left time: 51.2175s\n",
      "\titers: 500, epoch: 14 | loss: 0.0805607\n",
      "\tspeed: 0.0142s/iter; left time: 49.7431s\n",
      "Epoch: 14 cost time: 8.44920039176941\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1345381 Vali Loss: 0.0330430 Test Loss: 0.1089277\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1321850\n",
      "\tspeed: 0.0458s/iter; left time: 152.0775s\n",
      "\titers: 200, epoch: 15 | loss: 0.1116377\n",
      "\tspeed: 0.0143s/iter; left time: 45.9723s\n",
      "\titers: 300, epoch: 15 | loss: 0.1358414\n",
      "\tspeed: 0.0143s/iter; left time: 44.5286s\n",
      "\titers: 400, epoch: 15 | loss: 0.1307736\n",
      "\tspeed: 0.0143s/iter; left time: 43.0631s\n",
      "\titers: 500, epoch: 15 | loss: 0.1802043\n",
      "\tspeed: 0.0143s/iter; left time: 41.6505s\n",
      "Epoch: 15 cost time: 8.447641849517822\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1356780 Vali Loss: 0.0330492 Test Loss: 0.1089440\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1402374\n",
      "\tspeed: 0.0465s/iter; left time: 127.7859s\n",
      "\titers: 200, epoch: 16 | loss: 0.1551928\n",
      "\tspeed: 0.0142s/iter; left time: 37.7533s\n",
      "\titers: 300, epoch: 16 | loss: 0.1137916\n",
      "\tspeed: 0.0143s/iter; left time: 36.3626s\n",
      "\titers: 400, epoch: 16 | loss: 0.1630017\n",
      "\tspeed: 0.0143s/iter; left time: 34.9277s\n",
      "\titers: 500, epoch: 16 | loss: 0.1067045\n",
      "\tspeed: 0.0143s/iter; left time: 33.5062s\n",
      "Epoch: 16 cost time: 8.457226037979126\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1356732 Vali Loss: 0.0331464 Test Loss: 0.1089477\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10903660207986832, mae:0.19916637241840363\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1381665\n",
      "\tspeed: 0.0278s/iter; left time: 314.4081s\n",
      "\titers: 200, epoch: 1 | loss: 0.3449165\n",
      "\tspeed: 0.0160s/iter; left time: 179.6421s\n",
      "\titers: 300, epoch: 1 | loss: 0.2914426\n",
      "\tspeed: 0.0160s/iter; left time: 177.9033s\n",
      "\titers: 400, epoch: 1 | loss: 0.2373642\n",
      "\tspeed: 0.0161s/iter; left time: 176.6340s\n",
      "\titers: 500, epoch: 1 | loss: 0.2826465\n",
      "\tspeed: 0.0161s/iter; left time: 175.4556s\n",
      "Epoch: 1 cost time: 10.376337051391602\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2493833 Vali Loss: 0.0467754 Test Loss: 0.1420881\n",
      "Validation loss decreased (inf --> 0.046775).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1900936\n",
      "\tspeed: 0.0475s/iter; left time: 509.8713s\n",
      "\titers: 200, epoch: 2 | loss: 0.2193269\n",
      "\tspeed: 0.0144s/iter; left time: 153.3413s\n",
      "\titers: 300, epoch: 2 | loss: 0.1439174\n",
      "\tspeed: 0.0144s/iter; left time: 151.7634s\n",
      "\titers: 400, epoch: 2 | loss: 0.1681918\n",
      "\tspeed: 0.0144s/iter; left time: 150.3335s\n",
      "\titers: 500, epoch: 2 | loss: 0.2402062\n",
      "\tspeed: 0.0144s/iter; left time: 149.1038s\n",
      "Epoch: 2 cost time: 8.545698404312134\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2090211 Vali Loss: 0.0434395 Test Loss: 0.1385605\n",
      "Validation loss decreased (0.046775 --> 0.043440).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2056979\n",
      "\tspeed: 0.0457s/iter; left time: 464.7102s\n",
      "\titers: 200, epoch: 3 | loss: 0.2125686\n",
      "\tspeed: 0.0143s/iter; left time: 144.2126s\n",
      "\titers: 300, epoch: 3 | loss: 0.1561041\n",
      "\tspeed: 0.0143s/iter; left time: 142.3847s\n",
      "\titers: 400, epoch: 3 | loss: 0.1571902\n",
      "\tspeed: 0.0143s/iter; left time: 141.0359s\n",
      "\titers: 500, epoch: 3 | loss: 0.2057050\n",
      "\tspeed: 0.0144s/iter; left time: 140.4524s\n",
      "Epoch: 3 cost time: 8.46859335899353\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1796085 Vali Loss: 0.0421454 Test Loss: 0.1335378\n",
      "Validation loss decreased (0.043440 --> 0.042145).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2165372\n",
      "\tspeed: 0.0474s/iter; left time: 454.9470s\n",
      "\titers: 200, epoch: 4 | loss: 0.1942196\n",
      "\tspeed: 0.0144s/iter; left time: 136.5413s\n",
      "\titers: 300, epoch: 4 | loss: 0.1731071\n",
      "\tspeed: 0.0144s/iter; left time: 135.2055s\n",
      "\titers: 400, epoch: 4 | loss: 0.1457002\n",
      "\tspeed: 0.0144s/iter; left time: 133.5109s\n",
      "\titers: 500, epoch: 4 | loss: 0.1088841\n",
      "\tspeed: 0.0143s/iter; left time: 131.8516s\n",
      "Epoch: 4 cost time: 8.465720891952515\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1631498 Vali Loss: 0.0403996 Test Loss: 0.1263688\n",
      "Validation loss decreased (0.042145 --> 0.040400).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1325023\n",
      "\tspeed: 0.0462s/iter; left time: 416.8528s\n",
      "\titers: 200, epoch: 5 | loss: 0.1555424\n",
      "\tspeed: 0.0143s/iter; left time: 127.9026s\n",
      "\titers: 300, epoch: 5 | loss: 0.1323898\n",
      "\tspeed: 0.0143s/iter; left time: 126.2341s\n",
      "\titers: 400, epoch: 5 | loss: 0.2263586\n",
      "\tspeed: 0.0143s/iter; left time: 125.1155s\n",
      "\titers: 500, epoch: 5 | loss: 0.1418733\n",
      "\tspeed: 0.0143s/iter; left time: 123.6100s\n",
      "Epoch: 5 cost time: 8.454774856567383\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1533076 Vali Loss: 0.0393731 Test Loss: 0.1267292\n",
      "Validation loss decreased (0.040400 --> 0.039373).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1757096\n",
      "\tspeed: 0.0455s/iter; left time: 384.6481s\n",
      "\titers: 200, epoch: 6 | loss: 0.1593852\n",
      "\tspeed: 0.0143s/iter; left time: 119.5281s\n",
      "\titers: 300, epoch: 6 | loss: 0.1035415\n",
      "\tspeed: 0.0143s/iter; left time: 117.9671s\n",
      "\titers: 400, epoch: 6 | loss: 0.1843241\n",
      "\tspeed: 0.0143s/iter; left time: 116.3378s\n",
      "\titers: 500, epoch: 6 | loss: 0.1459927\n",
      "\tspeed: 0.0144s/iter; left time: 115.6713s\n",
      "Epoch: 6 cost time: 8.468841314315796\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1486886 Vali Loss: 0.0387617 Test Loss: 0.1253418\n",
      "Validation loss decreased (0.039373 --> 0.038762).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1954851\n",
      "\tspeed: 0.0459s/iter; left time: 361.8052s\n",
      "\titers: 200, epoch: 7 | loss: 0.0975353\n",
      "\tspeed: 0.0144s/iter; left time: 112.0767s\n",
      "\titers: 300, epoch: 7 | loss: 0.0966628\n",
      "\tspeed: 0.0144s/iter; left time: 110.4170s\n",
      "\titers: 400, epoch: 7 | loss: 0.2145246\n",
      "\tspeed: 0.0144s/iter; left time: 108.8482s\n",
      "\titers: 500, epoch: 7 | loss: 0.1329321\n",
      "\tspeed: 0.0144s/iter; left time: 107.3684s\n",
      "Epoch: 7 cost time: 8.479830503463745\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1470587 Vali Loss: 0.0387335 Test Loss: 0.1235380\n",
      "Validation loss decreased (0.038762 --> 0.038734).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1238119\n",
      "\tspeed: 0.0481s/iter; left time: 351.5699s\n",
      "\titers: 200, epoch: 8 | loss: 0.1432588\n",
      "\tspeed: 0.0161s/iter; left time: 115.9629s\n",
      "\titers: 300, epoch: 8 | loss: 0.1247279\n",
      "\tspeed: 0.0161s/iter; left time: 114.4182s\n",
      "\titers: 400, epoch: 8 | loss: 0.1522371\n",
      "\tspeed: 0.0161s/iter; left time: 112.8314s\n",
      "\titers: 500, epoch: 8 | loss: 0.0975019\n",
      "\tspeed: 0.0161s/iter; left time: 111.0902s\n",
      "Epoch: 8 cost time: 9.46516227722168\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1456697 Vali Loss: 0.0395853 Test Loss: 0.1261854\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2091442\n",
      "\tspeed: 0.0488s/iter; left time: 328.7253s\n",
      "\titers: 200, epoch: 9 | loss: 0.1126706\n",
      "\tspeed: 0.0143s/iter; left time: 94.6984s\n",
      "\titers: 300, epoch: 9 | loss: 0.0992757\n",
      "\tspeed: 0.0142s/iter; left time: 92.9942s\n",
      "\titers: 400, epoch: 9 | loss: 0.1102065\n",
      "\tspeed: 0.0143s/iter; left time: 91.8461s\n",
      "\titers: 500, epoch: 9 | loss: 0.1507562\n",
      "\tspeed: 0.0142s/iter; left time: 90.3063s\n",
      "Epoch: 9 cost time: 8.406576871871948\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1442083 Vali Loss: 0.0391426 Test Loss: 0.1253360\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1900224\n",
      "\tspeed: 0.0457s/iter; left time: 282.0995s\n",
      "\titers: 200, epoch: 10 | loss: 0.1614278\n",
      "\tspeed: 0.0143s/iter; left time: 86.9082s\n",
      "\titers: 300, epoch: 10 | loss: 0.1242339\n",
      "\tspeed: 0.0143s/iter; left time: 85.3619s\n",
      "\titers: 400, epoch: 10 | loss: 0.1666936\n",
      "\tspeed: 0.0143s/iter; left time: 83.8430s\n",
      "\titers: 500, epoch: 10 | loss: 0.1359811\n",
      "\tspeed: 0.0143s/iter; left time: 82.3971s\n",
      "Epoch: 10 cost time: 8.43296504020691\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1436869 Vali Loss: 0.0391266 Test Loss: 0.1257459\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12369126081466675, mae:0.22086134552955627\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2958392\n",
      "\tspeed: 0.0285s/iter; left time: 322.2408s\n",
      "\titers: 200, epoch: 1 | loss: 0.1971780\n",
      "\tspeed: 0.0169s/iter; left time: 189.1001s\n",
      "\titers: 300, epoch: 1 | loss: 0.2727989\n",
      "\tspeed: 0.0169s/iter; left time: 187.3281s\n",
      "\titers: 400, epoch: 1 | loss: 0.2513632\n",
      "\tspeed: 0.0169s/iter; left time: 185.5309s\n",
      "\titers: 500, epoch: 1 | loss: 0.2136229\n",
      "\tspeed: 0.0169s/iter; left time: 184.0538s\n",
      "Epoch: 1 cost time: 10.839314937591553\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2550540 Vali Loss: 0.0474312 Test Loss: 0.1496193\n",
      "Validation loss decreased (inf --> 0.047431).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2432267\n",
      "\tspeed: 0.0536s/iter; left time: 575.5611s\n",
      "\titers: 200, epoch: 2 | loss: 0.2498724\n",
      "\tspeed: 0.0184s/iter; left time: 195.6841s\n",
      "\titers: 300, epoch: 2 | loss: 0.2779619\n",
      "\tspeed: 0.0184s/iter; left time: 193.8712s\n",
      "\titers: 400, epoch: 2 | loss: 0.1651098\n",
      "\tspeed: 0.0184s/iter; left time: 192.3050s\n",
      "\titers: 500, epoch: 2 | loss: 0.1844755\n",
      "\tspeed: 0.0184s/iter; left time: 190.0112s\n",
      "Epoch: 2 cost time: 10.833738803863525\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2384399 Vali Loss: 0.0586837 Test Loss: 0.1700942\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2058848\n",
      "\tspeed: 0.0511s/iter; left time: 519.3342s\n",
      "\titers: 200, epoch: 3 | loss: 0.1983583\n",
      "\tspeed: 0.0169s/iter; left time: 170.0875s\n",
      "\titers: 300, epoch: 3 | loss: 0.3410145\n",
      "\tspeed: 0.0169s/iter; left time: 168.5601s\n",
      "\titers: 400, epoch: 3 | loss: 0.1681196\n",
      "\tspeed: 0.0169s/iter; left time: 166.9948s\n",
      "\titers: 500, epoch: 3 | loss: 0.1727709\n",
      "\tspeed: 0.0169s/iter; left time: 165.0291s\n",
      "Epoch: 3 cost time: 9.908249139785767\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2248720 Vali Loss: 0.0464250 Test Loss: 0.1407486\n",
      "Validation loss decreased (0.047431 --> 0.046425).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1574557\n",
      "\tspeed: 0.0507s/iter; left time: 485.8731s\n",
      "\titers: 200, epoch: 4 | loss: 0.2051149\n",
      "\tspeed: 0.0170s/iter; left time: 161.7069s\n",
      "\titers: 300, epoch: 4 | loss: 0.2613001\n",
      "\tspeed: 0.0170s/iter; left time: 159.5988s\n",
      "\titers: 400, epoch: 4 | loss: 0.2122735\n",
      "\tspeed: 0.0170s/iter; left time: 157.9913s\n",
      "\titers: 500, epoch: 4 | loss: 0.2093787\n",
      "\tspeed: 0.0170s/iter; left time: 156.0262s\n",
      "Epoch: 4 cost time: 9.998785734176636\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1972890 Vali Loss: 0.0443675 Test Loss: 0.1331053\n",
      "Validation loss decreased (0.046425 --> 0.044367).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1237682\n",
      "\tspeed: 0.0514s/iter; left time: 463.2443s\n",
      "\titers: 200, epoch: 5 | loss: 0.2852994\n",
      "\tspeed: 0.0169s/iter; left time: 150.6697s\n",
      "\titers: 300, epoch: 5 | loss: 0.1794762\n",
      "\tspeed: 0.0169s/iter; left time: 149.1781s\n",
      "\titers: 400, epoch: 5 | loss: 0.2154945\n",
      "\tspeed: 0.0169s/iter; left time: 147.1516s\n",
      "\titers: 500, epoch: 5 | loss: 0.1389254\n",
      "\tspeed: 0.0169s/iter; left time: 145.4861s\n",
      "Epoch: 5 cost time: 9.947157382965088\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1862711 Vali Loss: 0.0417370 Test Loss: 0.1288609\n",
      "Validation loss decreased (0.044367 --> 0.041737).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2081187\n",
      "\tspeed: 0.0506s/iter; left time: 427.8846s\n",
      "\titers: 200, epoch: 6 | loss: 0.1423266\n",
      "\tspeed: 0.0170s/iter; left time: 141.5545s\n",
      "\titers: 300, epoch: 6 | loss: 0.2063590\n",
      "\tspeed: 0.0170s/iter; left time: 140.0016s\n",
      "\titers: 400, epoch: 6 | loss: 0.2357511\n",
      "\tspeed: 0.0170s/iter; left time: 138.2698s\n",
      "\titers: 500, epoch: 6 | loss: 0.1809550\n",
      "\tspeed: 0.0170s/iter; left time: 136.5288s\n",
      "Epoch: 6 cost time: 9.975769281387329\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1806376 Vali Loss: 0.0412314 Test Loss: 0.1279439\n",
      "Validation loss decreased (0.041737 --> 0.041231).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1608773\n",
      "\tspeed: 0.0511s/iter; left time: 403.0290s\n",
      "\titers: 200, epoch: 7 | loss: 0.2030581\n",
      "\tspeed: 0.0169s/iter; left time: 131.2317s\n",
      "\titers: 300, epoch: 7 | loss: 0.2310962\n",
      "\tspeed: 0.0169s/iter; left time: 129.6130s\n",
      "\titers: 400, epoch: 7 | loss: 0.1222981\n",
      "\tspeed: 0.0169s/iter; left time: 127.8520s\n",
      "\titers: 500, epoch: 7 | loss: 0.1308530\n",
      "\tspeed: 0.0169s/iter; left time: 126.1580s\n",
      "Epoch: 7 cost time: 9.919497966766357\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1756744 Vali Loss: 0.0416139 Test Loss: 0.1276427\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1781857\n",
      "\tspeed: 0.0544s/iter; left time: 397.3546s\n",
      "\titers: 200, epoch: 8 | loss: 0.1287111\n",
      "\tspeed: 0.0168s/iter; left time: 121.3585s\n",
      "\titers: 300, epoch: 8 | loss: 0.1429879\n",
      "\tspeed: 0.0169s/iter; left time: 119.8929s\n",
      "\titers: 400, epoch: 8 | loss: 0.1149336\n",
      "\tspeed: 0.0168s/iter; left time: 118.1317s\n",
      "\titers: 500, epoch: 8 | loss: 0.1402981\n",
      "\tspeed: 0.0168s/iter; left time: 116.2740s\n",
      "Epoch: 8 cost time: 10.037273645401001\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1750025 Vali Loss: 0.0413127 Test Loss: 0.1273950\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1620007\n",
      "\tspeed: 0.0513s/iter; left time: 345.9794s\n",
      "\titers: 200, epoch: 9 | loss: 0.1230296\n",
      "\tspeed: 0.0185s/iter; left time: 122.6134s\n",
      "\titers: 300, epoch: 9 | loss: 0.1626625\n",
      "\tspeed: 0.0184s/iter; left time: 120.5394s\n",
      "\titers: 400, epoch: 9 | loss: 0.2064057\n",
      "\tspeed: 0.0184s/iter; left time: 118.6376s\n",
      "\titers: 500, epoch: 9 | loss: 0.2402835\n",
      "\tspeed: 0.0184s/iter; left time: 116.9165s\n",
      "Epoch: 9 cost time: 10.793215274810791\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1738157 Vali Loss: 0.0414313 Test Loss: 0.1275823\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1281076818704605, mae:0.2205001711845398\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2068312\n",
      "\tspeed: 0.0281s/iter; left time: 317.8935s\n",
      "\titers: 200, epoch: 1 | loss: 0.1475040\n",
      "\tspeed: 0.0164s/iter; left time: 183.3206s\n",
      "\titers: 300, epoch: 1 | loss: 0.2655107\n",
      "\tspeed: 0.0163s/iter; left time: 181.4744s\n",
      "\titers: 400, epoch: 1 | loss: 0.1900390\n",
      "\tspeed: 0.0163s/iter; left time: 179.8589s\n",
      "\titers: 500, epoch: 1 | loss: 0.1433915\n",
      "\tspeed: 0.0164s/iter; left time: 178.5096s\n",
      "Epoch: 1 cost time: 10.557101249694824\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2343052 Vali Loss: 0.0426553 Test Loss: 0.1378862\n",
      "Validation loss decreased (inf --> 0.042655).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1608283\n",
      "\tspeed: 0.0507s/iter; left time: 544.1490s\n",
      "\titers: 200, epoch: 2 | loss: 0.1833935\n",
      "\tspeed: 0.0164s/iter; left time: 174.2195s\n",
      "\titers: 300, epoch: 2 | loss: 0.2451425\n",
      "\tspeed: 0.0164s/iter; left time: 172.2778s\n",
      "\titers: 400, epoch: 2 | loss: 0.3889318\n",
      "\tspeed: 0.0164s/iter; left time: 170.7457s\n",
      "\titers: 500, epoch: 2 | loss: 0.2141431\n",
      "\tspeed: 0.0164s/iter; left time: 168.9845s\n",
      "Epoch: 2 cost time: 9.647379875183105\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2236655 Vali Loss: 0.0419637 Test Loss: 0.1399358\n",
      "Validation loss decreased (0.042655 --> 0.041964).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2036266\n",
      "\tspeed: 0.0500s/iter; left time: 508.3199s\n",
      "\titers: 200, epoch: 3 | loss: 0.2122304\n",
      "\tspeed: 0.0164s/iter; left time: 164.8407s\n",
      "\titers: 300, epoch: 3 | loss: 0.1900365\n",
      "\tspeed: 0.0164s/iter; left time: 162.9499s\n",
      "\titers: 400, epoch: 3 | loss: 0.2440022\n",
      "\tspeed: 0.0164s/iter; left time: 161.5711s\n",
      "\titers: 500, epoch: 3 | loss: 0.1065650\n",
      "\tspeed: 0.0163s/iter; left time: 159.4970s\n",
      "Epoch: 3 cost time: 9.670368194580078\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1956232 Vali Loss: 0.0376617 Test Loss: 0.1246283\n",
      "Validation loss decreased (0.041964 --> 0.037662).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1733230\n",
      "\tspeed: 0.0516s/iter; left time: 494.4700s\n",
      "\titers: 200, epoch: 4 | loss: 0.3698699\n",
      "\tspeed: 0.0179s/iter; left time: 169.6777s\n",
      "\titers: 300, epoch: 4 | loss: 0.1629041\n",
      "\tspeed: 0.0179s/iter; left time: 167.6495s\n",
      "\titers: 400, epoch: 4 | loss: 0.1191096\n",
      "\tspeed: 0.0179s/iter; left time: 165.8533s\n",
      "\titers: 500, epoch: 4 | loss: 0.2356152\n",
      "\tspeed: 0.0179s/iter; left time: 164.0790s\n",
      "Epoch: 4 cost time: 10.500868082046509\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1789920 Vali Loss: 0.0367390 Test Loss: 0.1175551\n",
      "Validation loss decreased (0.037662 --> 0.036739).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1773712\n",
      "\tspeed: 0.0523s/iter; left time: 471.4618s\n",
      "\titers: 200, epoch: 5 | loss: 0.2478509\n",
      "\tspeed: 0.0163s/iter; left time: 145.7352s\n",
      "\titers: 300, epoch: 5 | loss: 0.1592406\n",
      "\tspeed: 0.0163s/iter; left time: 144.1095s\n",
      "\titers: 400, epoch: 5 | loss: 0.1151561\n",
      "\tspeed: 0.0163s/iter; left time: 142.4761s\n",
      "\titers: 500, epoch: 5 | loss: 0.2273076\n",
      "\tspeed: 0.0163s/iter; left time: 140.7854s\n",
      "Epoch: 5 cost time: 9.639588117599487\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1693784 Vali Loss: 0.0352099 Test Loss: 0.1135109\n",
      "Validation loss decreased (0.036739 --> 0.035210).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1581204\n",
      "\tspeed: 0.0506s/iter; left time: 427.6195s\n",
      "\titers: 200, epoch: 6 | loss: 0.2331288\n",
      "\tspeed: 0.0163s/iter; left time: 136.3150s\n",
      "\titers: 300, epoch: 6 | loss: 0.2776801\n",
      "\tspeed: 0.0163s/iter; left time: 134.7062s\n",
      "\titers: 400, epoch: 6 | loss: 0.1604757\n",
      "\tspeed: 0.0163s/iter; left time: 133.0879s\n",
      "\titers: 500, epoch: 6 | loss: 0.1558830\n",
      "\tspeed: 0.0163s/iter; left time: 131.4517s\n",
      "Epoch: 6 cost time: 9.636428594589233\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1640407 Vali Loss: 0.0348077 Test Loss: 0.1118976\n",
      "Validation loss decreased (0.035210 --> 0.034808).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2196183\n",
      "\tspeed: 0.0531s/iter; left time: 418.5698s\n",
      "\titers: 200, epoch: 7 | loss: 0.1099205\n",
      "\tspeed: 0.0173s/iter; left time: 134.4823s\n",
      "\titers: 300, epoch: 7 | loss: 0.1736868\n",
      "\tspeed: 0.0179s/iter; left time: 137.2705s\n",
      "\titers: 400, epoch: 7 | loss: 0.2072969\n",
      "\tspeed: 0.0179s/iter; left time: 135.4448s\n",
      "\titers: 500, epoch: 7 | loss: 0.2354021\n",
      "\tspeed: 0.0179s/iter; left time: 133.7909s\n",
      "Epoch: 7 cost time: 10.291216850280762\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1619769 Vali Loss: 0.0341742 Test Loss: 0.1114734\n",
      "Validation loss decreased (0.034808 --> 0.034174).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0833036\n",
      "\tspeed: 0.0526s/iter; left time: 384.8718s\n",
      "\titers: 200, epoch: 8 | loss: 0.2457558\n",
      "\tspeed: 0.0176s/iter; left time: 127.1420s\n",
      "\titers: 300, epoch: 8 | loss: 0.1971235\n",
      "\tspeed: 0.0164s/iter; left time: 116.3334s\n",
      "\titers: 400, epoch: 8 | loss: 0.1781801\n",
      "\tspeed: 0.0163s/iter; left time: 114.5034s\n",
      "\titers: 500, epoch: 8 | loss: 0.1443961\n",
      "\tspeed: 0.0164s/iter; left time: 113.0711s\n",
      "Epoch: 8 cost time: 9.924705266952515\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1589763 Vali Loss: 0.0339690 Test Loss: 0.1106487\n",
      "Validation loss decreased (0.034174 --> 0.033969).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0992123\n",
      "\tspeed: 0.0530s/iter; left time: 357.5010s\n",
      "\titers: 200, epoch: 9 | loss: 0.1586944\n",
      "\tspeed: 0.0164s/iter; left time: 108.9021s\n",
      "\titers: 300, epoch: 9 | loss: 0.0967666\n",
      "\tspeed: 0.0164s/iter; left time: 107.1550s\n",
      "\titers: 400, epoch: 9 | loss: 0.1272036\n",
      "\tspeed: 0.0164s/iter; left time: 105.4709s\n",
      "\titers: 500, epoch: 9 | loss: 0.0852321\n",
      "\tspeed: 0.0164s/iter; left time: 103.7935s\n",
      "Epoch: 9 cost time: 9.662972927093506\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1580761 Vali Loss: 0.0339173 Test Loss: 0.1107529\n",
      "Validation loss decreased (0.033969 --> 0.033917).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1669910\n",
      "\tspeed: 0.0499s/iter; left time: 307.7782s\n",
      "\titers: 200, epoch: 10 | loss: 0.1650951\n",
      "\tspeed: 0.0164s/iter; left time: 99.3241s\n",
      "\titers: 300, epoch: 10 | loss: 0.1311248\n",
      "\tspeed: 0.0163s/iter; left time: 97.6031s\n",
      "\titers: 400, epoch: 10 | loss: 0.1945249\n",
      "\tspeed: 0.0163s/iter; left time: 95.8499s\n",
      "\titers: 500, epoch: 10 | loss: 0.1179071\n",
      "\tspeed: 0.0163s/iter; left time: 94.1885s\n",
      "Epoch: 10 cost time: 9.647021293640137\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1578447 Vali Loss: 0.0338990 Test Loss: 0.1106659\n",
      "Validation loss decreased (0.033917 --> 0.033899).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1007985\n",
      "\tspeed: 0.0507s/iter; left time: 284.1840s\n",
      "\titers: 200, epoch: 11 | loss: 0.1268860\n",
      "\tspeed: 0.0164s/iter; left time: 90.1351s\n",
      "\titers: 300, epoch: 11 | loss: 0.1053247\n",
      "\tspeed: 0.0164s/iter; left time: 88.4082s\n",
      "\titers: 400, epoch: 11 | loss: 0.1901793\n",
      "\tspeed: 0.0164s/iter; left time: 86.7814s\n",
      "\titers: 500, epoch: 11 | loss: 0.1574086\n",
      "\tspeed: 0.0164s/iter; left time: 85.1883s\n",
      "Epoch: 11 cost time: 9.641788721084595\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1576925 Vali Loss: 0.0340112 Test Loss: 0.1106179\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1106339\n",
      "\tspeed: 0.0485s/iter; left time: 244.1176s\n",
      "\titers: 200, epoch: 12 | loss: 0.1610774\n",
      "\tspeed: 0.0163s/iter; left time: 80.3285s\n",
      "\titers: 300, epoch: 12 | loss: 0.0931303\n",
      "\tspeed: 0.0163s/iter; left time: 78.6510s\n",
      "\titers: 400, epoch: 12 | loss: 0.1648269\n",
      "\tspeed: 0.0163s/iter; left time: 76.9417s\n",
      "\titers: 500, epoch: 12 | loss: 0.2387092\n",
      "\tspeed: 0.0163s/iter; left time: 75.3128s\n",
      "Epoch: 12 cost time: 9.548754215240479\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1574193 Vali Loss: 0.0339695 Test Loss: 0.1105909\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2163615\n",
      "\tspeed: 0.0500s/iter; left time: 222.9688s\n",
      "\titers: 200, epoch: 13 | loss: 0.2399940\n",
      "\tspeed: 0.0179s/iter; left time: 77.8466s\n",
      "\titers: 300, epoch: 13 | loss: 0.2056655\n",
      "\tspeed: 0.0166s/iter; left time: 70.7060s\n",
      "\titers: 400, epoch: 13 | loss: 0.1324357\n",
      "\tspeed: 0.0164s/iter; left time: 68.1810s\n",
      "\titers: 500, epoch: 13 | loss: 0.1442983\n",
      "\tspeed: 0.0164s/iter; left time: 66.5420s\n",
      "Epoch: 13 cost time: 9.939443349838257\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1575063 Vali Loss: 0.0340451 Test Loss: 0.1105785\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11083490401506424, mae:0.20302008092403412\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2738749\n",
      "\tspeed: 0.0275s/iter; left time: 310.3619s\n",
      "\titers: 200, epoch: 1 | loss: 0.2410658\n",
      "\tspeed: 0.0155s/iter; left time: 173.6929s\n",
      "\titers: 300, epoch: 1 | loss: 0.1838185\n",
      "\tspeed: 0.0155s/iter; left time: 171.9475s\n",
      "\titers: 400, epoch: 1 | loss: 0.3203909\n",
      "\tspeed: 0.0155s/iter; left time: 170.5481s\n",
      "\titers: 500, epoch: 1 | loss: 0.3899314\n",
      "\tspeed: 0.0155s/iter; left time: 169.4090s\n",
      "Epoch: 1 cost time: 10.090262413024902\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2626514 Vali Loss: 0.0484837 Test Loss: 0.1565852\n",
      "Validation loss decreased (inf --> 0.048484).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2507294\n",
      "\tspeed: 0.0485s/iter; left time: 520.7494s\n",
      "\titers: 200, epoch: 2 | loss: 0.1554500\n",
      "\tspeed: 0.0155s/iter; left time: 164.8120s\n",
      "\titers: 300, epoch: 2 | loss: 0.2832112\n",
      "\tspeed: 0.0155s/iter; left time: 163.0682s\n",
      "\titers: 400, epoch: 2 | loss: 0.2342743\n",
      "\tspeed: 0.0155s/iter; left time: 161.4259s\n",
      "\titers: 500, epoch: 2 | loss: 0.3202891\n",
      "\tspeed: 0.0155s/iter; left time: 159.6520s\n",
      "Epoch: 2 cost time: 9.138121604919434\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2514978 Vali Loss: 0.0754417 Test Loss: 0.2028981\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3472426\n",
      "\tspeed: 0.0493s/iter; left time: 500.6107s\n",
      "\titers: 200, epoch: 3 | loss: 0.1513417\n",
      "\tspeed: 0.0166s/iter; left time: 167.3722s\n",
      "\titers: 300, epoch: 3 | loss: 0.3537053\n",
      "\tspeed: 0.0155s/iter; left time: 154.5596s\n",
      "\titers: 400, epoch: 3 | loss: 0.2166460\n",
      "\tspeed: 0.0155s/iter; left time: 152.8640s\n",
      "\titers: 500, epoch: 3 | loss: 0.2753240\n",
      "\tspeed: 0.0155s/iter; left time: 151.3949s\n",
      "Epoch: 3 cost time: 9.391047954559326\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2626431 Vali Loss: 0.0480382 Test Loss: 0.1509399\n",
      "Validation loss decreased (0.048484 --> 0.048038).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1961316\n",
      "\tspeed: 0.0496s/iter; left time: 475.7817s\n",
      "\titers: 200, epoch: 4 | loss: 0.2935966\n",
      "\tspeed: 0.0154s/iter; left time: 146.2607s\n",
      "\titers: 300, epoch: 4 | loss: 0.3480114\n",
      "\tspeed: 0.0154s/iter; left time: 144.7810s\n",
      "\titers: 400, epoch: 4 | loss: 0.2189986\n",
      "\tspeed: 0.0154s/iter; left time: 143.3231s\n",
      "\titers: 500, epoch: 4 | loss: 0.2833834\n",
      "\tspeed: 0.0154s/iter; left time: 141.5829s\n",
      "Epoch: 4 cost time: 9.212220430374146\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2311800 Vali Loss: 0.0482368 Test Loss: 0.1470153\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1678876\n",
      "\tspeed: 0.0485s/iter; left time: 437.4913s\n",
      "\titers: 200, epoch: 5 | loss: 0.1962808\n",
      "\tspeed: 0.0155s/iter; left time: 138.2203s\n",
      "\titers: 300, epoch: 5 | loss: 0.1936736\n",
      "\tspeed: 0.0155s/iter; left time: 136.5428s\n",
      "\titers: 400, epoch: 5 | loss: 0.1584071\n",
      "\tspeed: 0.0155s/iter; left time: 135.3497s\n",
      "\titers: 500, epoch: 5 | loss: 0.2072518\n",
      "\tspeed: 0.0155s/iter; left time: 133.6773s\n",
      "Epoch: 5 cost time: 9.101691246032715\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2162158 Vali Loss: 0.0466264 Test Loss: 0.1401968\n",
      "Validation loss decreased (0.048038 --> 0.046626).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1975028\n",
      "\tspeed: 0.0494s/iter; left time: 417.1441s\n",
      "\titers: 200, epoch: 6 | loss: 0.1984609\n",
      "\tspeed: 0.0155s/iter; left time: 129.2948s\n",
      "\titers: 300, epoch: 6 | loss: 0.1290805\n",
      "\tspeed: 0.0155s/iter; left time: 127.7308s\n",
      "\titers: 400, epoch: 6 | loss: 0.1410978\n",
      "\tspeed: 0.0155s/iter; left time: 126.2343s\n",
      "\titers: 500, epoch: 6 | loss: 0.1558844\n",
      "\tspeed: 0.0155s/iter; left time: 124.8247s\n",
      "Epoch: 6 cost time: 9.136810541152954\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2068572 Vali Loss: 0.0456033 Test Loss: 0.1415516\n",
      "Validation loss decreased (0.046626 --> 0.045603).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2529387\n",
      "\tspeed: 0.0498s/iter; left time: 392.5874s\n",
      "\titers: 200, epoch: 7 | loss: 0.1949639\n",
      "\tspeed: 0.0155s/iter; left time: 120.7829s\n",
      "\titers: 300, epoch: 7 | loss: 0.1584079\n",
      "\tspeed: 0.0155s/iter; left time: 119.1620s\n",
      "\titers: 400, epoch: 7 | loss: 0.2333105\n",
      "\tspeed: 0.0155s/iter; left time: 117.4209s\n",
      "\titers: 500, epoch: 7 | loss: 0.2131604\n",
      "\tspeed: 0.0155s/iter; left time: 115.8617s\n",
      "Epoch: 7 cost time: 9.139897346496582\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2030053 Vali Loss: 0.0455402 Test Loss: 0.1392490\n",
      "Validation loss decreased (0.045603 --> 0.045540).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1815846\n",
      "\tspeed: 0.0483s/iter; left time: 352.8234s\n",
      "\titers: 200, epoch: 8 | loss: 0.2485581\n",
      "\tspeed: 0.0154s/iter; left time: 111.1243s\n",
      "\titers: 300, epoch: 8 | loss: 0.1278086\n",
      "\tspeed: 0.0154s/iter; left time: 109.6528s\n",
      "\titers: 400, epoch: 8 | loss: 0.1952574\n",
      "\tspeed: 0.0154s/iter; left time: 108.0397s\n",
      "\titers: 500, epoch: 8 | loss: 0.1229836\n",
      "\tspeed: 0.0154s/iter; left time: 106.3601s\n",
      "Epoch: 8 cost time: 9.109086036682129\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2005512 Vali Loss: 0.0445065 Test Loss: 0.1393705\n",
      "Validation loss decreased (0.045540 --> 0.044507).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1886239\n",
      "\tspeed: 0.0482s/iter; left time: 325.0373s\n",
      "\titers: 200, epoch: 9 | loss: 0.1721874\n",
      "\tspeed: 0.0155s/iter; left time: 103.2036s\n",
      "\titers: 300, epoch: 9 | loss: 0.2327887\n",
      "\tspeed: 0.0155s/iter; left time: 101.7076s\n",
      "\titers: 400, epoch: 9 | loss: 0.1441329\n",
      "\tspeed: 0.0155s/iter; left time: 99.9730s\n",
      "\titers: 500, epoch: 9 | loss: 0.1836031\n",
      "\tspeed: 0.0154s/iter; left time: 97.9152s\n",
      "Epoch: 9 cost time: 9.143268823623657\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1996132 Vali Loss: 0.0446082 Test Loss: 0.1392387\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2538546\n",
      "\tspeed: 0.0484s/iter; left time: 298.8069s\n",
      "\titers: 200, epoch: 10 | loss: 0.2266675\n",
      "\tspeed: 0.0154s/iter; left time: 93.6166s\n",
      "\titers: 300, epoch: 10 | loss: 0.1425828\n",
      "\tspeed: 0.0154s/iter; left time: 92.1785s\n",
      "\titers: 400, epoch: 10 | loss: 0.2254750\n",
      "\tspeed: 0.0154s/iter; left time: 90.4436s\n",
      "\titers: 500, epoch: 10 | loss: 0.3943844\n",
      "\tspeed: 0.0154s/iter; left time: 88.8987s\n",
      "Epoch: 10 cost time: 9.082751750946045\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2003246 Vali Loss: 0.0444149 Test Loss: 0.1389218\n",
      "Validation loss decreased (0.044507 --> 0.044415).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2145433\n",
      "\tspeed: 0.0501s/iter; left time: 280.7556s\n",
      "\titers: 200, epoch: 11 | loss: 0.1607526\n",
      "\tspeed: 0.0171s/iter; left time: 93.9033s\n",
      "\titers: 300, epoch: 11 | loss: 0.1815906\n",
      "\tspeed: 0.0157s/iter; left time: 84.5967s\n",
      "\titers: 400, epoch: 11 | loss: 0.2530977\n",
      "\tspeed: 0.0167s/iter; left time: 88.6080s\n",
      "\titers: 500, epoch: 11 | loss: 0.2577681\n",
      "\tspeed: 0.0170s/iter; left time: 88.6510s\n",
      "Epoch: 11 cost time: 9.810497760772705\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1997284 Vali Loss: 0.0445360 Test Loss: 0.1388688\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1345384\n",
      "\tspeed: 0.0496s/iter; left time: 249.5023s\n",
      "\titers: 200, epoch: 12 | loss: 0.3895433\n",
      "\tspeed: 0.0155s/iter; left time: 76.5096s\n",
      "\titers: 300, epoch: 12 | loss: 0.2682382\n",
      "\tspeed: 0.0155s/iter; left time: 74.7900s\n",
      "\titers: 400, epoch: 12 | loss: 0.1882094\n",
      "\tspeed: 0.0155s/iter; left time: 73.4105s\n",
      "\titers: 500, epoch: 12 | loss: 0.1769363\n",
      "\tspeed: 0.0155s/iter; left time: 71.8469s\n",
      "Epoch: 12 cost time: 9.14151406288147\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2000720 Vali Loss: 0.0445321 Test Loss: 0.1389161\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2760415\n",
      "\tspeed: 0.0473s/iter; left time: 211.1145s\n",
      "\titers: 200, epoch: 13 | loss: 0.1822870\n",
      "\tspeed: 0.0154s/iter; left time: 67.1599s\n",
      "\titers: 300, epoch: 13 | loss: 0.1951600\n",
      "\tspeed: 0.0154s/iter; left time: 65.5237s\n",
      "\titers: 400, epoch: 13 | loss: 0.2494859\n",
      "\tspeed: 0.0154s/iter; left time: 63.8932s\n",
      "\titers: 500, epoch: 13 | loss: 0.1471906\n",
      "\tspeed: 0.0153s/iter; left time: 62.3355s\n",
      "Epoch: 13 cost time: 9.048424005508423\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2007352 Vali Loss: 0.0444837 Test Loss: 0.1388606\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1390725076198578, mae:0.2316804677248001\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1979800\n",
      "\tspeed: 0.0326s/iter; left time: 368.8988s\n",
      "\titers: 200, epoch: 1 | loss: 0.2329361\n",
      "\tspeed: 0.0208s/iter; left time: 233.1651s\n",
      "\titers: 300, epoch: 1 | loss: 0.3890593\n",
      "\tspeed: 0.0208s/iter; left time: 231.2810s\n",
      "\titers: 400, epoch: 1 | loss: 0.1947859\n",
      "\tspeed: 0.0208s/iter; left time: 229.3678s\n",
      "\titers: 500, epoch: 1 | loss: 0.1655631\n",
      "\tspeed: 0.0208s/iter; left time: 227.0225s\n",
      "Epoch: 1 cost time: 13.11115312576294\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2447270 Vali Loss: 0.0476489 Test Loss: 0.1459083\n",
      "Validation loss decreased (inf --> 0.047649).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3815386\n",
      "\tspeed: 0.0610s/iter; left time: 654.7736s\n",
      "\titers: 200, epoch: 2 | loss: 0.1765963\n",
      "\tspeed: 0.0209s/iter; left time: 222.3061s\n",
      "\titers: 300, epoch: 2 | loss: 0.1362110\n",
      "\tspeed: 0.0210s/iter; left time: 220.9059s\n",
      "\titers: 400, epoch: 2 | loss: 0.1704368\n",
      "\tspeed: 0.0209s/iter; left time: 218.0653s\n",
      "\titers: 500, epoch: 2 | loss: 0.1821329\n",
      "\tspeed: 0.0209s/iter; left time: 216.1040s\n",
      "Epoch: 2 cost time: 12.284221649169922\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2095919 Vali Loss: 0.0445460 Test Loss: 0.1347510\n",
      "Validation loss decreased (0.047649 --> 0.044546).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1477922\n",
      "\tspeed: 0.0579s/iter; left time: 588.2886s\n",
      "\titers: 200, epoch: 3 | loss: 0.1658632\n",
      "\tspeed: 0.0186s/iter; left time: 187.3165s\n",
      "\titers: 300, epoch: 3 | loss: 0.2155154\n",
      "\tspeed: 0.0186s/iter; left time: 185.3815s\n",
      "\titers: 400, epoch: 3 | loss: 0.1262809\n",
      "\tspeed: 0.0186s/iter; left time: 183.4741s\n",
      "\titers: 500, epoch: 3 | loss: 0.1303732\n",
      "\tspeed: 0.0186s/iter; left time: 181.6676s\n",
      "Epoch: 3 cost time: 10.898059606552124\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1854190 Vali Loss: 0.0436595 Test Loss: 0.1318966\n",
      "Validation loss decreased (0.044546 --> 0.043659).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1870910\n",
      "\tspeed: 0.0578s/iter; left time: 554.5740s\n",
      "\titers: 200, epoch: 4 | loss: 0.2351542\n",
      "\tspeed: 0.0186s/iter; left time: 176.5074s\n",
      "\titers: 300, epoch: 4 | loss: 0.1840209\n",
      "\tspeed: 0.0186s/iter; left time: 174.6957s\n",
      "\titers: 400, epoch: 4 | loss: 0.0944235\n",
      "\tspeed: 0.0186s/iter; left time: 172.6014s\n",
      "\titers: 500, epoch: 4 | loss: 0.1628318\n",
      "\tspeed: 0.0186s/iter; left time: 170.7546s\n",
      "Epoch: 4 cost time: 10.881864070892334\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1660591 Vali Loss: 0.0401228 Test Loss: 0.1307222\n",
      "Validation loss decreased (0.043659 --> 0.040123).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2354830\n",
      "\tspeed: 0.0609s/iter; left time: 549.7587s\n",
      "\titers: 200, epoch: 5 | loss: 0.1616431\n",
      "\tspeed: 0.0209s/iter; left time: 186.7907s\n",
      "\titers: 300, epoch: 5 | loss: 0.1415432\n",
      "\tspeed: 0.0209s/iter; left time: 184.3843s\n",
      "\titers: 400, epoch: 5 | loss: 0.1302704\n",
      "\tspeed: 0.0209s/iter; left time: 182.3200s\n",
      "\titers: 500, epoch: 5 | loss: 0.1366715\n",
      "\tspeed: 0.0209s/iter; left time: 180.1316s\n",
      "Epoch: 5 cost time: 12.297125577926636\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1549483 Vali Loss: 0.0393205 Test Loss: 0.1318874\n",
      "Validation loss decreased (0.040123 --> 0.039320).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0862106\n",
      "\tspeed: 0.0599s/iter; left time: 505.8721s\n",
      "\titers: 200, epoch: 6 | loss: 0.1426957\n",
      "\tspeed: 0.0186s/iter; left time: 155.5936s\n",
      "\titers: 300, epoch: 6 | loss: 0.1217947\n",
      "\tspeed: 0.0187s/iter; left time: 153.9948s\n",
      "\titers: 400, epoch: 6 | loss: 0.1642987\n",
      "\tspeed: 0.0187s/iter; left time: 152.2064s\n",
      "\titers: 500, epoch: 6 | loss: 0.1378561\n",
      "\tspeed: 0.0187s/iter; left time: 150.3653s\n",
      "Epoch: 6 cost time: 10.968008756637573\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1476896 Vali Loss: 0.0385074 Test Loss: 0.1282430\n",
      "Validation loss decreased (0.039320 --> 0.038507).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1076406\n",
      "\tspeed: 0.0562s/iter; left time: 443.1389s\n",
      "\titers: 200, epoch: 7 | loss: 0.1230809\n",
      "\tspeed: 0.0186s/iter; left time: 144.6945s\n",
      "\titers: 300, epoch: 7 | loss: 0.1243722\n",
      "\tspeed: 0.0186s/iter; left time: 142.8189s\n",
      "\titers: 400, epoch: 7 | loss: 0.1548696\n",
      "\tspeed: 0.0186s/iter; left time: 140.9159s\n",
      "\titers: 500, epoch: 7 | loss: 0.2566475\n",
      "\tspeed: 0.0186s/iter; left time: 139.0841s\n",
      "Epoch: 7 cost time: 10.904294967651367\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1445968 Vali Loss: 0.0385820 Test Loss: 0.1284373\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1278794\n",
      "\tspeed: 0.0566s/iter; left time: 414.0556s\n",
      "\titers: 200, epoch: 8 | loss: 0.1256059\n",
      "\tspeed: 0.0186s/iter; left time: 134.3463s\n",
      "\titers: 300, epoch: 8 | loss: 0.1016735\n",
      "\tspeed: 0.0186s/iter; left time: 132.5265s\n",
      "\titers: 400, epoch: 8 | loss: 0.1710809\n",
      "\tspeed: 0.0186s/iter; left time: 130.6339s\n",
      "\titers: 500, epoch: 8 | loss: 0.1221571\n",
      "\tspeed: 0.0187s/iter; left time: 128.9319s\n",
      "Epoch: 8 cost time: 10.919054508209229\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1430321 Vali Loss: 0.0382561 Test Loss: 0.1283470\n",
      "Validation loss decreased (0.038507 --> 0.038256).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0890513\n",
      "\tspeed: 0.0595s/iter; left time: 401.0338s\n",
      "\titers: 200, epoch: 9 | loss: 0.1911742\n",
      "\tspeed: 0.0185s/iter; left time: 123.1864s\n",
      "\titers: 300, epoch: 9 | loss: 0.0837665\n",
      "\tspeed: 0.0186s/iter; left time: 121.3380s\n",
      "\titers: 400, epoch: 9 | loss: 0.1713939\n",
      "\tspeed: 0.0185s/iter; left time: 119.3337s\n",
      "\titers: 500, epoch: 9 | loss: 0.0705790\n",
      "\tspeed: 0.0185s/iter; left time: 117.4699s\n",
      "Epoch: 9 cost time: 10.873522996902466\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1432252 Vali Loss: 0.0386749 Test Loss: 0.1292707\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1382120\n",
      "\tspeed: 0.0579s/iter; left time: 357.0262s\n",
      "\titers: 200, epoch: 10 | loss: 0.1594550\n",
      "\tspeed: 0.0186s/iter; left time: 112.9534s\n",
      "\titers: 300, epoch: 10 | loss: 0.1034489\n",
      "\tspeed: 0.0186s/iter; left time: 111.0560s\n",
      "\titers: 400, epoch: 10 | loss: 0.1186969\n",
      "\tspeed: 0.0186s/iter; left time: 109.1687s\n",
      "\titers: 500, epoch: 10 | loss: 0.1209665\n",
      "\tspeed: 0.0186s/iter; left time: 107.3602s\n",
      "Epoch: 10 cost time: 10.919167518615723\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1419138 Vali Loss: 0.0382800 Test Loss: 0.1293405\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1135975\n",
      "\tspeed: 0.0573s/iter; left time: 321.1093s\n",
      "\titers: 200, epoch: 11 | loss: 0.2162677\n",
      "\tspeed: 0.0186s/iter; left time: 102.5436s\n",
      "\titers: 300, epoch: 11 | loss: 0.1270639\n",
      "\tspeed: 0.0186s/iter; left time: 100.6226s\n",
      "\titers: 400, epoch: 11 | loss: 0.0953551\n",
      "\tspeed: 0.0186s/iter; left time: 98.8273s\n",
      "\titers: 500, epoch: 11 | loss: 0.2087691\n",
      "\tspeed: 0.0186s/iter; left time: 96.9139s\n",
      "Epoch: 11 cost time: 10.94358205795288\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1425096 Vali Loss: 0.0383815 Test Loss: 0.1293128\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12851981818675995, mae:0.22300393879413605\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4062195\n",
      "\tspeed: 0.0299s/iter; left time: 337.8763s\n",
      "\titers: 200, epoch: 1 | loss: 0.2367173\n",
      "\tspeed: 0.0182s/iter; left time: 203.9359s\n",
      "\titers: 300, epoch: 1 | loss: 0.1880140\n",
      "\tspeed: 0.0182s/iter; left time: 202.0882s\n",
      "\titers: 400, epoch: 1 | loss: 0.1825474\n",
      "\tspeed: 0.0182s/iter; left time: 200.1999s\n",
      "\titers: 500, epoch: 1 | loss: 0.2249993\n",
      "\tspeed: 0.0182s/iter; left time: 198.6001s\n",
      "Epoch: 1 cost time: 11.601240873336792\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2210236 Vali Loss: 0.0382343 Test Loss: 0.1270257\n",
      "Validation loss decreased (inf --> 0.038234).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1443575\n",
      "\tspeed: 0.0562s/iter; left time: 603.6077s\n",
      "\titers: 200, epoch: 2 | loss: 0.2346351\n",
      "\tspeed: 0.0182s/iter; left time: 193.6052s\n",
      "\titers: 300, epoch: 2 | loss: 0.3214695\n",
      "\tspeed: 0.0182s/iter; left time: 191.6843s\n",
      "\titers: 400, epoch: 2 | loss: 0.1769039\n",
      "\tspeed: 0.0182s/iter; left time: 189.4387s\n",
      "\titers: 500, epoch: 2 | loss: 0.1559059\n",
      "\tspeed: 0.0181s/iter; left time: 187.4731s\n",
      "Epoch: 2 cost time: 10.68709397315979\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1931597 Vali Loss: 0.0417072 Test Loss: 0.1257905\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1181805\n",
      "\tspeed: 0.0573s/iter; left time: 581.9628s\n",
      "\titers: 200, epoch: 3 | loss: 0.1790282\n",
      "\tspeed: 0.0184s/iter; left time: 185.1240s\n",
      "\titers: 300, epoch: 3 | loss: 0.1843037\n",
      "\tspeed: 0.0184s/iter; left time: 183.1656s\n",
      "\titers: 400, epoch: 3 | loss: 0.1559345\n",
      "\tspeed: 0.0184s/iter; left time: 181.6440s\n",
      "\titers: 500, epoch: 3 | loss: 0.1333293\n",
      "\tspeed: 0.0184s/iter; left time: 179.5548s\n",
      "Epoch: 3 cost time: 10.742154598236084\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1680607 Vali Loss: 0.0361680 Test Loss: 0.1142172\n",
      "Validation loss decreased (0.038234 --> 0.036168).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1780870\n",
      "\tspeed: 0.0560s/iter; left time: 537.0598s\n",
      "\titers: 200, epoch: 4 | loss: 0.1038720\n",
      "\tspeed: 0.0182s/iter; left time: 172.5318s\n",
      "\titers: 300, epoch: 4 | loss: 0.2237743\n",
      "\tspeed: 0.0182s/iter; left time: 170.7578s\n",
      "\titers: 400, epoch: 4 | loss: 0.1229403\n",
      "\tspeed: 0.0182s/iter; left time: 168.8281s\n",
      "\titers: 500, epoch: 4 | loss: 0.1177608\n",
      "\tspeed: 0.0182s/iter; left time: 167.1235s\n",
      "Epoch: 4 cost time: 10.66884970664978\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1538203 Vali Loss: 0.0342794 Test Loss: 0.1126993\n",
      "Validation loss decreased (0.036168 --> 0.034279).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1363425\n",
      "\tspeed: 0.0560s/iter; left time: 505.2507s\n",
      "\titers: 200, epoch: 5 | loss: 0.1396369\n",
      "\tspeed: 0.0183s/iter; left time: 163.0772s\n",
      "\titers: 300, epoch: 5 | loss: 0.1429909\n",
      "\tspeed: 0.0183s/iter; left time: 161.2278s\n",
      "\titers: 400, epoch: 5 | loss: 0.1354701\n",
      "\tspeed: 0.0182s/iter; left time: 159.1485s\n",
      "\titers: 500, epoch: 5 | loss: 0.1390233\n",
      "\tspeed: 0.0183s/iter; left time: 157.4801s\n",
      "Epoch: 5 cost time: 10.7041654586792\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1457334 Vali Loss: 0.0337416 Test Loss: 0.1117296\n",
      "Validation loss decreased (0.034279 --> 0.033742).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2053926\n",
      "\tspeed: 0.0560s/iter; left time: 473.2295s\n",
      "\titers: 200, epoch: 6 | loss: 0.1320635\n",
      "\tspeed: 0.0182s/iter; left time: 152.3028s\n",
      "\titers: 300, epoch: 6 | loss: 0.1648138\n",
      "\tspeed: 0.0182s/iter; left time: 150.3460s\n",
      "\titers: 400, epoch: 6 | loss: 0.1140018\n",
      "\tspeed: 0.0183s/iter; left time: 148.9685s\n",
      "\titers: 500, epoch: 6 | loss: 0.0900856\n",
      "\tspeed: 0.0182s/iter; left time: 146.7108s\n",
      "Epoch: 6 cost time: 10.700132846832275\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1408665 Vali Loss: 0.0336674 Test Loss: 0.1124479\n",
      "Validation loss decreased (0.033742 --> 0.033667).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1324584\n",
      "\tspeed: 0.0564s/iter; left time: 444.6882s\n",
      "\titers: 200, epoch: 7 | loss: 0.1815987\n",
      "\tspeed: 0.0183s/iter; left time: 142.0195s\n",
      "\titers: 300, epoch: 7 | loss: 0.1007532\n",
      "\tspeed: 0.0182s/iter; left time: 140.0296s\n",
      "\titers: 400, epoch: 7 | loss: 0.1524494\n",
      "\tspeed: 0.0182s/iter; left time: 137.8700s\n",
      "\titers: 500, epoch: 7 | loss: 0.1171991\n",
      "\tspeed: 0.0182s/iter; left time: 136.2057s\n",
      "Epoch: 7 cost time: 10.666775941848755\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1371023 Vali Loss: 0.0332240 Test Loss: 0.1123765\n",
      "Validation loss decreased (0.033667 --> 0.033224).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1047971\n",
      "\tspeed: 0.0572s/iter; left time: 418.0156s\n",
      "\titers: 200, epoch: 8 | loss: 0.1174079\n",
      "\tspeed: 0.0181s/iter; left time: 130.5103s\n",
      "\titers: 300, epoch: 8 | loss: 0.1243928\n",
      "\tspeed: 0.0181s/iter; left time: 128.5077s\n",
      "\titers: 400, epoch: 8 | loss: 0.1190089\n",
      "\tspeed: 0.0181s/iter; left time: 126.7153s\n",
      "\titers: 500, epoch: 8 | loss: 0.1612852\n",
      "\tspeed: 0.0181s/iter; left time: 125.1172s\n",
      "Epoch: 8 cost time: 10.64265513420105\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1349000 Vali Loss: 0.0330598 Test Loss: 0.1125979\n",
      "Validation loss decreased (0.033224 --> 0.033060).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0788062\n",
      "\tspeed: 0.0557s/iter; left time: 375.1647s\n",
      "\titers: 200, epoch: 9 | loss: 0.1062236\n",
      "\tspeed: 0.0182s/iter; left time: 120.8928s\n",
      "\titers: 300, epoch: 9 | loss: 0.1499604\n",
      "\tspeed: 0.0182s/iter; left time: 119.2844s\n",
      "\titers: 400, epoch: 9 | loss: 0.1308397\n",
      "\tspeed: 0.0182s/iter; left time: 117.5093s\n",
      "\titers: 500, epoch: 9 | loss: 0.1163688\n",
      "\tspeed: 0.0182s/iter; left time: 115.3787s\n",
      "Epoch: 9 cost time: 10.688637018203735\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1347633 Vali Loss: 0.0328941 Test Loss: 0.1120978\n",
      "Validation loss decreased (0.033060 --> 0.032894).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1081880\n",
      "\tspeed: 0.0557s/iter; left time: 343.8169s\n",
      "\titers: 200, epoch: 10 | loss: 0.0949696\n",
      "\tspeed: 0.0183s/iter; left time: 111.0648s\n",
      "\titers: 300, epoch: 10 | loss: 0.1786988\n",
      "\tspeed: 0.0182s/iter; left time: 108.9389s\n",
      "\titers: 400, epoch: 10 | loss: 0.1464879\n",
      "\tspeed: 0.0183s/iter; left time: 107.2209s\n",
      "\titers: 500, epoch: 10 | loss: 0.1142340\n",
      "\tspeed: 0.0182s/iter; left time: 105.2005s\n",
      "Epoch: 10 cost time: 10.755906581878662\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1351930 Vali Loss: 0.0327680 Test Loss: 0.1121678\n",
      "Validation loss decreased (0.032894 --> 0.032768).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1656882\n",
      "\tspeed: 0.0552s/iter; left time: 309.3650s\n",
      "\titers: 200, epoch: 11 | loss: 0.1380123\n",
      "\tspeed: 0.0182s/iter; left time: 100.1090s\n",
      "\titers: 300, epoch: 11 | loss: 0.0956098\n",
      "\tspeed: 0.0182s/iter; left time: 98.2830s\n",
      "\titers: 400, epoch: 11 | loss: 0.1846106\n",
      "\tspeed: 0.0182s/iter; left time: 96.5004s\n",
      "\titers: 500, epoch: 11 | loss: 0.1490075\n",
      "\tspeed: 0.0182s/iter; left time: 94.6060s\n",
      "Epoch: 11 cost time: 10.642504930496216\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1345975 Vali Loss: 0.0328031 Test Loss: 0.1123543\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1867373\n",
      "\tspeed: 0.0566s/iter; left time: 284.7299s\n",
      "\titers: 200, epoch: 12 | loss: 0.1077482\n",
      "\tspeed: 0.0182s/iter; left time: 89.9878s\n",
      "\titers: 300, epoch: 12 | loss: 0.0930673\n",
      "\tspeed: 0.0182s/iter; left time: 88.0170s\n",
      "\titers: 400, epoch: 12 | loss: 0.1318797\n",
      "\tspeed: 0.0182s/iter; left time: 86.1607s\n",
      "\titers: 500, epoch: 12 | loss: 0.1422749\n",
      "\tspeed: 0.0182s/iter; left time: 84.4182s\n",
      "Epoch: 12 cost time: 10.74021053314209\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1345187 Vali Loss: 0.0327841 Test Loss: 0.1122793\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1140422\n",
      "\tspeed: 0.0553s/iter; left time: 246.5143s\n",
      "\titers: 200, epoch: 13 | loss: 0.1710508\n",
      "\tspeed: 0.0182s/iter; left time: 79.1951s\n",
      "\titers: 300, epoch: 13 | loss: 0.1083229\n",
      "\tspeed: 0.0182s/iter; left time: 77.3538s\n",
      "\titers: 400, epoch: 13 | loss: 0.1506232\n",
      "\tspeed: 0.0181s/iter; left time: 75.3389s\n",
      "\titers: 500, epoch: 13 | loss: 0.2035553\n",
      "\tspeed: 0.0181s/iter; left time: 73.5155s\n",
      "Epoch: 13 cost time: 10.615530014038086\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1355682 Vali Loss: 0.0328358 Test Loss: 0.1122939\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1123042106628418, mae:0.20286034047603607\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2797449\n",
      "\tspeed: 0.0326s/iter; left time: 368.5282s\n",
      "\titers: 200, epoch: 1 | loss: 0.1862150\n",
      "\tspeed: 0.0205s/iter; left time: 229.3529s\n",
      "\titers: 300, epoch: 1 | loss: 0.3039144\n",
      "\tspeed: 0.0205s/iter; left time: 228.0891s\n",
      "\titers: 400, epoch: 1 | loss: 0.2908368\n",
      "\tspeed: 0.0205s/iter; left time: 225.8859s\n",
      "\titers: 500, epoch: 1 | loss: 0.3222593\n",
      "\tspeed: 0.0199s/iter; left time: 216.9477s\n",
      "Epoch: 1 cost time: 12.74429702758789\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2490867 Vali Loss: 0.0448612 Test Loss: 0.1411850\n",
      "Validation loss decreased (inf --> 0.044861).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3115781\n",
      "\tspeed: 0.0579s/iter; left time: 621.1804s\n",
      "\titers: 200, epoch: 2 | loss: 0.3009275\n",
      "\tspeed: 0.0184s/iter; left time: 195.7296s\n",
      "\titers: 300, epoch: 2 | loss: 0.4145051\n",
      "\tspeed: 0.0183s/iter; left time: 192.8845s\n",
      "\titers: 400, epoch: 2 | loss: 0.1729305\n",
      "\tspeed: 0.0183s/iter; left time: 190.8061s\n",
      "\titers: 500, epoch: 2 | loss: 0.1927388\n",
      "\tspeed: 0.0183s/iter; left time: 188.8737s\n",
      "Epoch: 2 cost time: 10.792171955108643\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2132458 Vali Loss: 0.0427954 Test Loss: 0.1374245\n",
      "Validation loss decreased (0.044861 --> 0.042795).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1280790\n",
      "\tspeed: 0.0555s/iter; left time: 563.9455s\n",
      "\titers: 200, epoch: 3 | loss: 0.2055959\n",
      "\tspeed: 0.0182s/iter; left time: 183.3218s\n",
      "\titers: 300, epoch: 3 | loss: 0.2088492\n",
      "\tspeed: 0.0182s/iter; left time: 181.1762s\n",
      "\titers: 400, epoch: 3 | loss: 0.2187045\n",
      "\tspeed: 0.0182s/iter; left time: 179.2245s\n",
      "\titers: 500, epoch: 3 | loss: 0.1717113\n",
      "\tspeed: 0.0182s/iter; left time: 177.5106s\n",
      "Epoch: 3 cost time: 10.664822101593018\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1804804 Vali Loss: 0.0406638 Test Loss: 0.1281530\n",
      "Validation loss decreased (0.042795 --> 0.040664).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1274240\n",
      "\tspeed: 0.0575s/iter; left time: 551.8230s\n",
      "\titers: 200, epoch: 4 | loss: 0.1697613\n",
      "\tspeed: 0.0184s/iter; left time: 174.3866s\n",
      "\titers: 300, epoch: 4 | loss: 0.1532433\n",
      "\tspeed: 0.0184s/iter; left time: 172.3718s\n",
      "\titers: 400, epoch: 4 | loss: 0.1963158\n",
      "\tspeed: 0.0183s/iter; left time: 170.4148s\n",
      "\titers: 500, epoch: 4 | loss: 0.1010367\n",
      "\tspeed: 0.0183s/iter; left time: 168.5997s\n",
      "Epoch: 4 cost time: 10.795193910598755\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1598551 Vali Loss: 0.0383151 Test Loss: 0.1256488\n",
      "Validation loss decreased (0.040664 --> 0.038315).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1172608\n",
      "\tspeed: 0.0587s/iter; left time: 529.6851s\n",
      "\titers: 200, epoch: 5 | loss: 0.1494514\n",
      "\tspeed: 0.0206s/iter; left time: 183.5128s\n",
      "\titers: 300, epoch: 5 | loss: 0.1080816\n",
      "\tspeed: 0.0207s/iter; left time: 182.2461s\n",
      "\titers: 400, epoch: 5 | loss: 0.1401497\n",
      "\tspeed: 0.0208s/iter; left time: 181.6674s\n",
      "\titers: 500, epoch: 5 | loss: 0.0738565\n",
      "\tspeed: 0.0212s/iter; left time: 182.8644s\n",
      "Epoch: 5 cost time: 12.202595710754395\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1475491 Vali Loss: 0.0380924 Test Loss: 0.1269571\n",
      "Validation loss decreased (0.038315 --> 0.038092).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1919727\n",
      "\tspeed: 0.0629s/iter; left time: 531.4757s\n",
      "\titers: 200, epoch: 6 | loss: 0.0946734\n",
      "\tspeed: 0.0206s/iter; left time: 171.9608s\n",
      "\titers: 300, epoch: 6 | loss: 0.2285027\n",
      "\tspeed: 0.0206s/iter; left time: 169.6167s\n",
      "\titers: 400, epoch: 6 | loss: 0.1166387\n",
      "\tspeed: 0.0206s/iter; left time: 167.7097s\n",
      "\titers: 500, epoch: 6 | loss: 0.1483490\n",
      "\tspeed: 0.0206s/iter; left time: 165.5569s\n",
      "Epoch: 6 cost time: 12.324496984481812\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1427690 Vali Loss: 0.0374106 Test Loss: 0.1276500\n",
      "Validation loss decreased (0.038092 --> 0.037411).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1298775\n",
      "\tspeed: 0.0610s/iter; left time: 480.9496s\n",
      "\titers: 200, epoch: 7 | loss: 0.1101149\n",
      "\tspeed: 0.0183s/iter; left time: 142.6634s\n",
      "\titers: 300, epoch: 7 | loss: 0.1458274\n",
      "\tspeed: 0.0183s/iter; left time: 140.8128s\n",
      "\titers: 400, epoch: 7 | loss: 0.1348525\n",
      "\tspeed: 0.0183s/iter; left time: 138.7714s\n",
      "\titers: 500, epoch: 7 | loss: 0.1037479\n",
      "\tspeed: 0.0183s/iter; left time: 136.8540s\n",
      "Epoch: 7 cost time: 10.762275218963623\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1406734 Vali Loss: 0.0375035 Test Loss: 0.1276350\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1153444\n",
      "\tspeed: 0.0586s/iter; left time: 428.5507s\n",
      "\titers: 200, epoch: 8 | loss: 0.0958842\n",
      "\tspeed: 0.0183s/iter; left time: 132.0319s\n",
      "\titers: 300, epoch: 8 | loss: 0.1312107\n",
      "\tspeed: 0.0183s/iter; left time: 130.0459s\n",
      "\titers: 400, epoch: 8 | loss: 0.1608980\n",
      "\tspeed: 0.0183s/iter; left time: 127.9883s\n",
      "\titers: 500, epoch: 8 | loss: 0.1197002\n",
      "\tspeed: 0.0182s/iter; left time: 125.9911s\n",
      "Epoch: 8 cost time: 10.742356538772583\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1382457 Vali Loss: 0.0379177 Test Loss: 0.1280280\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1367284\n",
      "\tspeed: 0.0566s/iter; left time: 381.4437s\n",
      "\titers: 200, epoch: 9 | loss: 0.1761189\n",
      "\tspeed: 0.0181s/iter; left time: 120.4398s\n",
      "\titers: 300, epoch: 9 | loss: 0.1626104\n",
      "\tspeed: 0.0181s/iter; left time: 118.5878s\n",
      "\titers: 400, epoch: 9 | loss: 0.2542892\n",
      "\tspeed: 0.0202s/iter; left time: 129.9366s\n",
      "\titers: 500, epoch: 9 | loss: 0.1088795\n",
      "\tspeed: 0.0205s/iter; left time: 129.9902s\n",
      "Epoch: 9 cost time: 11.418935060501099\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1378798 Vali Loss: 0.0379509 Test Loss: 0.1280553\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12781256437301636, mae:0.22418509423732758\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2265389\n",
      "\tspeed: 0.0340s/iter; left time: 384.5241s\n",
      "\titers: 200, epoch: 1 | loss: 0.2037170\n",
      "\tspeed: 0.0220s/iter; left time: 246.2110s\n",
      "\titers: 300, epoch: 1 | loss: 0.2611356\n",
      "\tspeed: 0.0220s/iter; left time: 244.6762s\n",
      "\titers: 400, epoch: 1 | loss: 0.3736100\n",
      "\tspeed: 0.0220s/iter; left time: 242.0488s\n",
      "\titers: 500, epoch: 1 | loss: 0.3269736\n",
      "\tspeed: 0.0219s/iter; left time: 238.8339s\n",
      "Epoch: 1 cost time: 13.780522584915161\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2639419 Vali Loss: 0.0465951 Test Loss: 0.1468236\n",
      "Validation loss decreased (inf --> 0.046595).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3247042\n",
      "\tspeed: 0.0635s/iter; left time: 681.4150s\n",
      "\titers: 200, epoch: 2 | loss: 0.2474189\n",
      "\tspeed: 0.0220s/iter; left time: 234.1854s\n",
      "\titers: 300, epoch: 2 | loss: 0.2419414\n",
      "\tspeed: 0.0220s/iter; left time: 231.7278s\n",
      "\titers: 400, epoch: 2 | loss: 0.3111796\n",
      "\tspeed: 0.0220s/iter; left time: 229.6074s\n",
      "\titers: 500, epoch: 2 | loss: 0.2558926\n",
      "\tspeed: 0.0220s/iter; left time: 227.5018s\n",
      "Epoch: 2 cost time: 12.888434886932373\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2886750 Vali Loss: 0.0646475 Test Loss: 0.1807764\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4931170\n",
      "\tspeed: 0.0678s/iter; left time: 689.1295s\n",
      "\titers: 200, epoch: 3 | loss: 0.2203465\n",
      "\tspeed: 0.0227s/iter; left time: 228.1807s\n",
      "\titers: 300, epoch: 3 | loss: 0.3041163\n",
      "\tspeed: 0.0221s/iter; left time: 219.9952s\n",
      "\titers: 400, epoch: 3 | loss: 0.2258451\n",
      "\tspeed: 0.0221s/iter; left time: 217.9872s\n",
      "\titers: 500, epoch: 3 | loss: 0.1609135\n",
      "\tspeed: 0.0221s/iter; left time: 215.5131s\n",
      "Epoch: 3 cost time: 13.159153461456299\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2678624 Vali Loss: 0.0592624 Test Loss: 0.1633920\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1885575\n",
      "\tspeed: 0.0641s/iter; left time: 615.2616s\n",
      "\titers: 200, epoch: 4 | loss: 0.2064136\n",
      "\tspeed: 0.0220s/iter; left time: 209.1760s\n",
      "\titers: 300, epoch: 4 | loss: 0.2522214\n",
      "\tspeed: 0.0220s/iter; left time: 206.6168s\n",
      "\titers: 400, epoch: 4 | loss: 0.2098793\n",
      "\tspeed: 0.0220s/iter; left time: 204.5435s\n",
      "\titers: 500, epoch: 4 | loss: 0.3203610\n",
      "\tspeed: 0.0220s/iter; left time: 202.3057s\n",
      "Epoch: 4 cost time: 12.835817098617554\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2464373 Vali Loss: 0.0537332 Test Loss: 0.1535708\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14701522886753082, mae:0.238888680934906\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2473196\n",
      "\tspeed: 0.0330s/iter; left time: 373.4403s\n",
      "\titers: 200, epoch: 1 | loss: 0.1257081\n",
      "\tspeed: 0.0212s/iter; left time: 237.9155s\n",
      "\titers: 300, epoch: 1 | loss: 0.1898378\n",
      "\tspeed: 0.0213s/iter; left time: 235.9009s\n",
      "\titers: 400, epoch: 1 | loss: 0.2509260\n",
      "\tspeed: 0.0213s/iter; left time: 233.9049s\n",
      "\titers: 500, epoch: 1 | loss: 0.2319994\n",
      "\tspeed: 0.0212s/iter; left time: 231.4051s\n",
      "Epoch: 1 cost time: 13.344077587127686\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2497239 Vali Loss: 0.0487074 Test Loss: 0.1458330\n",
      "Validation loss decreased (inf --> 0.048707).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2340444\n",
      "\tspeed: 0.0621s/iter; left time: 666.0970s\n",
      "\titers: 200, epoch: 2 | loss: 0.1911051\n",
      "\tspeed: 0.0213s/iter; left time: 226.2055s\n",
      "\titers: 300, epoch: 2 | loss: 0.3099531\n",
      "\tspeed: 0.0213s/iter; left time: 224.0059s\n",
      "\titers: 400, epoch: 2 | loss: 0.2114587\n",
      "\tspeed: 0.0213s/iter; left time: 221.8096s\n",
      "\titers: 500, epoch: 2 | loss: 0.2178767\n",
      "\tspeed: 0.0213s/iter; left time: 219.5872s\n",
      "Epoch: 2 cost time: 12.448843240737915\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2483829 Vali Loss: 0.0637570 Test Loss: 0.1865307\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2303837\n",
      "\tspeed: 0.0640s/iter; left time: 650.6406s\n",
      "\titers: 200, epoch: 3 | loss: 0.2040206\n",
      "\tspeed: 0.0232s/iter; left time: 233.1907s\n",
      "\titers: 300, epoch: 3 | loss: 0.2155994\n",
      "\tspeed: 0.0232s/iter; left time: 231.1404s\n",
      "\titers: 400, epoch: 3 | loss: 0.1691144\n",
      "\tspeed: 0.0215s/iter; left time: 211.5621s\n",
      "\titers: 500, epoch: 3 | loss: 0.2280182\n",
      "\tspeed: 0.0212s/iter; left time: 206.9027s\n",
      "Epoch: 3 cost time: 12.998172760009766\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2552357 Vali Loss: 0.0537550 Test Loss: 0.1570746\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1636872\n",
      "\tspeed: 0.0604s/iter; left time: 579.6031s\n",
      "\titers: 200, epoch: 4 | loss: 0.2324255\n",
      "\tspeed: 0.0213s/iter; left time: 202.5147s\n",
      "\titers: 300, epoch: 4 | loss: 0.2311193\n",
      "\tspeed: 0.0213s/iter; left time: 200.1086s\n",
      "\titers: 400, epoch: 4 | loss: 0.2508463\n",
      "\tspeed: 0.0213s/iter; left time: 198.1786s\n",
      "\titers: 500, epoch: 4 | loss: 0.1465907\n",
      "\tspeed: 0.0213s/iter; left time: 195.8179s\n",
      "Epoch: 4 cost time: 12.419177055358887\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2251661 Vali Loss: 0.0478529 Test Loss: 0.1428172\n",
      "Validation loss decreased (0.048707 --> 0.047853).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3381991\n",
      "\tspeed: 0.0618s/iter; left time: 557.6425s\n",
      "\titers: 200, epoch: 5 | loss: 0.1688681\n",
      "\tspeed: 0.0213s/iter; left time: 190.3391s\n",
      "\titers: 300, epoch: 5 | loss: 0.2440124\n",
      "\tspeed: 0.0213s/iter; left time: 187.7860s\n",
      "\titers: 400, epoch: 5 | loss: 0.2265646\n",
      "\tspeed: 0.0213s/iter; left time: 185.7200s\n",
      "\titers: 500, epoch: 5 | loss: 0.1937955\n",
      "\tspeed: 0.0213s/iter; left time: 183.4994s\n",
      "Epoch: 5 cost time: 12.438358306884766\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2147581 Vali Loss: 0.0463137 Test Loss: 0.1410170\n",
      "Validation loss decreased (0.047853 --> 0.046314).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1619216\n",
      "\tspeed: 0.0629s/iter; left time: 531.9385s\n",
      "\titers: 200, epoch: 6 | loss: 0.2720591\n",
      "\tspeed: 0.0213s/iter; left time: 178.0981s\n",
      "\titers: 300, epoch: 6 | loss: 0.2588261\n",
      "\tspeed: 0.0213s/iter; left time: 175.7096s\n",
      "\titers: 400, epoch: 6 | loss: 0.1421844\n",
      "\tspeed: 0.0212s/iter; left time: 172.6402s\n",
      "\titers: 500, epoch: 6 | loss: 0.1271585\n",
      "\tspeed: 0.0212s/iter; left time: 170.7774s\n",
      "Epoch: 6 cost time: 12.411394357681274\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2085781 Vali Loss: 0.0456530 Test Loss: 0.1389347\n",
      "Validation loss decreased (0.046314 --> 0.045653).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1957026\n",
      "\tspeed: 0.0624s/iter; left time: 492.0800s\n",
      "\titers: 200, epoch: 7 | loss: 0.2440834\n",
      "\tspeed: 0.0212s/iter; left time: 164.7704s\n",
      "\titers: 300, epoch: 7 | loss: 0.2426528\n",
      "\tspeed: 0.0212s/iter; left time: 163.1175s\n",
      "\titers: 400, epoch: 7 | loss: 0.1396694\n",
      "\tspeed: 0.0212s/iter; left time: 160.7041s\n",
      "\titers: 500, epoch: 7 | loss: 0.2076509\n",
      "\tspeed: 0.0212s/iter; left time: 158.6725s\n",
      "Epoch: 7 cost time: 12.393557786941528\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2040462 Vali Loss: 0.0450921 Test Loss: 0.1383753\n",
      "Validation loss decreased (0.045653 --> 0.045092).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1679175\n",
      "\tspeed: 0.0628s/iter; left time: 459.2231s\n",
      "\titers: 200, epoch: 8 | loss: 0.2228707\n",
      "\tspeed: 0.0213s/iter; left time: 153.6960s\n",
      "\titers: 300, epoch: 8 | loss: 0.2266141\n",
      "\tspeed: 0.0213s/iter; left time: 151.5136s\n",
      "\titers: 400, epoch: 8 | loss: 0.1868207\n",
      "\tspeed: 0.0213s/iter; left time: 149.3869s\n",
      "\titers: 500, epoch: 8 | loss: 0.2489204\n",
      "\tspeed: 0.0213s/iter; left time: 147.2493s\n",
      "Epoch: 8 cost time: 12.433119773864746\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2040983 Vali Loss: 0.0444818 Test Loss: 0.1387045\n",
      "Validation loss decreased (0.045092 --> 0.044482).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1514863\n",
      "\tspeed: 0.0615s/iter; left time: 414.7690s\n",
      "\titers: 200, epoch: 9 | loss: 0.3282959\n",
      "\tspeed: 0.0214s/iter; left time: 141.9404s\n",
      "\titers: 300, epoch: 9 | loss: 0.2165562\n",
      "\tspeed: 0.0214s/iter; left time: 139.7263s\n",
      "\titers: 400, epoch: 9 | loss: 0.1657842\n",
      "\tspeed: 0.0214s/iter; left time: 137.6240s\n",
      "\titers: 500, epoch: 9 | loss: 0.1373940\n",
      "\tspeed: 0.0214s/iter; left time: 135.4979s\n",
      "Epoch: 9 cost time: 12.488882541656494\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2025875 Vali Loss: 0.0441560 Test Loss: 0.1378595\n",
      "Validation loss decreased (0.044482 --> 0.044156).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1225090\n",
      "\tspeed: 0.0657s/iter; left time: 405.7408s\n",
      "\titers: 200, epoch: 10 | loss: 0.2219221\n",
      "\tspeed: 0.0212s/iter; left time: 128.9439s\n",
      "\titers: 300, epoch: 10 | loss: 0.1584260\n",
      "\tspeed: 0.0212s/iter; left time: 126.8476s\n",
      "\titers: 400, epoch: 10 | loss: 0.1926998\n",
      "\tspeed: 0.0212s/iter; left time: 124.7356s\n",
      "\titers: 500, epoch: 10 | loss: 0.2200271\n",
      "\tspeed: 0.0212s/iter; left time: 122.6237s\n",
      "Epoch: 10 cost time: 12.42766809463501\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2016098 Vali Loss: 0.0441128 Test Loss: 0.1378013\n",
      "Validation loss decreased (0.044156 --> 0.044113).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.3134599\n",
      "\tspeed: 0.0614s/iter; left time: 343.8801s\n",
      "\titers: 200, epoch: 11 | loss: 0.2411440\n",
      "\tspeed: 0.0212s/iter; left time: 116.4036s\n",
      "\titers: 300, epoch: 11 | loss: 0.1653250\n",
      "\tspeed: 0.0212s/iter; left time: 114.2856s\n",
      "\titers: 400, epoch: 11 | loss: 0.1539716\n",
      "\tspeed: 0.0212s/iter; left time: 112.1400s\n",
      "\titers: 500, epoch: 11 | loss: 0.2373441\n",
      "\tspeed: 0.0212s/iter; left time: 110.0470s\n",
      "Epoch: 11 cost time: 12.394959449768066\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2021595 Vali Loss: 0.0440318 Test Loss: 0.1377459\n",
      "Validation loss decreased (0.044113 --> 0.044032).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2353914\n",
      "\tspeed: 0.0648s/iter; left time: 325.8739s\n",
      "\titers: 200, epoch: 12 | loss: 0.3114094\n",
      "\tspeed: 0.0213s/iter; left time: 105.0887s\n",
      "\titers: 300, epoch: 12 | loss: 0.1593771\n",
      "\tspeed: 0.0213s/iter; left time: 102.9272s\n",
      "\titers: 400, epoch: 12 | loss: 0.3602030\n",
      "\tspeed: 0.0213s/iter; left time: 100.8142s\n",
      "\titers: 500, epoch: 12 | loss: 0.1722682\n",
      "\tspeed: 0.0213s/iter; left time: 98.6283s\n",
      "Epoch: 12 cost time: 12.605692863464355\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2014231 Vali Loss: 0.0440433 Test Loss: 0.1377519\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1846561\n",
      "\tspeed: 0.0608s/iter; left time: 271.2194s\n",
      "\titers: 200, epoch: 13 | loss: 0.2406921\n",
      "\tspeed: 0.0213s/iter; left time: 92.8893s\n",
      "\titers: 300, epoch: 13 | loss: 0.2706851\n",
      "\tspeed: 0.0213s/iter; left time: 90.7479s\n",
      "\titers: 400, epoch: 13 | loss: 0.1599784\n",
      "\tspeed: 0.0213s/iter; left time: 88.5814s\n",
      "\titers: 500, epoch: 13 | loss: 0.2028231\n",
      "\tspeed: 0.0213s/iter; left time: 86.4278s\n",
      "Epoch: 13 cost time: 12.471432209014893\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2015567 Vali Loss: 0.0441264 Test Loss: 0.1377645\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1936617\n",
      "\tspeed: 0.0611s/iter; left time: 237.6599s\n",
      "\titers: 200, epoch: 14 | loss: 0.2265234\n",
      "\tspeed: 0.0212s/iter; left time: 80.4971s\n",
      "\titers: 300, epoch: 14 | loss: 0.2004135\n",
      "\tspeed: 0.0212s/iter; left time: 78.3596s\n",
      "\titers: 400, epoch: 14 | loss: 0.1715649\n",
      "\tspeed: 0.0212s/iter; left time: 76.2309s\n",
      "\titers: 500, epoch: 14 | loss: 0.2285334\n",
      "\tspeed: 0.0212s/iter; left time: 74.0887s\n",
      "Epoch: 14 cost time: 12.407430171966553\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2025883 Vali Loss: 0.0440201 Test Loss: 0.1377260\n",
      "Validation loss decreased (0.044032 --> 0.044020).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1840862\n",
      "\tspeed: 0.0614s/iter; left time: 203.7724s\n",
      "\titers: 200, epoch: 15 | loss: 0.1368307\n",
      "\tspeed: 0.0212s/iter; left time: 68.4344s\n",
      "\titers: 300, epoch: 15 | loss: 0.1548997\n",
      "\tspeed: 0.0212s/iter; left time: 66.2904s\n",
      "\titers: 400, epoch: 15 | loss: 0.1958587\n",
      "\tspeed: 0.0212s/iter; left time: 64.1780s\n",
      "\titers: 500, epoch: 15 | loss: 0.1892700\n",
      "\tspeed: 0.0212s/iter; left time: 62.0440s\n",
      "Epoch: 15 cost time: 12.43505597114563\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2019070 Vali Loss: 0.0439447 Test Loss: 0.1377172\n",
      "Validation loss decreased (0.044020 --> 0.043945).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1813580\n",
      "\tspeed: 0.0622s/iter; left time: 171.0237s\n",
      "\titers: 200, epoch: 16 | loss: 0.1363460\n",
      "\tspeed: 0.0213s/iter; left time: 56.5031s\n",
      "\titers: 300, epoch: 16 | loss: 0.2831162\n",
      "\tspeed: 0.0213s/iter; left time: 54.3708s\n",
      "\titers: 400, epoch: 16 | loss: 0.2373928\n",
      "\tspeed: 0.0213s/iter; left time: 52.2524s\n",
      "\titers: 500, epoch: 16 | loss: 0.2242551\n",
      "\tspeed: 0.0213s/iter; left time: 50.1286s\n",
      "Epoch: 16 cost time: 12.474010944366455\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.2016798 Vali Loss: 0.0439457 Test Loss: 0.1377160\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.2514225\n",
      "\tspeed: 0.0624s/iter; left time: 136.1214s\n",
      "\titers: 200, epoch: 17 | loss: 0.3110075\n",
      "\tspeed: 0.0212s/iter; left time: 44.1890s\n",
      "\titers: 300, epoch: 17 | loss: 0.2045411\n",
      "\tspeed: 0.0212s/iter; left time: 42.0567s\n",
      "\titers: 400, epoch: 17 | loss: 0.2309391\n",
      "\tspeed: 0.0212s/iter; left time: 39.9312s\n",
      "\titers: 500, epoch: 17 | loss: 0.1832193\n",
      "\tspeed: 0.0213s/iter; left time: 37.8602s\n",
      "Epoch: 17 cost time: 12.369378566741943\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.2015760 Vali Loss: 0.0441191 Test Loss: 0.1377131\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1943845\n",
      "\tspeed: 0.0599s/iter; left time: 96.5484s\n",
      "\titers: 200, epoch: 18 | loss: 0.3082238\n",
      "\tspeed: 0.0213s/iter; left time: 32.1391s\n",
      "\titers: 300, epoch: 18 | loss: 0.2240575\n",
      "\tspeed: 0.0213s/iter; left time: 29.9897s\n",
      "\titers: 400, epoch: 18 | loss: 0.2067386\n",
      "\tspeed: 0.0213s/iter; left time: 27.8605s\n",
      "\titers: 500, epoch: 18 | loss: 0.2080798\n",
      "\tspeed: 0.0213s/iter; left time: 25.7412s\n",
      "Epoch: 18 cost time: 12.39993405342102\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.2016109 Vali Loss: 0.0440172 Test Loss: 0.1377118\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13791650533676147, mae:0.23043178021907806\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3004980\n",
      "\tspeed: 0.0319s/iter; left time: 360.8350s\n",
      "\titers: 200, epoch: 1 | loss: 0.1989134\n",
      "\tspeed: 0.0200s/iter; left time: 224.2434s\n",
      "\titers: 300, epoch: 1 | loss: 0.3044634\n",
      "\tspeed: 0.0200s/iter; left time: 222.3918s\n",
      "\titers: 400, epoch: 1 | loss: 0.2734650\n",
      "\tspeed: 0.0200s/iter; left time: 219.7967s\n",
      "\titers: 500, epoch: 1 | loss: 0.2556673\n",
      "\tspeed: 0.0201s/iter; left time: 218.9171s\n",
      "Epoch: 1 cost time: 12.662327766418457\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2702008 Vali Loss: 0.0588236 Test Loss: 0.1728681\n",
      "Validation loss decreased (inf --> 0.058824).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1507782\n",
      "\tspeed: 0.0613s/iter; left time: 657.5711s\n",
      "\titers: 200, epoch: 2 | loss: 0.4606629\n",
      "\tspeed: 0.0202s/iter; left time: 214.9508s\n",
      "\titers: 300, epoch: 2 | loss: 0.3003632\n",
      "\tspeed: 0.0201s/iter; left time: 212.0100s\n",
      "\titers: 400, epoch: 2 | loss: 0.2307945\n",
      "\tspeed: 0.0214s/iter; left time: 223.4712s\n",
      "\titers: 500, epoch: 2 | loss: 0.1799846\n",
      "\tspeed: 0.0221s/iter; left time: 228.2249s\n",
      "Epoch: 2 cost time: 12.344701051712036\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2892339 Vali Loss: 0.0639202 Test Loss: 0.1825890\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2069496\n",
      "\tspeed: 0.0625s/iter; left time: 634.6483s\n",
      "\titers: 200, epoch: 3 | loss: 0.2117680\n",
      "\tspeed: 0.0212s/iter; left time: 213.1089s\n",
      "\titers: 300, epoch: 3 | loss: 0.2944541\n",
      "\tspeed: 0.0201s/iter; left time: 200.0300s\n",
      "\titers: 400, epoch: 3 | loss: 0.2844866\n",
      "\tspeed: 0.0201s/iter; left time: 197.8222s\n",
      "\titers: 500, epoch: 3 | loss: 0.2531351\n",
      "\tspeed: 0.0201s/iter; left time: 195.8718s\n",
      "Epoch: 3 cost time: 12.035943269729614\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2709073 Vali Loss: 0.0599584 Test Loss: 0.1668101\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1952371\n",
      "\tspeed: 0.0579s/iter; left time: 555.5181s\n",
      "\titers: 200, epoch: 4 | loss: 0.3761272\n",
      "\tspeed: 0.0201s/iter; left time: 190.4014s\n",
      "\titers: 300, epoch: 4 | loss: 0.1631347\n",
      "\tspeed: 0.0201s/iter; left time: 188.3439s\n",
      "\titers: 400, epoch: 4 | loss: 0.2734665\n",
      "\tspeed: 0.0201s/iter; left time: 186.3587s\n",
      "\titers: 500, epoch: 4 | loss: 0.1820910\n",
      "\tspeed: 0.0200s/iter; left time: 184.2448s\n",
      "Epoch: 4 cost time: 11.694469690322876\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2504997 Vali Loss: 0.0569234 Test Loss: 0.1600959\n",
      "Validation loss decreased (0.058824 --> 0.056923).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2042698\n",
      "\tspeed: 0.0602s/iter; left time: 543.2510s\n",
      "\titers: 200, epoch: 5 | loss: 0.3836574\n",
      "\tspeed: 0.0200s/iter; left time: 178.1731s\n",
      "\titers: 300, epoch: 5 | loss: 0.2233628\n",
      "\tspeed: 0.0199s/iter; left time: 175.9069s\n",
      "\titers: 400, epoch: 5 | loss: 0.2100992\n",
      "\tspeed: 0.0199s/iter; left time: 173.9053s\n",
      "\titers: 500, epoch: 5 | loss: 0.2138872\n",
      "\tspeed: 0.0199s/iter; left time: 171.7816s\n",
      "Epoch: 5 cost time: 11.681588649749756\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2398774 Vali Loss: 0.0535849 Test Loss: 0.1552137\n",
      "Validation loss decreased (0.056923 --> 0.053585).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2638490\n",
      "\tspeed: 0.0597s/iter; left time: 504.9447s\n",
      "\titers: 200, epoch: 6 | loss: 0.2066480\n",
      "\tspeed: 0.0201s/iter; left time: 167.7511s\n",
      "\titers: 300, epoch: 6 | loss: 0.2113417\n",
      "\tspeed: 0.0201s/iter; left time: 165.6652s\n",
      "\titers: 400, epoch: 6 | loss: 0.1573086\n",
      "\tspeed: 0.0201s/iter; left time: 163.6042s\n",
      "\titers: 500, epoch: 6 | loss: 0.3463879\n",
      "\tspeed: 0.0201s/iter; left time: 161.5444s\n",
      "Epoch: 6 cost time: 11.728840589523315\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2346667 Vali Loss: 0.0539009 Test Loss: 0.1536500\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2611862\n",
      "\tspeed: 0.0575s/iter; left time: 453.4397s\n",
      "\titers: 200, epoch: 7 | loss: 0.2718537\n",
      "\tspeed: 0.0201s/iter; left time: 156.4750s\n",
      "\titers: 300, epoch: 7 | loss: 0.2823459\n",
      "\tspeed: 0.0201s/iter; left time: 154.5195s\n",
      "\titers: 400, epoch: 7 | loss: 0.3576207\n",
      "\tspeed: 0.0201s/iter; left time: 152.4989s\n",
      "\titers: 500, epoch: 7 | loss: 0.1743409\n",
      "\tspeed: 0.0201s/iter; left time: 150.4741s\n",
      "Epoch: 7 cost time: 11.742376565933228\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2333656 Vali Loss: 0.0536834 Test Loss: 0.1523338\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2495489\n",
      "\tspeed: 0.0587s/iter; left time: 428.8077s\n",
      "\titers: 200, epoch: 8 | loss: 0.4250316\n",
      "\tspeed: 0.0201s/iter; left time: 145.1406s\n",
      "\titers: 300, epoch: 8 | loss: 0.2799234\n",
      "\tspeed: 0.0201s/iter; left time: 143.0228s\n",
      "\titers: 400, epoch: 8 | loss: 0.1764279\n",
      "\tspeed: 0.0201s/iter; left time: 140.9903s\n",
      "\titers: 500, epoch: 8 | loss: 0.2514960\n",
      "\tspeed: 0.0201s/iter; left time: 138.9612s\n",
      "Epoch: 8 cost time: 11.73278284072876\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2304643 Vali Loss: 0.0535885 Test Loss: 0.1519138\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15540441870689392, mae:0.24983768165111542\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1513899\n",
      "\tspeed: 0.0214s/iter; left time: 241.7464s\n",
      "\titers: 200, epoch: 1 | loss: 0.1708261\n",
      "\tspeed: 0.0098s/iter; left time: 110.3223s\n",
      "\titers: 300, epoch: 1 | loss: 0.2072538\n",
      "\tspeed: 0.0099s/iter; left time: 109.6272s\n",
      "\titers: 400, epoch: 1 | loss: 0.1874213\n",
      "\tspeed: 0.0098s/iter; left time: 108.2622s\n",
      "\titers: 500, epoch: 1 | loss: 0.1462569\n",
      "\tspeed: 0.0099s/iter; left time: 107.5373s\n",
      "Epoch: 1 cost time: 6.815077543258667\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1782682 Vali Loss: 0.0488664 Test Loss: 0.1440767\n",
      "Validation loss decreased (inf --> 0.048866).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1783890\n",
      "\tspeed: 0.0356s/iter; left time: 382.0493s\n",
      "\titers: 200, epoch: 2 | loss: 0.2236717\n",
      "\tspeed: 0.0099s/iter; left time: 105.4459s\n",
      "\titers: 300, epoch: 2 | loss: 0.1199877\n",
      "\tspeed: 0.0099s/iter; left time: 104.7411s\n",
      "\titers: 400, epoch: 2 | loss: 0.1373761\n",
      "\tspeed: 0.0099s/iter; left time: 103.3658s\n",
      "\titers: 500, epoch: 2 | loss: 0.1278298\n",
      "\tspeed: 0.0099s/iter; left time: 102.2962s\n",
      "Epoch: 2 cost time: 5.963961362838745\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1318140 Vali Loss: 0.0394691 Test Loss: 0.1216232\n",
      "Validation loss decreased (0.048866 --> 0.039469).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0813655\n",
      "\tspeed: 0.0358s/iter; left time: 363.5461s\n",
      "\titers: 200, epoch: 3 | loss: 0.1225430\n",
      "\tspeed: 0.0100s/iter; left time: 100.5132s\n",
      "\titers: 300, epoch: 3 | loss: 0.0964295\n",
      "\tspeed: 0.0100s/iter; left time: 99.6828s\n",
      "\titers: 400, epoch: 3 | loss: 0.1345346\n",
      "\tspeed: 0.0100s/iter; left time: 98.3770s\n",
      "\titers: 500, epoch: 3 | loss: 0.1609011\n",
      "\tspeed: 0.0100s/iter; left time: 97.3653s\n",
      "Epoch: 3 cost time: 5.9879162311553955\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1044723 Vali Loss: 0.0375850 Test Loss: 0.1230151\n",
      "Validation loss decreased (0.039469 --> 0.037585).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1030105\n",
      "\tspeed: 0.0372s/iter; left time: 356.7666s\n",
      "\titers: 200, epoch: 4 | loss: 0.0546012\n",
      "\tspeed: 0.0099s/iter; left time: 94.0465s\n",
      "\titers: 300, epoch: 4 | loss: 0.0743815\n",
      "\tspeed: 0.0104s/iter; left time: 97.8144s\n",
      "\titers: 400, epoch: 4 | loss: 0.0863305\n",
      "\tspeed: 0.0101s/iter; left time: 93.9017s\n",
      "\titers: 500, epoch: 4 | loss: 0.0859571\n",
      "\tspeed: 0.0099s/iter; left time: 90.8848s\n",
      "Epoch: 4 cost time: 6.15201473236084\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0912220 Vali Loss: 0.0352333 Test Loss: 0.1292616\n",
      "Validation loss decreased (0.037585 --> 0.035233).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0888323\n",
      "\tspeed: 0.0360s/iter; left time: 324.4400s\n",
      "\titers: 200, epoch: 5 | loss: 0.0870868\n",
      "\tspeed: 0.0100s/iter; left time: 88.9712s\n",
      "\titers: 300, epoch: 5 | loss: 0.0659903\n",
      "\tspeed: 0.0100s/iter; left time: 87.9765s\n",
      "\titers: 400, epoch: 5 | loss: 0.0912737\n",
      "\tspeed: 0.0100s/iter; left time: 87.0411s\n",
      "\titers: 500, epoch: 5 | loss: 0.0844885\n",
      "\tspeed: 0.0100s/iter; left time: 85.8585s\n",
      "Epoch: 5 cost time: 6.003388166427612\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0837980 Vali Loss: 0.0351836 Test Loss: 0.1282767\n",
      "Validation loss decreased (0.035233 --> 0.035184).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0707374\n",
      "\tspeed: 0.0355s/iter; left time: 300.3699s\n",
      "\titers: 200, epoch: 6 | loss: 0.0545981\n",
      "\tspeed: 0.0099s/iter; left time: 83.0443s\n",
      "\titers: 300, epoch: 6 | loss: 0.0826763\n",
      "\tspeed: 0.0099s/iter; left time: 81.7011s\n",
      "\titers: 400, epoch: 6 | loss: 0.0825454\n",
      "\tspeed: 0.0099s/iter; left time: 80.6406s\n",
      "\titers: 500, epoch: 6 | loss: 0.0937315\n",
      "\tspeed: 0.0099s/iter; left time: 79.8521s\n",
      "Epoch: 6 cost time: 5.954099893569946\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0797587 Vali Loss: 0.0353576 Test Loss: 0.1341149\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0572066\n",
      "\tspeed: 0.0387s/iter; left time: 304.7592s\n",
      "\titers: 200, epoch: 7 | loss: 0.0860813\n",
      "\tspeed: 0.0112s/iter; left time: 86.8087s\n",
      "\titers: 300, epoch: 7 | loss: 0.0820051\n",
      "\tspeed: 0.0112s/iter; left time: 85.6822s\n",
      "\titers: 400, epoch: 7 | loss: 0.0914026\n",
      "\tspeed: 0.0100s/iter; left time: 75.4313s\n",
      "\titers: 500, epoch: 7 | loss: 0.0608955\n",
      "\tspeed: 0.0099s/iter; left time: 74.1850s\n",
      "Epoch: 7 cost time: 6.297482967376709\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0778621 Vali Loss: 0.0352114 Test Loss: 0.1327515\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0954885\n",
      "\tspeed: 0.0376s/iter; left time: 274.9680s\n",
      "\titers: 200, epoch: 8 | loss: 0.0881027\n",
      "\tspeed: 0.0102s/iter; left time: 73.7550s\n",
      "\titers: 300, epoch: 8 | loss: 0.0847088\n",
      "\tspeed: 0.0099s/iter; left time: 70.7006s\n",
      "\titers: 400, epoch: 8 | loss: 0.1198116\n",
      "\tspeed: 0.0099s/iter; left time: 69.6327s\n",
      "\titers: 500, epoch: 8 | loss: 0.0666076\n",
      "\tspeed: 0.0099s/iter; left time: 68.4982s\n",
      "Epoch: 8 cost time: 6.097086668014526\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0767807 Vali Loss: 0.0349456 Test Loss: 0.1335279\n",
      "Validation loss decreased (0.035184 --> 0.034946).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0757837\n",
      "\tspeed: 0.0358s/iter; left time: 241.1797s\n",
      "\titers: 200, epoch: 9 | loss: 0.0716201\n",
      "\tspeed: 0.0100s/iter; left time: 66.7190s\n",
      "\titers: 300, epoch: 9 | loss: 0.0531166\n",
      "\tspeed: 0.0100s/iter; left time: 65.6681s\n",
      "\titers: 400, epoch: 9 | loss: 0.0605998\n",
      "\tspeed: 0.0100s/iter; left time: 64.3003s\n",
      "\titers: 500, epoch: 9 | loss: 0.0825795\n",
      "\tspeed: 0.0100s/iter; left time: 63.4343s\n",
      "Epoch: 9 cost time: 6.0216310024261475\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0762669 Vali Loss: 0.0351849 Test Loss: 0.1326815\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0987884\n",
      "\tspeed: 0.0357s/iter; left time: 220.3130s\n",
      "\titers: 200, epoch: 10 | loss: 0.0958489\n",
      "\tspeed: 0.0099s/iter; left time: 60.3865s\n",
      "\titers: 300, epoch: 10 | loss: 0.0612050\n",
      "\tspeed: 0.0099s/iter; left time: 59.2075s\n",
      "\titers: 400, epoch: 10 | loss: 0.0695738\n",
      "\tspeed: 0.0099s/iter; left time: 58.3898s\n",
      "\titers: 500, epoch: 10 | loss: 0.0914397\n",
      "\tspeed: 0.0100s/iter; left time: 57.4740s\n",
      "Epoch: 10 cost time: 6.003925085067749\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0759110 Vali Loss: 0.0349752 Test Loss: 0.1329615\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0891797\n",
      "\tspeed: 0.0352s/iter; left time: 197.3920s\n",
      "\titers: 200, epoch: 11 | loss: 0.0654194\n",
      "\tspeed: 0.0099s/iter; left time: 54.6608s\n",
      "\titers: 300, epoch: 11 | loss: 0.0499686\n",
      "\tspeed: 0.0100s/iter; left time: 53.9063s\n",
      "\titers: 400, epoch: 11 | loss: 0.0801086\n",
      "\tspeed: 0.0100s/iter; left time: 52.9112s\n",
      "\titers: 500, epoch: 11 | loss: 0.0725849\n",
      "\tspeed: 0.0099s/iter; left time: 51.7395s\n",
      "Epoch: 11 cost time: 5.9545488357543945\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0758053 Vali Loss: 0.0349229 Test Loss: 0.1328993\n",
      "Validation loss decreased (0.034946 --> 0.034923).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.0766716\n",
      "\tspeed: 0.0371s/iter; left time: 186.6595s\n",
      "\titers: 200, epoch: 12 | loss: 0.1050293\n",
      "\tspeed: 0.0099s/iter; left time: 48.7254s\n",
      "\titers: 300, epoch: 12 | loss: 0.0558165\n",
      "\tspeed: 0.0098s/iter; left time: 47.5305s\n",
      "\titers: 400, epoch: 12 | loss: 0.0781581\n",
      "\tspeed: 0.0098s/iter; left time: 46.5388s\n",
      "\titers: 500, epoch: 12 | loss: 0.1023269\n",
      "\tspeed: 0.0098s/iter; left time: 45.5708s\n",
      "Epoch: 12 cost time: 5.909122467041016\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0757633 Vali Loss: 0.0349610 Test Loss: 0.1328122\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.0476927\n",
      "\tspeed: 0.0368s/iter; left time: 164.3388s\n",
      "\titers: 200, epoch: 13 | loss: 0.0836869\n",
      "\tspeed: 0.0099s/iter; left time: 43.3242s\n",
      "\titers: 300, epoch: 13 | loss: 0.0739400\n",
      "\tspeed: 0.0099s/iter; left time: 42.3083s\n",
      "\titers: 400, epoch: 13 | loss: 0.0708614\n",
      "\tspeed: 0.0099s/iter; left time: 41.2769s\n",
      "\titers: 500, epoch: 13 | loss: 0.0761347\n",
      "\tspeed: 0.0099s/iter; left time: 40.0374s\n",
      "Epoch: 13 cost time: 5.952649831771851\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0757305 Vali Loss: 0.0349727 Test Loss: 0.1329227\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.0647223\n",
      "\tspeed: 0.0373s/iter; left time: 145.1175s\n",
      "\titers: 200, epoch: 14 | loss: 0.0526018\n",
      "\tspeed: 0.0098s/iter; left time: 37.2943s\n",
      "\titers: 300, epoch: 14 | loss: 0.0859268\n",
      "\tspeed: 0.0098s/iter; left time: 36.2837s\n",
      "\titers: 400, epoch: 14 | loss: 0.0826752\n",
      "\tspeed: 0.0098s/iter; left time: 35.1743s\n",
      "\titers: 500, epoch: 14 | loss: 0.0700437\n",
      "\tspeed: 0.0098s/iter; left time: 34.2124s\n",
      "Epoch: 14 cost time: 5.876570463180542\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.0756925 Vali Loss: 0.0349790 Test Loss: 0.1329065\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13309873640537262, mae:0.23310883343219757\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1296794\n",
      "\tspeed: 0.0213s/iter; left time: 240.4788s\n",
      "\titers: 200, epoch: 1 | loss: 0.1537673\n",
      "\tspeed: 0.0098s/iter; left time: 109.2857s\n",
      "\titers: 300, epoch: 1 | loss: 0.1868851\n",
      "\tspeed: 0.0098s/iter; left time: 108.5247s\n",
      "\titers: 400, epoch: 1 | loss: 0.1413876\n",
      "\tspeed: 0.0098s/iter; left time: 107.2722s\n",
      "\titers: 500, epoch: 1 | loss: 0.1430622\n",
      "\tspeed: 0.0098s/iter; left time: 106.5423s\n",
      "Epoch: 1 cost time: 6.764409303665161\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1467088 Vali Loss: 0.0350296 Test Loss: 0.1152290\n",
      "Validation loss decreased (inf --> 0.035030).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1114759\n",
      "\tspeed: 0.0372s/iter; left time: 398.7151s\n",
      "\titers: 200, epoch: 2 | loss: 0.1284230\n",
      "\tspeed: 0.0097s/iter; left time: 103.5369s\n",
      "\titers: 300, epoch: 2 | loss: 0.0688358\n",
      "\tspeed: 0.0097s/iter; left time: 102.3752s\n",
      "\titers: 400, epoch: 2 | loss: 0.1128854\n",
      "\tspeed: 0.0097s/iter; left time: 101.4674s\n",
      "\titers: 500, epoch: 2 | loss: 0.1115896\n",
      "\tspeed: 0.0097s/iter; left time: 100.1793s\n",
      "Epoch: 2 cost time: 5.847932815551758\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1114781 Vali Loss: 0.0329555 Test Loss: 0.1099542\n",
      "Validation loss decreased (0.035030 --> 0.032956).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0609866\n",
      "\tspeed: 0.0366s/iter; left time: 371.5205s\n",
      "\titers: 200, epoch: 3 | loss: 0.0976384\n",
      "\tspeed: 0.0097s/iter; left time: 97.9281s\n",
      "\titers: 300, epoch: 3 | loss: 0.1724140\n",
      "\tspeed: 0.0097s/iter; left time: 96.8879s\n",
      "\titers: 400, epoch: 3 | loss: 0.0866911\n",
      "\tspeed: 0.0097s/iter; left time: 95.7572s\n",
      "\titers: 500, epoch: 3 | loss: 0.0692270\n",
      "\tspeed: 0.0097s/iter; left time: 94.9171s\n",
      "Epoch: 3 cost time: 5.827683925628662\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0921401 Vali Loss: 0.0308499 Test Loss: 0.1111345\n",
      "Validation loss decreased (0.032956 --> 0.030850).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0585588\n",
      "\tspeed: 0.0367s/iter; left time: 352.2482s\n",
      "\titers: 200, epoch: 4 | loss: 0.0662341\n",
      "\tspeed: 0.0098s/iter; left time: 92.7198s\n",
      "\titers: 300, epoch: 4 | loss: 0.0571444\n",
      "\tspeed: 0.0098s/iter; left time: 91.6553s\n",
      "\titers: 400, epoch: 4 | loss: 0.0533146\n",
      "\tspeed: 0.0097s/iter; left time: 90.3320s\n",
      "\titers: 500, epoch: 4 | loss: 0.0809954\n",
      "\tspeed: 0.0097s/iter; left time: 89.5496s\n",
      "Epoch: 4 cost time: 5.853921175003052\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0807265 Vali Loss: 0.0310337 Test Loss: 0.1128686\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0621617\n",
      "\tspeed: 0.0344s/iter; left time: 310.3356s\n",
      "\titers: 200, epoch: 5 | loss: 0.1101965\n",
      "\tspeed: 0.0097s/iter; left time: 86.5481s\n",
      "\titers: 300, epoch: 5 | loss: 0.0706058\n",
      "\tspeed: 0.0097s/iter; left time: 85.5728s\n",
      "\titers: 400, epoch: 5 | loss: 0.0821538\n",
      "\tspeed: 0.0097s/iter; left time: 84.9707s\n",
      "\titers: 500, epoch: 5 | loss: 0.0852641\n",
      "\tspeed: 0.0097s/iter; left time: 83.7128s\n",
      "Epoch: 5 cost time: 5.821242570877075\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0744010 Vali Loss: 0.0319490 Test Loss: 0.1164137\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0432204\n",
      "\tspeed: 0.0340s/iter; left time: 287.4810s\n",
      "\titers: 200, epoch: 6 | loss: 0.1052784\n",
      "\tspeed: 0.0109s/iter; left time: 91.0897s\n",
      "\titers: 300, epoch: 6 | loss: 0.1071064\n",
      "\tspeed: 0.0109s/iter; left time: 90.0712s\n",
      "\titers: 400, epoch: 6 | loss: 0.0691435\n",
      "\tspeed: 0.0109s/iter; left time: 88.7564s\n",
      "\titers: 500, epoch: 6 | loss: 0.0560922\n",
      "\tspeed: 0.0109s/iter; left time: 87.6583s\n",
      "Epoch: 6 cost time: 6.38397479057312\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0713012 Vali Loss: 0.0315096 Test Loss: 0.1154929\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11129950731992722, mae:0.20474335551261902\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1780639\n",
      "\tspeed: 0.0229s/iter; left time: 258.9588s\n",
      "\titers: 200, epoch: 1 | loss: 0.1372520\n",
      "\tspeed: 0.0109s/iter; left time: 122.1786s\n",
      "\titers: 300, epoch: 1 | loss: 0.1191107\n",
      "\tspeed: 0.0109s/iter; left time: 120.6978s\n",
      "\titers: 400, epoch: 1 | loss: 0.0929553\n",
      "\tspeed: 0.0109s/iter; left time: 119.9427s\n",
      "\titers: 500, epoch: 1 | loss: 0.1663685\n",
      "\tspeed: 0.0109s/iter; left time: 119.0684s\n",
      "Epoch: 1 cost time: 7.46794319152832\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1820953 Vali Loss: 0.0429141 Test Loss: 0.1349617\n",
      "Validation loss decreased (inf --> 0.042914).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1548477\n",
      "\tspeed: 0.0368s/iter; left time: 395.0436s\n",
      "\titers: 200, epoch: 2 | loss: 0.2209670\n",
      "\tspeed: 0.0097s/iter; left time: 103.4650s\n",
      "\titers: 300, epoch: 2 | loss: 0.1052781\n",
      "\tspeed: 0.0097s/iter; left time: 102.3513s\n",
      "\titers: 400, epoch: 2 | loss: 0.1976978\n",
      "\tspeed: 0.0097s/iter; left time: 101.5779s\n",
      "\titers: 500, epoch: 2 | loss: 0.1454997\n",
      "\tspeed: 0.0097s/iter; left time: 100.4227s\n",
      "Epoch: 2 cost time: 5.927467584609985\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1344605 Vali Loss: 0.0395189 Test Loss: 0.1307461\n",
      "Validation loss decreased (0.042914 --> 0.039519).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0897920\n",
      "\tspeed: 0.0370s/iter; left time: 375.7287s\n",
      "\titers: 200, epoch: 3 | loss: 0.0834154\n",
      "\tspeed: 0.0098s/iter; left time: 98.5085s\n",
      "\titers: 300, epoch: 3 | loss: 0.1152317\n",
      "\tspeed: 0.0098s/iter; left time: 97.5295s\n",
      "\titers: 400, epoch: 3 | loss: 0.0778258\n",
      "\tspeed: 0.0098s/iter; left time: 96.4942s\n",
      "\titers: 500, epoch: 3 | loss: 0.0889585\n",
      "\tspeed: 0.0098s/iter; left time: 95.5956s\n",
      "Epoch: 3 cost time: 5.880545377731323\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1071869 Vali Loss: 0.0380230 Test Loss: 0.1316162\n",
      "Validation loss decreased (0.039519 --> 0.038023).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0710069\n",
      "\tspeed: 0.0356s/iter; left time: 341.4007s\n",
      "\titers: 200, epoch: 4 | loss: 0.0664880\n",
      "\tspeed: 0.0097s/iter; left time: 91.8440s\n",
      "\titers: 300, epoch: 4 | loss: 0.0639720\n",
      "\tspeed: 0.0097s/iter; left time: 90.9314s\n",
      "\titers: 400, epoch: 4 | loss: 0.1217632\n",
      "\tspeed: 0.0097s/iter; left time: 89.9861s\n",
      "\titers: 500, epoch: 4 | loss: 0.0955939\n",
      "\tspeed: 0.0097s/iter; left time: 89.4619s\n",
      "Epoch: 4 cost time: 5.839489936828613\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0925274 Vali Loss: 0.0394200 Test Loss: 0.1415058\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0904921\n",
      "\tspeed: 0.0354s/iter; left time: 319.0771s\n",
      "\titers: 200, epoch: 5 | loss: 0.0606137\n",
      "\tspeed: 0.0097s/iter; left time: 86.5958s\n",
      "\titers: 300, epoch: 5 | loss: 0.0757486\n",
      "\tspeed: 0.0097s/iter; left time: 85.5841s\n",
      "\titers: 400, epoch: 5 | loss: 0.0715954\n",
      "\tspeed: 0.0097s/iter; left time: 84.5760s\n",
      "\titers: 500, epoch: 5 | loss: 0.0930780\n",
      "\tspeed: 0.0097s/iter; left time: 83.6377s\n",
      "Epoch: 5 cost time: 5.852719783782959\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0848515 Vali Loss: 0.0375565 Test Loss: 0.1370987\n",
      "Validation loss decreased (0.038023 --> 0.037556).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0669339\n",
      "\tspeed: 0.0359s/iter; left time: 303.6314s\n",
      "\titers: 200, epoch: 6 | loss: 0.0817979\n",
      "\tspeed: 0.0111s/iter; left time: 93.0961s\n",
      "\titers: 300, epoch: 6 | loss: 0.0868388\n",
      "\tspeed: 0.0111s/iter; left time: 91.9144s\n",
      "\titers: 400, epoch: 6 | loss: 0.0847087\n",
      "\tspeed: 0.0111s/iter; left time: 90.7985s\n",
      "\titers: 500, epoch: 6 | loss: 0.0854470\n",
      "\tspeed: 0.0111s/iter; left time: 89.7021s\n",
      "Epoch: 6 cost time: 6.504245042800903\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0807174 Vali Loss: 0.0377012 Test Loss: 0.1391779\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0496188\n",
      "\tspeed: 0.0369s/iter; left time: 290.8545s\n",
      "\titers: 200, epoch: 7 | loss: 0.0840604\n",
      "\tspeed: 0.0111s/iter; left time: 86.5654s\n",
      "\titers: 300, epoch: 7 | loss: 0.0612264\n",
      "\tspeed: 0.0099s/iter; left time: 75.8290s\n",
      "\titers: 400, epoch: 7 | loss: 0.0641417\n",
      "\tspeed: 0.0111s/iter; left time: 84.2730s\n",
      "\titers: 500, epoch: 7 | loss: 0.0803898\n",
      "\tspeed: 0.0111s/iter; left time: 83.2183s\n",
      "Epoch: 7 cost time: 6.5381786823272705\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0786098 Vali Loss: 0.0379949 Test Loss: 0.1402828\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0726003\n",
      "\tspeed: 0.0380s/iter; left time: 277.8787s\n",
      "\titers: 200, epoch: 8 | loss: 0.0959347\n",
      "\tspeed: 0.0103s/iter; left time: 73.9417s\n",
      "\titers: 300, epoch: 8 | loss: 0.0586260\n",
      "\tspeed: 0.0111s/iter; left time: 79.2777s\n",
      "\titers: 400, epoch: 8 | loss: 0.0814930\n",
      "\tspeed: 0.0112s/iter; left time: 78.2018s\n",
      "\titers: 500, epoch: 8 | loss: 0.0738687\n",
      "\tspeed: 0.0112s/iter; left time: 77.3146s\n",
      "Epoch: 8 cost time: 6.547569513320923\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0774923 Vali Loss: 0.0378750 Test Loss: 0.1392616\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13732154667377472, mae:0.2374982088804245\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2075656\n",
      "\tspeed: 0.0242s/iter; left time: 273.7404s\n",
      "\titers: 200, epoch: 1 | loss: 0.1891302\n",
      "\tspeed: 0.0124s/iter; left time: 139.4448s\n",
      "\titers: 300, epoch: 1 | loss: 0.1438425\n",
      "\tspeed: 0.0124s/iter; left time: 137.8342s\n",
      "\titers: 400, epoch: 1 | loss: 0.0960854\n",
      "\tspeed: 0.0123s/iter; left time: 134.8660s\n",
      "\titers: 500, epoch: 1 | loss: 0.1460082\n",
      "\tspeed: 0.0114s/iter; left time: 124.7929s\n",
      "Epoch: 1 cost time: 8.127431631088257\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1726389 Vali Loss: 0.0416023 Test Loss: 0.1333482\n",
      "Validation loss decreased (inf --> 0.041602).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1125999\n",
      "\tspeed: 0.0388s/iter; left time: 416.2951s\n",
      "\titers: 200, epoch: 2 | loss: 0.1601015\n",
      "\tspeed: 0.0114s/iter; left time: 121.3882s\n",
      "\titers: 300, epoch: 2 | loss: 0.0863702\n",
      "\tspeed: 0.0114s/iter; left time: 120.3847s\n",
      "\titers: 400, epoch: 2 | loss: 0.0722242\n",
      "\tspeed: 0.0114s/iter; left time: 119.0907s\n",
      "\titers: 500, epoch: 2 | loss: 0.1252543\n",
      "\tspeed: 0.0114s/iter; left time: 118.1074s\n",
      "Epoch: 2 cost time: 6.843969821929932\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1206118 Vali Loss: 0.0371007 Test Loss: 0.1300596\n",
      "Validation loss decreased (0.041602 --> 0.037101).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1112814\n",
      "\tspeed: 0.0402s/iter; left time: 408.6979s\n",
      "\titers: 200, epoch: 3 | loss: 0.1019169\n",
      "\tspeed: 0.0115s/iter; left time: 115.2773s\n",
      "\titers: 300, epoch: 3 | loss: 0.0533755\n",
      "\tspeed: 0.0114s/iter; left time: 113.9814s\n",
      "\titers: 400, epoch: 3 | loss: 0.0745492\n",
      "\tspeed: 0.0114s/iter; left time: 112.6414s\n",
      "\titers: 500, epoch: 3 | loss: 0.0664679\n",
      "\tspeed: 0.0114s/iter; left time: 111.5658s\n",
      "Epoch: 3 cost time: 6.844347953796387\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0850507 Vali Loss: 0.0382536 Test Loss: 0.1343134\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0650218\n",
      "\tspeed: 0.0414s/iter; left time: 396.9250s\n",
      "\titers: 200, epoch: 4 | loss: 0.0675176\n",
      "\tspeed: 0.0125s/iter; left time: 118.4367s\n",
      "\titers: 300, epoch: 4 | loss: 0.0475311\n",
      "\tspeed: 0.0125s/iter; left time: 117.4095s\n",
      "\titers: 400, epoch: 4 | loss: 0.0494996\n",
      "\tspeed: 0.0125s/iter; left time: 116.4612s\n",
      "\titers: 500, epoch: 4 | loss: 0.0569383\n",
      "\tspeed: 0.0125s/iter; left time: 114.7312s\n",
      "Epoch: 4 cost time: 7.437259912490845\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0661452 Vali Loss: 0.0368891 Test Loss: 0.1419170\n",
      "Validation loss decreased (0.037101 --> 0.036889).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0638862\n",
      "\tspeed: 0.0394s/iter; left time: 355.1909s\n",
      "\titers: 200, epoch: 5 | loss: 0.0473970\n",
      "\tspeed: 0.0115s/iter; left time: 102.2047s\n",
      "\titers: 300, epoch: 5 | loss: 0.0737297\n",
      "\tspeed: 0.0115s/iter; left time: 101.2360s\n",
      "\titers: 400, epoch: 5 | loss: 0.0438493\n",
      "\tspeed: 0.0115s/iter; left time: 100.0078s\n",
      "\titers: 500, epoch: 5 | loss: 0.0572657\n",
      "\tspeed: 0.0115s/iter; left time: 98.7392s\n",
      "Epoch: 5 cost time: 6.843087673187256\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0570209 Vali Loss: 0.0372287 Test Loss: 0.1375001\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0528088\n",
      "\tspeed: 0.0384s/iter; left time: 324.3291s\n",
      "\titers: 200, epoch: 6 | loss: 0.0441225\n",
      "\tspeed: 0.0115s/iter; left time: 95.6209s\n",
      "\titers: 300, epoch: 6 | loss: 0.0589389\n",
      "\tspeed: 0.0115s/iter; left time: 94.5231s\n",
      "\titers: 400, epoch: 6 | loss: 0.0440373\n",
      "\tspeed: 0.0115s/iter; left time: 93.3821s\n",
      "\titers: 500, epoch: 6 | loss: 0.0396450\n",
      "\tspeed: 0.0115s/iter; left time: 92.2295s\n",
      "Epoch: 6 cost time: 6.833966255187988\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0524589 Vali Loss: 0.0373383 Test Loss: 0.1398008\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0418494\n",
      "\tspeed: 0.0383s/iter; left time: 301.7096s\n",
      "\titers: 200, epoch: 7 | loss: 0.0407792\n",
      "\tspeed: 0.0114s/iter; left time: 88.9962s\n",
      "\titers: 300, epoch: 7 | loss: 0.0543232\n",
      "\tspeed: 0.0114s/iter; left time: 87.8452s\n",
      "\titers: 400, epoch: 7 | loss: 0.0451226\n",
      "\tspeed: 0.0114s/iter; left time: 86.7136s\n",
      "\titers: 500, epoch: 7 | loss: 0.0532091\n",
      "\tspeed: 0.0114s/iter; left time: 85.5655s\n",
      "Epoch: 7 cost time: 6.818583011627197\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0501880 Vali Loss: 0.0371221 Test Loss: 0.1395622\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1421547830104828, mae:0.2445380836725235\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.0821424\n",
      "\tspeed: 0.0239s/iter; left time: 270.5470s\n",
      "\titers: 200, epoch: 1 | loss: 0.2055272\n",
      "\tspeed: 0.0121s/iter; left time: 135.2596s\n",
      "\titers: 300, epoch: 1 | loss: 0.1187363\n",
      "\tspeed: 0.0121s/iter; left time: 134.1861s\n",
      "\titers: 400, epoch: 1 | loss: 0.1494484\n",
      "\tspeed: 0.0121s/iter; left time: 132.7615s\n",
      "\titers: 500, epoch: 1 | loss: 0.1010472\n",
      "\tspeed: 0.0121s/iter; left time: 131.6243s\n",
      "Epoch: 1 cost time: 8.115211248397827\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1439106 Vali Loss: 0.0382256 Test Loss: 0.1186200\n",
      "Validation loss decreased (inf --> 0.038226).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1050378\n",
      "\tspeed: 0.0406s/iter; left time: 435.3888s\n",
      "\titers: 200, epoch: 2 | loss: 0.0721160\n",
      "\tspeed: 0.0111s/iter; left time: 117.6235s\n",
      "\titers: 300, epoch: 2 | loss: 0.1025319\n",
      "\tspeed: 0.0110s/iter; left time: 116.3181s\n",
      "\titers: 400, epoch: 2 | loss: 0.1451430\n",
      "\tspeed: 0.0111s/iter; left time: 115.3589s\n",
      "\titers: 500, epoch: 2 | loss: 0.1316334\n",
      "\tspeed: 0.0110s/iter; left time: 114.1129s\n",
      "Epoch: 2 cost time: 6.665112733840942\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1032850 Vali Loss: 0.0366824 Test Loss: 0.1188014\n",
      "Validation loss decreased (0.038226 --> 0.036682).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0892414\n",
      "\tspeed: 0.0388s/iter; left time: 394.7126s\n",
      "\titers: 200, epoch: 3 | loss: 0.0581202\n",
      "\tspeed: 0.0111s/iter; left time: 111.1955s\n",
      "\titers: 300, epoch: 3 | loss: 0.0749623\n",
      "\tspeed: 0.0110s/iter; left time: 109.8804s\n",
      "\titers: 400, epoch: 3 | loss: 0.0981017\n",
      "\tspeed: 0.0110s/iter; left time: 108.8076s\n",
      "\titers: 500, epoch: 3 | loss: 0.0787319\n",
      "\tspeed: 0.0110s/iter; left time: 107.8197s\n",
      "Epoch: 3 cost time: 6.617070436477661\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0738748 Vali Loss: 0.0344610 Test Loss: 0.1209547\n",
      "Validation loss decreased (0.036682 --> 0.034461).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0760859\n",
      "\tspeed: 0.0378s/iter; left time: 362.4301s\n",
      "\titers: 200, epoch: 4 | loss: 0.0513707\n",
      "\tspeed: 0.0111s/iter; left time: 105.0792s\n",
      "\titers: 300, epoch: 4 | loss: 0.0340439\n",
      "\tspeed: 0.0111s/iter; left time: 104.0293s\n",
      "\titers: 400, epoch: 4 | loss: 0.0534885\n",
      "\tspeed: 0.0111s/iter; left time: 103.1325s\n",
      "\titers: 500, epoch: 4 | loss: 0.0699914\n",
      "\tspeed: 0.0111s/iter; left time: 102.1838s\n",
      "Epoch: 4 cost time: 6.64502215385437\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0581782 Vali Loss: 0.0338879 Test Loss: 0.1185862\n",
      "Validation loss decreased (0.034461 --> 0.033888).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0432031\n",
      "\tspeed: 0.0401s/iter; left time: 361.6829s\n",
      "\titers: 200, epoch: 5 | loss: 0.0489614\n",
      "\tspeed: 0.0111s/iter; left time: 99.0721s\n",
      "\titers: 300, epoch: 5 | loss: 0.0570353\n",
      "\tspeed: 0.0111s/iter; left time: 98.1132s\n",
      "\titers: 400, epoch: 5 | loss: 0.0587425\n",
      "\tspeed: 0.0111s/iter; left time: 96.9736s\n",
      "\titers: 500, epoch: 5 | loss: 0.0486464\n",
      "\tspeed: 0.0111s/iter; left time: 95.8519s\n",
      "Epoch: 5 cost time: 6.647135972976685\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0501565 Vali Loss: 0.0345894 Test Loss: 0.1200429\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0685314\n",
      "\tspeed: 0.0388s/iter; left time: 328.2875s\n",
      "\titers: 200, epoch: 6 | loss: 0.0445221\n",
      "\tspeed: 0.0122s/iter; left time: 101.6589s\n",
      "\titers: 300, epoch: 6 | loss: 0.0449592\n",
      "\tspeed: 0.0121s/iter; left time: 100.0590s\n",
      "\titers: 400, epoch: 6 | loss: 0.0366026\n",
      "\tspeed: 0.0121s/iter; left time: 98.7977s\n",
      "\titers: 500, epoch: 6 | loss: 0.0413571\n",
      "\tspeed: 0.0121s/iter; left time: 97.6111s\n",
      "Epoch: 6 cost time: 7.207422733306885\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0462577 Vali Loss: 0.0350961 Test Loss: 0.1203088\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0393076\n",
      "\tspeed: 0.0395s/iter; left time: 311.2288s\n",
      "\titers: 200, epoch: 7 | loss: 0.0463046\n",
      "\tspeed: 0.0121s/iter; left time: 94.2976s\n",
      "\titers: 300, epoch: 7 | loss: 0.0442514\n",
      "\tspeed: 0.0122s/iter; left time: 93.6283s\n",
      "\titers: 400, epoch: 7 | loss: 0.0389046\n",
      "\tspeed: 0.0121s/iter; left time: 91.8751s\n",
      "\titers: 500, epoch: 7 | loss: 0.0392465\n",
      "\tspeed: 0.0121s/iter; left time: 90.7587s\n",
      "Epoch: 7 cost time: 7.21053671836853\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0443031 Vali Loss: 0.0349080 Test Loss: 0.1201539\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11877221614122391, mae:0.21709507703781128\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1887072\n",
      "\tspeed: 0.0235s/iter; left time: 265.5318s\n",
      "\titers: 200, epoch: 1 | loss: 0.1759307\n",
      "\tspeed: 0.0113s/iter; left time: 126.8827s\n",
      "\titers: 300, epoch: 1 | loss: 0.2258705\n",
      "\tspeed: 0.0113s/iter; left time: 125.4351s\n",
      "\titers: 400, epoch: 1 | loss: 0.1309080\n",
      "\tspeed: 0.0113s/iter; left time: 124.4595s\n",
      "\titers: 500, epoch: 1 | loss: 0.1294963\n",
      "\tspeed: 0.0113s/iter; left time: 123.7174s\n",
      "Epoch: 1 cost time: 7.720759630203247\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1765686 Vali Loss: 0.0423134 Test Loss: 0.1332751\n",
      "Validation loss decreased (inf --> 0.042313).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1715980\n",
      "\tspeed: 0.0385s/iter; left time: 412.6575s\n",
      "\titers: 200, epoch: 2 | loss: 0.1917507\n",
      "\tspeed: 0.0102s/iter; left time: 108.8288s\n",
      "\titers: 300, epoch: 2 | loss: 0.0993218\n",
      "\tspeed: 0.0103s/iter; left time: 107.9865s\n",
      "\titers: 400, epoch: 2 | loss: 0.0857142\n",
      "\tspeed: 0.0102s/iter; left time: 106.7690s\n",
      "\titers: 500, epoch: 2 | loss: 0.1163648\n",
      "\tspeed: 0.0102s/iter; left time: 105.8188s\n",
      "Epoch: 2 cost time: 6.165524959564209\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1256958 Vali Loss: 0.0456945 Test Loss: 0.1378805\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1145572\n",
      "\tspeed: 0.0363s/iter; left time: 368.7864s\n",
      "\titers: 200, epoch: 3 | loss: 0.0797853\n",
      "\tspeed: 0.0103s/iter; left time: 103.6650s\n",
      "\titers: 300, epoch: 3 | loss: 0.0870536\n",
      "\tspeed: 0.0103s/iter; left time: 102.5899s\n",
      "\titers: 400, epoch: 3 | loss: 0.0757016\n",
      "\tspeed: 0.0103s/iter; left time: 101.3541s\n",
      "\titers: 500, epoch: 3 | loss: 0.1084943\n",
      "\tspeed: 0.0102s/iter; left time: 100.0305s\n",
      "Epoch: 3 cost time: 6.167678356170654\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0880999 Vali Loss: 0.0434391 Test Loss: 0.1412951\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0574450\n",
      "\tspeed: 0.0363s/iter; left time: 348.6281s\n",
      "\titers: 200, epoch: 4 | loss: 0.0721468\n",
      "\tspeed: 0.0102s/iter; left time: 97.1638s\n",
      "\titers: 300, epoch: 4 | loss: 0.0654138\n",
      "\tspeed: 0.0103s/iter; left time: 96.2996s\n",
      "\titers: 400, epoch: 4 | loss: 0.0990636\n",
      "\tspeed: 0.0103s/iter; left time: 95.4524s\n",
      "\titers: 500, epoch: 4 | loss: 0.0713094\n",
      "\tspeed: 0.0102s/iter; left time: 94.1315s\n",
      "Epoch: 4 cost time: 6.1442790031433105\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0684439 Vali Loss: 0.0446042 Test Loss: 0.1375321\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1334594041109085, mae:0.23299375176429749\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1846062\n",
      "\tspeed: 0.0253s/iter; left time: 285.6287s\n",
      "\titers: 200, epoch: 1 | loss: 0.1460764\n",
      "\tspeed: 0.0137s/iter; left time: 153.0818s\n",
      "\titers: 300, epoch: 1 | loss: 0.2441318\n",
      "\tspeed: 0.0136s/iter; left time: 151.2038s\n",
      "\titers: 400, epoch: 1 | loss: 0.1463033\n",
      "\tspeed: 0.0136s/iter; left time: 149.9602s\n",
      "\titers: 500, epoch: 1 | loss: 0.1389970\n",
      "\tspeed: 0.0137s/iter; left time: 148.9832s\n",
      "Epoch: 1 cost time: 8.989021301269531\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1763569 Vali Loss: 0.0408216 Test Loss: 0.1298443\n",
      "Validation loss decreased (inf --> 0.040822).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1671756\n",
      "\tspeed: 0.0449s/iter; left time: 482.1804s\n",
      "\titers: 200, epoch: 2 | loss: 0.1170338\n",
      "\tspeed: 0.0136s/iter; left time: 144.0642s\n",
      "\titers: 300, epoch: 2 | loss: 0.1050745\n",
      "\tspeed: 0.0135s/iter; left time: 142.3382s\n",
      "\titers: 400, epoch: 2 | loss: 0.1136662\n",
      "\tspeed: 0.0135s/iter; left time: 141.0914s\n",
      "\titers: 500, epoch: 2 | loss: 0.1486110\n",
      "\tspeed: 0.0135s/iter; left time: 139.5268s\n",
      "Epoch: 2 cost time: 8.002887487411499\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1301342 Vali Loss: 0.0375515 Test Loss: 0.1284046\n",
      "Validation loss decreased (0.040822 --> 0.037552).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1702181\n",
      "\tspeed: 0.0451s/iter; left time: 458.6202s\n",
      "\titers: 200, epoch: 3 | loss: 0.0794896\n",
      "\tspeed: 0.0137s/iter; left time: 137.7625s\n",
      "\titers: 300, epoch: 3 | loss: 0.0642059\n",
      "\tspeed: 0.0137s/iter; left time: 136.3141s\n",
      "\titers: 400, epoch: 3 | loss: 0.0825356\n",
      "\tspeed: 0.0137s/iter; left time: 134.9923s\n",
      "\titers: 500, epoch: 3 | loss: 0.0720266\n",
      "\tspeed: 0.0137s/iter; left time: 133.3584s\n",
      "Epoch: 3 cost time: 8.100081443786621\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0985180 Vali Loss: 0.0344121 Test Loss: 0.1251863\n",
      "Validation loss decreased (0.037552 --> 0.034412).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0837777\n",
      "\tspeed: 0.0464s/iter; left time: 444.5881s\n",
      "\titers: 200, epoch: 4 | loss: 0.0715140\n",
      "\tspeed: 0.0136s/iter; left time: 129.1184s\n",
      "\titers: 300, epoch: 4 | loss: 0.1077494\n",
      "\tspeed: 0.0136s/iter; left time: 127.9726s\n",
      "\titers: 400, epoch: 4 | loss: 0.0905709\n",
      "\tspeed: 0.0136s/iter; left time: 126.3546s\n",
      "\titers: 500, epoch: 4 | loss: 0.0786342\n",
      "\tspeed: 0.0136s/iter; left time: 125.0781s\n",
      "Epoch: 4 cost time: 8.100306749343872\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0826425 Vali Loss: 0.0340419 Test Loss: 0.1338193\n",
      "Validation loss decreased (0.034412 --> 0.034042).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0822442\n",
      "\tspeed: 0.0454s/iter; left time: 409.6006s\n",
      "\titers: 200, epoch: 5 | loss: 0.0708540\n",
      "\tspeed: 0.0136s/iter; left time: 121.1705s\n",
      "\titers: 300, epoch: 5 | loss: 0.0620825\n",
      "\tspeed: 0.0136s/iter; left time: 119.8905s\n",
      "\titers: 400, epoch: 5 | loss: 0.0708510\n",
      "\tspeed: 0.0136s/iter; left time: 118.5397s\n",
      "\titers: 500, epoch: 5 | loss: 0.0573040\n",
      "\tspeed: 0.0136s/iter; left time: 117.1170s\n",
      "Epoch: 5 cost time: 8.067023754119873\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0745537 Vali Loss: 0.0338613 Test Loss: 0.1355065\n",
      "Validation loss decreased (0.034042 --> 0.033861).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0625130\n",
      "\tspeed: 0.0460s/iter; left time: 388.6232s\n",
      "\titers: 200, epoch: 6 | loss: 0.1003545\n",
      "\tspeed: 0.0136s/iter; left time: 113.4476s\n",
      "\titers: 300, epoch: 6 | loss: 0.0615088\n",
      "\tspeed: 0.0136s/iter; left time: 112.0945s\n",
      "\titers: 400, epoch: 6 | loss: 0.0667660\n",
      "\tspeed: 0.0136s/iter; left time: 110.7447s\n",
      "\titers: 500, epoch: 6 | loss: 0.0611838\n",
      "\tspeed: 0.0136s/iter; left time: 109.3652s\n",
      "Epoch: 6 cost time: 8.055323839187622\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0702705 Vali Loss: 0.0333649 Test Loss: 0.1425450\n",
      "Validation loss decreased (0.033861 --> 0.033365).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0654657\n",
      "\tspeed: 0.0456s/iter; left time: 359.7506s\n",
      "\titers: 200, epoch: 7 | loss: 0.0705935\n",
      "\tspeed: 0.0136s/iter; left time: 106.1311s\n",
      "\titers: 300, epoch: 7 | loss: 0.0509100\n",
      "\tspeed: 0.0136s/iter; left time: 104.6014s\n",
      "\titers: 400, epoch: 7 | loss: 0.0633409\n",
      "\tspeed: 0.0136s/iter; left time: 103.2258s\n",
      "\titers: 500, epoch: 7 | loss: 0.0975587\n",
      "\tspeed: 0.0136s/iter; left time: 101.8052s\n",
      "Epoch: 7 cost time: 8.082286357879639\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0681824 Vali Loss: 0.0336655 Test Loss: 0.1403572\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0956806\n",
      "\tspeed: 0.0452s/iter; left time: 330.8063s\n",
      "\titers: 200, epoch: 8 | loss: 0.0627450\n",
      "\tspeed: 0.0137s/iter; left time: 98.7426s\n",
      "\titers: 300, epoch: 8 | loss: 0.0663303\n",
      "\tspeed: 0.0137s/iter; left time: 97.3525s\n",
      "\titers: 400, epoch: 8 | loss: 0.0545766\n",
      "\tspeed: 0.0137s/iter; left time: 95.9847s\n",
      "\titers: 500, epoch: 8 | loss: 0.0640488\n",
      "\tspeed: 0.0137s/iter; left time: 94.6241s\n",
      "Epoch: 8 cost time: 8.129994869232178\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0669320 Vali Loss: 0.0336026 Test Loss: 0.1410938\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0882823\n",
      "\tspeed: 0.0455s/iter; left time: 306.5015s\n",
      "\titers: 200, epoch: 9 | loss: 0.0625098\n",
      "\tspeed: 0.0151s/iter; left time: 100.4885s\n",
      "\titers: 300, epoch: 9 | loss: 0.0696202\n",
      "\tspeed: 0.0167s/iter; left time: 108.9736s\n",
      "\titers: 400, epoch: 9 | loss: 0.0970610\n",
      "\tspeed: 0.0163s/iter; left time: 104.7046s\n",
      "\titers: 500, epoch: 9 | loss: 0.0535186\n",
      "\tspeed: 0.0158s/iter; left time: 100.0336s\n",
      "Epoch: 9 cost time: 9.235925912857056\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0663572 Vali Loss: 0.0335445 Test Loss: 0.1410328\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14278028905391693, mae:0.2392059564590454\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2086009\n",
      "\tspeed: 0.0250s/iter; left time: 282.0467s\n",
      "\titers: 200, epoch: 1 | loss: 0.1268833\n",
      "\tspeed: 0.0135s/iter; left time: 151.0621s\n",
      "\titers: 300, epoch: 1 | loss: 0.0825019\n",
      "\tspeed: 0.0135s/iter; left time: 149.4465s\n",
      "\titers: 400, epoch: 1 | loss: 0.1786777\n",
      "\tspeed: 0.0135s/iter; left time: 148.6024s\n",
      "\titers: 500, epoch: 1 | loss: 0.1494922\n",
      "\tspeed: 0.0135s/iter; left time: 147.1702s\n",
      "Epoch: 1 cost time: 8.882765769958496\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1467718 Vali Loss: 0.0342594 Test Loss: 0.1109782\n",
      "Validation loss decreased (inf --> 0.034259).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0795360\n",
      "\tspeed: 0.0473s/iter; left time: 507.9208s\n",
      "\titers: 200, epoch: 2 | loss: 0.1822288\n",
      "\tspeed: 0.0134s/iter; left time: 142.6120s\n",
      "\titers: 300, epoch: 2 | loss: 0.1266771\n",
      "\tspeed: 0.0135s/iter; left time: 141.7247s\n",
      "\titers: 400, epoch: 2 | loss: 0.0741273\n",
      "\tspeed: 0.0134s/iter; left time: 139.8737s\n",
      "\titers: 500, epoch: 2 | loss: 0.0941638\n",
      "\tspeed: 0.0134s/iter; left time: 138.5346s\n",
      "Epoch: 2 cost time: 7.987172365188599\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1086508 Vali Loss: 0.0347206 Test Loss: 0.1152831\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1089271\n",
      "\tspeed: 0.0474s/iter; left time: 481.2440s\n",
      "\titers: 200, epoch: 3 | loss: 0.0491227\n",
      "\tspeed: 0.0134s/iter; left time: 134.7198s\n",
      "\titers: 300, epoch: 3 | loss: 0.0805668\n",
      "\tspeed: 0.0134s/iter; left time: 133.5506s\n",
      "\titers: 400, epoch: 3 | loss: 0.0633435\n",
      "\tspeed: 0.0134s/iter; left time: 132.3824s\n",
      "\titers: 500, epoch: 3 | loss: 0.0806450\n",
      "\tspeed: 0.0134s/iter; left time: 131.0201s\n",
      "Epoch: 3 cost time: 8.009812593460083\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0849287 Vali Loss: 0.0339566 Test Loss: 0.1128228\n",
      "Validation loss decreased (0.034259 --> 0.033957).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0430873\n",
      "\tspeed: 0.0456s/iter; left time: 437.6705s\n",
      "\titers: 200, epoch: 4 | loss: 0.0846040\n",
      "\tspeed: 0.0136s/iter; left time: 128.6509s\n",
      "\titers: 300, epoch: 4 | loss: 0.0854110\n",
      "\tspeed: 0.0135s/iter; left time: 127.0536s\n",
      "\titers: 400, epoch: 4 | loss: 0.0510526\n",
      "\tspeed: 0.0135s/iter; left time: 125.7608s\n",
      "\titers: 500, epoch: 4 | loss: 0.0482591\n",
      "\tspeed: 0.0135s/iter; left time: 124.0822s\n",
      "Epoch: 4 cost time: 8.046814918518066\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0707148 Vali Loss: 0.0325064 Test Loss: 0.1138103\n",
      "Validation loss decreased (0.033957 --> 0.032506).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0418888\n",
      "\tspeed: 0.0450s/iter; left time: 405.5050s\n",
      "\titers: 200, epoch: 5 | loss: 0.0535754\n",
      "\tspeed: 0.0134s/iter; left time: 119.6209s\n",
      "\titers: 300, epoch: 5 | loss: 0.0558173\n",
      "\tspeed: 0.0134s/iter; left time: 118.4091s\n",
      "\titers: 400, epoch: 5 | loss: 0.0423305\n",
      "\tspeed: 0.0134s/iter; left time: 116.9570s\n",
      "\titers: 500, epoch: 5 | loss: 0.0710006\n",
      "\tspeed: 0.0134s/iter; left time: 115.6490s\n",
      "Epoch: 5 cost time: 7.96323823928833\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0630975 Vali Loss: 0.0331896 Test Loss: 0.1157545\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0559983\n",
      "\tspeed: 0.0460s/iter; left time: 388.6570s\n",
      "\titers: 200, epoch: 6 | loss: 0.0775681\n",
      "\tspeed: 0.0134s/iter; left time: 112.0778s\n",
      "\titers: 300, epoch: 6 | loss: 0.0614349\n",
      "\tspeed: 0.0134s/iter; left time: 110.8677s\n",
      "\titers: 400, epoch: 6 | loss: 0.0877111\n",
      "\tspeed: 0.0134s/iter; left time: 109.6073s\n",
      "\titers: 500, epoch: 6 | loss: 0.0537601\n",
      "\tspeed: 0.0134s/iter; left time: 108.2539s\n",
      "Epoch: 6 cost time: 7.990378141403198\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0592694 Vali Loss: 0.0337582 Test Loss: 0.1174152\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0422037\n",
      "\tspeed: 0.0460s/iter; left time: 362.2612s\n",
      "\titers: 200, epoch: 7 | loss: 0.0650079\n",
      "\tspeed: 0.0135s/iter; left time: 105.3910s\n",
      "\titers: 300, epoch: 7 | loss: 0.0573400\n",
      "\tspeed: 0.0135s/iter; left time: 103.7487s\n",
      "\titers: 400, epoch: 7 | loss: 0.0773056\n",
      "\tspeed: 0.0135s/iter; left time: 102.3324s\n",
      "\titers: 500, epoch: 7 | loss: 0.0634231\n",
      "\tspeed: 0.0135s/iter; left time: 100.8746s\n",
      "Epoch: 7 cost time: 8.008410215377808\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0573789 Vali Loss: 0.0333042 Test Loss: 0.1173780\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1139611005783081, mae:0.2082395851612091\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1144149\n",
      "\tspeed: 0.0252s/iter; left time: 284.7158s\n",
      "\titers: 200, epoch: 1 | loss: 0.2406504\n",
      "\tspeed: 0.0134s/iter; left time: 150.2339s\n",
      "\titers: 300, epoch: 1 | loss: 0.2033081\n",
      "\tspeed: 0.0134s/iter; left time: 148.6007s\n",
      "\titers: 400, epoch: 1 | loss: 0.1833813\n",
      "\tspeed: 0.0134s/iter; left time: 147.3582s\n",
      "\titers: 500, epoch: 1 | loss: 0.2188929\n",
      "\tspeed: 0.0134s/iter; left time: 146.4056s\n",
      "Epoch: 1 cost time: 8.872413635253906\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1830724 Vali Loss: 0.0439534 Test Loss: 0.1360427\n",
      "Validation loss decreased (inf --> 0.043953).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1161361\n",
      "\tspeed: 0.0443s/iter; left time: 475.3628s\n",
      "\titers: 200, epoch: 2 | loss: 0.1070751\n",
      "\tspeed: 0.0133s/iter; left time: 141.8372s\n",
      "\titers: 300, epoch: 2 | loss: 0.0788092\n",
      "\tspeed: 0.0134s/iter; left time: 140.6175s\n",
      "\titers: 400, epoch: 2 | loss: 0.1083257\n",
      "\tspeed: 0.0133s/iter; left time: 139.1059s\n",
      "\titers: 500, epoch: 2 | loss: 0.1613746\n",
      "\tspeed: 0.0134s/iter; left time: 137.9462s\n",
      "Epoch: 2 cost time: 7.9257652759552\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1330781 Vali Loss: 0.0425666 Test Loss: 0.1355622\n",
      "Validation loss decreased (0.043953 --> 0.042567).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1262835\n",
      "\tspeed: 0.0449s/iter; left time: 456.4993s\n",
      "\titers: 200, epoch: 3 | loss: 0.1231543\n",
      "\tspeed: 0.0134s/iter; left time: 134.7937s\n",
      "\titers: 300, epoch: 3 | loss: 0.1130273\n",
      "\tspeed: 0.0134s/iter; left time: 133.5371s\n",
      "\titers: 400, epoch: 3 | loss: 0.0933835\n",
      "\tspeed: 0.0134s/iter; left time: 131.9778s\n",
      "\titers: 500, epoch: 3 | loss: 0.0828833\n",
      "\tspeed: 0.0134s/iter; left time: 130.8776s\n",
      "Epoch: 3 cost time: 7.959874629974365\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1018775 Vali Loss: 0.0402669 Test Loss: 0.1315529\n",
      "Validation loss decreased (0.042567 --> 0.040267).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0914271\n",
      "\tspeed: 0.0442s/iter; left time: 424.0340s\n",
      "\titers: 200, epoch: 4 | loss: 0.0881497\n",
      "\tspeed: 0.0134s/iter; left time: 126.9179s\n",
      "\titers: 300, epoch: 4 | loss: 0.1122045\n",
      "\tspeed: 0.0134s/iter; left time: 125.6801s\n",
      "\titers: 400, epoch: 4 | loss: 0.0924591\n",
      "\tspeed: 0.0134s/iter; left time: 124.1910s\n",
      "\titers: 500, epoch: 4 | loss: 0.0600994\n",
      "\tspeed: 0.0134s/iter; left time: 123.1078s\n",
      "Epoch: 4 cost time: 7.933874130249023\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0849475 Vali Loss: 0.0398159 Test Loss: 0.1358591\n",
      "Validation loss decreased (0.040267 --> 0.039816).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0953159\n",
      "\tspeed: 0.0452s/iter; left time: 407.3871s\n",
      "\titers: 200, epoch: 5 | loss: 0.0844836\n",
      "\tspeed: 0.0135s/iter; left time: 120.4714s\n",
      "\titers: 300, epoch: 5 | loss: 0.0806522\n",
      "\tspeed: 0.0135s/iter; left time: 119.0989s\n",
      "\titers: 400, epoch: 5 | loss: 0.1095421\n",
      "\tspeed: 0.0135s/iter; left time: 117.5686s\n",
      "\titers: 500, epoch: 5 | loss: 0.0639920\n",
      "\tspeed: 0.0135s/iter; left time: 116.3398s\n",
      "Epoch: 5 cost time: 7.9931418895721436\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0758155 Vali Loss: 0.0397634 Test Loss: 0.1374840\n",
      "Validation loss decreased (0.039816 --> 0.039763).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0751844\n",
      "\tspeed: 0.0493s/iter; left time: 416.8253s\n",
      "\titers: 200, epoch: 6 | loss: 0.0710672\n",
      "\tspeed: 0.0151s/iter; left time: 126.0852s\n",
      "\titers: 300, epoch: 6 | loss: 0.0579034\n",
      "\tspeed: 0.0151s/iter; left time: 124.6281s\n",
      "\titers: 400, epoch: 6 | loss: 0.1137909\n",
      "\tspeed: 0.0151s/iter; left time: 122.7402s\n",
      "\titers: 500, epoch: 6 | loss: 0.0788763\n",
      "\tspeed: 0.0140s/iter; left time: 112.9238s\n",
      "Epoch: 6 cost time: 8.698703050613403\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0710732 Vali Loss: 0.0392962 Test Loss: 0.1376793\n",
      "Validation loss decreased (0.039763 --> 0.039296).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0663552\n",
      "\tspeed: 0.0447s/iter; left time: 352.0698s\n",
      "\titers: 200, epoch: 7 | loss: 0.0596826\n",
      "\tspeed: 0.0135s/iter; left time: 105.2372s\n",
      "\titers: 300, epoch: 7 | loss: 0.0695392\n",
      "\tspeed: 0.0135s/iter; left time: 103.9101s\n",
      "\titers: 400, epoch: 7 | loss: 0.0918856\n",
      "\tspeed: 0.0135s/iter; left time: 102.6412s\n",
      "\titers: 500, epoch: 7 | loss: 0.0480557\n",
      "\tspeed: 0.0135s/iter; left time: 101.0962s\n",
      "Epoch: 7 cost time: 8.069202661514282\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0685185 Vali Loss: 0.0394485 Test Loss: 0.1380076\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0535319\n",
      "\tspeed: 0.0437s/iter; left time: 319.1831s\n",
      "\titers: 200, epoch: 8 | loss: 0.0492976\n",
      "\tspeed: 0.0133s/iter; left time: 96.1874s\n",
      "\titers: 300, epoch: 8 | loss: 0.0699289\n",
      "\tspeed: 0.0133s/iter; left time: 94.9063s\n",
      "\titers: 400, epoch: 8 | loss: 0.0725985\n",
      "\tspeed: 0.0134s/iter; left time: 93.7366s\n",
      "\titers: 500, epoch: 8 | loss: 0.0543380\n",
      "\tspeed: 0.0133s/iter; left time: 92.2325s\n",
      "Epoch: 8 cost time: 7.866165399551392\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0673541 Vali Loss: 0.0400491 Test Loss: 0.1392234\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0756388\n",
      "\tspeed: 0.0453s/iter; left time: 305.1627s\n",
      "\titers: 200, epoch: 9 | loss: 0.0615586\n",
      "\tspeed: 0.0135s/iter; left time: 89.6676s\n",
      "\titers: 300, epoch: 9 | loss: 0.0448848\n",
      "\tspeed: 0.0135s/iter; left time: 88.4749s\n",
      "\titers: 400, epoch: 9 | loss: 0.0649287\n",
      "\tspeed: 0.0135s/iter; left time: 87.0241s\n",
      "\titers: 500, epoch: 9 | loss: 0.0648801\n",
      "\tspeed: 0.0135s/iter; left time: 85.7405s\n",
      "Epoch: 9 cost time: 8.144781112670898\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0667382 Vali Loss: 0.0394727 Test Loss: 0.1387659\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13790731132030487, mae:0.24107909202575684\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2078299\n",
      "\tspeed: 0.0299s/iter; left time: 338.2921s\n",
      "\titers: 200, epoch: 1 | loss: 0.1424055\n",
      "\tspeed: 0.0177s/iter; left time: 198.6591s\n",
      "\titers: 300, epoch: 1 | loss: 0.1897625\n",
      "\tspeed: 0.0178s/iter; left time: 197.2951s\n",
      "\titers: 400, epoch: 1 | loss: 0.1409163\n",
      "\tspeed: 0.0178s/iter; left time: 195.6991s\n",
      "\titers: 500, epoch: 1 | loss: 0.1303341\n",
      "\tspeed: 0.0178s/iter; left time: 193.8146s\n",
      "Epoch: 1 cost time: 11.394493103027344\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1713149 Vali Loss: 0.0450306 Test Loss: 0.1379519\n",
      "Validation loss decreased (inf --> 0.045031).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1361812\n",
      "\tspeed: 0.0527s/iter; left time: 566.0586s\n",
      "\titers: 200, epoch: 2 | loss: 0.1315437\n",
      "\tspeed: 0.0178s/iter; left time: 189.3635s\n",
      "\titers: 300, epoch: 2 | loss: 0.1162446\n",
      "\tspeed: 0.0178s/iter; left time: 187.3443s\n",
      "\titers: 400, epoch: 2 | loss: 0.0781730\n",
      "\tspeed: 0.0178s/iter; left time: 185.4151s\n",
      "\titers: 500, epoch: 2 | loss: 0.0761840\n",
      "\tspeed: 0.0178s/iter; left time: 183.5739s\n",
      "Epoch: 2 cost time: 10.440245389938354\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1173301 Vali Loss: 0.0351438 Test Loss: 0.1238170\n",
      "Validation loss decreased (0.045031 --> 0.035144).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0778482\n",
      "\tspeed: 0.0512s/iter; left time: 519.9251s\n",
      "\titers: 200, epoch: 3 | loss: 0.0825490\n",
      "\tspeed: 0.0163s/iter; left time: 164.3218s\n",
      "\titers: 300, epoch: 3 | loss: 0.1426406\n",
      "\tspeed: 0.0163s/iter; left time: 162.6235s\n",
      "\titers: 400, epoch: 3 | loss: 0.0660927\n",
      "\tspeed: 0.0163s/iter; left time: 160.9285s\n",
      "\titers: 500, epoch: 3 | loss: 0.0541380\n",
      "\tspeed: 0.0163s/iter; left time: 159.4271s\n",
      "Epoch: 3 cost time: 9.612198114395142\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0766610 Vali Loss: 0.0350214 Test Loss: 0.1284641\n",
      "Validation loss decreased (0.035144 --> 0.035021).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0515998\n",
      "\tspeed: 0.0498s/iter; left time: 477.8186s\n",
      "\titers: 200, epoch: 4 | loss: 0.0599067\n",
      "\tspeed: 0.0163s/iter; left time: 155.0982s\n",
      "\titers: 300, epoch: 4 | loss: 0.0413344\n",
      "\tspeed: 0.0164s/iter; left time: 154.1632s\n",
      "\titers: 400, epoch: 4 | loss: 0.0718858\n",
      "\tspeed: 0.0164s/iter; left time: 152.5855s\n",
      "\titers: 500, epoch: 4 | loss: 0.0584151\n",
      "\tspeed: 0.0164s/iter; left time: 150.7370s\n",
      "Epoch: 4 cost time: 9.6574866771698\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0558121 Vali Loss: 0.0368052 Test Loss: 0.1358294\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0419310\n",
      "\tspeed: 0.0504s/iter; left time: 454.4853s\n",
      "\titers: 200, epoch: 5 | loss: 0.0496591\n",
      "\tspeed: 0.0163s/iter; left time: 145.8533s\n",
      "\titers: 300, epoch: 5 | loss: 0.0489003\n",
      "\tspeed: 0.0163s/iter; left time: 144.1862s\n",
      "\titers: 400, epoch: 5 | loss: 0.0590560\n",
      "\tspeed: 0.0163s/iter; left time: 142.4145s\n",
      "\titers: 500, epoch: 5 | loss: 0.0397884\n",
      "\tspeed: 0.0163s/iter; left time: 140.9090s\n",
      "Epoch: 5 cost time: 9.635277271270752\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0466488 Vali Loss: 0.0354055 Test Loss: 0.1407815\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0395191\n",
      "\tspeed: 0.0515s/iter; left time: 435.4149s\n",
      "\titers: 200, epoch: 6 | loss: 0.0326661\n",
      "\tspeed: 0.0163s/iter; left time: 136.4583s\n",
      "\titers: 300, epoch: 6 | loss: 0.0438034\n",
      "\tspeed: 0.0163s/iter; left time: 134.8899s\n",
      "\titers: 400, epoch: 6 | loss: 0.0483762\n",
      "\tspeed: 0.0163s/iter; left time: 133.0532s\n",
      "\titers: 500, epoch: 6 | loss: 0.0372350\n",
      "\tspeed: 0.0163s/iter; left time: 131.3526s\n",
      "Epoch: 6 cost time: 9.602179288864136\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0420321 Vali Loss: 0.0355620 Test Loss: 0.1407559\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12866666913032532, mae:0.23136582970619202\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1477773\n",
      "\tspeed: 0.0274s/iter; left time: 309.9178s\n",
      "\titers: 200, epoch: 1 | loss: 0.0753202\n",
      "\tspeed: 0.0158s/iter; left time: 177.1375s\n",
      "\titers: 300, epoch: 1 | loss: 0.1597915\n",
      "\tspeed: 0.0158s/iter; left time: 175.3575s\n",
      "\titers: 400, epoch: 1 | loss: 0.1282118\n",
      "\tspeed: 0.0158s/iter; left time: 173.8594s\n",
      "\titers: 500, epoch: 1 | loss: 0.0819353\n",
      "\tspeed: 0.0158s/iter; left time: 172.3019s\n",
      "Epoch: 1 cost time: 10.227953910827637\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1416919 Vali Loss: 0.0340319 Test Loss: 0.1118738\n",
      "Validation loss decreased (inf --> 0.034032).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1035396\n",
      "\tspeed: 0.0495s/iter; left time: 531.4050s\n",
      "\titers: 200, epoch: 2 | loss: 0.0897047\n",
      "\tspeed: 0.0159s/iter; left time: 169.4592s\n",
      "\titers: 300, epoch: 2 | loss: 0.1571794\n",
      "\tspeed: 0.0159s/iter; left time: 167.7646s\n",
      "\titers: 400, epoch: 2 | loss: 0.0875617\n",
      "\tspeed: 0.0159s/iter; left time: 166.0461s\n",
      "\titers: 500, epoch: 2 | loss: 0.0733796\n",
      "\tspeed: 0.0159s/iter; left time: 164.6765s\n",
      "Epoch: 2 cost time: 9.393481254577637\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.0999425 Vali Loss: 0.0362646 Test Loss: 0.1160469\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0676384\n",
      "\tspeed: 0.0489s/iter; left time: 496.9477s\n",
      "\titers: 200, epoch: 3 | loss: 0.0603915\n",
      "\tspeed: 0.0158s/iter; left time: 159.4311s\n",
      "\titers: 300, epoch: 3 | loss: 0.0641372\n",
      "\tspeed: 0.0159s/iter; left time: 158.0344s\n",
      "\titers: 400, epoch: 3 | loss: 0.0787638\n",
      "\tspeed: 0.0159s/iter; left time: 156.3682s\n",
      "\titers: 500, epoch: 3 | loss: 0.0494916\n",
      "\tspeed: 0.0159s/iter; left time: 154.7947s\n",
      "Epoch: 3 cost time: 9.346500158309937\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0663488 Vali Loss: 0.0348005 Test Loss: 0.1191276\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0568060\n",
      "\tspeed: 0.0495s/iter; left time: 474.7051s\n",
      "\titers: 200, epoch: 4 | loss: 0.0655669\n",
      "\tspeed: 0.0158s/iter; left time: 149.9872s\n",
      "\titers: 300, epoch: 4 | loss: 0.0402272\n",
      "\tspeed: 0.0158s/iter; left time: 148.3447s\n",
      "\titers: 400, epoch: 4 | loss: 0.0446116\n",
      "\tspeed: 0.0158s/iter; left time: 146.6136s\n",
      "\titers: 500, epoch: 4 | loss: 0.0549300\n",
      "\tspeed: 0.0158s/iter; left time: 144.9008s\n",
      "Epoch: 4 cost time: 9.324583768844604\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0491865 Vali Loss: 0.0349448 Test Loss: 0.1214562\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11205563694238663, mae:0.20530520379543304\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1708764\n",
      "\tspeed: 0.0280s/iter; left time: 316.3273s\n",
      "\titers: 200, epoch: 1 | loss: 0.1193258\n",
      "\tspeed: 0.0161s/iter; left time: 180.3134s\n",
      "\titers: 300, epoch: 1 | loss: 0.1656508\n",
      "\tspeed: 0.0161s/iter; left time: 178.8348s\n",
      "\titers: 400, epoch: 1 | loss: 0.1511722\n",
      "\tspeed: 0.0161s/iter; left time: 176.9747s\n",
      "\titers: 500, epoch: 1 | loss: 0.2388563\n",
      "\tspeed: 0.0161s/iter; left time: 175.6477s\n",
      "Epoch: 1 cost time: 10.427111864089966\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1793625 Vali Loss: 0.0441069 Test Loss: 0.1392629\n",
      "Validation loss decreased (inf --> 0.044107).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1864589\n",
      "\tspeed: 0.0483s/iter; left time: 518.1492s\n",
      "\titers: 200, epoch: 2 | loss: 0.0733957\n",
      "\tspeed: 0.0147s/iter; left time: 155.7823s\n",
      "\titers: 300, epoch: 2 | loss: 0.1169123\n",
      "\tspeed: 0.0146s/iter; left time: 154.1904s\n",
      "\titers: 400, epoch: 2 | loss: 0.1325214\n",
      "\tspeed: 0.0146s/iter; left time: 152.6366s\n",
      "\titers: 500, epoch: 2 | loss: 0.1146732\n",
      "\tspeed: 0.0146s/iter; left time: 151.1193s\n",
      "Epoch: 2 cost time: 8.655827283859253\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1177451 Vali Loss: 0.0400731 Test Loss: 0.1421569\n",
      "Validation loss decreased (0.044107 --> 0.040073).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0823320\n",
      "\tspeed: 0.0478s/iter; left time: 485.2151s\n",
      "\titers: 200, epoch: 3 | loss: 0.0564177\n",
      "\tspeed: 0.0146s/iter; left time: 147.0375s\n",
      "\titers: 300, epoch: 3 | loss: 0.0987329\n",
      "\tspeed: 0.0146s/iter; left time: 145.5171s\n",
      "\titers: 400, epoch: 3 | loss: 0.0602564\n",
      "\tspeed: 0.0146s/iter; left time: 143.9746s\n",
      "\titers: 500, epoch: 3 | loss: 0.0829141\n",
      "\tspeed: 0.0146s/iter; left time: 142.5362s\n",
      "Epoch: 3 cost time: 8.640058755874634\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0726184 Vali Loss: 0.0375189 Test Loss: 0.1363660\n",
      "Validation loss decreased (0.040073 --> 0.037519).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0425757\n",
      "\tspeed: 0.0478s/iter; left time: 458.8941s\n",
      "\titers: 200, epoch: 4 | loss: 0.0856920\n",
      "\tspeed: 0.0146s/iter; left time: 138.8573s\n",
      "\titers: 300, epoch: 4 | loss: 0.0566378\n",
      "\tspeed: 0.0146s/iter; left time: 137.3810s\n",
      "\titers: 400, epoch: 4 | loss: 0.0437738\n",
      "\tspeed: 0.0146s/iter; left time: 135.9012s\n",
      "\titers: 500, epoch: 4 | loss: 0.0620285\n",
      "\tspeed: 0.0147s/iter; left time: 134.6843s\n",
      "Epoch: 4 cost time: 8.67498230934143\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0526851 Vali Loss: 0.0370105 Test Loss: 0.1403879\n",
      "Validation loss decreased (0.037519 --> 0.037011).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0406114\n",
      "\tspeed: 0.0484s/iter; left time: 436.4284s\n",
      "\titers: 200, epoch: 5 | loss: 0.0422068\n",
      "\tspeed: 0.0161s/iter; left time: 143.3088s\n",
      "\titers: 300, epoch: 5 | loss: 0.0453009\n",
      "\tspeed: 0.0162s/iter; left time: 142.5762s\n",
      "\titers: 400, epoch: 5 | loss: 0.0435832\n",
      "\tspeed: 0.0162s/iter; left time: 140.8981s\n",
      "\titers: 500, epoch: 5 | loss: 0.0368674\n",
      "\tspeed: 0.0162s/iter; left time: 139.4807s\n",
      "Epoch: 5 cost time: 9.462120056152344\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0436435 Vali Loss: 0.0378411 Test Loss: 0.1423287\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0305940\n",
      "\tspeed: 0.0506s/iter; left time: 427.4910s\n",
      "\titers: 200, epoch: 6 | loss: 0.0309110\n",
      "\tspeed: 0.0158s/iter; left time: 131.8192s\n",
      "\titers: 300, epoch: 6 | loss: 0.0290665\n",
      "\tspeed: 0.0157s/iter; left time: 129.6337s\n",
      "\titers: 400, epoch: 6 | loss: 0.0337679\n",
      "\tspeed: 0.0162s/iter; left time: 132.3789s\n",
      "\titers: 500, epoch: 6 | loss: 0.0298539\n",
      "\tspeed: 0.0162s/iter; left time: 130.0741s\n",
      "Epoch: 6 cost time: 9.39762568473816\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0393141 Vali Loss: 0.0380284 Test Loss: 0.1433180\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0388735\n",
      "\tspeed: 0.0487s/iter; left time: 384.0113s\n",
      "\titers: 200, epoch: 7 | loss: 0.0317292\n",
      "\tspeed: 0.0159s/iter; left time: 123.5484s\n",
      "\titers: 300, epoch: 7 | loss: 0.0503877\n",
      "\tspeed: 0.0162s/iter; left time: 124.2605s\n",
      "\titers: 400, epoch: 7 | loss: 0.0319478\n",
      "\tspeed: 0.0161s/iter; left time: 122.3642s\n",
      "\titers: 500, epoch: 7 | loss: 0.0518978\n",
      "\tspeed: 0.0161s/iter; left time: 120.7877s\n",
      "Epoch: 7 cost time: 9.407744884490967\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0371862 Vali Loss: 0.0383767 Test Loss: 0.1444984\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14058546721935272, mae:0.2441914826631546\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1207708\n",
      "\tspeed: 0.0291s/iter; left time: 329.3618s\n",
      "\titers: 200, epoch: 1 | loss: 0.1658366\n",
      "\tspeed: 0.0176s/iter; left time: 196.8576s\n",
      "\titers: 300, epoch: 1 | loss: 0.2600677\n",
      "\tspeed: 0.0175s/iter; left time: 194.7626s\n",
      "\titers: 400, epoch: 1 | loss: 0.1324711\n",
      "\tspeed: 0.0176s/iter; left time: 193.1656s\n",
      "\titers: 500, epoch: 1 | loss: 0.1258223\n",
      "\tspeed: 0.0176s/iter; left time: 191.3945s\n",
      "Epoch: 1 cost time: 11.227256774902344\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1767459 Vali Loss: 0.0456234 Test Loss: 0.1338982\n",
      "Validation loss decreased (inf --> 0.045623).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1645802\n",
      "\tspeed: 0.0569s/iter; left time: 610.2725s\n",
      "\titers: 200, epoch: 2 | loss: 0.1161484\n",
      "\tspeed: 0.0197s/iter; left time: 209.7566s\n",
      "\titers: 300, epoch: 2 | loss: 0.1286412\n",
      "\tspeed: 0.0196s/iter; left time: 206.4297s\n",
      "\titers: 400, epoch: 2 | loss: 0.1164942\n",
      "\tspeed: 0.0196s/iter; left time: 204.6089s\n",
      "\titers: 500, epoch: 2 | loss: 0.1442048\n",
      "\tspeed: 0.0196s/iter; left time: 202.4468s\n",
      "Epoch: 2 cost time: 11.496079444885254\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1249090 Vali Loss: 0.0433283 Test Loss: 0.1317271\n",
      "Validation loss decreased (0.045623 --> 0.043328).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0977896\n",
      "\tspeed: 0.0558s/iter; left time: 567.2931s\n",
      "\titers: 200, epoch: 3 | loss: 0.0681730\n",
      "\tspeed: 0.0175s/iter; left time: 176.3985s\n",
      "\titers: 300, epoch: 3 | loss: 0.0849765\n",
      "\tspeed: 0.0175s/iter; left time: 174.4720s\n",
      "\titers: 400, epoch: 3 | loss: 0.0804550\n",
      "\tspeed: 0.0175s/iter; left time: 172.8585s\n",
      "\titers: 500, epoch: 3 | loss: 0.0725421\n",
      "\tspeed: 0.0175s/iter; left time: 171.1685s\n",
      "Epoch: 3 cost time: 10.26699686050415\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0941606 Vali Loss: 0.0353566 Test Loss: 0.1254870\n",
      "Validation loss decreased (0.043328 --> 0.035357).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0793846\n",
      "\tspeed: 0.0535s/iter; left time: 513.0299s\n",
      "\titers: 200, epoch: 4 | loss: 0.0919872\n",
      "\tspeed: 0.0175s/iter; left time: 166.0062s\n",
      "\titers: 300, epoch: 4 | loss: 0.0671227\n",
      "\tspeed: 0.0175s/iter; left time: 164.1991s\n",
      "\titers: 400, epoch: 4 | loss: 0.0602730\n",
      "\tspeed: 0.0175s/iter; left time: 162.4086s\n",
      "\titers: 500, epoch: 4 | loss: 0.0878569\n",
      "\tspeed: 0.0175s/iter; left time: 160.6217s\n",
      "Epoch: 4 cost time: 10.257752656936646\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0772238 Vali Loss: 0.0342536 Test Loss: 0.1399447\n",
      "Validation loss decreased (0.035357 --> 0.034254).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0656800\n",
      "\tspeed: 0.0559s/iter; left time: 504.1452s\n",
      "\titers: 200, epoch: 5 | loss: 0.0561583\n",
      "\tspeed: 0.0176s/iter; left time: 156.7911s\n",
      "\titers: 300, epoch: 5 | loss: 0.0804801\n",
      "\tspeed: 0.0176s/iter; left time: 154.9833s\n",
      "\titers: 400, epoch: 5 | loss: 0.0545313\n",
      "\tspeed: 0.0175s/iter; left time: 152.9262s\n",
      "\titers: 500, epoch: 5 | loss: 0.0810072\n",
      "\tspeed: 0.0175s/iter; left time: 150.9893s\n",
      "Epoch: 5 cost time: 10.275804996490479\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0680126 Vali Loss: 0.0335107 Test Loss: 0.1322938\n",
      "Validation loss decreased (0.034254 --> 0.033511).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0404243\n",
      "\tspeed: 0.0544s/iter; left time: 459.8221s\n",
      "\titers: 200, epoch: 6 | loss: 0.0678751\n",
      "\tspeed: 0.0176s/iter; left time: 146.5929s\n",
      "\titers: 300, epoch: 6 | loss: 0.0676996\n",
      "\tspeed: 0.0176s/iter; left time: 144.8052s\n",
      "\titers: 400, epoch: 6 | loss: 0.0885342\n",
      "\tspeed: 0.0175s/iter; left time: 142.9604s\n",
      "\titers: 500, epoch: 6 | loss: 0.0566523\n",
      "\tspeed: 0.0175s/iter; left time: 141.0203s\n",
      "Epoch: 6 cost time: 10.307605504989624\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0635844 Vali Loss: 0.0332794 Test Loss: 0.1336828\n",
      "Validation loss decreased (0.033511 --> 0.033279).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0589511\n",
      "\tspeed: 0.0537s/iter; left time: 423.0915s\n",
      "\titers: 200, epoch: 7 | loss: 0.0571943\n",
      "\tspeed: 0.0175s/iter; left time: 136.5262s\n",
      "\titers: 300, epoch: 7 | loss: 0.0717927\n",
      "\tspeed: 0.0175s/iter; left time: 134.7085s\n",
      "\titers: 400, epoch: 7 | loss: 0.0719622\n",
      "\tspeed: 0.0175s/iter; left time: 132.8706s\n",
      "\titers: 500, epoch: 7 | loss: 0.0520488\n",
      "\tspeed: 0.0176s/iter; left time: 131.3774s\n",
      "Epoch: 7 cost time: 10.265719890594482\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0611406 Vali Loss: 0.0330539 Test Loss: 0.1343281\n",
      "Validation loss decreased (0.033279 --> 0.033054).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0647300\n",
      "\tspeed: 0.0535s/iter; left time: 391.5036s\n",
      "\titers: 200, epoch: 8 | loss: 0.0595467\n",
      "\tspeed: 0.0179s/iter; left time: 128.8483s\n",
      "\titers: 300, epoch: 8 | loss: 0.0460011\n",
      "\tspeed: 0.0196s/iter; left time: 139.6272s\n",
      "\titers: 400, epoch: 8 | loss: 0.0597768\n",
      "\tspeed: 0.0196s/iter; left time: 137.5903s\n",
      "\titers: 500, epoch: 8 | loss: 0.0478682\n",
      "\tspeed: 0.0196s/iter; left time: 135.6579s\n",
      "Epoch: 8 cost time: 11.074920177459717\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0599008 Vali Loss: 0.0333627 Test Loss: 0.1343927\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0450619\n",
      "\tspeed: 0.0549s/iter; left time: 370.0631s\n",
      "\titers: 200, epoch: 9 | loss: 0.0991942\n",
      "\tspeed: 0.0176s/iter; left time: 116.7296s\n",
      "\titers: 300, epoch: 9 | loss: 0.0457475\n",
      "\tspeed: 0.0176s/iter; left time: 114.9674s\n",
      "\titers: 400, epoch: 9 | loss: 0.0412745\n",
      "\tspeed: 0.0176s/iter; left time: 113.1698s\n",
      "\titers: 500, epoch: 9 | loss: 0.0411304\n",
      "\tspeed: 0.0176s/iter; left time: 111.6112s\n",
      "Epoch: 9 cost time: 10.29896068572998\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0593183 Vali Loss: 0.0332272 Test Loss: 0.1348051\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0575047\n",
      "\tspeed: 0.0589s/iter; left time: 363.6527s\n",
      "\titers: 200, epoch: 10 | loss: 0.0695797\n",
      "\tspeed: 0.0188s/iter; left time: 114.3574s\n",
      "\titers: 300, epoch: 10 | loss: 0.0439022\n",
      "\tspeed: 0.0175s/iter; left time: 104.6999s\n",
      "\titers: 400, epoch: 10 | loss: 0.0652481\n",
      "\tspeed: 0.0175s/iter; left time: 102.8767s\n",
      "\titers: 500, epoch: 10 | loss: 0.0322872\n",
      "\tspeed: 0.0175s/iter; left time: 101.1163s\n",
      "Epoch: 10 cost time: 10.744315147399902\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0589843 Vali Loss: 0.0331905 Test Loss: 0.1361058\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13452480733394623, mae:0.23368285596370697\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3226862\n",
      "\tspeed: 0.0312s/iter; left time: 352.6082s\n",
      "\titers: 200, epoch: 1 | loss: 0.1668440\n",
      "\tspeed: 0.0193s/iter; left time: 215.9531s\n",
      "\titers: 300, epoch: 1 | loss: 0.1776613\n",
      "\tspeed: 0.0193s/iter; left time: 214.3866s\n",
      "\titers: 400, epoch: 1 | loss: 0.1115717\n",
      "\tspeed: 0.0194s/iter; left time: 212.8951s\n",
      "\titers: 500, epoch: 1 | loss: 0.1671815\n",
      "\tspeed: 0.0193s/iter; left time: 210.6275s\n",
      "Epoch: 1 cost time: 12.25816011428833\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1464359 Vali Loss: 0.0346369 Test Loss: 0.1122974\n",
      "Validation loss decreased (inf --> 0.034637).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0989168\n",
      "\tspeed: 0.0557s/iter; left time: 597.5979s\n",
      "\titers: 200, epoch: 2 | loss: 0.1485835\n",
      "\tspeed: 0.0173s/iter; left time: 183.8817s\n",
      "\titers: 300, epoch: 2 | loss: 0.1692001\n",
      "\tspeed: 0.0188s/iter; left time: 197.9012s\n",
      "\titers: 400, epoch: 2 | loss: 0.0990008\n",
      "\tspeed: 0.0198s/iter; left time: 206.2864s\n",
      "\titers: 500, epoch: 2 | loss: 0.0971779\n",
      "\tspeed: 0.0198s/iter; left time: 204.4747s\n",
      "Epoch: 2 cost time: 10.980872392654419\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1085422 Vali Loss: 0.0356164 Test Loss: 0.1155539\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0768530\n",
      "\tspeed: 0.0602s/iter; left time: 611.5406s\n",
      "\titers: 200, epoch: 3 | loss: 0.0811924\n",
      "\tspeed: 0.0198s/iter; left time: 198.7993s\n",
      "\titers: 300, epoch: 3 | loss: 0.0978360\n",
      "\tspeed: 0.0198s/iter; left time: 197.4649s\n",
      "\titers: 400, epoch: 3 | loss: 0.0615743\n",
      "\tspeed: 0.0198s/iter; left time: 194.8161s\n",
      "\titers: 500, epoch: 3 | loss: 0.0687462\n",
      "\tspeed: 0.0198s/iter; left time: 193.0090s\n",
      "Epoch: 3 cost time: 11.614497900009155\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0823146 Vali Loss: 0.0338312 Test Loss: 0.1130264\n",
      "Validation loss decreased (0.034637 --> 0.033831).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0664183\n",
      "\tspeed: 0.0596s/iter; left time: 571.1978s\n",
      "\titers: 200, epoch: 4 | loss: 0.0526979\n",
      "\tspeed: 0.0197s/iter; left time: 187.2910s\n",
      "\titers: 300, epoch: 4 | loss: 0.0824772\n",
      "\tspeed: 0.0197s/iter; left time: 185.4263s\n",
      "\titers: 400, epoch: 4 | loss: 0.0836798\n",
      "\tspeed: 0.0197s/iter; left time: 183.3815s\n",
      "\titers: 500, epoch: 4 | loss: 0.0645510\n",
      "\tspeed: 0.0198s/iter; left time: 181.6098s\n",
      "Epoch: 4 cost time: 11.381749868392944\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0654951 Vali Loss: 0.0329143 Test Loss: 0.1138199\n",
      "Validation loss decreased (0.033831 --> 0.032914).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0522060\n",
      "\tspeed: 0.0553s/iter; left time: 498.7700s\n",
      "\titers: 200, epoch: 5 | loss: 0.0592664\n",
      "\tspeed: 0.0173s/iter; left time: 154.5379s\n",
      "\titers: 300, epoch: 5 | loss: 0.0512895\n",
      "\tspeed: 0.0173s/iter; left time: 152.3429s\n",
      "\titers: 400, epoch: 5 | loss: 0.0640452\n",
      "\tspeed: 0.0173s/iter; left time: 150.6813s\n",
      "\titers: 500, epoch: 5 | loss: 0.0600721\n",
      "\tspeed: 0.0173s/iter; left time: 148.8054s\n",
      "Epoch: 5 cost time: 10.145564794540405\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0567611 Vali Loss: 0.0334743 Test Loss: 0.1158179\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0593614\n",
      "\tspeed: 0.0559s/iter; left time: 472.3175s\n",
      "\titers: 200, epoch: 6 | loss: 0.0394505\n",
      "\tspeed: 0.0193s/iter; left time: 160.8311s\n",
      "\titers: 300, epoch: 6 | loss: 0.0592579\n",
      "\tspeed: 0.0192s/iter; left time: 158.7277s\n",
      "\titers: 400, epoch: 6 | loss: 0.0466528\n",
      "\tspeed: 0.0192s/iter; left time: 156.8316s\n",
      "\titers: 500, epoch: 6 | loss: 0.0352588\n",
      "\tspeed: 0.0192s/iter; left time: 154.9051s\n",
      "Epoch: 6 cost time: 11.301153898239136\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0526633 Vali Loss: 0.0331158 Test Loss: 0.1148413\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0470458\n",
      "\tspeed: 0.0554s/iter; left time: 436.5758s\n",
      "\titers: 200, epoch: 7 | loss: 0.0658160\n",
      "\tspeed: 0.0172s/iter; left time: 133.6483s\n",
      "\titers: 300, epoch: 7 | loss: 0.0436879\n",
      "\tspeed: 0.0172s/iter; left time: 131.7486s\n",
      "\titers: 400, epoch: 7 | loss: 0.0596417\n",
      "\tspeed: 0.0172s/iter; left time: 130.2541s\n",
      "\titers: 500, epoch: 7 | loss: 0.0392330\n",
      "\tspeed: 0.0172s/iter; left time: 128.4162s\n",
      "Epoch: 7 cost time: 10.039403915405273\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0504884 Vali Loss: 0.0333423 Test Loss: 0.1157486\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11395978182554245, mae:0.20874567329883575\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2526689\n",
      "\tspeed: 0.0287s/iter; left time: 324.2911s\n",
      "\titers: 200, epoch: 1 | loss: 0.1506011\n",
      "\tspeed: 0.0171s/iter; left time: 192.0688s\n",
      "\titers: 300, epoch: 1 | loss: 0.1895043\n",
      "\tspeed: 0.0171s/iter; left time: 190.3639s\n",
      "\titers: 400, epoch: 1 | loss: 0.1955848\n",
      "\tspeed: 0.0172s/iter; left time: 188.7121s\n",
      "\titers: 500, epoch: 1 | loss: 0.2660136\n",
      "\tspeed: 0.0172s/iter; left time: 187.1760s\n",
      "Epoch: 1 cost time: 10.991947412490845\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1832555 Vali Loss: 0.0414752 Test Loss: 0.1353214\n",
      "Validation loss decreased (inf --> 0.041475).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1894857\n",
      "\tspeed: 0.0582s/iter; left time: 624.7995s\n",
      "\titers: 200, epoch: 2 | loss: 0.2140321\n",
      "\tspeed: 0.0194s/iter; left time: 206.5487s\n",
      "\titers: 300, epoch: 2 | loss: 0.2231693\n",
      "\tspeed: 0.0194s/iter; left time: 203.9767s\n",
      "\titers: 400, epoch: 2 | loss: 0.1023709\n",
      "\tspeed: 0.0193s/iter; left time: 201.5676s\n",
      "\titers: 500, epoch: 2 | loss: 0.0706140\n",
      "\tspeed: 0.0193s/iter; left time: 199.2758s\n",
      "Epoch: 2 cost time: 11.339491605758667\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1284721 Vali Loss: 0.0430929 Test Loss: 0.1399398\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1038830\n",
      "\tspeed: 0.0573s/iter; left time: 582.2424s\n",
      "\titers: 200, epoch: 3 | loss: 0.1072234\n",
      "\tspeed: 0.0173s/iter; left time: 174.4965s\n",
      "\titers: 300, epoch: 3 | loss: 0.0985372\n",
      "\tspeed: 0.0173s/iter; left time: 171.9896s\n",
      "\titers: 400, epoch: 3 | loss: 0.0693403\n",
      "\tspeed: 0.0173s/iter; left time: 170.6013s\n",
      "\titers: 500, epoch: 3 | loss: 0.0678159\n",
      "\tspeed: 0.0173s/iter; left time: 168.4032s\n",
      "Epoch: 3 cost time: 10.170094966888428\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0921640 Vali Loss: 0.0411940 Test Loss: 0.1413373\n",
      "Validation loss decreased (0.041475 --> 0.041194).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0639332\n",
      "\tspeed: 0.0536s/iter; left time: 514.5439s\n",
      "\titers: 200, epoch: 4 | loss: 0.0903152\n",
      "\tspeed: 0.0172s/iter; left time: 163.1889s\n",
      "\titers: 300, epoch: 4 | loss: 0.0720685\n",
      "\tspeed: 0.0172s/iter; left time: 161.0896s\n",
      "\titers: 400, epoch: 4 | loss: 0.0802401\n",
      "\tspeed: 0.0171s/iter; left time: 159.3260s\n",
      "\titers: 500, epoch: 4 | loss: 0.0611074\n",
      "\tspeed: 0.0172s/iter; left time: 157.6467s\n",
      "Epoch: 4 cost time: 10.049198150634766\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0717763 Vali Loss: 0.0385932 Test Loss: 0.1380353\n",
      "Validation loss decreased (0.041194 --> 0.038593).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0552789\n",
      "\tspeed: 0.0543s/iter; left time: 489.7856s\n",
      "\titers: 200, epoch: 5 | loss: 0.0573784\n",
      "\tspeed: 0.0171s/iter; left time: 152.1922s\n",
      "\titers: 300, epoch: 5 | loss: 0.0559159\n",
      "\tspeed: 0.0170s/iter; left time: 150.3799s\n",
      "\titers: 400, epoch: 5 | loss: 0.0563369\n",
      "\tspeed: 0.0170s/iter; left time: 148.5962s\n",
      "\titers: 500, epoch: 5 | loss: 0.0454566\n",
      "\tspeed: 0.0170s/iter; left time: 146.9207s\n",
      "Epoch: 5 cost time: 9.991517066955566\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0615526 Vali Loss: 0.0402327 Test Loss: 0.1463692\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0563857\n",
      "\tspeed: 0.0525s/iter; left time: 443.7516s\n",
      "\titers: 200, epoch: 6 | loss: 0.0544896\n",
      "\tspeed: 0.0171s/iter; left time: 142.6596s\n",
      "\titers: 300, epoch: 6 | loss: 0.0795186\n",
      "\tspeed: 0.0171s/iter; left time: 140.9883s\n",
      "\titers: 400, epoch: 6 | loss: 0.0644698\n",
      "\tspeed: 0.0171s/iter; left time: 139.2646s\n",
      "\titers: 500, epoch: 6 | loss: 0.0572942\n",
      "\tspeed: 0.0171s/iter; left time: 137.5242s\n",
      "Epoch: 6 cost time: 10.029054641723633\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0564596 Vali Loss: 0.0390845 Test Loss: 0.1458327\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0485623\n",
      "\tspeed: 0.0548s/iter; left time: 431.7514s\n",
      "\titers: 200, epoch: 7 | loss: 0.0468063\n",
      "\tspeed: 0.0172s/iter; left time: 133.6767s\n",
      "\titers: 300, epoch: 7 | loss: 0.0684707\n",
      "\tspeed: 0.0172s/iter; left time: 131.8773s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643950\n",
      "\tspeed: 0.0172s/iter; left time: 130.1603s\n",
      "\titers: 500, epoch: 7 | loss: 0.0415587\n",
      "\tspeed: 0.0172s/iter; left time: 128.4303s\n",
      "Epoch: 7 cost time: 10.065946340560913\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0539953 Vali Loss: 0.0394990 Test Loss: 0.1465579\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13822001218795776, mae:0.23914697766304016\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1888228\n",
      "\tspeed: 0.0352s/iter; left time: 397.2396s\n",
      "\titers: 200, epoch: 1 | loss: 0.1582943\n",
      "\tspeed: 0.0232s/iter; left time: 259.4727s\n",
      "\titers: 300, epoch: 1 | loss: 0.1563228\n",
      "\tspeed: 0.0232s/iter; left time: 257.2971s\n",
      "\titers: 400, epoch: 1 | loss: 0.2523674\n",
      "\tspeed: 0.0231s/iter; left time: 254.3235s\n",
      "\titers: 500, epoch: 1 | loss: 0.2365478\n",
      "\tspeed: 0.0231s/iter; left time: 252.0371s\n",
      "Epoch: 1 cost time: 14.446568250656128\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1755242 Vali Loss: 0.0442618 Test Loss: 0.1377467\n",
      "Validation loss decreased (inf --> 0.044262).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1871693\n",
      "\tspeed: 0.0658s/iter; left time: 706.1471s\n",
      "\titers: 200, epoch: 2 | loss: 0.0924348\n",
      "\tspeed: 0.0214s/iter; left time: 227.1194s\n",
      "\titers: 300, epoch: 2 | loss: 0.1252180\n",
      "\tspeed: 0.0221s/iter; left time: 233.0483s\n",
      "\titers: 400, epoch: 2 | loss: 0.1210898\n",
      "\tspeed: 0.0223s/iter; left time: 232.5018s\n",
      "\titers: 500, epoch: 2 | loss: 0.0729622\n",
      "\tspeed: 0.0223s/iter; left time: 230.2079s\n",
      "Epoch: 2 cost time: 12.826947689056396\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1170748 Vali Loss: 0.0391907 Test Loss: 0.1344381\n",
      "Validation loss decreased (0.044262 --> 0.039191).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0959562\n",
      "\tspeed: 0.0677s/iter; left time: 688.1851s\n",
      "\titers: 200, epoch: 3 | loss: 0.0675417\n",
      "\tspeed: 0.0216s/iter; left time: 217.7877s\n",
      "\titers: 300, epoch: 3 | loss: 0.0984169\n",
      "\tspeed: 0.0214s/iter; left time: 213.2232s\n",
      "\titers: 400, epoch: 3 | loss: 0.0519896\n",
      "\tspeed: 0.0214s/iter; left time: 211.0112s\n",
      "\titers: 500, epoch: 3 | loss: 0.0565442\n",
      "\tspeed: 0.0214s/iter; left time: 208.8638s\n",
      "Epoch: 3 cost time: 12.723251819610596\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0725646 Vali Loss: 0.0358213 Test Loss: 0.1385621\n",
      "Validation loss decreased (0.039191 --> 0.035821).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0500028\n",
      "\tspeed: 0.0655s/iter; left time: 628.3863s\n",
      "\titers: 200, epoch: 4 | loss: 0.0390338\n",
      "\tspeed: 0.0215s/iter; left time: 203.9752s\n",
      "\titers: 300, epoch: 4 | loss: 0.0459121\n",
      "\tspeed: 0.0215s/iter; left time: 201.5770s\n",
      "\titers: 400, epoch: 4 | loss: 0.0498361\n",
      "\tspeed: 0.0214s/iter; left time: 198.7949s\n",
      "\titers: 500, epoch: 4 | loss: 0.0820427\n",
      "\tspeed: 0.0214s/iter; left time: 196.7982s\n",
      "Epoch: 4 cost time: 12.711302042007446\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0506902 Vali Loss: 0.0382494 Test Loss: 0.1395986\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0575483\n",
      "\tspeed: 0.0648s/iter; left time: 584.7742s\n",
      "\titers: 200, epoch: 5 | loss: 0.0359831\n",
      "\tspeed: 0.0224s/iter; left time: 199.7875s\n",
      "\titers: 300, epoch: 5 | loss: 0.0293592\n",
      "\tspeed: 0.0224s/iter; left time: 197.2629s\n",
      "\titers: 400, epoch: 5 | loss: 0.0327550\n",
      "\tspeed: 0.0224s/iter; left time: 195.2832s\n",
      "\titers: 500, epoch: 5 | loss: 0.0358799\n",
      "\tspeed: 0.0224s/iter; left time: 193.0790s\n",
      "Epoch: 5 cost time: 13.050045013427734\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0410975 Vali Loss: 0.0380475 Test Loss: 0.1372982\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0283897\n",
      "\tspeed: 0.0641s/iter; left time: 541.9514s\n",
      "\titers: 200, epoch: 6 | loss: 0.0320395\n",
      "\tspeed: 0.0232s/iter; left time: 193.4781s\n",
      "\titers: 300, epoch: 6 | loss: 0.0325276\n",
      "\tspeed: 0.0231s/iter; left time: 190.9426s\n",
      "\titers: 400, epoch: 6 | loss: 0.0307756\n",
      "\tspeed: 0.0231s/iter; left time: 188.6622s\n",
      "\titers: 500, epoch: 6 | loss: 0.0497869\n",
      "\tspeed: 0.0231s/iter; left time: 186.3337s\n",
      "Epoch: 6 cost time: 13.499301195144653\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0364597 Vali Loss: 0.0385614 Test Loss: 0.1396468\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13873861730098724, mae:0.24120382964611053\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1323162\n",
      "\tspeed: 0.0345s/iter; left time: 390.1510s\n",
      "\titers: 200, epoch: 1 | loss: 0.0824137\n",
      "\tspeed: 0.0224s/iter; left time: 251.3283s\n",
      "\titers: 300, epoch: 1 | loss: 0.1541070\n",
      "\tspeed: 0.0225s/iter; left time: 249.4896s\n",
      "\titers: 400, epoch: 1 | loss: 0.1056086\n",
      "\tspeed: 0.0225s/iter; left time: 247.2837s\n",
      "\titers: 500, epoch: 1 | loss: 0.1541315\n",
      "\tspeed: 0.0224s/iter; left time: 244.6248s\n",
      "Epoch: 1 cost time: 14.064595937728882\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1440308 Vali Loss: 0.0353116 Test Loss: 0.1119556\n",
      "Validation loss decreased (inf --> 0.035312).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1237014\n",
      "\tspeed: 0.0623s/iter; left time: 668.6089s\n",
      "\titers: 200, epoch: 2 | loss: 0.0813918\n",
      "\tspeed: 0.0206s/iter; left time: 219.1649s\n",
      "\titers: 300, epoch: 2 | loss: 0.1027531\n",
      "\tspeed: 0.0206s/iter; left time: 216.8261s\n",
      "\titers: 400, epoch: 2 | loss: 0.0933123\n",
      "\tspeed: 0.0206s/iter; left time: 215.0736s\n",
      "\titers: 500, epoch: 2 | loss: 0.0625068\n",
      "\tspeed: 0.0207s/iter; left time: 213.7842s\n",
      "Epoch: 2 cost time: 12.110306739807129\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.0988597 Vali Loss: 0.0343631 Test Loss: 0.1173671\n",
      "Validation loss decreased (0.035312 --> 0.034363).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0641420\n",
      "\tspeed: 0.0606s/iter; left time: 616.0512s\n",
      "\titers: 200, epoch: 3 | loss: 0.0536526\n",
      "\tspeed: 0.0207s/iter; left time: 207.9695s\n",
      "\titers: 300, epoch: 3 | loss: 0.0694883\n",
      "\tspeed: 0.0207s/iter; left time: 205.7418s\n",
      "\titers: 400, epoch: 3 | loss: 0.0559392\n",
      "\tspeed: 0.0207s/iter; left time: 203.7613s\n",
      "\titers: 500, epoch: 3 | loss: 0.0553465\n",
      "\tspeed: 0.0206s/iter; left time: 201.4946s\n",
      "Epoch: 3 cost time: 12.081169843673706\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0621782 Vali Loss: 0.0342525 Test Loss: 0.1219186\n",
      "Validation loss decreased (0.034363 --> 0.034253).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0366462\n",
      "\tspeed: 0.0623s/iter; left time: 597.1284s\n",
      "\titers: 200, epoch: 4 | loss: 0.0427919\n",
      "\tspeed: 0.0205s/iter; left time: 194.8806s\n",
      "\titers: 300, epoch: 4 | loss: 0.0457355\n",
      "\tspeed: 0.0205s/iter; left time: 192.7960s\n",
      "\titers: 400, epoch: 4 | loss: 0.0432367\n",
      "\tspeed: 0.0205s/iter; left time: 190.7941s\n",
      "\titers: 500, epoch: 4 | loss: 0.0349271\n",
      "\tspeed: 0.0205s/iter; left time: 188.6823s\n",
      "Epoch: 4 cost time: 12.015088081359863\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0436198 Vali Loss: 0.0343519 Test Loss: 0.1205175\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0357755\n",
      "\tspeed: 0.0598s/iter; left time: 539.5460s\n",
      "\titers: 200, epoch: 5 | loss: 0.0355270\n",
      "\tspeed: 0.0207s/iter; left time: 184.4979s\n",
      "\titers: 300, epoch: 5 | loss: 0.0306037\n",
      "\tspeed: 0.0207s/iter; left time: 182.4268s\n",
      "\titers: 400, epoch: 5 | loss: 0.0420264\n",
      "\tspeed: 0.0207s/iter; left time: 180.3363s\n",
      "\titers: 500, epoch: 5 | loss: 0.0385151\n",
      "\tspeed: 0.0207s/iter; left time: 178.2707s\n",
      "Epoch: 5 cost time: 12.05589771270752\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0357689 Vali Loss: 0.0346102 Test Loss: 0.1223844\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0234594\n",
      "\tspeed: 0.0614s/iter; left time: 518.9624s\n",
      "\titers: 200, epoch: 6 | loss: 0.0269406\n",
      "\tspeed: 0.0206s/iter; left time: 172.3710s\n",
      "\titers: 300, epoch: 6 | loss: 0.0281651\n",
      "\tspeed: 0.0206s/iter; left time: 170.3451s\n",
      "\titers: 400, epoch: 6 | loss: 0.0390316\n",
      "\tspeed: 0.0206s/iter; left time: 168.2727s\n",
      "\titers: 500, epoch: 6 | loss: 0.0268994\n",
      "\tspeed: 0.0206s/iter; left time: 166.2037s\n",
      "Epoch: 6 cost time: 12.043020248413086\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0320021 Vali Loss: 0.0347968 Test Loss: 0.1230432\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12209206074476242, mae:0.2197163850069046\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1993479\n",
      "\tspeed: 0.0329s/iter; left time: 372.3471s\n",
      "\titers: 200, epoch: 1 | loss: 0.1782197\n",
      "\tspeed: 0.0210s/iter; left time: 234.9644s\n",
      "\titers: 300, epoch: 1 | loss: 0.1975592\n",
      "\tspeed: 0.0210s/iter; left time: 233.1408s\n",
      "\titers: 400, epoch: 1 | loss: 0.2005119\n",
      "\tspeed: 0.0210s/iter; left time: 231.2028s\n",
      "\titers: 500, epoch: 1 | loss: 0.1762355\n",
      "\tspeed: 0.0209s/iter; left time: 228.3444s\n",
      "Epoch: 1 cost time: 13.2682785987854\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1741566 Vali Loss: 0.0434220 Test Loss: 0.1350538\n",
      "Validation loss decreased (inf --> 0.043422).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0638883\n",
      "\tspeed: 0.0623s/iter; left time: 668.9219s\n",
      "\titers: 200, epoch: 2 | loss: 0.1771397\n",
      "\tspeed: 0.0218s/iter; left time: 231.6555s\n",
      "\titers: 300, epoch: 2 | loss: 0.0777541\n",
      "\tspeed: 0.0217s/iter; left time: 228.7621s\n",
      "\titers: 400, epoch: 2 | loss: 0.0867392\n",
      "\tspeed: 0.0217s/iter; left time: 225.9781s\n",
      "\titers: 500, epoch: 2 | loss: 0.0822950\n",
      "\tspeed: 0.0217s/iter; left time: 223.7740s\n",
      "Epoch: 2 cost time: 12.685627937316895\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1135138 Vali Loss: 0.0410773 Test Loss: 0.1439481\n",
      "Validation loss decreased (0.043422 --> 0.041077).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0571769\n",
      "\tspeed: 0.0625s/iter; left time: 634.5798s\n",
      "\titers: 200, epoch: 3 | loss: 0.0578771\n",
      "\tspeed: 0.0191s/iter; left time: 191.8252s\n",
      "\titers: 300, epoch: 3 | loss: 0.0776076\n",
      "\tspeed: 0.0191s/iter; left time: 189.8743s\n",
      "\titers: 400, epoch: 3 | loss: 0.0569135\n",
      "\tspeed: 0.0190s/iter; left time: 187.8449s\n",
      "\titers: 500, epoch: 3 | loss: 0.0758753\n",
      "\tspeed: 0.0190s/iter; left time: 185.5777s\n",
      "Epoch: 3 cost time: 11.344019412994385\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0699668 Vali Loss: 0.0388080 Test Loss: 0.1375320\n",
      "Validation loss decreased (0.041077 --> 0.038808).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0319529\n",
      "\tspeed: 0.0592s/iter; left time: 568.0137s\n",
      "\titers: 200, epoch: 4 | loss: 0.0382339\n",
      "\tspeed: 0.0191s/iter; left time: 181.2300s\n",
      "\titers: 300, epoch: 4 | loss: 0.0422708\n",
      "\tspeed: 0.0195s/iter; left time: 183.1233s\n",
      "\titers: 400, epoch: 4 | loss: 0.0494687\n",
      "\tspeed: 0.0219s/iter; left time: 203.4562s\n",
      "\titers: 500, epoch: 4 | loss: 0.0360663\n",
      "\tspeed: 0.0219s/iter; left time: 201.2498s\n",
      "Epoch: 4 cost time: 11.98630428314209\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0483091 Vali Loss: 0.0379723 Test Loss: 0.1417125\n",
      "Validation loss decreased (0.038808 --> 0.037972).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0414709\n",
      "\tspeed: 0.0627s/iter; left time: 565.6404s\n",
      "\titers: 200, epoch: 5 | loss: 0.0288012\n",
      "\tspeed: 0.0218s/iter; left time: 194.5883s\n",
      "\titers: 300, epoch: 5 | loss: 0.0336405\n",
      "\tspeed: 0.0218s/iter; left time: 192.4145s\n",
      "\titers: 400, epoch: 5 | loss: 0.0298387\n",
      "\tspeed: 0.0218s/iter; left time: 190.2065s\n",
      "\titers: 500, epoch: 5 | loss: 0.0345208\n",
      "\tspeed: 0.0218s/iter; left time: 188.0566s\n",
      "Epoch: 5 cost time: 12.738741159439087\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0384476 Vali Loss: 0.0387255 Test Loss: 0.1445214\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0388057\n",
      "\tspeed: 0.0641s/iter; left time: 541.4455s\n",
      "\titers: 200, epoch: 6 | loss: 0.0374171\n",
      "\tspeed: 0.0218s/iter; left time: 182.4624s\n",
      "\titers: 300, epoch: 6 | loss: 0.0292357\n",
      "\tspeed: 0.0218s/iter; left time: 180.2320s\n",
      "\titers: 400, epoch: 6 | loss: 0.0309061\n",
      "\tspeed: 0.0218s/iter; left time: 177.3227s\n",
      "\titers: 500, epoch: 6 | loss: 0.0376697\n",
      "\tspeed: 0.0217s/iter; left time: 175.0721s\n",
      "Epoch: 6 cost time: 12.747246980667114\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0338836 Vali Loss: 0.0390634 Test Loss: 0.1482659\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0233778\n",
      "\tspeed: 0.0635s/iter; left time: 500.3741s\n",
      "\titers: 200, epoch: 7 | loss: 0.0288248\n",
      "\tspeed: 0.0234s/iter; left time: 182.0683s\n",
      "\titers: 300, epoch: 7 | loss: 0.0329367\n",
      "\tspeed: 0.0234s/iter; left time: 179.5224s\n",
      "\titers: 400, epoch: 7 | loss: 0.0305000\n",
      "\tspeed: 0.0234s/iter; left time: 177.2657s\n",
      "\titers: 500, epoch: 7 | loss: 0.0248829\n",
      "\tspeed: 0.0214s/iter; left time: 160.4197s\n",
      "Epoch: 7 cost time: 13.261168956756592\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0316444 Vali Loss: 0.0388381 Test Loss: 0.1495141\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14190757274627686, mae:0.24510309100151062\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1661820\n",
      "\tspeed: 0.0222s/iter; left time: 250.5343s\n",
      "\titers: 200, epoch: 1 | loss: 0.1882960\n",
      "\tspeed: 0.0105s/iter; left time: 117.7247s\n",
      "\titers: 300, epoch: 1 | loss: 0.2432883\n",
      "\tspeed: 0.0105s/iter; left time: 116.8787s\n",
      "\titers: 400, epoch: 1 | loss: 0.2106672\n",
      "\tspeed: 0.0105s/iter; left time: 115.6001s\n",
      "\titers: 500, epoch: 1 | loss: 0.1615628\n",
      "\tspeed: 0.0105s/iter; left time: 114.7469s\n",
      "Epoch: 1 cost time: 7.208001136779785\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2132680 Vali Loss: 0.0489154 Test Loss: 0.1486931\n",
      "Validation loss decreased (inf --> 0.048915).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2554823\n",
      "\tspeed: 0.0375s/iter; left time: 402.4319s\n",
      "\titers: 200, epoch: 2 | loss: 0.2682980\n",
      "\tspeed: 0.0104s/iter; left time: 110.9440s\n",
      "\titers: 300, epoch: 2 | loss: 0.1647600\n",
      "\tspeed: 0.0105s/iter; left time: 110.1984s\n",
      "\titers: 400, epoch: 2 | loss: 0.1572925\n",
      "\tspeed: 0.0104s/iter; left time: 108.6291s\n",
      "\titers: 500, epoch: 2 | loss: 0.1673287\n",
      "\tspeed: 0.0104s/iter; left time: 107.7169s\n",
      "Epoch: 2 cost time: 6.253747224807739\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1726924 Vali Loss: 0.0418939 Test Loss: 0.1265778\n",
      "Validation loss decreased (0.048915 --> 0.041894).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1181338\n",
      "\tspeed: 0.0380s/iter; left time: 386.3301s\n",
      "\titers: 200, epoch: 3 | loss: 0.2001222\n",
      "\tspeed: 0.0106s/iter; left time: 106.9446s\n",
      "\titers: 300, epoch: 3 | loss: 0.1377586\n",
      "\tspeed: 0.0106s/iter; left time: 105.4844s\n",
      "\titers: 400, epoch: 3 | loss: 0.1699878\n",
      "\tspeed: 0.0106s/iter; left time: 104.4411s\n",
      "\titers: 500, epoch: 3 | loss: 0.2055827\n",
      "\tspeed: 0.0105s/iter; left time: 102.8840s\n",
      "Epoch: 3 cost time: 6.408422231674194\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1537922 Vali Loss: 0.0383821 Test Loss: 0.1203843\n",
      "Validation loss decreased (0.041894 --> 0.038382).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1481126\n",
      "\tspeed: 0.0373s/iter; left time: 357.4696s\n",
      "\titers: 200, epoch: 4 | loss: 0.0932456\n",
      "\tspeed: 0.0105s/iter; left time: 99.9391s\n",
      "\titers: 300, epoch: 4 | loss: 0.1380324\n",
      "\tspeed: 0.0106s/iter; left time: 99.2184s\n",
      "\titers: 400, epoch: 4 | loss: 0.1560184\n",
      "\tspeed: 0.0106s/iter; left time: 98.0826s\n",
      "\titers: 500, epoch: 4 | loss: 0.1368372\n",
      "\tspeed: 0.0105s/iter; left time: 96.8999s\n",
      "Epoch: 4 cost time: 6.333551645278931\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1447483 Vali Loss: 0.0380562 Test Loss: 0.1202246\n",
      "Validation loss decreased (0.038382 --> 0.038056).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1547439\n",
      "\tspeed: 0.0361s/iter; left time: 325.9894s\n",
      "\titers: 200, epoch: 5 | loss: 0.1538793\n",
      "\tspeed: 0.0105s/iter; left time: 93.3850s\n",
      "\titers: 300, epoch: 5 | loss: 0.1062656\n",
      "\tspeed: 0.0105s/iter; left time: 92.3735s\n",
      "\titers: 400, epoch: 5 | loss: 0.1278121\n",
      "\tspeed: 0.0105s/iter; left time: 91.1731s\n",
      "\titers: 500, epoch: 5 | loss: 0.1534337\n",
      "\tspeed: 0.0104s/iter; left time: 89.9021s\n",
      "Epoch: 5 cost time: 6.292460680007935\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1410183 Vali Loss: 0.0372186 Test Loss: 0.1193272\n",
      "Validation loss decreased (0.038056 --> 0.037219).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1385755\n",
      "\tspeed: 0.0373s/iter; left time: 314.9893s\n",
      "\titers: 200, epoch: 6 | loss: 0.0998583\n",
      "\tspeed: 0.0105s/iter; left time: 87.9593s\n",
      "\titers: 300, epoch: 6 | loss: 0.1453860\n",
      "\tspeed: 0.0106s/iter; left time: 87.2800s\n",
      "\titers: 400, epoch: 6 | loss: 0.1546631\n",
      "\tspeed: 0.0106s/iter; left time: 86.3644s\n",
      "\titers: 500, epoch: 6 | loss: 0.1294211\n",
      "\tspeed: 0.0106s/iter; left time: 84.9852s\n",
      "Epoch: 6 cost time: 6.276424169540405\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1377903 Vali Loss: 0.0368099 Test Loss: 0.1188920\n",
      "Validation loss decreased (0.037219 --> 0.036810).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1413189\n",
      "\tspeed: 0.0378s/iter; left time: 297.5613s\n",
      "\titers: 200, epoch: 7 | loss: 0.2008557\n",
      "\tspeed: 0.0104s/iter; left time: 81.2336s\n",
      "\titers: 300, epoch: 7 | loss: 0.1179606\n",
      "\tspeed: 0.0105s/iter; left time: 80.3890s\n",
      "\titers: 400, epoch: 7 | loss: 0.1264820\n",
      "\tspeed: 0.0105s/iter; left time: 79.7143s\n",
      "\titers: 500, epoch: 7 | loss: 0.1174680\n",
      "\tspeed: 0.0105s/iter; left time: 78.3173s\n",
      "Epoch: 7 cost time: 6.332913875579834\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1374366 Vali Loss: 0.0371287 Test Loss: 0.1191760\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1442029\n",
      "\tspeed: 0.0374s/iter; left time: 273.4814s\n",
      "\titers: 200, epoch: 8 | loss: 0.1226638\n",
      "\tspeed: 0.0106s/iter; left time: 76.1778s\n",
      "\titers: 300, epoch: 8 | loss: 0.1857536\n",
      "\tspeed: 0.0105s/iter; left time: 74.7902s\n",
      "\titers: 400, epoch: 8 | loss: 0.1947010\n",
      "\tspeed: 0.0121s/iter; left time: 84.5276s\n",
      "\titers: 500, epoch: 8 | loss: 0.1027075\n",
      "\tspeed: 0.0120s/iter; left time: 83.1993s\n",
      "Epoch: 8 cost time: 6.700424671173096\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1363312 Vali Loss: 0.0368068 Test Loss: 0.1189497\n",
      "Validation loss decreased (0.036810 --> 0.036807).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1744780\n",
      "\tspeed: 0.0384s/iter; left time: 259.0737s\n",
      "\titers: 200, epoch: 9 | loss: 0.1239212\n",
      "\tspeed: 0.0117s/iter; left time: 77.9696s\n",
      "\titers: 300, epoch: 9 | loss: 0.0659825\n",
      "\tspeed: 0.0117s/iter; left time: 76.7691s\n",
      "\titers: 400, epoch: 9 | loss: 0.1558087\n",
      "\tspeed: 0.0117s/iter; left time: 75.6209s\n",
      "\titers: 500, epoch: 9 | loss: 0.1267672\n",
      "\tspeed: 0.0118s/iter; left time: 74.6009s\n",
      "Epoch: 9 cost time: 6.9901556968688965\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1359967 Vali Loss: 0.0371288 Test Loss: 0.1191462\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1579017\n",
      "\tspeed: 0.0372s/iter; left time: 229.5839s\n",
      "\titers: 200, epoch: 10 | loss: 0.1459462\n",
      "\tspeed: 0.0105s/iter; left time: 63.7480s\n",
      "\titers: 300, epoch: 10 | loss: 0.1129273\n",
      "\tspeed: 0.0105s/iter; left time: 62.6696s\n",
      "\titers: 400, epoch: 10 | loss: 0.1299830\n",
      "\tspeed: 0.0105s/iter; left time: 61.6381s\n",
      "\titers: 500, epoch: 10 | loss: 0.1774755\n",
      "\tspeed: 0.0105s/iter; left time: 60.5790s\n",
      "Epoch: 10 cost time: 6.240526914596558\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1356069 Vali Loss: 0.0369601 Test Loss: 0.1190087\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1467692\n",
      "\tspeed: 0.0373s/iter; left time: 208.8255s\n",
      "\titers: 200, epoch: 11 | loss: 0.1327035\n",
      "\tspeed: 0.0107s/iter; left time: 59.1006s\n",
      "\titers: 300, epoch: 11 | loss: 0.1024538\n",
      "\tspeed: 0.0104s/iter; left time: 56.3033s\n",
      "\titers: 400, epoch: 11 | loss: 0.1856325\n",
      "\tspeed: 0.0104s/iter; left time: 55.2510s\n",
      "\titers: 500, epoch: 11 | loss: 0.1188164\n",
      "\tspeed: 0.0104s/iter; left time: 54.2002s\n",
      "Epoch: 11 cost time: 6.388766765594482\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1367319 Vali Loss: 0.0368543 Test Loss: 0.1190960\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1191156804561615, mae:0.21371115744113922\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1632898\n",
      "\tspeed: 0.0237s/iter; left time: 267.3663s\n",
      "\titers: 200, epoch: 1 | loss: 0.1962866\n",
      "\tspeed: 0.0116s/iter; left time: 130.2413s\n",
      "\titers: 300, epoch: 1 | loss: 0.2366278\n",
      "\tspeed: 0.0116s/iter; left time: 128.5154s\n",
      "\titers: 400, epoch: 1 | loss: 0.1839257\n",
      "\tspeed: 0.0116s/iter; left time: 127.4407s\n",
      "\titers: 500, epoch: 1 | loss: 0.1764508\n",
      "\tspeed: 0.0116s/iter; left time: 126.7231s\n",
      "Epoch: 1 cost time: 7.869370937347412\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1842088 Vali Loss: 0.0355790 Test Loss: 0.1113996\n",
      "Validation loss decreased (inf --> 0.035579).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1228776\n",
      "\tspeed: 0.0371s/iter; left time: 398.2102s\n",
      "\titers: 200, epoch: 2 | loss: 0.1663849\n",
      "\tspeed: 0.0104s/iter; left time: 110.3444s\n",
      "\titers: 300, epoch: 2 | loss: 0.1062605\n",
      "\tspeed: 0.0104s/iter; left time: 109.0489s\n",
      "\titers: 400, epoch: 2 | loss: 0.1292071\n",
      "\tspeed: 0.0103s/iter; left time: 107.7233s\n",
      "\titers: 500, epoch: 2 | loss: 0.1493803\n",
      "\tspeed: 0.0104s/iter; left time: 107.1318s\n",
      "Epoch: 2 cost time: 6.19862699508667\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1527847 Vali Loss: 0.0330815 Test Loss: 0.1088629\n",
      "Validation loss decreased (0.035579 --> 0.033082).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0841182\n",
      "\tspeed: 0.0367s/iter; left time: 373.1085s\n",
      "\titers: 200, epoch: 3 | loss: 0.1208962\n",
      "\tspeed: 0.0116s/iter; left time: 116.5664s\n",
      "\titers: 300, epoch: 3 | loss: 0.2659534\n",
      "\tspeed: 0.0116s/iter; left time: 115.8029s\n",
      "\titers: 400, epoch: 3 | loss: 0.1304747\n",
      "\tspeed: 0.0116s/iter; left time: 114.5739s\n",
      "\titers: 500, epoch: 3 | loss: 0.1050824\n",
      "\tspeed: 0.0116s/iter; left time: 113.1316s\n",
      "Epoch: 3 cost time: 6.913269519805908\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1381241 Vali Loss: 0.0310028 Test Loss: 0.1073266\n",
      "Validation loss decreased (0.033082 --> 0.031003).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1128397\n",
      "\tspeed: 0.0378s/iter; left time: 362.3650s\n",
      "\titers: 200, epoch: 4 | loss: 0.0994237\n",
      "\tspeed: 0.0103s/iter; left time: 97.6135s\n",
      "\titers: 300, epoch: 4 | loss: 0.1038814\n",
      "\tspeed: 0.0103s/iter; left time: 96.3808s\n",
      "\titers: 400, epoch: 4 | loss: 0.0991919\n",
      "\tspeed: 0.0103s/iter; left time: 95.4212s\n",
      "\titers: 500, epoch: 4 | loss: 0.1753294\n",
      "\tspeed: 0.0103s/iter; left time: 94.6595s\n",
      "Epoch: 4 cost time: 6.16164755821228\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1310582 Vali Loss: 0.0308950 Test Loss: 0.1073900\n",
      "Validation loss decreased (0.031003 --> 0.030895).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0961585\n",
      "\tspeed: 0.0361s/iter; left time: 326.0591s\n",
      "\titers: 200, epoch: 5 | loss: 0.1698866\n",
      "\tspeed: 0.0103s/iter; left time: 91.7829s\n",
      "\titers: 300, epoch: 5 | loss: 0.1103002\n",
      "\tspeed: 0.0103s/iter; left time: 90.6214s\n",
      "\titers: 400, epoch: 5 | loss: 0.1169631\n",
      "\tspeed: 0.0103s/iter; left time: 89.5298s\n",
      "\titers: 500, epoch: 5 | loss: 0.1244035\n",
      "\tspeed: 0.0103s/iter; left time: 88.7473s\n",
      "Epoch: 5 cost time: 6.169874668121338\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1278323 Vali Loss: 0.0307792 Test Loss: 0.1089362\n",
      "Validation loss decreased (0.030895 --> 0.030779).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0868208\n",
      "\tspeed: 0.0380s/iter; left time: 321.4548s\n",
      "\titers: 200, epoch: 6 | loss: 0.1824658\n",
      "\tspeed: 0.0115s/iter; left time: 95.7844s\n",
      "\titers: 300, epoch: 6 | loss: 0.1520076\n",
      "\tspeed: 0.0103s/iter; left time: 84.9660s\n",
      "\titers: 400, epoch: 6 | loss: 0.1010766\n",
      "\tspeed: 0.0103s/iter; left time: 84.1178s\n",
      "\titers: 500, epoch: 6 | loss: 0.0868167\n",
      "\tspeed: 0.0103s/iter; left time: 82.7574s\n",
      "Epoch: 6 cost time: 6.409346103668213\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1265988 Vali Loss: 0.0305154 Test Loss: 0.1082917\n",
      "Validation loss decreased (0.030779 --> 0.030515).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1266780\n",
      "\tspeed: 0.0372s/iter; left time: 293.3849s\n",
      "\titers: 200, epoch: 7 | loss: 0.1439131\n",
      "\tspeed: 0.0102s/iter; left time: 79.4561s\n",
      "\titers: 300, epoch: 7 | loss: 0.2094483\n",
      "\tspeed: 0.0102s/iter; left time: 78.6409s\n",
      "\titers: 400, epoch: 7 | loss: 0.1722923\n",
      "\tspeed: 0.0102s/iter; left time: 77.5148s\n",
      "\titers: 500, epoch: 7 | loss: 0.1462710\n",
      "\tspeed: 0.0102s/iter; left time: 76.3445s\n",
      "Epoch: 7 cost time: 6.116265296936035\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1256750 Vali Loss: 0.0302904 Test Loss: 0.1083232\n",
      "Validation loss decreased (0.030515 --> 0.030290).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1091781\n",
      "\tspeed: 0.0363s/iter; left time: 265.1469s\n",
      "\titers: 200, epoch: 8 | loss: 0.0930290\n",
      "\tspeed: 0.0102s/iter; left time: 73.6124s\n",
      "\titers: 300, epoch: 8 | loss: 0.1188539\n",
      "\tspeed: 0.0102s/iter; left time: 72.7208s\n",
      "\titers: 400, epoch: 8 | loss: 0.1192022\n",
      "\tspeed: 0.0102s/iter; left time: 71.6108s\n",
      "\titers: 500, epoch: 8 | loss: 0.0936297\n",
      "\tspeed: 0.0102s/iter; left time: 70.4356s\n",
      "Epoch: 8 cost time: 6.121912717819214\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1259217 Vali Loss: 0.0301730 Test Loss: 0.1081155\n",
      "Validation loss decreased (0.030290 --> 0.030173).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1100343\n",
      "\tspeed: 0.0363s/iter; left time: 244.7299s\n",
      "\titers: 200, epoch: 9 | loss: 0.1464002\n",
      "\tspeed: 0.0102s/iter; left time: 68.0085s\n",
      "\titers: 300, epoch: 9 | loss: 0.1102298\n",
      "\tspeed: 0.0102s/iter; left time: 66.9552s\n",
      "\titers: 400, epoch: 9 | loss: 0.1361244\n",
      "\tspeed: 0.0102s/iter; left time: 65.8991s\n",
      "\titers: 500, epoch: 9 | loss: 0.1263459\n",
      "\tspeed: 0.0102s/iter; left time: 64.9015s\n",
      "Epoch: 9 cost time: 6.1091883182525635\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1245330 Vali Loss: 0.0302652 Test Loss: 0.1083011\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1255611\n",
      "\tspeed: 0.0359s/iter; left time: 221.7361s\n",
      "\titers: 200, epoch: 10 | loss: 0.1017917\n",
      "\tspeed: 0.0103s/iter; left time: 62.3151s\n",
      "\titers: 300, epoch: 10 | loss: 0.1145379\n",
      "\tspeed: 0.0103s/iter; left time: 61.2688s\n",
      "\titers: 400, epoch: 10 | loss: 0.0913101\n",
      "\tspeed: 0.0103s/iter; left time: 60.2210s\n",
      "\titers: 500, epoch: 10 | loss: 0.0766238\n",
      "\tspeed: 0.0103s/iter; left time: 59.2169s\n",
      "Epoch: 10 cost time: 6.153545141220093\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1257018 Vali Loss: 0.0302718 Test Loss: 0.1083023\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1139698\n",
      "\tspeed: 0.0355s/iter; left time: 198.8509s\n",
      "\titers: 200, epoch: 11 | loss: 0.1388310\n",
      "\tspeed: 0.0102s/iter; left time: 55.9751s\n",
      "\titers: 300, epoch: 11 | loss: 0.1469189\n",
      "\tspeed: 0.0102s/iter; left time: 54.9612s\n",
      "\titers: 400, epoch: 11 | loss: 0.1414951\n",
      "\tspeed: 0.0102s/iter; left time: 54.1433s\n",
      "\titers: 500, epoch: 11 | loss: 0.1077557\n",
      "\tspeed: 0.0103s/iter; left time: 53.6893s\n",
      "Epoch: 11 cost time: 6.108328104019165\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1238729 Vali Loss: 0.0301528 Test Loss: 0.1083141\n",
      "Validation loss decreased (0.030173 --> 0.030153).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1152398\n",
      "\tspeed: 0.0362s/iter; left time: 182.0764s\n",
      "\titers: 200, epoch: 12 | loss: 0.1277250\n",
      "\tspeed: 0.0103s/iter; left time: 50.8075s\n",
      "\titers: 300, epoch: 12 | loss: 0.1104521\n",
      "\tspeed: 0.0103s/iter; left time: 49.7488s\n",
      "\titers: 400, epoch: 12 | loss: 0.1891102\n",
      "\tspeed: 0.0103s/iter; left time: 48.6234s\n",
      "\titers: 500, epoch: 12 | loss: 0.1129850\n",
      "\tspeed: 0.0103s/iter; left time: 47.6546s\n",
      "Epoch: 12 cost time: 6.158369064331055\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1244618 Vali Loss: 0.0302571 Test Loss: 0.1083152\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1319189\n",
      "\tspeed: 0.0358s/iter; left time: 159.6063s\n",
      "\titers: 200, epoch: 13 | loss: 0.1576452\n",
      "\tspeed: 0.0102s/iter; left time: 44.5675s\n",
      "\titers: 300, epoch: 13 | loss: 0.0837472\n",
      "\tspeed: 0.0102s/iter; left time: 43.4670s\n",
      "\titers: 400, epoch: 13 | loss: 0.1625548\n",
      "\tspeed: 0.0102s/iter; left time: 42.4604s\n",
      "\titers: 500, epoch: 13 | loss: 0.1285427\n",
      "\tspeed: 0.0102s/iter; left time: 41.3606s\n",
      "Epoch: 13 cost time: 6.073928594589233\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1248096 Vali Loss: 0.0301347 Test Loss: 0.1083239\n",
      "Validation loss decreased (0.030153 --> 0.030135).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1196261\n",
      "\tspeed: 0.0366s/iter; left time: 142.3014s\n",
      "\titers: 200, epoch: 14 | loss: 0.0963889\n",
      "\tspeed: 0.0103s/iter; left time: 39.0954s\n",
      "\titers: 300, epoch: 14 | loss: 0.1159598\n",
      "\tspeed: 0.0103s/iter; left time: 37.9580s\n",
      "\titers: 400, epoch: 14 | loss: 0.1377869\n",
      "\tspeed: 0.0103s/iter; left time: 36.9006s\n",
      "\titers: 500, epoch: 14 | loss: 0.1410586\n",
      "\tspeed: 0.0103s/iter; left time: 35.8943s\n",
      "Epoch: 14 cost time: 6.183748483657837\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1254393 Vali Loss: 0.0301758 Test Loss: 0.1083240\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.0806293\n",
      "\tspeed: 0.0371s/iter; left time: 123.3593s\n",
      "\titers: 200, epoch: 15 | loss: 0.1490056\n",
      "\tspeed: 0.0103s/iter; left time: 33.0714s\n",
      "\titers: 300, epoch: 15 | loss: 0.1265745\n",
      "\tspeed: 0.0103s/iter; left time: 32.0125s\n",
      "\titers: 400, epoch: 15 | loss: 0.1200736\n",
      "\tspeed: 0.0103s/iter; left time: 31.0989s\n",
      "\titers: 500, epoch: 15 | loss: 0.0979574\n",
      "\tspeed: 0.0103s/iter; left time: 29.9696s\n",
      "Epoch: 15 cost time: 6.126640796661377\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1243336 Vali Loss: 0.0301346 Test Loss: 0.1083250\n",
      "Validation loss decreased (0.030135 --> 0.030135).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.1814100\n",
      "\tspeed: 0.0360s/iter; left time: 99.0077s\n",
      "\titers: 200, epoch: 16 | loss: 0.1212711\n",
      "\tspeed: 0.0102s/iter; left time: 27.1071s\n",
      "\titers: 300, epoch: 16 | loss: 0.1605145\n",
      "\tspeed: 0.0102s/iter; left time: 26.1193s\n",
      "\titers: 400, epoch: 16 | loss: 0.1443062\n",
      "\tspeed: 0.0103s/iter; left time: 25.1426s\n",
      "\titers: 500, epoch: 16 | loss: 0.2023915\n",
      "\tspeed: 0.0103s/iter; left time: 24.1744s\n",
      "Epoch: 16 cost time: 6.172207355499268\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1248819 Vali Loss: 0.0301262 Test Loss: 0.1083253\n",
      "Validation loss decreased (0.030135 --> 0.030126).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "\titers: 100, epoch: 17 | loss: 0.0958868\n",
      "\tspeed: 0.0367s/iter; left time: 79.9544s\n",
      "\titers: 200, epoch: 17 | loss: 0.1025478\n",
      "\tspeed: 0.0113s/iter; left time: 23.4627s\n",
      "\titers: 300, epoch: 17 | loss: 0.1280416\n",
      "\tspeed: 0.0103s/iter; left time: 20.3776s\n",
      "\titers: 400, epoch: 17 | loss: 0.1144716\n",
      "\tspeed: 0.0102s/iter; left time: 19.2558s\n",
      "\titers: 500, epoch: 17 | loss: 0.0766145\n",
      "\tspeed: 0.0102s/iter; left time: 18.2323s\n",
      "Epoch: 17 cost time: 6.365556716918945\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1247482 Vali Loss: 0.0302355 Test Loss: 0.1083252\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1366455\n",
      "\tspeed: 0.0353s/iter; left time: 56.8831s\n",
      "\titers: 200, epoch: 18 | loss: 0.1215379\n",
      "\tspeed: 0.0102s/iter; left time: 15.4112s\n",
      "\titers: 300, epoch: 18 | loss: 0.0830668\n",
      "\tspeed: 0.0102s/iter; left time: 14.3901s\n",
      "\titers: 400, epoch: 18 | loss: 0.2063132\n",
      "\tspeed: 0.0102s/iter; left time: 13.4156s\n",
      "\titers: 500, epoch: 18 | loss: 0.1021243\n",
      "\tspeed: 0.0102s/iter; left time: 12.3598s\n",
      "Epoch: 18 cost time: 6.065615892410278\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.1247364 Vali Loss: 0.0302222 Test Loss: 0.1083251\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "\titers: 100, epoch: 19 | loss: 0.1282438\n",
      "\tspeed: 0.0369s/iter; left time: 38.4137s\n",
      "\titers: 200, epoch: 19 | loss: 0.1352768\n",
      "\tspeed: 0.0103s/iter; left time: 9.6758s\n",
      "\titers: 300, epoch: 19 | loss: 0.1141587\n",
      "\tspeed: 0.0103s/iter; left time: 8.6235s\n",
      "\titers: 400, epoch: 19 | loss: 0.1481847\n",
      "\tspeed: 0.0103s/iter; left time: 7.5959s\n",
      "\titers: 500, epoch: 19 | loss: 0.1201239\n",
      "\tspeed: 0.0103s/iter; left time: 6.5710s\n",
      "Epoch: 19 cost time: 6.247210741043091\n",
      "Epoch: 19, Steps: 570 | Train Loss: 0.1244544 Vali Loss: 0.0302172 Test Loss: 0.1083252\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10847773402929306, mae:0.19956164062023163\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2209077\n",
      "\tspeed: 0.0235s/iter; left time: 265.5957s\n",
      "\titers: 200, epoch: 1 | loss: 0.1631805\n",
      "\tspeed: 0.0116s/iter; left time: 129.7721s\n",
      "\titers: 300, epoch: 1 | loss: 0.1428613\n",
      "\tspeed: 0.0116s/iter; left time: 128.2919s\n",
      "\titers: 400, epoch: 1 | loss: 0.1150904\n",
      "\tspeed: 0.0116s/iter; left time: 127.1577s\n",
      "\titers: 500, epoch: 1 | loss: 0.2018290\n",
      "\tspeed: 0.0116s/iter; left time: 126.5637s\n",
      "Epoch: 1 cost time: 7.841349124908447\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2200478 Vali Loss: 0.0456244 Test Loss: 0.1333508\n",
      "Validation loss decreased (inf --> 0.045624).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1866460\n",
      "\tspeed: 0.0382s/iter; left time: 410.3139s\n",
      "\titers: 200, epoch: 2 | loss: 0.2389505\n",
      "\tspeed: 0.0103s/iter; left time: 109.4272s\n",
      "\titers: 300, epoch: 2 | loss: 0.1493279\n",
      "\tspeed: 0.0103s/iter; left time: 108.3167s\n",
      "\titers: 400, epoch: 2 | loss: 0.2726944\n",
      "\tspeed: 0.0103s/iter; left time: 107.4735s\n",
      "\titers: 500, epoch: 2 | loss: 0.2013200\n",
      "\tspeed: 0.0103s/iter; left time: 106.2248s\n",
      "Epoch: 2 cost time: 6.2765953540802\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1774382 Vali Loss: 0.0395741 Test Loss: 0.1253181\n",
      "Validation loss decreased (0.045624 --> 0.039574).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1207781\n",
      "\tspeed: 0.0371s/iter; left time: 377.2339s\n",
      "\titers: 200, epoch: 3 | loss: 0.1570719\n",
      "\tspeed: 0.0103s/iter; left time: 103.3707s\n",
      "\titers: 300, epoch: 3 | loss: 0.1950693\n",
      "\tspeed: 0.0103s/iter; left time: 102.1385s\n",
      "\titers: 400, epoch: 3 | loss: 0.1283651\n",
      "\tspeed: 0.0102s/iter; left time: 100.9523s\n",
      "\titers: 500, epoch: 3 | loss: 0.1199954\n",
      "\tspeed: 0.0102s/iter; left time: 99.8487s\n",
      "Epoch: 3 cost time: 6.127661228179932\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1601210 Vali Loss: 0.0392600 Test Loss: 0.1258912\n",
      "Validation loss decreased (0.039574 --> 0.039260).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0857576\n",
      "\tspeed: 0.0376s/iter; left time: 360.2417s\n",
      "\titers: 200, epoch: 4 | loss: 0.1435241\n",
      "\tspeed: 0.0116s/iter; left time: 109.8028s\n",
      "\titers: 300, epoch: 4 | loss: 0.1204739\n",
      "\tspeed: 0.0116s/iter; left time: 108.8739s\n",
      "\titers: 400, epoch: 4 | loss: 0.1990416\n",
      "\tspeed: 0.0116s/iter; left time: 108.0054s\n",
      "\titers: 500, epoch: 4 | loss: 0.1360075\n",
      "\tspeed: 0.0103s/iter; left time: 94.4807s\n",
      "Epoch: 4 cost time: 6.654064416885376\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1509492 Vali Loss: 0.0388266 Test Loss: 0.1260145\n",
      "Validation loss decreased (0.039260 --> 0.038827).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1325593\n",
      "\tspeed: 0.0370s/iter; left time: 333.4112s\n",
      "\titers: 200, epoch: 5 | loss: 0.1007131\n",
      "\tspeed: 0.0116s/iter; left time: 103.3162s\n",
      "\titers: 300, epoch: 5 | loss: 0.1569553\n",
      "\tspeed: 0.0116s/iter; left time: 101.9664s\n",
      "\titers: 400, epoch: 5 | loss: 0.1501538\n",
      "\tspeed: 0.0116s/iter; left time: 101.0960s\n",
      "\titers: 500, epoch: 5 | loss: 0.1873112\n",
      "\tspeed: 0.0116s/iter; left time: 99.6747s\n",
      "Epoch: 5 cost time: 6.878689765930176\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1477954 Vali Loss: 0.0387457 Test Loss: 0.1248546\n",
      "Validation loss decreased (0.038827 --> 0.038746).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1031383\n",
      "\tspeed: 0.0364s/iter; left time: 307.6909s\n",
      "\titers: 200, epoch: 6 | loss: 0.1437926\n",
      "\tspeed: 0.0102s/iter; left time: 85.5695s\n",
      "\titers: 300, epoch: 6 | loss: 0.1388285\n",
      "\tspeed: 0.0102s/iter; left time: 84.3835s\n",
      "\titers: 400, epoch: 6 | loss: 0.1307970\n",
      "\tspeed: 0.0102s/iter; left time: 83.3025s\n",
      "\titers: 500, epoch: 6 | loss: 0.1496309\n",
      "\tspeed: 0.0102s/iter; left time: 82.2481s\n",
      "Epoch: 6 cost time: 6.12692666053772\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1451480 Vali Loss: 0.0380487 Test Loss: 0.1235802\n",
      "Validation loss decreased (0.038746 --> 0.038049).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1069827\n",
      "\tspeed: 0.0367s/iter; left time: 289.0270s\n",
      "\titers: 200, epoch: 7 | loss: 0.1461647\n",
      "\tspeed: 0.0103s/iter; left time: 80.0886s\n",
      "\titers: 300, epoch: 7 | loss: 0.1125339\n",
      "\tspeed: 0.0103s/iter; left time: 79.3070s\n",
      "\titers: 400, epoch: 7 | loss: 0.0938165\n",
      "\tspeed: 0.0103s/iter; left time: 78.0901s\n",
      "\titers: 500, epoch: 7 | loss: 0.1562604\n",
      "\tspeed: 0.0103s/iter; left time: 76.7676s\n",
      "Epoch: 7 cost time: 6.1953113079071045\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1428303 Vali Loss: 0.0379655 Test Loss: 0.1234711\n",
      "Validation loss decreased (0.038049 --> 0.037965).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1077710\n",
      "\tspeed: 0.0366s/iter; left time: 267.7613s\n",
      "\titers: 200, epoch: 8 | loss: 0.1454666\n",
      "\tspeed: 0.0103s/iter; left time: 73.9128s\n",
      "\titers: 300, epoch: 8 | loss: 0.1180467\n",
      "\tspeed: 0.0103s/iter; left time: 72.9523s\n",
      "\titers: 400, epoch: 8 | loss: 0.1400131\n",
      "\tspeed: 0.0103s/iter; left time: 71.9738s\n",
      "\titers: 500, epoch: 8 | loss: 0.1238756\n",
      "\tspeed: 0.0103s/iter; left time: 70.9122s\n",
      "Epoch: 8 cost time: 6.1830153465271\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1415453 Vali Loss: 0.0379648 Test Loss: 0.1236288\n",
      "Validation loss decreased (0.037965 --> 0.037965).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1197457\n",
      "\tspeed: 0.0385s/iter; left time: 259.7859s\n",
      "\titers: 200, epoch: 9 | loss: 0.1663950\n",
      "\tspeed: 0.0103s/iter; left time: 68.1398s\n",
      "\titers: 300, epoch: 9 | loss: 0.1219669\n",
      "\tspeed: 0.0103s/iter; left time: 67.2912s\n",
      "\titers: 400, epoch: 9 | loss: 0.1165582\n",
      "\tspeed: 0.0103s/iter; left time: 66.1291s\n",
      "\titers: 500, epoch: 9 | loss: 0.1473963\n",
      "\tspeed: 0.0103s/iter; left time: 65.2429s\n",
      "Epoch: 9 cost time: 6.157060384750366\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1408470 Vali Loss: 0.0381282 Test Loss: 0.1237652\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1218221\n",
      "\tspeed: 0.0360s/iter; left time: 221.9904s\n",
      "\titers: 200, epoch: 10 | loss: 0.1730519\n",
      "\tspeed: 0.0103s/iter; left time: 62.3285s\n",
      "\titers: 300, epoch: 10 | loss: 0.1865845\n",
      "\tspeed: 0.0103s/iter; left time: 61.3602s\n",
      "\titers: 400, epoch: 10 | loss: 0.1654164\n",
      "\tspeed: 0.0103s/iter; left time: 60.3178s\n",
      "\titers: 500, epoch: 10 | loss: 0.1427130\n",
      "\tspeed: 0.0103s/iter; left time: 59.3151s\n",
      "Epoch: 10 cost time: 6.1498496532440186\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1427504 Vali Loss: 0.0381587 Test Loss: 0.1238057\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1540829\n",
      "\tspeed: 0.0356s/iter; left time: 199.3808s\n",
      "\titers: 200, epoch: 11 | loss: 0.1129410\n",
      "\tspeed: 0.0103s/iter; left time: 56.6798s\n",
      "\titers: 300, epoch: 11 | loss: 0.1937230\n",
      "\tspeed: 0.0103s/iter; left time: 55.6345s\n",
      "\titers: 400, epoch: 11 | loss: 0.1644548\n",
      "\tspeed: 0.0103s/iter; left time: 54.6029s\n",
      "\titers: 500, epoch: 11 | loss: 0.1550523\n",
      "\tspeed: 0.0103s/iter; left time: 53.5809s\n",
      "Epoch: 11 cost time: 6.150158405303955\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1425434 Vali Loss: 0.0379050 Test Loss: 0.1238274\n",
      "Validation loss decreased (0.037965 --> 0.037905).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1438372\n",
      "\tspeed: 0.0363s/iter; left time: 182.8017s\n",
      "\titers: 200, epoch: 12 | loss: 0.1561584\n",
      "\tspeed: 0.0102s/iter; left time: 50.4662s\n",
      "\titers: 300, epoch: 12 | loss: 0.1043206\n",
      "\tspeed: 0.0102s/iter; left time: 49.4456s\n",
      "\titers: 400, epoch: 12 | loss: 0.1268635\n",
      "\tspeed: 0.0102s/iter; left time: 48.4109s\n",
      "\titers: 500, epoch: 12 | loss: 0.0814305\n",
      "\tspeed: 0.0102s/iter; left time: 47.3846s\n",
      "Epoch: 12 cost time: 6.131193161010742\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1417405 Vali Loss: 0.0379074 Test Loss: 0.1238480\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1408719\n",
      "\tspeed: 0.0355s/iter; left time: 158.3655s\n",
      "\titers: 200, epoch: 13 | loss: 0.1521952\n",
      "\tspeed: 0.0103s/iter; left time: 44.7221s\n",
      "\titers: 300, epoch: 13 | loss: 0.1613741\n",
      "\tspeed: 0.0103s/iter; left time: 43.7016s\n",
      "\titers: 400, epoch: 13 | loss: 0.1620815\n",
      "\tspeed: 0.0103s/iter; left time: 42.6737s\n",
      "\titers: 500, epoch: 13 | loss: 0.1417292\n",
      "\tspeed: 0.0103s/iter; left time: 41.6374s\n",
      "Epoch: 13 cost time: 6.131879568099976\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1424282 Vali Loss: 0.0379697 Test Loss: 0.1238488\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1171110\n",
      "\tspeed: 0.0357s/iter; left time: 138.9802s\n",
      "\titers: 200, epoch: 14 | loss: 0.1668206\n",
      "\tspeed: 0.0102s/iter; left time: 38.7040s\n",
      "\titers: 300, epoch: 14 | loss: 0.1107595\n",
      "\tspeed: 0.0102s/iter; left time: 37.7094s\n",
      "\titers: 400, epoch: 14 | loss: 0.0863452\n",
      "\tspeed: 0.0102s/iter; left time: 36.6842s\n",
      "\titers: 500, epoch: 14 | loss: 0.1830739\n",
      "\tspeed: 0.0102s/iter; left time: 35.6677s\n",
      "Epoch: 14 cost time: 6.100139617919922\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1414670 Vali Loss: 0.0378636 Test Loss: 0.1238594\n",
      "Validation loss decreased (0.037905 --> 0.037864).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1206651\n",
      "\tspeed: 0.0364s/iter; left time: 120.7250s\n",
      "\titers: 200, epoch: 15 | loss: 0.1962299\n",
      "\tspeed: 0.0103s/iter; left time: 33.2092s\n",
      "\titers: 300, epoch: 15 | loss: 0.0880813\n",
      "\tspeed: 0.0103s/iter; left time: 32.1462s\n",
      "\titers: 400, epoch: 15 | loss: 0.0686535\n",
      "\tspeed: 0.0103s/iter; left time: 31.1094s\n",
      "\titers: 500, epoch: 15 | loss: 0.2198229\n",
      "\tspeed: 0.0103s/iter; left time: 30.0689s\n",
      "Epoch: 15 cost time: 6.170158624649048\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1417189 Vali Loss: 0.0379597 Test Loss: 0.1238618\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.1515440\n",
      "\tspeed: 0.0352s/iter; left time: 96.8353s\n",
      "\titers: 200, epoch: 16 | loss: 0.1062499\n",
      "\tspeed: 0.0103s/iter; left time: 27.1836s\n",
      "\titers: 300, epoch: 16 | loss: 0.1378633\n",
      "\tspeed: 0.0102s/iter; left time: 26.1217s\n",
      "\titers: 400, epoch: 16 | loss: 0.1257110\n",
      "\tspeed: 0.0102s/iter; left time: 25.1012s\n",
      "\titers: 500, epoch: 16 | loss: 0.1545956\n",
      "\tspeed: 0.0102s/iter; left time: 24.0931s\n",
      "Epoch: 16 cost time: 6.1100122928619385\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1420061 Vali Loss: 0.0381102 Test Loss: 0.1238612\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "\titers: 100, epoch: 17 | loss: 0.1119800\n",
      "\tspeed: 0.0367s/iter; left time: 80.0318s\n",
      "\titers: 200, epoch: 17 | loss: 0.0862425\n",
      "\tspeed: 0.0103s/iter; left time: 21.3474s\n",
      "\titers: 300, epoch: 17 | loss: 0.0969094\n",
      "\tspeed: 0.0103s/iter; left time: 20.3183s\n",
      "\titers: 400, epoch: 17 | loss: 0.1524913\n",
      "\tspeed: 0.0103s/iter; left time: 19.2937s\n",
      "\titers: 500, epoch: 17 | loss: 0.0960862\n",
      "\tspeed: 0.0103s/iter; left time: 18.2647s\n",
      "Epoch: 17 cost time: 6.125229597091675\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1422323 Vali Loss: 0.0381015 Test Loss: 0.1238618\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1240406259894371, mae:0.2200387865304947\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2494522\n",
      "\tspeed: 0.0251s/iter; left time: 283.4774s\n",
      "\titers: 200, epoch: 1 | loss: 0.2129330\n",
      "\tspeed: 0.0129s/iter; left time: 145.0078s\n",
      "\titers: 300, epoch: 1 | loss: 0.1523527\n",
      "\tspeed: 0.0129s/iter; left time: 143.3534s\n",
      "\titers: 400, epoch: 1 | loss: 0.1196123\n",
      "\tspeed: 0.0129s/iter; left time: 142.2852s\n",
      "\titers: 500, epoch: 1 | loss: 0.1970649\n",
      "\tspeed: 0.0129s/iter; left time: 141.0794s\n",
      "Epoch: 1 cost time: 8.638608694076538\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2074562 Vali Loss: 0.0426341 Test Loss: 0.1295917\n",
      "Validation loss decreased (inf --> 0.042634).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1755087\n",
      "\tspeed: 0.0418s/iter; left time: 449.0341s\n",
      "\titers: 200, epoch: 2 | loss: 0.1553559\n",
      "\tspeed: 0.0119s/iter; left time: 126.2638s\n",
      "\titers: 300, epoch: 2 | loss: 0.1324689\n",
      "\tspeed: 0.0119s/iter; left time: 125.1277s\n",
      "\titers: 400, epoch: 2 | loss: 0.0915983\n",
      "\tspeed: 0.0119s/iter; left time: 123.8174s\n",
      "\titers: 500, epoch: 2 | loss: 0.1688437\n",
      "\tspeed: 0.0119s/iter; left time: 122.9617s\n",
      "Epoch: 2 cost time: 7.117243528366089\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1654598 Vali Loss: 0.0396928 Test Loss: 0.1245952\n",
      "Validation loss decreased (0.042634 --> 0.039693).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1581594\n",
      "\tspeed: 0.0412s/iter; left time: 418.6888s\n",
      "\titers: 200, epoch: 3 | loss: 0.1721560\n",
      "\tspeed: 0.0127s/iter; left time: 128.2711s\n",
      "\titers: 300, epoch: 3 | loss: 0.1140315\n",
      "\tspeed: 0.0127s/iter; left time: 126.9280s\n",
      "\titers: 400, epoch: 3 | loss: 0.1244937\n",
      "\tspeed: 0.0128s/iter; left time: 125.7967s\n",
      "\titers: 500, epoch: 3 | loss: 0.1060665\n",
      "\tspeed: 0.0127s/iter; left time: 124.2067s\n",
      "Epoch: 3 cost time: 7.587965250015259\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1411879 Vali Loss: 0.0367619 Test Loss: 0.1210644\n",
      "Validation loss decreased (0.039693 --> 0.036762).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1151561\n",
      "\tspeed: 0.0420s/iter; left time: 402.9067s\n",
      "\titers: 200, epoch: 4 | loss: 0.1654308\n",
      "\tspeed: 0.0127s/iter; left time: 120.9562s\n",
      "\titers: 300, epoch: 4 | loss: 0.0820960\n",
      "\tspeed: 0.0119s/iter; left time: 111.4393s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956668\n",
      "\tspeed: 0.0119s/iter; left time: 110.3678s\n",
      "\titers: 500, epoch: 4 | loss: 0.1009726\n",
      "\tspeed: 0.0119s/iter; left time: 109.1275s\n",
      "Epoch: 4 cost time: 7.246025562286377\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1289499 Vali Loss: 0.0352489 Test Loss: 0.1231693\n",
      "Validation loss decreased (0.036762 --> 0.035249).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1123193\n",
      "\tspeed: 0.0394s/iter; left time: 355.0309s\n",
      "\titers: 200, epoch: 5 | loss: 0.1100611\n",
      "\tspeed: 0.0118s/iter; left time: 105.6235s\n",
      "\titers: 300, epoch: 5 | loss: 0.1749112\n",
      "\tspeed: 0.0118s/iter; left time: 104.3966s\n",
      "\titers: 400, epoch: 5 | loss: 0.1129525\n",
      "\tspeed: 0.0118s/iter; left time: 103.2208s\n",
      "\titers: 500, epoch: 5 | loss: 0.1089170\n",
      "\tspeed: 0.0118s/iter; left time: 102.0491s\n",
      "Epoch: 5 cost time: 7.0756988525390625\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1233984 Vali Loss: 0.0358403 Test Loss: 0.1185238\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1055258\n",
      "\tspeed: 0.0386s/iter; left time: 326.1343s\n",
      "\titers: 200, epoch: 6 | loss: 0.0744367\n",
      "\tspeed: 0.0119s/iter; left time: 99.1327s\n",
      "\titers: 300, epoch: 6 | loss: 0.1253520\n",
      "\tspeed: 0.0119s/iter; left time: 97.9431s\n",
      "\titers: 400, epoch: 6 | loss: 0.1410496\n",
      "\tspeed: 0.0119s/iter; left time: 96.7485s\n",
      "\titers: 500, epoch: 6 | loss: 0.0853137\n",
      "\tspeed: 0.0119s/iter; left time: 95.5843s\n",
      "Epoch: 6 cost time: 7.107334852218628\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1182106 Vali Loss: 0.0345302 Test Loss: 0.1180393\n",
      "Validation loss decreased (0.035249 --> 0.034530).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1296884\n",
      "\tspeed: 0.0398s/iter; left time: 313.3094s\n",
      "\titers: 200, epoch: 7 | loss: 0.1308485\n",
      "\tspeed: 0.0127s/iter; left time: 98.9300s\n",
      "\titers: 300, epoch: 7 | loss: 0.1279680\n",
      "\tspeed: 0.0127s/iter; left time: 97.1684s\n",
      "\titers: 400, epoch: 7 | loss: 0.1000333\n",
      "\tspeed: 0.0119s/iter; left time: 90.0434s\n",
      "\titers: 500, epoch: 7 | loss: 0.1005313\n",
      "\tspeed: 0.0119s/iter; left time: 88.8640s\n",
      "Epoch: 7 cost time: 7.310508489608765\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1169174 Vali Loss: 0.0343042 Test Loss: 0.1196582\n",
      "Validation loss decreased (0.034530 --> 0.034304).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0725371\n",
      "\tspeed: 0.0422s/iter; left time: 308.6909s\n",
      "\titers: 200, epoch: 8 | loss: 0.1225613\n",
      "\tspeed: 0.0130s/iter; left time: 93.7183s\n",
      "\titers: 300, epoch: 8 | loss: 0.0996163\n",
      "\tspeed: 0.0129s/iter; left time: 91.9585s\n",
      "\titers: 400, epoch: 8 | loss: 0.0855558\n",
      "\tspeed: 0.0129s/iter; left time: 90.7407s\n",
      "\titers: 500, epoch: 8 | loss: 0.0986222\n",
      "\tspeed: 0.0129s/iter; left time: 89.4310s\n",
      "Epoch: 8 cost time: 7.713958263397217\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1168448 Vali Loss: 0.0341010 Test Loss: 0.1192758\n",
      "Validation loss decreased (0.034304 --> 0.034101).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0960807\n",
      "\tspeed: 0.0419s/iter; left time: 282.7568s\n",
      "\titers: 200, epoch: 9 | loss: 0.1103233\n",
      "\tspeed: 0.0118s/iter; left time: 78.6182s\n",
      "\titers: 300, epoch: 9 | loss: 0.1214779\n",
      "\tspeed: 0.0118s/iter; left time: 77.3058s\n",
      "\titers: 400, epoch: 9 | loss: 0.0919171\n",
      "\tspeed: 0.0118s/iter; left time: 76.0580s\n",
      "\titers: 500, epoch: 9 | loss: 0.1461041\n",
      "\tspeed: 0.0118s/iter; left time: 74.8309s\n",
      "Epoch: 9 cost time: 7.076043605804443\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1163970 Vali Loss: 0.0340609 Test Loss: 0.1190358\n",
      "Validation loss decreased (0.034101 --> 0.034061).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1156739\n",
      "\tspeed: 0.0397s/iter; left time: 244.8792s\n",
      "\titers: 200, epoch: 10 | loss: 0.1168119\n",
      "\tspeed: 0.0118s/iter; left time: 71.6317s\n",
      "\titers: 300, epoch: 10 | loss: 0.0976095\n",
      "\tspeed: 0.0118s/iter; left time: 70.4634s\n",
      "\titers: 400, epoch: 10 | loss: 0.0884077\n",
      "\tspeed: 0.0118s/iter; left time: 69.2752s\n",
      "\titers: 500, epoch: 10 | loss: 0.1521445\n",
      "\tspeed: 0.0118s/iter; left time: 68.0986s\n",
      "Epoch: 10 cost time: 7.102535963058472\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1171683 Vali Loss: 0.0342109 Test Loss: 0.1189131\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0946117\n",
      "\tspeed: 0.0393s/iter; left time: 220.3769s\n",
      "\titers: 200, epoch: 11 | loss: 0.1715285\n",
      "\tspeed: 0.0118s/iter; left time: 64.9770s\n",
      "\titers: 300, epoch: 11 | loss: 0.1023363\n",
      "\tspeed: 0.0118s/iter; left time: 63.8202s\n",
      "\titers: 400, epoch: 11 | loss: 0.1114139\n",
      "\tspeed: 0.0118s/iter; left time: 62.5949s\n",
      "\titers: 500, epoch: 11 | loss: 0.1463158\n",
      "\tspeed: 0.0118s/iter; left time: 61.4312s\n",
      "Epoch: 11 cost time: 7.009037256240845\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1154145 Vali Loss: 0.0342090 Test Loss: 0.1189196\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1003364\n",
      "\tspeed: 0.0390s/iter; left time: 196.2166s\n",
      "\titers: 200, epoch: 12 | loss: 0.0884850\n",
      "\tspeed: 0.0129s/iter; left time: 63.7594s\n",
      "\titers: 300, epoch: 12 | loss: 0.0798764\n",
      "\tspeed: 0.0129s/iter; left time: 62.4789s\n",
      "\titers: 400, epoch: 12 | loss: 0.0917849\n",
      "\tspeed: 0.0129s/iter; left time: 61.1908s\n",
      "\titers: 500, epoch: 12 | loss: 0.1094390\n",
      "\tspeed: 0.0129s/iter; left time: 59.9552s\n",
      "Epoch: 12 cost time: 7.649098873138428\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1151760 Vali Loss: 0.0342856 Test Loss: 0.1188759\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11919538676738739, mae:0.2178240269422531\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1061446\n",
      "\tspeed: 0.0231s/iter; left time: 260.8877s\n",
      "\titers: 200, epoch: 1 | loss: 0.2938389\n",
      "\tspeed: 0.0114s/iter; left time: 127.5657s\n",
      "\titers: 300, epoch: 1 | loss: 0.1393132\n",
      "\tspeed: 0.0114s/iter; left time: 126.2150s\n",
      "\titers: 400, epoch: 1 | loss: 0.1742748\n",
      "\tspeed: 0.0114s/iter; left time: 125.2043s\n",
      "\titers: 500, epoch: 1 | loss: 0.1219428\n",
      "\tspeed: 0.0114s/iter; left time: 124.0937s\n",
      "Epoch: 1 cost time: 7.709277153015137\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1809858 Vali Loss: 0.0358772 Test Loss: 0.1120169\n",
      "Validation loss decreased (inf --> 0.035877).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1285634\n",
      "\tspeed: 0.0389s/iter; left time: 417.7519s\n",
      "\titers: 200, epoch: 2 | loss: 0.1006706\n",
      "\tspeed: 0.0115s/iter; left time: 122.3823s\n",
      "\titers: 300, epoch: 2 | loss: 0.1497626\n",
      "\tspeed: 0.0115s/iter; left time: 120.9259s\n",
      "\titers: 400, epoch: 2 | loss: 0.2290181\n",
      "\tspeed: 0.0115s/iter; left time: 119.9077s\n",
      "\titers: 500, epoch: 2 | loss: 0.2421417\n",
      "\tspeed: 0.0115s/iter; left time: 118.5764s\n",
      "Epoch: 2 cost time: 6.86345362663269\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1469245 Vali Loss: 0.0362476 Test Loss: 0.1125096\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1461935\n",
      "\tspeed: 0.0408s/iter; left time: 414.7938s\n",
      "\titers: 200, epoch: 3 | loss: 0.0874312\n",
      "\tspeed: 0.0114s/iter; left time: 114.9302s\n",
      "\titers: 300, epoch: 3 | loss: 0.1629128\n",
      "\tspeed: 0.0114s/iter; left time: 113.6591s\n",
      "\titers: 400, epoch: 3 | loss: 0.1303105\n",
      "\tspeed: 0.0114s/iter; left time: 112.6394s\n",
      "\titers: 500, epoch: 3 | loss: 0.1554490\n",
      "\tspeed: 0.0114s/iter; left time: 111.6555s\n",
      "Epoch: 3 cost time: 6.804374694824219\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1299274 Vali Loss: 0.0332274 Test Loss: 0.1086616\n",
      "Validation loss decreased (0.035877 --> 0.033227).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1100514\n",
      "\tspeed: 0.0380s/iter; left time: 364.1145s\n",
      "\titers: 200, epoch: 4 | loss: 0.0898602\n",
      "\tspeed: 0.0115s/iter; left time: 108.8907s\n",
      "\titers: 300, epoch: 4 | loss: 0.0719668\n",
      "\tspeed: 0.0115s/iter; left time: 107.7354s\n",
      "\titers: 400, epoch: 4 | loss: 0.1100080\n",
      "\tspeed: 0.0115s/iter; left time: 106.6724s\n",
      "\titers: 500, epoch: 4 | loss: 0.1275932\n",
      "\tspeed: 0.0115s/iter; left time: 105.4187s\n",
      "Epoch: 4 cost time: 6.862810850143433\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1192766 Vali Loss: 0.0334128 Test Loss: 0.1083727\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0998100\n",
      "\tspeed: 0.0393s/iter; left time: 354.4202s\n",
      "\titers: 200, epoch: 5 | loss: 0.0951665\n",
      "\tspeed: 0.0115s/iter; left time: 102.2202s\n",
      "\titers: 300, epoch: 5 | loss: 0.1616226\n",
      "\tspeed: 0.0114s/iter; left time: 100.8026s\n",
      "\titers: 400, epoch: 5 | loss: 0.0922074\n",
      "\tspeed: 0.0114s/iter; left time: 99.7286s\n",
      "\titers: 500, epoch: 5 | loss: 0.1030626\n",
      "\tspeed: 0.0114s/iter; left time: 98.5524s\n",
      "Epoch: 5 cost time: 6.81291127204895\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1137036 Vali Loss: 0.0332378 Test Loss: 0.1093978\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1444074\n",
      "\tspeed: 0.0388s/iter; left time: 327.7421s\n",
      "\titers: 200, epoch: 6 | loss: 0.1259873\n",
      "\tspeed: 0.0114s/iter; left time: 95.4108s\n",
      "\titers: 300, epoch: 6 | loss: 0.0823032\n",
      "\tspeed: 0.0114s/iter; left time: 94.2778s\n",
      "\titers: 400, epoch: 6 | loss: 0.0880875\n",
      "\tspeed: 0.0114s/iter; left time: 93.1121s\n",
      "\titers: 500, epoch: 6 | loss: 0.0839217\n",
      "\tspeed: 0.0114s/iter; left time: 91.8189s\n",
      "Epoch: 6 cost time: 6.820871353149414\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1108379 Vali Loss: 0.0329414 Test Loss: 0.1084211\n",
      "Validation loss decreased (0.033227 --> 0.032941).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0801231\n",
      "\tspeed: 0.0377s/iter; left time: 296.7297s\n",
      "\titers: 200, epoch: 7 | loss: 0.1289176\n",
      "\tspeed: 0.0114s/iter; left time: 89.0453s\n",
      "\titers: 300, epoch: 7 | loss: 0.0806299\n",
      "\tspeed: 0.0114s/iter; left time: 87.7427s\n",
      "\titers: 400, epoch: 7 | loss: 0.0882797\n",
      "\tspeed: 0.0114s/iter; left time: 86.5623s\n",
      "\titers: 500, epoch: 7 | loss: 0.0941452\n",
      "\tspeed: 0.0114s/iter; left time: 85.4240s\n",
      "Epoch: 7 cost time: 6.812136888504028\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1095375 Vali Loss: 0.0327797 Test Loss: 0.1082623\n",
      "Validation loss decreased (0.032941 --> 0.032780).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0955910\n",
      "\tspeed: 0.0388s/iter; left time: 283.3729s\n",
      "\titers: 200, epoch: 8 | loss: 0.1279755\n",
      "\tspeed: 0.0114s/iter; left time: 82.4946s\n",
      "\titers: 300, epoch: 8 | loss: 0.0893521\n",
      "\tspeed: 0.0114s/iter; left time: 81.2943s\n",
      "\titers: 400, epoch: 8 | loss: 0.1013135\n",
      "\tspeed: 0.0114s/iter; left time: 80.1954s\n",
      "\titers: 500, epoch: 8 | loss: 0.1093070\n",
      "\tspeed: 0.0114s/iter; left time: 79.0467s\n",
      "Epoch: 8 cost time: 6.83205771446228\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1093392 Vali Loss: 0.0331353 Test Loss: 0.1088808\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1575224\n",
      "\tspeed: 0.0371s/iter; left time: 249.8831s\n",
      "\titers: 200, epoch: 9 | loss: 0.0802651\n",
      "\tspeed: 0.0114s/iter; left time: 76.0085s\n",
      "\titers: 300, epoch: 9 | loss: 0.1117401\n",
      "\tspeed: 0.0115s/iter; left time: 74.9695s\n",
      "\titers: 400, epoch: 9 | loss: 0.1396321\n",
      "\tspeed: 0.0115s/iter; left time: 73.8239s\n",
      "\titers: 500, epoch: 9 | loss: 0.1535444\n",
      "\tspeed: 0.0115s/iter; left time: 72.6532s\n",
      "Epoch: 9 cost time: 6.818120002746582\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1096551 Vali Loss: 0.0328618 Test Loss: 0.1087080\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1076999\n",
      "\tspeed: 0.0372s/iter; left time: 229.8381s\n",
      "\titers: 200, epoch: 10 | loss: 0.0950411\n",
      "\tspeed: 0.0115s/iter; left time: 69.5969s\n",
      "\titers: 300, epoch: 10 | loss: 0.0886016\n",
      "\tspeed: 0.0115s/iter; left time: 68.4630s\n",
      "\titers: 400, epoch: 10 | loss: 0.1124933\n",
      "\tspeed: 0.0115s/iter; left time: 67.2698s\n",
      "\titers: 500, epoch: 10 | loss: 0.1248781\n",
      "\tspeed: 0.0115s/iter; left time: 66.1077s\n",
      "Epoch: 10 cost time: 6.834472417831421\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1088378 Vali Loss: 0.0329134 Test Loss: 0.1087530\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10842026770114899, mae:0.19971726834774017\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2666608\n",
      "\tspeed: 0.0236s/iter; left time: 266.2809s\n",
      "\titers: 200, epoch: 1 | loss: 0.2344479\n",
      "\tspeed: 0.0109s/iter; left time: 121.9217s\n",
      "\titers: 300, epoch: 1 | loss: 0.2681766\n",
      "\tspeed: 0.0108s/iter; left time: 120.4185s\n",
      "\titers: 400, epoch: 1 | loss: 0.1758684\n",
      "\tspeed: 0.0108s/iter; left time: 119.3052s\n",
      "\titers: 500, epoch: 1 | loss: 0.1921344\n",
      "\tspeed: 0.0109s/iter; left time: 118.4869s\n",
      "Epoch: 1 cost time: 7.511661767959595\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2114521 Vali Loss: 0.0422289 Test Loss: 0.1334401\n",
      "Validation loss decreased (inf --> 0.042229).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2222213\n",
      "\tspeed: 0.0384s/iter; left time: 411.9950s\n",
      "\titers: 200, epoch: 2 | loss: 0.2736513\n",
      "\tspeed: 0.0109s/iter; left time: 115.8333s\n",
      "\titers: 300, epoch: 2 | loss: 0.1277289\n",
      "\tspeed: 0.0109s/iter; left time: 114.6102s\n",
      "\titers: 400, epoch: 2 | loss: 0.1396603\n",
      "\tspeed: 0.0109s/iter; left time: 114.0300s\n",
      "\titers: 500, epoch: 2 | loss: 0.1350700\n",
      "\tspeed: 0.0110s/iter; left time: 113.2392s\n",
      "Epoch: 2 cost time: 6.53412938117981\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1713502 Vali Loss: 0.0425691 Test Loss: 0.1256566\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1520723\n",
      "\tspeed: 0.0380s/iter; left time: 385.9178s\n",
      "\titers: 200, epoch: 3 | loss: 0.1291768\n",
      "\tspeed: 0.0109s/iter; left time: 109.4658s\n",
      "\titers: 300, epoch: 3 | loss: 0.1434106\n",
      "\tspeed: 0.0109s/iter; left time: 108.5722s\n",
      "\titers: 400, epoch: 3 | loss: 0.1218131\n",
      "\tspeed: 0.0109s/iter; left time: 107.5853s\n",
      "\titers: 500, epoch: 3 | loss: 0.1572614\n",
      "\tspeed: 0.0109s/iter; left time: 106.2815s\n",
      "Epoch: 3 cost time: 6.464886665344238\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1472495 Vali Loss: 0.0413527 Test Loss: 0.1271744\n",
      "Validation loss decreased (0.042229 --> 0.041353).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1284194\n",
      "\tspeed: 0.0372s/iter; left time: 356.4136s\n",
      "\titers: 200, epoch: 4 | loss: 0.1104371\n",
      "\tspeed: 0.0109s/iter; left time: 103.8586s\n",
      "\titers: 300, epoch: 4 | loss: 0.1248381\n",
      "\tspeed: 0.0109s/iter; left time: 102.6541s\n",
      "\titers: 400, epoch: 4 | loss: 0.2190025\n",
      "\tspeed: 0.0109s/iter; left time: 101.5415s\n",
      "\titers: 500, epoch: 4 | loss: 0.1397098\n",
      "\tspeed: 0.0109s/iter; left time: 100.2684s\n",
      "Epoch: 4 cost time: 6.520285367965698\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1364210 Vali Loss: 0.0399190 Test Loss: 0.1251965\n",
      "Validation loss decreased (0.041353 --> 0.039919).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1347206\n",
      "\tspeed: 0.0372s/iter; left time: 335.5799s\n",
      "\titers: 200, epoch: 5 | loss: 0.1331736\n",
      "\tspeed: 0.0109s/iter; left time: 97.2130s\n",
      "\titers: 300, epoch: 5 | loss: 0.1527700\n",
      "\tspeed: 0.0110s/iter; left time: 96.5999s\n",
      "\titers: 400, epoch: 5 | loss: 0.1086815\n",
      "\tspeed: 0.0109s/iter; left time: 95.1575s\n",
      "\titers: 500, epoch: 5 | loss: 0.2206286\n",
      "\tspeed: 0.0109s/iter; left time: 94.0552s\n",
      "Epoch: 5 cost time: 6.496492624282837\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1303347 Vali Loss: 0.0386281 Test Loss: 0.1224689\n",
      "Validation loss decreased (0.039919 --> 0.038628).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1115298\n",
      "\tspeed: 0.0366s/iter; left time: 309.6832s\n",
      "\titers: 200, epoch: 6 | loss: 0.1223001\n",
      "\tspeed: 0.0109s/iter; left time: 90.8880s\n",
      "\titers: 300, epoch: 6 | loss: 0.1253359\n",
      "\tspeed: 0.0109s/iter; left time: 89.7875s\n",
      "\titers: 400, epoch: 6 | loss: 0.1120289\n",
      "\tspeed: 0.0109s/iter; left time: 88.7512s\n",
      "\titers: 500, epoch: 6 | loss: 0.1296152\n",
      "\tspeed: 0.0109s/iter; left time: 87.5903s\n",
      "Epoch: 6 cost time: 6.478749513626099\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1268731 Vali Loss: 0.0391719 Test Loss: 0.1254759\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1168155\n",
      "\tspeed: 0.0360s/iter; left time: 283.8929s\n",
      "\titers: 200, epoch: 7 | loss: 0.1033156\n",
      "\tspeed: 0.0109s/iter; left time: 85.0183s\n",
      "\titers: 300, epoch: 7 | loss: 0.0881221\n",
      "\tspeed: 0.0109s/iter; left time: 83.7956s\n",
      "\titers: 400, epoch: 7 | loss: 0.1157171\n",
      "\tspeed: 0.0109s/iter; left time: 82.7540s\n",
      "\titers: 500, epoch: 7 | loss: 0.0893376\n",
      "\tspeed: 0.0109s/iter; left time: 81.7387s\n",
      "Epoch: 7 cost time: 6.493412017822266\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1247528 Vali Loss: 0.0385924 Test Loss: 0.1243976\n",
      "Validation loss decreased (0.038628 --> 0.038592).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0976155\n",
      "\tspeed: 0.0386s/iter; left time: 282.0587s\n",
      "\titers: 200, epoch: 8 | loss: 0.1807430\n",
      "\tspeed: 0.0121s/iter; left time: 87.4223s\n",
      "\titers: 300, epoch: 8 | loss: 0.1563657\n",
      "\tspeed: 0.0121s/iter; left time: 85.9442s\n",
      "\titers: 400, epoch: 8 | loss: 0.1276036\n",
      "\tspeed: 0.0121s/iter; left time: 84.5397s\n",
      "\titers: 500, epoch: 8 | loss: 0.0864860\n",
      "\tspeed: 0.0121s/iter; left time: 83.6081s\n",
      "Epoch: 8 cost time: 7.21411657333374\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1243899 Vali Loss: 0.0384668 Test Loss: 0.1241978\n",
      "Validation loss decreased (0.038592 --> 0.038467).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1099533\n",
      "\tspeed: 0.0390s/iter; left time: 262.6372s\n",
      "\titers: 200, epoch: 9 | loss: 0.0833538\n",
      "\tspeed: 0.0109s/iter; left time: 72.6393s\n",
      "\titers: 300, epoch: 9 | loss: 0.1339110\n",
      "\tspeed: 0.0109s/iter; left time: 71.5047s\n",
      "\titers: 400, epoch: 9 | loss: 0.1569254\n",
      "\tspeed: 0.0109s/iter; left time: 70.3754s\n",
      "\titers: 500, epoch: 9 | loss: 0.0781038\n",
      "\tspeed: 0.0109s/iter; left time: 69.2540s\n",
      "Epoch: 9 cost time: 6.544881820678711\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1230334 Vali Loss: 0.0385081 Test Loss: 0.1237361\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1158035\n",
      "\tspeed: 0.0366s/iter; left time: 225.6543s\n",
      "\titers: 200, epoch: 10 | loss: 0.0832513\n",
      "\tspeed: 0.0109s/iter; left time: 66.1736s\n",
      "\titers: 300, epoch: 10 | loss: 0.1323909\n",
      "\tspeed: 0.0109s/iter; left time: 64.8529s\n",
      "\titers: 400, epoch: 10 | loss: 0.0938356\n",
      "\tspeed: 0.0108s/iter; left time: 63.5338s\n",
      "\titers: 500, epoch: 10 | loss: 0.1200961\n",
      "\tspeed: 0.0109s/iter; left time: 62.6710s\n",
      "Epoch: 10 cost time: 6.487130880355835\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1232119 Vali Loss: 0.0384464 Test Loss: 0.1238844\n",
      "Validation loss decreased (0.038467 --> 0.038446).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0880294\n",
      "\tspeed: 0.0398s/iter; left time: 222.8606s\n",
      "\titers: 200, epoch: 11 | loss: 0.1333495\n",
      "\tspeed: 0.0109s/iter; left time: 59.8149s\n",
      "\titers: 300, epoch: 11 | loss: 0.1132784\n",
      "\tspeed: 0.0109s/iter; left time: 58.6916s\n",
      "\titers: 400, epoch: 11 | loss: 0.0909003\n",
      "\tspeed: 0.0109s/iter; left time: 57.6113s\n",
      "\titers: 500, epoch: 11 | loss: 0.0962447\n",
      "\tspeed: 0.0109s/iter; left time: 56.4939s\n",
      "Epoch: 11 cost time: 6.495711803436279\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1235934 Vali Loss: 0.0384384 Test Loss: 0.1238350\n",
      "Validation loss decreased (0.038446 --> 0.038438).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1400836\n",
      "\tspeed: 0.0379s/iter; left time: 190.7318s\n",
      "\titers: 200, epoch: 12 | loss: 0.1498244\n",
      "\tspeed: 0.0109s/iter; left time: 53.5342s\n",
      "\titers: 300, epoch: 12 | loss: 0.1078468\n",
      "\tspeed: 0.0108s/iter; left time: 52.2638s\n",
      "\titers: 400, epoch: 12 | loss: 0.1485666\n",
      "\tspeed: 0.0108s/iter; left time: 51.2245s\n",
      "\titers: 500, epoch: 12 | loss: 0.1239595\n",
      "\tspeed: 0.0108s/iter; left time: 50.2232s\n",
      "Epoch: 12 cost time: 6.483982563018799\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1226309 Vali Loss: 0.0384481 Test Loss: 0.1238305\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.0807109\n",
      "\tspeed: 0.0390s/iter; left time: 174.1732s\n",
      "\titers: 200, epoch: 13 | loss: 0.1681882\n",
      "\tspeed: 0.0109s/iter; left time: 47.6644s\n",
      "\titers: 300, epoch: 13 | loss: 0.0886898\n",
      "\tspeed: 0.0109s/iter; left time: 46.4852s\n",
      "\titers: 400, epoch: 13 | loss: 0.0915739\n",
      "\tspeed: 0.0109s/iter; left time: 45.3911s\n",
      "\titers: 500, epoch: 13 | loss: 0.1158995\n",
      "\tspeed: 0.0109s/iter; left time: 44.2437s\n",
      "Epoch: 13 cost time: 6.542429208755493\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1236351 Vali Loss: 0.0384767 Test Loss: 0.1238470\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1881920\n",
      "\tspeed: 0.0380s/iter; left time: 147.8622s\n",
      "\titers: 200, epoch: 14 | loss: 0.1902320\n",
      "\tspeed: 0.0109s/iter; left time: 41.1333s\n",
      "\titers: 300, epoch: 14 | loss: 0.1145264\n",
      "\tspeed: 0.0108s/iter; left time: 40.0392s\n",
      "\titers: 400, epoch: 14 | loss: 0.1251795\n",
      "\tspeed: 0.0108s/iter; left time: 38.9265s\n",
      "\titers: 500, epoch: 14 | loss: 0.1174224\n",
      "\tspeed: 0.0108s/iter; left time: 37.8431s\n",
      "Epoch: 14 cost time: 6.457972288131714\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1234293 Vali Loss: 0.0384395 Test Loss: 0.1238526\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1240076944231987, mae:0.22189994156360626\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2047569\n",
      "\tspeed: 0.0264s/iter; left time: 297.9763s\n",
      "\titers: 200, epoch: 1 | loss: 0.1721311\n",
      "\tspeed: 0.0145s/iter; left time: 162.3853s\n",
      "\titers: 300, epoch: 1 | loss: 0.2918872\n",
      "\tspeed: 0.0145s/iter; left time: 160.7487s\n",
      "\titers: 400, epoch: 1 | loss: 0.1840911\n",
      "\tspeed: 0.0145s/iter; left time: 159.6042s\n",
      "\titers: 500, epoch: 1 | loss: 0.1514521\n",
      "\tspeed: 0.0145s/iter; left time: 158.4163s\n",
      "Epoch: 1 cost time: 9.50637698173523\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2111790 Vali Loss: 0.0435493 Test Loss: 0.1358985\n",
      "Validation loss decreased (inf --> 0.043549).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2041141\n",
      "\tspeed: 0.0473s/iter; left time: 507.9645s\n",
      "\titers: 200, epoch: 2 | loss: 0.1462274\n",
      "\tspeed: 0.0146s/iter; left time: 155.2287s\n",
      "\titers: 300, epoch: 2 | loss: 0.1372207\n",
      "\tspeed: 0.0146s/iter; left time: 153.4718s\n",
      "\titers: 400, epoch: 2 | loss: 0.1192545\n",
      "\tspeed: 0.0146s/iter; left time: 152.1580s\n",
      "\titers: 500, epoch: 2 | loss: 0.1763059\n",
      "\tspeed: 0.0146s/iter; left time: 150.3726s\n",
      "Epoch: 2 cost time: 8.608175039291382\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1698193 Vali Loss: 0.0397341 Test Loss: 0.1301434\n",
      "Validation loss decreased (0.043549 --> 0.039734).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2607693\n",
      "\tspeed: 0.0472s/iter; left time: 479.2085s\n",
      "\titers: 200, epoch: 3 | loss: 0.1628471\n",
      "\tspeed: 0.0144s/iter; left time: 144.5917s\n",
      "\titers: 300, epoch: 3 | loss: 0.1069581\n",
      "\tspeed: 0.0144s/iter; left time: 143.5057s\n",
      "\titers: 400, epoch: 3 | loss: 0.1171424\n",
      "\tspeed: 0.0144s/iter; left time: 142.4686s\n",
      "\titers: 500, epoch: 3 | loss: 0.1060702\n",
      "\tspeed: 0.0144s/iter; left time: 140.9300s\n",
      "Epoch: 3 cost time: 8.492679834365845\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1490578 Vali Loss: 0.0359175 Test Loss: 0.1201296\n",
      "Validation loss decreased (0.039734 --> 0.035917).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1515058\n",
      "\tspeed: 0.0465s/iter; left time: 445.9076s\n",
      "\titers: 200, epoch: 4 | loss: 0.1164116\n",
      "\tspeed: 0.0144s/iter; left time: 136.5197s\n",
      "\titers: 300, epoch: 4 | loss: 0.1514658\n",
      "\tspeed: 0.0144s/iter; left time: 135.0818s\n",
      "\titers: 400, epoch: 4 | loss: 0.1478230\n",
      "\tspeed: 0.0144s/iter; left time: 133.6885s\n",
      "\titers: 500, epoch: 4 | loss: 0.1313840\n",
      "\tspeed: 0.0144s/iter; left time: 132.1854s\n",
      "Epoch: 4 cost time: 8.490906238555908\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1398823 Vali Loss: 0.0353378 Test Loss: 0.1230004\n",
      "Validation loss decreased (0.035917 --> 0.035338).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1263737\n",
      "\tspeed: 0.0474s/iter; left time: 428.0292s\n",
      "\titers: 200, epoch: 5 | loss: 0.1141983\n",
      "\tspeed: 0.0144s/iter; left time: 128.6208s\n",
      "\titers: 300, epoch: 5 | loss: 0.1274197\n",
      "\tspeed: 0.0144s/iter; left time: 127.3378s\n",
      "\titers: 400, epoch: 5 | loss: 0.1358247\n",
      "\tspeed: 0.0144s/iter; left time: 125.7287s\n",
      "\titers: 500, epoch: 5 | loss: 0.1271198\n",
      "\tspeed: 0.0144s/iter; left time: 124.3361s\n",
      "Epoch: 5 cost time: 8.499338865280151\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1352223 Vali Loss: 0.0348664 Test Loss: 0.1199729\n",
      "Validation loss decreased (0.035338 --> 0.034866).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0976155\n",
      "\tspeed: 0.0457s/iter; left time: 386.1819s\n",
      "\titers: 200, epoch: 6 | loss: 0.1496920\n",
      "\tspeed: 0.0145s/iter; left time: 120.8954s\n",
      "\titers: 300, epoch: 6 | loss: 0.1000441\n",
      "\tspeed: 0.0145s/iter; left time: 119.4416s\n",
      "\titers: 400, epoch: 6 | loss: 0.0984927\n",
      "\tspeed: 0.0144s/iter; left time: 117.3288s\n",
      "\titers: 500, epoch: 6 | loss: 0.1439775\n",
      "\tspeed: 0.0144s/iter; left time: 115.5399s\n",
      "Epoch: 6 cost time: 8.520895957946777\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1312461 Vali Loss: 0.0346095 Test Loss: 0.1215637\n",
      "Validation loss decreased (0.034866 --> 0.034610).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1307121\n",
      "\tspeed: 0.0466s/iter; left time: 367.5101s\n",
      "\titers: 200, epoch: 7 | loss: 0.1020568\n",
      "\tspeed: 0.0144s/iter; left time: 112.2583s\n",
      "\titers: 300, epoch: 7 | loss: 0.1043422\n",
      "\tspeed: 0.0144s/iter; left time: 110.9189s\n",
      "\titers: 400, epoch: 7 | loss: 0.1314693\n",
      "\tspeed: 0.0144s/iter; left time: 109.2964s\n",
      "\titers: 500, epoch: 7 | loss: 0.2119968\n",
      "\tspeed: 0.0145s/iter; left time: 108.1697s\n",
      "Epoch: 7 cost time: 8.493776559829712\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1305236 Vali Loss: 0.0346760 Test Loss: 0.1211119\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1867866\n",
      "\tspeed: 0.0452s/iter; left time: 330.4904s\n",
      "\titers: 200, epoch: 8 | loss: 0.1295784\n",
      "\tspeed: 0.0145s/iter; left time: 104.7619s\n",
      "\titers: 300, epoch: 8 | loss: 0.1420640\n",
      "\tspeed: 0.0145s/iter; left time: 103.1691s\n",
      "\titers: 400, epoch: 8 | loss: 0.0948894\n",
      "\tspeed: 0.0145s/iter; left time: 101.6321s\n",
      "\titers: 500, epoch: 8 | loss: 0.1524995\n",
      "\tspeed: 0.0145s/iter; left time: 100.1891s\n",
      "Epoch: 8 cost time: 8.522740840911865\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1304179 Vali Loss: 0.0348385 Test Loss: 0.1214825\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1644307\n",
      "\tspeed: 0.0473s/iter; left time: 318.7408s\n",
      "\titers: 200, epoch: 9 | loss: 0.1492928\n",
      "\tspeed: 0.0162s/iter; left time: 107.7666s\n",
      "\titers: 300, epoch: 9 | loss: 0.1900343\n",
      "\tspeed: 0.0163s/iter; left time: 106.3266s\n",
      "\titers: 400, epoch: 9 | loss: 0.1507646\n",
      "\tspeed: 0.0162s/iter; left time: 104.5254s\n",
      "\titers: 500, epoch: 9 | loss: 0.1052462\n",
      "\tspeed: 0.0162s/iter; left time: 103.0355s\n",
      "Epoch: 9 cost time: 9.512165069580078\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1295061 Vali Loss: 0.0347421 Test Loss: 0.1210433\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12174582481384277, mae:0.21724450588226318\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2714466\n",
      "\tspeed: 0.0277s/iter; left time: 312.4901s\n",
      "\titers: 200, epoch: 1 | loss: 0.1833450\n",
      "\tspeed: 0.0162s/iter; left time: 181.8854s\n",
      "\titers: 300, epoch: 1 | loss: 0.1116743\n",
      "\tspeed: 0.0162s/iter; left time: 180.1014s\n",
      "\titers: 400, epoch: 1 | loss: 0.2457210\n",
      "\tspeed: 0.0163s/iter; left time: 178.7760s\n",
      "\titers: 500, epoch: 1 | loss: 0.1758959\n",
      "\tspeed: 0.0162s/iter; left time: 177.0983s\n",
      "Epoch: 1 cost time: 10.447898626327515\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1832130 Vali Loss: 0.0354777 Test Loss: 0.1102525\n",
      "Validation loss decreased (inf --> 0.035478).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1255521\n",
      "\tspeed: 0.0486s/iter; left time: 521.4476s\n",
      "\titers: 200, epoch: 2 | loss: 0.2327390\n",
      "\tspeed: 0.0160s/iter; left time: 169.5655s\n",
      "\titers: 300, epoch: 2 | loss: 0.1772514\n",
      "\tspeed: 0.0153s/iter; left time: 161.0476s\n",
      "\titers: 400, epoch: 2 | loss: 0.1205304\n",
      "\tspeed: 0.0142s/iter; left time: 147.8592s\n",
      "\titers: 500, epoch: 2 | loss: 0.1233689\n",
      "\tspeed: 0.0142s/iter; left time: 146.4762s\n",
      "Epoch: 2 cost time: 8.848949193954468\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1506715 Vali Loss: 0.0339201 Test Loss: 0.1126614\n",
      "Validation loss decreased (0.035478 --> 0.033920).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1473355\n",
      "\tspeed: 0.0470s/iter; left time: 477.2603s\n",
      "\titers: 200, epoch: 3 | loss: 0.0864547\n",
      "\tspeed: 0.0142s/iter; left time: 142.6469s\n",
      "\titers: 300, epoch: 3 | loss: 0.1745018\n",
      "\tspeed: 0.0142s/iter; left time: 141.4723s\n",
      "\titers: 400, epoch: 3 | loss: 0.1090745\n",
      "\tspeed: 0.0142s/iter; left time: 139.8246s\n",
      "\titers: 500, epoch: 3 | loss: 0.1212042\n",
      "\tspeed: 0.0142s/iter; left time: 138.6447s\n",
      "Epoch: 3 cost time: 8.357187747955322\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1344462 Vali Loss: 0.0329960 Test Loss: 0.1109708\n",
      "Validation loss decreased (0.033920 --> 0.032996).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1262608\n",
      "\tspeed: 0.0470s/iter; left time: 450.6967s\n",
      "\titers: 200, epoch: 4 | loss: 0.1213391\n",
      "\tspeed: 0.0141s/iter; left time: 134.0097s\n",
      "\titers: 300, epoch: 4 | loss: 0.1373242\n",
      "\tspeed: 0.0141s/iter; left time: 132.4513s\n",
      "\titers: 400, epoch: 4 | loss: 0.0776137\n",
      "\tspeed: 0.0141s/iter; left time: 131.0822s\n",
      "\titers: 500, epoch: 4 | loss: 0.0916152\n",
      "\tspeed: 0.0141s/iter; left time: 129.8459s\n",
      "Epoch: 4 cost time: 8.360148191452026\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1285917 Vali Loss: 0.0319500 Test Loss: 0.1105838\n",
      "Validation loss decreased (0.032996 --> 0.031950).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0646266\n",
      "\tspeed: 0.0450s/iter; left time: 406.1411s\n",
      "\titers: 200, epoch: 5 | loss: 0.1230586\n",
      "\tspeed: 0.0141s/iter; left time: 126.0475s\n",
      "\titers: 300, epoch: 5 | loss: 0.1696747\n",
      "\tspeed: 0.0141s/iter; left time: 124.6139s\n",
      "\titers: 400, epoch: 5 | loss: 0.1049808\n",
      "\tspeed: 0.0141s/iter; left time: 123.1180s\n",
      "\titers: 500, epoch: 5 | loss: 0.1418797\n",
      "\tspeed: 0.0141s/iter; left time: 121.8379s\n",
      "Epoch: 5 cost time: 8.327671527862549\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1240557 Vali Loss: 0.0317498 Test Loss: 0.1098400\n",
      "Validation loss decreased (0.031950 --> 0.031750).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1115341\n",
      "\tspeed: 0.0447s/iter; left time: 377.3970s\n",
      "\titers: 200, epoch: 6 | loss: 0.2230012\n",
      "\tspeed: 0.0142s/iter; left time: 118.5680s\n",
      "\titers: 300, epoch: 6 | loss: 0.1876464\n",
      "\tspeed: 0.0142s/iter; left time: 117.1932s\n",
      "\titers: 400, epoch: 6 | loss: 0.1356148\n",
      "\tspeed: 0.0142s/iter; left time: 115.7307s\n",
      "\titers: 500, epoch: 6 | loss: 0.1370379\n",
      "\tspeed: 0.0142s/iter; left time: 114.3048s\n",
      "Epoch: 6 cost time: 8.386096715927124\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1222151 Vali Loss: 0.0315075 Test Loss: 0.1099989\n",
      "Validation loss decreased (0.031750 --> 0.031507).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0917359\n",
      "\tspeed: 0.0465s/iter; left time: 366.6627s\n",
      "\titers: 200, epoch: 7 | loss: 0.1327952\n",
      "\tspeed: 0.0142s/iter; left time: 110.3167s\n",
      "\titers: 300, epoch: 7 | loss: 0.1693450\n",
      "\tspeed: 0.0142s/iter; left time: 108.7969s\n",
      "\titers: 400, epoch: 7 | loss: 0.1225072\n",
      "\tspeed: 0.0142s/iter; left time: 107.3446s\n",
      "\titers: 500, epoch: 7 | loss: 0.1464634\n",
      "\tspeed: 0.0142s/iter; left time: 105.9093s\n",
      "Epoch: 7 cost time: 8.349441528320312\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1204937 Vali Loss: 0.0312481 Test Loss: 0.1096683\n",
      "Validation loss decreased (0.031507 --> 0.031248).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0855149\n",
      "\tspeed: 0.0446s/iter; left time: 325.7354s\n",
      "\titers: 200, epoch: 8 | loss: 0.1063012\n",
      "\tspeed: 0.0141s/iter; left time: 101.3760s\n",
      "\titers: 300, epoch: 8 | loss: 0.1573913\n",
      "\tspeed: 0.0141s/iter; left time: 99.9763s\n",
      "\titers: 400, epoch: 8 | loss: 0.1176097\n",
      "\tspeed: 0.0141s/iter; left time: 98.5935s\n",
      "\titers: 500, epoch: 8 | loss: 0.1097314\n",
      "\tspeed: 0.0141s/iter; left time: 97.4194s\n",
      "Epoch: 8 cost time: 8.293718338012695\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1196738 Vali Loss: 0.0313694 Test Loss: 0.1097663\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0974496\n",
      "\tspeed: 0.0479s/iter; left time: 322.7501s\n",
      "\titers: 200, epoch: 9 | loss: 0.1121854\n",
      "\tspeed: 0.0141s/iter; left time: 93.5899s\n",
      "\titers: 300, epoch: 9 | loss: 0.1125480\n",
      "\tspeed: 0.0141s/iter; left time: 92.2949s\n",
      "\titers: 400, epoch: 9 | loss: 0.0968294\n",
      "\tspeed: 0.0142s/iter; left time: 91.5283s\n",
      "\titers: 500, epoch: 9 | loss: 0.1291035\n",
      "\tspeed: 0.0142s/iter; left time: 89.9178s\n",
      "Epoch: 9 cost time: 8.334866046905518\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1191862 Vali Loss: 0.0312849 Test Loss: 0.1098480\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1136008\n",
      "\tspeed: 0.0472s/iter; left time: 291.0735s\n",
      "\titers: 200, epoch: 10 | loss: 0.1118148\n",
      "\tspeed: 0.0140s/iter; left time: 85.1259s\n",
      "\titers: 300, epoch: 10 | loss: 0.1124306\n",
      "\tspeed: 0.0140s/iter; left time: 83.8144s\n",
      "\titers: 400, epoch: 10 | loss: 0.1297452\n",
      "\tspeed: 0.0140s/iter; left time: 82.3110s\n",
      "\titers: 500, epoch: 10 | loss: 0.1475537\n",
      "\tspeed: 0.0140s/iter; left time: 80.9695s\n",
      "Epoch: 10 cost time: 8.295629739761353\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1196228 Vali Loss: 0.0313251 Test Loss: 0.1098910\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10982311517000198, mae:0.20000316202640533\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1340132\n",
      "\tspeed: 0.0262s/iter; left time: 296.2761s\n",
      "\titers: 200, epoch: 1 | loss: 0.2731330\n",
      "\tspeed: 0.0142s/iter; left time: 158.7000s\n",
      "\titers: 300, epoch: 1 | loss: 0.2700988\n",
      "\tspeed: 0.0141s/iter; left time: 156.9354s\n",
      "\titers: 400, epoch: 1 | loss: 0.2357663\n",
      "\tspeed: 0.0142s/iter; left time: 156.1861s\n",
      "\titers: 500, epoch: 1 | loss: 0.2441259\n",
      "\tspeed: 0.0142s/iter; left time: 154.7633s\n",
      "Epoch: 1 cost time: 9.335127830505371\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2175356 Vali Loss: 0.0434821 Test Loss: 0.1324463\n",
      "Validation loss decreased (inf --> 0.043482).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1540688\n",
      "\tspeed: 0.0465s/iter; left time: 498.5301s\n",
      "\titers: 200, epoch: 2 | loss: 0.1154824\n",
      "\tspeed: 0.0144s/iter; left time: 153.0561s\n",
      "\titers: 300, epoch: 2 | loss: 0.1036720\n",
      "\tspeed: 0.0143s/iter; left time: 151.1037s\n",
      "\titers: 400, epoch: 2 | loss: 0.1394471\n",
      "\tspeed: 0.0144s/iter; left time: 149.9447s\n",
      "\titers: 500, epoch: 2 | loss: 0.2109354\n",
      "\tspeed: 0.0143s/iter; left time: 148.0245s\n",
      "Epoch: 2 cost time: 8.505199432373047\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1765220 Vali Loss: 0.0419425 Test Loss: 0.1278307\n",
      "Validation loss decreased (0.043482 --> 0.041942).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1866744\n",
      "\tspeed: 0.0497s/iter; left time: 505.0127s\n",
      "\titers: 200, epoch: 3 | loss: 0.1807503\n",
      "\tspeed: 0.0143s/iter; left time: 143.5908s\n",
      "\titers: 300, epoch: 3 | loss: 0.1508223\n",
      "\tspeed: 0.0143s/iter; left time: 142.2383s\n",
      "\titers: 400, epoch: 3 | loss: 0.1283116\n",
      "\tspeed: 0.0143s/iter; left time: 140.9466s\n",
      "\titers: 500, epoch: 3 | loss: 0.1517522\n",
      "\tspeed: 0.0143s/iter; left time: 139.1195s\n",
      "Epoch: 3 cost time: 8.423991203308105\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1552826 Vali Loss: 0.0393618 Test Loss: 0.1237901\n",
      "Validation loss decreased (0.041942 --> 0.039362).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1693730\n",
      "\tspeed: 0.0483s/iter; left time: 463.5939s\n",
      "\titers: 200, epoch: 4 | loss: 0.1441906\n",
      "\tspeed: 0.0161s/iter; left time: 152.3662s\n",
      "\titers: 300, epoch: 4 | loss: 0.1830863\n",
      "\tspeed: 0.0161s/iter; left time: 151.0512s\n",
      "\titers: 400, epoch: 4 | loss: 0.1477285\n",
      "\tspeed: 0.0162s/iter; left time: 150.8341s\n",
      "\titers: 500, epoch: 4 | loss: 0.1046303\n",
      "\tspeed: 0.0161s/iter; left time: 148.0098s\n",
      "Epoch: 4 cost time: 9.47012448310852\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1463910 Vali Loss: 0.0377049 Test Loss: 0.1226720\n",
      "Validation loss decreased (0.039362 --> 0.037705).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1406279\n",
      "\tspeed: 0.0476s/iter; left time: 429.5291s\n",
      "\titers: 200, epoch: 5 | loss: 0.1610852\n",
      "\tspeed: 0.0143s/iter; left time: 127.3588s\n",
      "\titers: 300, epoch: 5 | loss: 0.1367887\n",
      "\tspeed: 0.0143s/iter; left time: 126.0372s\n",
      "\titers: 400, epoch: 5 | loss: 0.1876882\n",
      "\tspeed: 0.0143s/iter; left time: 124.6150s\n",
      "\titers: 500, epoch: 5 | loss: 0.1052222\n",
      "\tspeed: 0.0143s/iter; left time: 123.1503s\n",
      "Epoch: 5 cost time: 8.463184118270874\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1414276 Vali Loss: 0.0380426 Test Loss: 0.1222891\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2116309\n",
      "\tspeed: 0.0478s/iter; left time: 403.6054s\n",
      "\titers: 200, epoch: 6 | loss: 0.1298434\n",
      "\tspeed: 0.0142s/iter; left time: 118.1812s\n",
      "\titers: 300, epoch: 6 | loss: 0.0935109\n",
      "\tspeed: 0.0142s/iter; left time: 116.7539s\n",
      "\titers: 400, epoch: 6 | loss: 0.1730927\n",
      "\tspeed: 0.0142s/iter; left time: 115.3914s\n",
      "\titers: 500, epoch: 6 | loss: 0.1374914\n",
      "\tspeed: 0.0141s/iter; left time: 113.8082s\n",
      "Epoch: 6 cost time: 8.358580589294434\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1391261 Vali Loss: 0.0377288 Test Loss: 0.1226217\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1686033\n",
      "\tspeed: 0.0451s/iter; left time: 355.5178s\n",
      "\titers: 200, epoch: 7 | loss: 0.0903399\n",
      "\tspeed: 0.0142s/iter; left time: 110.4844s\n",
      "\titers: 300, epoch: 7 | loss: 0.1061012\n",
      "\tspeed: 0.0142s/iter; left time: 109.1887s\n",
      "\titers: 400, epoch: 7 | loss: 0.1717959\n",
      "\tspeed: 0.0142s/iter; left time: 107.6472s\n",
      "\titers: 500, epoch: 7 | loss: 0.1215039\n",
      "\tspeed: 0.0142s/iter; left time: 106.2326s\n",
      "Epoch: 7 cost time: 8.388734817504883\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1389024 Vali Loss: 0.0375392 Test Loss: 0.1220629\n",
      "Validation loss decreased (0.037705 --> 0.037539).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1050478\n",
      "\tspeed: 0.0470s/iter; left time: 343.4505s\n",
      "\titers: 200, epoch: 8 | loss: 0.1073643\n",
      "\tspeed: 0.0143s/iter; left time: 102.9595s\n",
      "\titers: 300, epoch: 8 | loss: 0.1274797\n",
      "\tspeed: 0.0143s/iter; left time: 101.3580s\n",
      "\titers: 400, epoch: 8 | loss: 0.1463042\n",
      "\tspeed: 0.0142s/iter; left time: 99.8670s\n",
      "\titers: 500, epoch: 8 | loss: 0.1025157\n",
      "\tspeed: 0.0143s/iter; left time: 98.4887s\n",
      "Epoch: 8 cost time: 8.438206672668457\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1376869 Vali Loss: 0.0380538 Test Loss: 0.1229385\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1774512\n",
      "\tspeed: 0.0471s/iter; left time: 317.3949s\n",
      "\titers: 200, epoch: 9 | loss: 0.1372954\n",
      "\tspeed: 0.0143s/iter; left time: 94.8770s\n",
      "\titers: 300, epoch: 9 | loss: 0.0735341\n",
      "\tspeed: 0.0143s/iter; left time: 93.4591s\n",
      "\titers: 400, epoch: 9 | loss: 0.1188553\n",
      "\tspeed: 0.0143s/iter; left time: 92.0070s\n",
      "\titers: 500, epoch: 9 | loss: 0.1156157\n",
      "\tspeed: 0.0143s/iter; left time: 90.5375s\n",
      "Epoch: 9 cost time: 8.428909063339233\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1363195 Vali Loss: 0.0376753 Test Loss: 0.1227020\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1548845\n",
      "\tspeed: 0.0466s/iter; left time: 287.4612s\n",
      "\titers: 200, epoch: 10 | loss: 0.1635989\n",
      "\tspeed: 0.0142s/iter; left time: 86.4282s\n",
      "\titers: 300, epoch: 10 | loss: 0.1354944\n",
      "\tspeed: 0.0142s/iter; left time: 84.9886s\n",
      "\titers: 400, epoch: 10 | loss: 0.1608072\n",
      "\tspeed: 0.0142s/iter; left time: 83.5375s\n",
      "\titers: 500, epoch: 10 | loss: 0.1307496\n",
      "\tspeed: 0.0142s/iter; left time: 82.1204s\n",
      "Epoch: 10 cost time: 8.50164794921875\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1368901 Vali Loss: 0.0376425 Test Loss: 0.1224218\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12225611507892609, mae:0.21893946826457977\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2642496\n",
      "\tspeed: 0.0287s/iter; left time: 323.8656s\n",
      "\titers: 200, epoch: 1 | loss: 0.1672373\n",
      "\tspeed: 0.0170s/iter; left time: 190.3563s\n",
      "\titers: 300, epoch: 1 | loss: 0.2110314\n",
      "\tspeed: 0.0170s/iter; left time: 188.6129s\n",
      "\titers: 400, epoch: 1 | loss: 0.1752806\n",
      "\tspeed: 0.0170s/iter; left time: 186.7932s\n",
      "\titers: 500, epoch: 1 | loss: 0.1534884\n",
      "\tspeed: 0.0170s/iter; left time: 185.2254s\n",
      "Epoch: 1 cost time: 10.901410579681396\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2048573 Vali Loss: 0.0442407 Test Loss: 0.1334456\n",
      "Validation loss decreased (inf --> 0.044241).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1922308\n",
      "\tspeed: 0.0517s/iter; left time: 554.8256s\n",
      "\titers: 200, epoch: 2 | loss: 0.1989005\n",
      "\tspeed: 0.0170s/iter; left time: 180.3888s\n",
      "\titers: 300, epoch: 2 | loss: 0.1904143\n",
      "\tspeed: 0.0170s/iter; left time: 178.6941s\n",
      "\titers: 400, epoch: 2 | loss: 0.1112681\n",
      "\tspeed: 0.0170s/iter; left time: 176.8145s\n",
      "\titers: 500, epoch: 2 | loss: 0.1280198\n",
      "\tspeed: 0.0169s/iter; left time: 175.0545s\n",
      "Epoch: 2 cost time: 10.01930570602417\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1616140 Vali Loss: 0.0378118 Test Loss: 0.1226735\n",
      "Validation loss decreased (0.044241 --> 0.037812).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1237671\n",
      "\tspeed: 0.0512s/iter; left time: 520.2287s\n",
      "\titers: 200, epoch: 3 | loss: 0.1354339\n",
      "\tspeed: 0.0170s/iter; left time: 170.6324s\n",
      "\titers: 300, epoch: 3 | loss: 0.2374647\n",
      "\tspeed: 0.0170s/iter; left time: 169.1357s\n",
      "\titers: 400, epoch: 3 | loss: 0.1247521\n",
      "\tspeed: 0.0170s/iter; left time: 167.4133s\n",
      "\titers: 500, epoch: 3 | loss: 0.0987477\n",
      "\tspeed: 0.0170s/iter; left time: 165.8069s\n",
      "Epoch: 3 cost time: 9.988650798797607\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1346405 Vali Loss: 0.0368421 Test Loss: 0.1211564\n",
      "Validation loss decreased (0.037812 --> 0.036842).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0937603\n",
      "\tspeed: 0.0536s/iter; left time: 514.0853s\n",
      "\titers: 200, epoch: 4 | loss: 0.1194085\n",
      "\tspeed: 0.0169s/iter; left time: 159.9309s\n",
      "\titers: 300, epoch: 4 | loss: 0.1261603\n",
      "\tspeed: 0.0169s/iter; left time: 158.3203s\n",
      "\titers: 400, epoch: 4 | loss: 0.1470387\n",
      "\tspeed: 0.0168s/iter; left time: 156.3342s\n",
      "\titers: 500, epoch: 4 | loss: 0.1260980\n",
      "\tspeed: 0.0168s/iter; left time: 154.5190s\n",
      "Epoch: 4 cost time: 9.885355949401855\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1197446 Vali Loss: 0.0364672 Test Loss: 0.1205641\n",
      "Validation loss decreased (0.036842 --> 0.036467).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0868400\n",
      "\tspeed: 0.0521s/iter; left time: 470.0196s\n",
      "\titers: 200, epoch: 5 | loss: 0.1205349\n",
      "\tspeed: 0.0169s/iter; left time: 150.4195s\n",
      "\titers: 300, epoch: 5 | loss: 0.1141921\n",
      "\tspeed: 0.0169s/iter; left time: 148.8620s\n",
      "\titers: 400, epoch: 5 | loss: 0.1284914\n",
      "\tspeed: 0.0169s/iter; left time: 146.9789s\n",
      "\titers: 500, epoch: 5 | loss: 0.0882356\n",
      "\tspeed: 0.0169s/iter; left time: 145.4862s\n",
      "Epoch: 5 cost time: 9.91375470161438\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1131571 Vali Loss: 0.0348938 Test Loss: 0.1195340\n",
      "Validation loss decreased (0.036467 --> 0.034894).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1147439\n",
      "\tspeed: 0.0539s/iter; left time: 455.1141s\n",
      "\titers: 200, epoch: 6 | loss: 0.0958291\n",
      "\tspeed: 0.0169s/iter; left time: 140.7462s\n",
      "\titers: 300, epoch: 6 | loss: 0.1061931\n",
      "\tspeed: 0.0169s/iter; left time: 139.1633s\n",
      "\titers: 400, epoch: 6 | loss: 0.1339055\n",
      "\tspeed: 0.0169s/iter; left time: 137.4283s\n",
      "\titers: 500, epoch: 6 | loss: 0.1064150\n",
      "\tspeed: 0.0169s/iter; left time: 135.7335s\n",
      "Epoch: 6 cost time: 9.913726329803467\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1091125 Vali Loss: 0.0349584 Test Loss: 0.1170439\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1292955\n",
      "\tspeed: 0.0521s/iter; left time: 410.3388s\n",
      "\titers: 200, epoch: 7 | loss: 0.1387319\n",
      "\tspeed: 0.0169s/iter; left time: 131.6385s\n",
      "\titers: 300, epoch: 7 | loss: 0.1260076\n",
      "\tspeed: 0.0169s/iter; left time: 130.0636s\n",
      "\titers: 400, epoch: 7 | loss: 0.0798721\n",
      "\tspeed: 0.0169s/iter; left time: 128.2056s\n",
      "\titers: 500, epoch: 7 | loss: 0.0855768\n",
      "\tspeed: 0.0169s/iter; left time: 126.4639s\n",
      "Epoch: 7 cost time: 9.908849477767944\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1072664 Vali Loss: 0.0340650 Test Loss: 0.1183059\n",
      "Validation loss decreased (0.034894 --> 0.034065).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0993857\n",
      "\tspeed: 0.0538s/iter; left time: 393.6178s\n",
      "\titers: 200, epoch: 8 | loss: 0.0821224\n",
      "\tspeed: 0.0184s/iter; left time: 132.9264s\n",
      "\titers: 300, epoch: 8 | loss: 0.1028370\n",
      "\tspeed: 0.0184s/iter; left time: 130.9933s\n",
      "\titers: 400, epoch: 8 | loss: 0.0694930\n",
      "\tspeed: 0.0184s/iter; left time: 129.0314s\n",
      "\titers: 500, epoch: 8 | loss: 0.0738068\n",
      "\tspeed: 0.0184s/iter; left time: 127.1717s\n",
      "Epoch: 8 cost time: 10.813576221466064\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1070141 Vali Loss: 0.0343289 Test Loss: 0.1180926\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0767808\n",
      "\tspeed: 0.0569s/iter; left time: 383.2542s\n",
      "\titers: 200, epoch: 9 | loss: 0.0842962\n",
      "\tspeed: 0.0184s/iter; left time: 122.2378s\n",
      "\titers: 300, epoch: 9 | loss: 0.0946802\n",
      "\tspeed: 0.0184s/iter; left time: 120.5539s\n",
      "\titers: 400, epoch: 9 | loss: 0.0880783\n",
      "\tspeed: 0.0184s/iter; left time: 118.6769s\n",
      "\titers: 500, epoch: 9 | loss: 0.1501595\n",
      "\tspeed: 0.0184s/iter; left time: 116.6712s\n",
      "Epoch: 9 cost time: 10.717708110809326\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1058018 Vali Loss: 0.0342370 Test Loss: 0.1178512\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1148590\n",
      "\tspeed: 0.0507s/iter; left time: 312.7898s\n",
      "\titers: 200, epoch: 10 | loss: 0.0611055\n",
      "\tspeed: 0.0169s/iter; left time: 102.4960s\n",
      "\titers: 300, epoch: 10 | loss: 0.1586422\n",
      "\tspeed: 0.0169s/iter; left time: 100.8191s\n",
      "\titers: 400, epoch: 10 | loss: 0.0999018\n",
      "\tspeed: 0.0169s/iter; left time: 99.0809s\n",
      "\titers: 500, epoch: 10 | loss: 0.0855434\n",
      "\tspeed: 0.0169s/iter; left time: 97.3955s\n",
      "Epoch: 10 cost time: 9.942793369293213\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1060700 Vali Loss: 0.0345414 Test Loss: 0.1175926\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1184559091925621, mae:0.21707218885421753\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1555285\n",
      "\tspeed: 0.0301s/iter; left time: 340.3317s\n",
      "\titers: 200, epoch: 1 | loss: 0.1058938\n",
      "\tspeed: 0.0178s/iter; left time: 199.5219s\n",
      "\titers: 300, epoch: 1 | loss: 0.1976846\n",
      "\tspeed: 0.0178s/iter; left time: 197.7498s\n",
      "\titers: 400, epoch: 1 | loss: 0.1717134\n",
      "\tspeed: 0.0179s/iter; left time: 196.6362s\n",
      "\titers: 500, epoch: 1 | loss: 0.1015578\n",
      "\tspeed: 0.0178s/iter; left time: 194.2650s\n",
      "Epoch: 1 cost time: 11.442708015441895\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1783853 Vali Loss: 0.0342684 Test Loss: 0.1115713\n",
      "Validation loss decreased (inf --> 0.034268).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1322298\n",
      "\tspeed: 0.0534s/iter; left time: 573.0550s\n",
      "\titers: 200, epoch: 2 | loss: 0.1116886\n",
      "\tspeed: 0.0164s/iter; left time: 174.5407s\n",
      "\titers: 300, epoch: 2 | loss: 0.2077729\n",
      "\tspeed: 0.0164s/iter; left time: 172.7728s\n",
      "\titers: 400, epoch: 2 | loss: 0.1784001\n",
      "\tspeed: 0.0164s/iter; left time: 170.8714s\n",
      "\titers: 500, epoch: 2 | loss: 0.1166745\n",
      "\tspeed: 0.0164s/iter; left time: 169.1540s\n",
      "Epoch: 2 cost time: 9.69466257095337\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1438919 Vali Loss: 0.0351572 Test Loss: 0.1138219\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1210392\n",
      "\tspeed: 0.0489s/iter; left time: 497.1605s\n",
      "\titers: 200, epoch: 3 | loss: 0.1237201\n",
      "\tspeed: 0.0163s/iter; left time: 164.1580s\n",
      "\titers: 300, epoch: 3 | loss: 0.1078374\n",
      "\tspeed: 0.0163s/iter; left time: 162.7010s\n",
      "\titers: 400, epoch: 3 | loss: 0.1568393\n",
      "\tspeed: 0.0163s/iter; left time: 160.9920s\n",
      "\titers: 500, epoch: 3 | loss: 0.0859967\n",
      "\tspeed: 0.0163s/iter; left time: 159.1964s\n",
      "Epoch: 3 cost time: 9.593924045562744\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1227649 Vali Loss: 0.0325438 Test Loss: 0.1098657\n",
      "Validation loss decreased (0.034268 --> 0.032544).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1230057\n",
      "\tspeed: 0.0538s/iter; left time: 516.0879s\n",
      "\titers: 200, epoch: 4 | loss: 0.1713229\n",
      "\tspeed: 0.0163s/iter; left time: 154.7821s\n",
      "\titers: 300, epoch: 4 | loss: 0.0925769\n",
      "\tspeed: 0.0163s/iter; left time: 153.3437s\n",
      "\titers: 400, epoch: 4 | loss: 0.0831985\n",
      "\tspeed: 0.0163s/iter; left time: 151.3935s\n",
      "\titers: 500, epoch: 4 | loss: 0.1476100\n",
      "\tspeed: 0.0163s/iter; left time: 149.9752s\n",
      "Epoch: 4 cost time: 9.610456943511963\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1103549 Vali Loss: 0.0322846 Test Loss: 0.1100513\n",
      "Validation loss decreased (0.032544 --> 0.032285).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1093887\n",
      "\tspeed: 0.0501s/iter; left time: 451.7870s\n",
      "\titers: 200, epoch: 5 | loss: 0.1200557\n",
      "\tspeed: 0.0164s/iter; left time: 146.2521s\n",
      "\titers: 300, epoch: 5 | loss: 0.1096876\n",
      "\tspeed: 0.0164s/iter; left time: 144.5394s\n",
      "\titers: 400, epoch: 5 | loss: 0.0723158\n",
      "\tspeed: 0.0164s/iter; left time: 142.9048s\n",
      "\titers: 500, epoch: 5 | loss: 0.1374758\n",
      "\tspeed: 0.0164s/iter; left time: 141.0225s\n",
      "Epoch: 5 cost time: 9.67451000213623\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1053676 Vali Loss: 0.0319508 Test Loss: 0.1103467\n",
      "Validation loss decreased (0.032285 --> 0.031951).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0999631\n",
      "\tspeed: 0.0505s/iter; left time: 426.3947s\n",
      "\titers: 200, epoch: 6 | loss: 0.1338409\n",
      "\tspeed: 0.0164s/iter; left time: 136.8307s\n",
      "\titers: 300, epoch: 6 | loss: 0.1689444\n",
      "\tspeed: 0.0164s/iter; left time: 135.0754s\n",
      "\titers: 400, epoch: 6 | loss: 0.1071086\n",
      "\tspeed: 0.0164s/iter; left time: 133.5143s\n",
      "\titers: 500, epoch: 6 | loss: 0.1048287\n",
      "\tspeed: 0.0164s/iter; left time: 131.9675s\n",
      "Epoch: 6 cost time: 9.667218685150146\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1028331 Vali Loss: 0.0320684 Test Loss: 0.1101003\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1261138\n",
      "\tspeed: 0.0510s/iter; left time: 402.1552s\n",
      "\titers: 200, epoch: 7 | loss: 0.0776426\n",
      "\tspeed: 0.0163s/iter; left time: 126.7873s\n",
      "\titers: 300, epoch: 7 | loss: 0.0916797\n",
      "\tspeed: 0.0163s/iter; left time: 125.2982s\n",
      "\titers: 400, epoch: 7 | loss: 0.1496750\n",
      "\tspeed: 0.0163s/iter; left time: 123.4780s\n",
      "\titers: 500, epoch: 7 | loss: 0.0992560\n",
      "\tspeed: 0.0163s/iter; left time: 121.9269s\n",
      "Epoch: 7 cost time: 9.588154554367065\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1012986 Vali Loss: 0.0322577 Test Loss: 0.1099045\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0672307\n",
      "\tspeed: 0.0490s/iter; left time: 358.0729s\n",
      "\titers: 200, epoch: 8 | loss: 0.1146743\n",
      "\tspeed: 0.0163s/iter; left time: 117.3839s\n",
      "\titers: 300, epoch: 8 | loss: 0.1142103\n",
      "\tspeed: 0.0163s/iter; left time: 115.7145s\n",
      "\titers: 400, epoch: 8 | loss: 0.1174164\n",
      "\tspeed: 0.0163s/iter; left time: 114.2377s\n",
      "\titers: 500, epoch: 8 | loss: 0.0964760\n",
      "\tspeed: 0.0163s/iter; left time: 112.7636s\n",
      "Epoch: 8 cost time: 9.572313785552979\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0999613 Vali Loss: 0.0321173 Test Loss: 0.1101281\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11049826443195343, mae:0.2042028158903122\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2063830\n",
      "\tspeed: 0.0269s/iter; left time: 304.2459s\n",
      "\titers: 200, epoch: 1 | loss: 0.1870340\n",
      "\tspeed: 0.0153s/iter; left time: 171.4860s\n",
      "\titers: 300, epoch: 1 | loss: 0.1797046\n",
      "\tspeed: 0.0153s/iter; left time: 169.6033s\n",
      "\titers: 400, epoch: 1 | loss: 0.1959289\n",
      "\tspeed: 0.0153s/iter; left time: 168.1316s\n",
      "\titers: 500, epoch: 1 | loss: 0.3154422\n",
      "\tspeed: 0.0153s/iter; left time: 166.7182s\n",
      "Epoch: 1 cost time: 9.924736499786377\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2134551 Vali Loss: 0.0435308 Test Loss: 0.1349582\n",
      "Validation loss decreased (inf --> 0.043531).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2284414\n",
      "\tspeed: 0.0486s/iter; left time: 521.1461s\n",
      "\titers: 200, epoch: 2 | loss: 0.1208357\n",
      "\tspeed: 0.0154s/iter; left time: 163.5693s\n",
      "\titers: 300, epoch: 2 | loss: 0.1812132\n",
      "\tspeed: 0.0154s/iter; left time: 161.7981s\n",
      "\titers: 400, epoch: 2 | loss: 0.1583438\n",
      "\tspeed: 0.0154s/iter; left time: 160.5297s\n",
      "\titers: 500, epoch: 2 | loss: 0.2020065\n",
      "\tspeed: 0.0154s/iter; left time: 159.0624s\n",
      "Epoch: 2 cost time: 9.085787773132324\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1656884 Vali Loss: 0.0411948 Test Loss: 0.1304179\n",
      "Validation loss decreased (0.043531 --> 0.041195).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1608097\n",
      "\tspeed: 0.0484s/iter; left time: 491.3647s\n",
      "\titers: 200, epoch: 3 | loss: 0.0874227\n",
      "\tspeed: 0.0153s/iter; left time: 154.4113s\n",
      "\titers: 300, epoch: 3 | loss: 0.1694377\n",
      "\tspeed: 0.0153s/iter; left time: 152.6281s\n",
      "\titers: 400, epoch: 3 | loss: 0.1177407\n",
      "\tspeed: 0.0153s/iter; left time: 151.0804s\n",
      "\titers: 500, epoch: 3 | loss: 0.1368724\n",
      "\tspeed: 0.0153s/iter; left time: 149.5720s\n",
      "Epoch: 3 cost time: 9.055989265441895\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1359576 Vali Loss: 0.0376442 Test Loss: 0.1299606\n",
      "Validation loss decreased (0.041195 --> 0.037644).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0844448\n",
      "\tspeed: 0.0476s/iter; left time: 456.6962s\n",
      "\titers: 200, epoch: 4 | loss: 0.1618553\n",
      "\tspeed: 0.0153s/iter; left time: 145.0323s\n",
      "\titers: 300, epoch: 4 | loss: 0.1517792\n",
      "\tspeed: 0.0153s/iter; left time: 143.5383s\n",
      "\titers: 400, epoch: 4 | loss: 0.1055960\n",
      "\tspeed: 0.0153s/iter; left time: 141.9326s\n",
      "\titers: 500, epoch: 4 | loss: 0.1550536\n",
      "\tspeed: 0.0153s/iter; left time: 140.4308s\n",
      "Epoch: 4 cost time: 8.985569953918457\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1223496 Vali Loss: 0.0364399 Test Loss: 0.1311539\n",
      "Validation loss decreased (0.037644 --> 0.036440).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1185952\n",
      "\tspeed: 0.0491s/iter; left time: 442.8820s\n",
      "\titers: 200, epoch: 5 | loss: 0.0921470\n",
      "\tspeed: 0.0170s/iter; left time: 151.5387s\n",
      "\titers: 300, epoch: 5 | loss: 0.0994880\n",
      "\tspeed: 0.0166s/iter; left time: 146.8342s\n",
      "\titers: 400, epoch: 5 | loss: 0.0897295\n",
      "\tspeed: 0.0154s/iter; left time: 134.2504s\n",
      "\titers: 500, epoch: 5 | loss: 0.0979272\n",
      "\tspeed: 0.0154s/iter; left time: 132.7726s\n",
      "Epoch: 5 cost time: 9.532302856445312\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1141438 Vali Loss: 0.0361733 Test Loss: 0.1294257\n",
      "Validation loss decreased (0.036440 --> 0.036173).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0919634\n",
      "\tspeed: 0.0500s/iter; left time: 422.2434s\n",
      "\titers: 200, epoch: 6 | loss: 0.0713519\n",
      "\tspeed: 0.0154s/iter; left time: 128.1896s\n",
      "\titers: 300, epoch: 6 | loss: 0.0640771\n",
      "\tspeed: 0.0154s/iter; left time: 126.8629s\n",
      "\titers: 400, epoch: 6 | loss: 0.0877847\n",
      "\tspeed: 0.0153s/iter; left time: 125.0300s\n",
      "\titers: 500, epoch: 6 | loss: 0.0983191\n",
      "\tspeed: 0.0154s/iter; left time: 123.7464s\n",
      "Epoch: 6 cost time: 9.050387620925903\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1107003 Vali Loss: 0.0353952 Test Loss: 0.1319505\n",
      "Validation loss decreased (0.036173 --> 0.035395).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1243195\n",
      "\tspeed: 0.0482s/iter; left time: 379.9651s\n",
      "\titers: 200, epoch: 7 | loss: 0.0919826\n",
      "\tspeed: 0.0154s/iter; left time: 119.9540s\n",
      "\titers: 300, epoch: 7 | loss: 0.1036018\n",
      "\tspeed: 0.0154s/iter; left time: 118.3080s\n",
      "\titers: 400, epoch: 7 | loss: 0.1400468\n",
      "\tspeed: 0.0154s/iter; left time: 116.7731s\n",
      "\titers: 500, epoch: 7 | loss: 0.1277194\n",
      "\tspeed: 0.0154s/iter; left time: 115.2007s\n",
      "Epoch: 7 cost time: 9.046965837478638\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1090898 Vali Loss: 0.0352193 Test Loss: 0.1302587\n",
      "Validation loss decreased (0.035395 --> 0.035219).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0812892\n",
      "\tspeed: 0.0503s/iter; left time: 367.6735s\n",
      "\titers: 200, epoch: 8 | loss: 0.1107979\n",
      "\tspeed: 0.0170s/iter; left time: 122.4991s\n",
      "\titers: 300, epoch: 8 | loss: 0.0703831\n",
      "\tspeed: 0.0170s/iter; left time: 120.7892s\n",
      "\titers: 400, epoch: 8 | loss: 0.1055723\n",
      "\tspeed: 0.0170s/iter; left time: 119.0831s\n",
      "\titers: 500, epoch: 8 | loss: 0.0826987\n",
      "\tspeed: 0.0170s/iter; left time: 117.4227s\n",
      "Epoch: 8 cost time: 9.985109329223633\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1076544 Vali Loss: 0.0354851 Test Loss: 0.1311845\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0923762\n",
      "\tspeed: 0.0494s/iter; left time: 333.1899s\n",
      "\titers: 200, epoch: 9 | loss: 0.0993528\n",
      "\tspeed: 0.0170s/iter; left time: 112.8904s\n",
      "\titers: 300, epoch: 9 | loss: 0.1253577\n",
      "\tspeed: 0.0170s/iter; left time: 111.0782s\n",
      "\titers: 400, epoch: 9 | loss: 0.0924837\n",
      "\tspeed: 0.0170s/iter; left time: 109.4876s\n",
      "\titers: 500, epoch: 9 | loss: 0.1125001\n",
      "\tspeed: 0.0170s/iter; left time: 107.6664s\n",
      "Epoch: 9 cost time: 9.9627103805542\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1072153 Vali Loss: 0.0354216 Test Loss: 0.1312207\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1162508\n",
      "\tspeed: 0.0491s/iter; left time: 302.8614s\n",
      "\titers: 200, epoch: 10 | loss: 0.1318796\n",
      "\tspeed: 0.0153s/iter; left time: 92.8572s\n",
      "\titers: 300, epoch: 10 | loss: 0.0907145\n",
      "\tspeed: 0.0153s/iter; left time: 91.3280s\n",
      "\titers: 400, epoch: 10 | loss: 0.1265460\n",
      "\tspeed: 0.0153s/iter; left time: 89.8652s\n",
      "\titers: 500, epoch: 10 | loss: 0.1289427\n",
      "\tspeed: 0.0153s/iter; left time: 88.3539s\n",
      "Epoch: 10 cost time: 9.008822441101074\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1082149 Vali Loss: 0.0353202 Test Loss: 0.1312223\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13045193254947662, mae:0.23218737542629242\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1391820\n",
      "\tspeed: 0.0303s/iter; left time: 342.2291s\n",
      "\titers: 200, epoch: 1 | loss: 0.2029080\n",
      "\tspeed: 0.0187s/iter; left time: 209.3768s\n",
      "\titers: 300, epoch: 1 | loss: 0.3258815\n",
      "\tspeed: 0.0187s/iter; left time: 207.2815s\n",
      "\titers: 400, epoch: 1 | loss: 0.1845980\n",
      "\tspeed: 0.0187s/iter; left time: 205.4004s\n",
      "\titers: 500, epoch: 1 | loss: 0.1627738\n",
      "\tspeed: 0.0187s/iter; left time: 203.7960s\n",
      "Epoch: 1 cost time: 11.862347602844238\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2102097 Vali Loss: 0.0448969 Test Loss: 0.1337774\n",
      "Validation loss decreased (inf --> 0.044897).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2993554\n",
      "\tspeed: 0.0574s/iter; left time: 615.6492s\n",
      "\titers: 200, epoch: 2 | loss: 0.1644480\n",
      "\tspeed: 0.0187s/iter; left time: 198.4521s\n",
      "\titers: 300, epoch: 2 | loss: 0.1396863\n",
      "\tspeed: 0.0187s/iter; left time: 197.2293s\n",
      "\titers: 400, epoch: 2 | loss: 0.1331915\n",
      "\tspeed: 0.0187s/iter; left time: 195.2727s\n",
      "\titers: 500, epoch: 2 | loss: 0.1542030\n",
      "\tspeed: 0.0187s/iter; left time: 193.2091s\n",
      "Epoch: 2 cost time: 10.953672409057617\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1688071 Vali Loss: 0.0415520 Test Loss: 0.1278588\n",
      "Validation loss decreased (0.044897 --> 0.041552).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1257951\n",
      "\tspeed: 0.0566s/iter; left time: 575.2830s\n",
      "\titers: 200, epoch: 3 | loss: 0.1143206\n",
      "\tspeed: 0.0186s/iter; left time: 186.6600s\n",
      "\titers: 300, epoch: 3 | loss: 0.1279311\n",
      "\tspeed: 0.0186s/iter; left time: 184.8944s\n",
      "\titers: 400, epoch: 3 | loss: 0.1258806\n",
      "\tspeed: 0.0186s/iter; left time: 183.0877s\n",
      "\titers: 500, epoch: 3 | loss: 0.0975429\n",
      "\tspeed: 0.0186s/iter; left time: 181.1537s\n",
      "Epoch: 3 cost time: 10.89474630355835\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1449889 Vali Loss: 0.0376083 Test Loss: 0.1195712\n",
      "Validation loss decreased (0.041552 --> 0.037608).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1677614\n",
      "\tspeed: 0.0563s/iter; left time: 539.7606s\n",
      "\titers: 200, epoch: 4 | loss: 0.1814447\n",
      "\tspeed: 0.0186s/iter; left time: 176.9077s\n",
      "\titers: 300, epoch: 4 | loss: 0.1515219\n",
      "\tspeed: 0.0186s/iter; left time: 174.7588s\n",
      "\titers: 400, epoch: 4 | loss: 0.1079124\n",
      "\tspeed: 0.0186s/iter; left time: 172.6327s\n",
      "\titers: 500, epoch: 4 | loss: 0.1271523\n",
      "\tspeed: 0.0186s/iter; left time: 170.9346s\n",
      "Epoch: 4 cost time: 10.903175592422485\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1339066 Vali Loss: 0.0367072 Test Loss: 0.1212885\n",
      "Validation loss decreased (0.037608 --> 0.036707).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1465802\n",
      "\tspeed: 0.0579s/iter; left time: 522.1551s\n",
      "\titers: 200, epoch: 5 | loss: 0.1292708\n",
      "\tspeed: 0.0201s/iter; left time: 179.6273s\n",
      "\titers: 300, epoch: 5 | loss: 0.1207413\n",
      "\tspeed: 0.0188s/iter; left time: 165.6437s\n",
      "\titers: 400, epoch: 5 | loss: 0.1002995\n",
      "\tspeed: 0.0188s/iter; left time: 164.1074s\n",
      "\titers: 500, epoch: 5 | loss: 0.1350384\n",
      "\tspeed: 0.0188s/iter; left time: 161.7380s\n",
      "Epoch: 5 cost time: 11.348000049591064\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1279273 Vali Loss: 0.0351292 Test Loss: 0.1215083\n",
      "Validation loss decreased (0.036707 --> 0.035129).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0720300\n",
      "\tspeed: 0.0568s/iter; left time: 479.9640s\n",
      "\titers: 200, epoch: 6 | loss: 0.1259978\n",
      "\tspeed: 0.0186s/iter; left time: 155.4399s\n",
      "\titers: 300, epoch: 6 | loss: 0.0900185\n",
      "\tspeed: 0.0186s/iter; left time: 153.2885s\n",
      "\titers: 400, epoch: 6 | loss: 0.1716787\n",
      "\tspeed: 0.0186s/iter; left time: 151.4791s\n",
      "\titers: 500, epoch: 6 | loss: 0.1148341\n",
      "\tspeed: 0.0186s/iter; left time: 149.6488s\n",
      "Epoch: 6 cost time: 10.875641107559204\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1261035 Vali Loss: 0.0348515 Test Loss: 0.1216410\n",
      "Validation loss decreased (0.035129 --> 0.034851).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0961893\n",
      "\tspeed: 0.0600s/iter; left time: 472.9703s\n",
      "\titers: 200, epoch: 7 | loss: 0.1358449\n",
      "\tspeed: 0.0186s/iter; left time: 144.5508s\n",
      "\titers: 300, epoch: 7 | loss: 0.1310754\n",
      "\tspeed: 0.0185s/iter; left time: 142.2689s\n",
      "\titers: 400, epoch: 7 | loss: 0.1342554\n",
      "\tspeed: 0.0186s/iter; left time: 140.6454s\n",
      "\titers: 500, epoch: 7 | loss: 0.1742030\n",
      "\tspeed: 0.0186s/iter; left time: 138.7958s\n",
      "Epoch: 7 cost time: 10.856202125549316\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1236303 Vali Loss: 0.0349391 Test Loss: 0.1203975\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1173634\n",
      "\tspeed: 0.0549s/iter; left time: 401.3803s\n",
      "\titers: 200, epoch: 8 | loss: 0.0968303\n",
      "\tspeed: 0.0187s/iter; left time: 135.0491s\n",
      "\titers: 300, epoch: 8 | loss: 0.0950346\n",
      "\tspeed: 0.0187s/iter; left time: 132.9497s\n",
      "\titers: 400, epoch: 8 | loss: 0.1503787\n",
      "\tspeed: 0.0187s/iter; left time: 131.0872s\n",
      "\titers: 500, epoch: 8 | loss: 0.0932398\n",
      "\tspeed: 0.0186s/iter; left time: 128.8500s\n",
      "Epoch: 8 cost time: 10.922219276428223\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1230835 Vali Loss: 0.0348350 Test Loss: 0.1202207\n",
      "Validation loss decreased (0.034851 --> 0.034835).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0810930\n",
      "\tspeed: 0.0558s/iter; left time: 376.3928s\n",
      "\titers: 200, epoch: 9 | loss: 0.1697227\n",
      "\tspeed: 0.0185s/iter; left time: 123.0317s\n",
      "\titers: 300, epoch: 9 | loss: 0.0800193\n",
      "\tspeed: 0.0185s/iter; left time: 121.0212s\n",
      "\titers: 400, epoch: 9 | loss: 0.1001454\n",
      "\tspeed: 0.0186s/iter; left time: 120.0662s\n",
      "\titers: 500, epoch: 9 | loss: 0.0645827\n",
      "\tspeed: 0.0186s/iter; left time: 118.1089s\n",
      "Epoch: 9 cost time: 10.854018449783325\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1228311 Vali Loss: 0.0350171 Test Loss: 0.1199624\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1279257\n",
      "\tspeed: 0.0558s/iter; left time: 344.3900s\n",
      "\titers: 200, epoch: 10 | loss: 0.1399926\n",
      "\tspeed: 0.0187s/iter; left time: 113.7350s\n",
      "\titers: 300, epoch: 10 | loss: 0.0904599\n",
      "\tspeed: 0.0187s/iter; left time: 111.5358s\n",
      "\titers: 400, epoch: 10 | loss: 0.1062682\n",
      "\tspeed: 0.0187s/iter; left time: 109.6424s\n",
      "\titers: 500, epoch: 10 | loss: 0.1082861\n",
      "\tspeed: 0.0187s/iter; left time: 107.9367s\n",
      "Epoch: 10 cost time: 10.951215028762817\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1224287 Vali Loss: 0.0346467 Test Loss: 0.1207519\n",
      "Validation loss decreased (0.034835 --> 0.034647).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1097813\n",
      "\tspeed: 0.0565s/iter; left time: 316.6498s\n",
      "\titers: 200, epoch: 11 | loss: 0.1797248\n",
      "\tspeed: 0.0186s/iter; left time: 102.0699s\n",
      "\titers: 300, epoch: 11 | loss: 0.1089150\n",
      "\tspeed: 0.0186s/iter; left time: 100.3037s\n",
      "\titers: 400, epoch: 11 | loss: 0.0766252\n",
      "\tspeed: 0.0186s/iter; left time: 98.4168s\n",
      "\titers: 500, epoch: 11 | loss: 0.1812996\n",
      "\tspeed: 0.0185s/iter; left time: 96.4273s\n",
      "Epoch: 11 cost time: 10.860699892044067\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1225361 Vali Loss: 0.0347175 Test Loss: 0.1206732\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1092781\n",
      "\tspeed: 0.0581s/iter; left time: 292.3518s\n",
      "\titers: 200, epoch: 12 | loss: 0.1323999\n",
      "\tspeed: 0.0187s/iter; left time: 92.1682s\n",
      "\titers: 300, epoch: 12 | loss: 0.0689266\n",
      "\tspeed: 0.0187s/iter; left time: 90.2303s\n",
      "\titers: 400, epoch: 12 | loss: 0.1341790\n",
      "\tspeed: 0.0187s/iter; left time: 88.3085s\n",
      "\titers: 500, epoch: 12 | loss: 0.1126035\n",
      "\tspeed: 0.0187s/iter; left time: 86.4607s\n",
      "Epoch: 12 cost time: 10.906403303146362\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1226761 Vali Loss: 0.0345742 Test Loss: 0.1206448\n",
      "Validation loss decreased (0.034647 --> 0.034574).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1307561\n",
      "\tspeed: 0.0573s/iter; left time: 255.6728s\n",
      "\titers: 200, epoch: 13 | loss: 0.1051752\n",
      "\tspeed: 0.0187s/iter; left time: 81.7069s\n",
      "\titers: 300, epoch: 13 | loss: 0.1009801\n",
      "\tspeed: 0.0186s/iter; left time: 79.4130s\n",
      "\titers: 400, epoch: 13 | loss: 0.0950372\n",
      "\tspeed: 0.0187s/iter; left time: 77.7518s\n",
      "\titers: 500, epoch: 13 | loss: 0.1825068\n",
      "\tspeed: 0.0187s/iter; left time: 75.8951s\n",
      "Epoch: 13 cost time: 10.927869081497192\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1217264 Vali Loss: 0.0346610 Test Loss: 0.1205635\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.0927252\n",
      "\tspeed: 0.0564s/iter; left time: 219.5535s\n",
      "\titers: 200, epoch: 14 | loss: 0.0829122\n",
      "\tspeed: 0.0186s/iter; left time: 70.6327s\n",
      "\titers: 300, epoch: 14 | loss: 0.1211418\n",
      "\tspeed: 0.0185s/iter; left time: 68.1881s\n",
      "\titers: 400, epoch: 14 | loss: 0.1085075\n",
      "\tspeed: 0.0185s/iter; left time: 66.5552s\n",
      "\titers: 500, epoch: 14 | loss: 0.1663618\n",
      "\tspeed: 0.0185s/iter; left time: 64.5847s\n",
      "Epoch: 14 cost time: 10.912145614624023\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1223126 Vali Loss: 0.0347010 Test Loss: 0.1205924\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.0744673\n",
      "\tspeed: 0.0582s/iter; left time: 193.2680s\n",
      "\titers: 200, epoch: 15 | loss: 0.1225159\n",
      "\tspeed: 0.0187s/iter; left time: 60.1529s\n",
      "\titers: 300, epoch: 15 | loss: 0.0960377\n",
      "\tspeed: 0.0186s/iter; left time: 58.0989s\n",
      "\titers: 400, epoch: 15 | loss: 0.1466675\n",
      "\tspeed: 0.0187s/iter; left time: 56.4494s\n",
      "\titers: 500, epoch: 15 | loss: 0.1094004\n",
      "\tspeed: 0.0186s/iter; left time: 54.3426s\n",
      "Epoch: 15 cost time: 10.913602352142334\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1223412 Vali Loss: 0.0346324 Test Loss: 0.1206264\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1208055168390274, mae:0.21598626673221588\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3847196\n",
      "\tspeed: 0.0297s/iter; left time: 335.7955s\n",
      "\titers: 200, epoch: 1 | loss: 0.1989159\n",
      "\tspeed: 0.0182s/iter; left time: 203.8501s\n",
      "\titers: 300, epoch: 1 | loss: 0.1971601\n",
      "\tspeed: 0.0182s/iter; left time: 201.8805s\n",
      "\titers: 400, epoch: 1 | loss: 0.1440938\n",
      "\tspeed: 0.0182s/iter; left time: 200.0072s\n",
      "\titers: 500, epoch: 1 | loss: 0.2128971\n",
      "\tspeed: 0.0182s/iter; left time: 198.3655s\n",
      "Epoch: 1 cost time: 11.58071517944336\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1825047 Vali Loss: 0.0356258 Test Loss: 0.1134578\n",
      "Validation loss decreased (inf --> 0.035626).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1396966\n",
      "\tspeed: 0.0582s/iter; left time: 624.2412s\n",
      "\titers: 200, epoch: 2 | loss: 0.2030740\n",
      "\tspeed: 0.0182s/iter; left time: 193.9053s\n",
      "\titers: 300, epoch: 2 | loss: 0.1893833\n",
      "\tspeed: 0.0182s/iter; left time: 191.3504s\n",
      "\titers: 400, epoch: 2 | loss: 0.1223289\n",
      "\tspeed: 0.0181s/iter; left time: 188.8583s\n",
      "\titers: 500, epoch: 2 | loss: 0.1044574\n",
      "\tspeed: 0.0181s/iter; left time: 187.0559s\n",
      "Epoch: 2 cost time: 10.66267466545105\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1498083 Vali Loss: 0.0350216 Test Loss: 0.1101953\n",
      "Validation loss decreased (0.035626 --> 0.035022).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0859063\n",
      "\tspeed: 0.0574s/iter; left time: 582.9142s\n",
      "\titers: 200, epoch: 3 | loss: 0.1234981\n",
      "\tspeed: 0.0182s/iter; left time: 183.3296s\n",
      "\titers: 300, epoch: 3 | loss: 0.1394759\n",
      "\tspeed: 0.0182s/iter; left time: 181.6225s\n",
      "\titers: 400, epoch: 3 | loss: 0.1237412\n",
      "\tspeed: 0.0182s/iter; left time: 179.4881s\n",
      "\titers: 500, epoch: 3 | loss: 0.1110266\n",
      "\tspeed: 0.0182s/iter; left time: 177.6329s\n",
      "Epoch: 3 cost time: 10.648946762084961\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1336981 Vali Loss: 0.0334302 Test Loss: 0.1088812\n",
      "Validation loss decreased (0.035022 --> 0.033430).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1316584\n",
      "\tspeed: 0.0594s/iter; left time: 569.4281s\n",
      "\titers: 200, epoch: 4 | loss: 0.0832696\n",
      "\tspeed: 0.0187s/iter; left time: 177.3288s\n",
      "\titers: 300, epoch: 4 | loss: 0.1686378\n",
      "\tspeed: 0.0182s/iter; left time: 171.2950s\n",
      "\titers: 400, epoch: 4 | loss: 0.1234927\n",
      "\tspeed: 0.0182s/iter; left time: 169.1957s\n",
      "\titers: 500, epoch: 4 | loss: 0.1014236\n",
      "\tspeed: 0.0182s/iter; left time: 167.2627s\n",
      "Epoch: 4 cost time: 10.91895341873169\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1239407 Vali Loss: 0.0317841 Test Loss: 0.1084288\n",
      "Validation loss decreased (0.033430 --> 0.031784).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0984245\n",
      "\tspeed: 0.0550s/iter; left time: 495.8276s\n",
      "\titers: 200, epoch: 5 | loss: 0.1071849\n",
      "\tspeed: 0.0182s/iter; left time: 162.2432s\n",
      "\titers: 300, epoch: 5 | loss: 0.1203405\n",
      "\tspeed: 0.0182s/iter; left time: 160.6381s\n",
      "\titers: 400, epoch: 5 | loss: 0.1267540\n",
      "\tspeed: 0.0182s/iter; left time: 158.9731s\n",
      "\titers: 500, epoch: 5 | loss: 0.1044165\n",
      "\tspeed: 0.0182s/iter; left time: 157.1797s\n",
      "Epoch: 5 cost time: 10.683056354522705\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1184903 Vali Loss: 0.0319481 Test Loss: 0.1089144\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1653547\n",
      "\tspeed: 0.0576s/iter; left time: 486.7017s\n",
      "\titers: 200, epoch: 6 | loss: 0.0927161\n",
      "\tspeed: 0.0183s/iter; left time: 152.5890s\n",
      "\titers: 300, epoch: 6 | loss: 0.1343766\n",
      "\tspeed: 0.0183s/iter; left time: 150.8955s\n",
      "\titers: 400, epoch: 6 | loss: 0.0845097\n",
      "\tspeed: 0.0182s/iter; left time: 148.6313s\n",
      "\titers: 500, epoch: 6 | loss: 0.0728186\n",
      "\tspeed: 0.0182s/iter; left time: 146.7543s\n",
      "Epoch: 6 cost time: 10.731748342514038\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1161634 Vali Loss: 0.0314554 Test Loss: 0.1089152\n",
      "Validation loss decreased (0.031784 --> 0.031455).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1205332\n",
      "\tspeed: 0.0577s/iter; left time: 454.6186s\n",
      "\titers: 200, epoch: 7 | loss: 0.1601523\n",
      "\tspeed: 0.0183s/iter; left time: 142.4152s\n",
      "\titers: 300, epoch: 7 | loss: 0.0827276\n",
      "\tspeed: 0.0183s/iter; left time: 140.4818s\n",
      "\titers: 400, epoch: 7 | loss: 0.1225206\n",
      "\tspeed: 0.0182s/iter; left time: 138.3121s\n",
      "\titers: 500, epoch: 7 | loss: 0.0939513\n",
      "\tspeed: 0.0183s/iter; left time: 136.6426s\n",
      "Epoch: 7 cost time: 10.714097499847412\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1140560 Vali Loss: 0.0314774 Test Loss: 0.1093833\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0813401\n",
      "\tspeed: 0.0561s/iter; left time: 410.2226s\n",
      "\titers: 200, epoch: 8 | loss: 0.0968648\n",
      "\tspeed: 0.0182s/iter; left time: 130.9620s\n",
      "\titers: 300, epoch: 8 | loss: 0.1148806\n",
      "\tspeed: 0.0182s/iter; left time: 129.1771s\n",
      "\titers: 400, epoch: 8 | loss: 0.1035178\n",
      "\tspeed: 0.0181s/iter; left time: 126.9936s\n",
      "\titers: 500, epoch: 8 | loss: 0.1377087\n",
      "\tspeed: 0.0181s/iter; left time: 125.0032s\n",
      "Epoch: 8 cost time: 10.579122543334961\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1131360 Vali Loss: 0.0315018 Test Loss: 0.1090401\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0662846\n",
      "\tspeed: 0.0587s/iter; left time: 395.3889s\n",
      "\titers: 200, epoch: 9 | loss: 0.0926176\n",
      "\tspeed: 0.0205s/iter; left time: 135.8190s\n",
      "\titers: 300, epoch: 9 | loss: 0.1140424\n",
      "\tspeed: 0.0205s/iter; left time: 134.0146s\n",
      "\titers: 400, epoch: 9 | loss: 0.1118420\n",
      "\tspeed: 0.0204s/iter; left time: 131.4248s\n",
      "\titers: 500, epoch: 9 | loss: 0.0876799\n",
      "\tspeed: 0.0205s/iter; left time: 129.7779s\n",
      "Epoch: 9 cost time: 11.931885957717896\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1132773 Vali Loss: 0.0314580 Test Loss: 0.1089168\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10906100273132324, mae:0.20073860883712769\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2622037\n",
      "\tspeed: 0.0326s/iter; left time: 368.0577s\n",
      "\titers: 200, epoch: 1 | loss: 0.1800064\n",
      "\tspeed: 0.0207s/iter; left time: 231.3811s\n",
      "\titers: 300, epoch: 1 | loss: 0.2144991\n",
      "\tspeed: 0.0206s/iter; left time: 229.2005s\n",
      "\titers: 400, epoch: 1 | loss: 0.2300204\n",
      "\tspeed: 0.0196s/iter; left time: 215.4945s\n",
      "\titers: 500, epoch: 1 | loss: 0.3149802\n",
      "\tspeed: 0.0185s/iter; left time: 201.6557s\n",
      "Epoch: 1 cost time: 12.53683066368103\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2178458 Vali Loss: 0.0412033 Test Loss: 0.1345358\n",
      "Validation loss decreased (inf --> 0.041203).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2365496\n",
      "\tspeed: 0.0597s/iter; left time: 640.3960s\n",
      "\titers: 200, epoch: 2 | loss: 0.2591004\n",
      "\tspeed: 0.0213s/iter; left time: 226.6610s\n",
      "\titers: 300, epoch: 2 | loss: 0.3177629\n",
      "\tspeed: 0.0212s/iter; left time: 223.4036s\n",
      "\titers: 400, epoch: 2 | loss: 0.1723979\n",
      "\tspeed: 0.0212s/iter; left time: 221.5476s\n",
      "\titers: 500, epoch: 2 | loss: 0.0800868\n",
      "\tspeed: 0.0212s/iter; left time: 218.9512s\n",
      "Epoch: 2 cost time: 12.421179056167603\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1743670 Vali Loss: 0.0447428 Test Loss: 0.1367654\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1199422\n",
      "\tspeed: 0.0613s/iter; left time: 623.1216s\n",
      "\titers: 200, epoch: 3 | loss: 0.1599562\n",
      "\tspeed: 0.0182s/iter; left time: 182.8351s\n",
      "\titers: 300, epoch: 3 | loss: 0.1599515\n",
      "\tspeed: 0.0182s/iter; left time: 181.3097s\n",
      "\titers: 400, epoch: 3 | loss: 0.1801567\n",
      "\tspeed: 0.0182s/iter; left time: 179.6543s\n",
      "\titers: 500, epoch: 3 | loss: 0.1400699\n",
      "\tspeed: 0.0182s/iter; left time: 177.9144s\n",
      "Epoch: 3 cost time: 10.67023777961731\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1508053 Vali Loss: 0.0413297 Test Loss: 0.1289753\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1095312\n",
      "\tspeed: 0.0570s/iter; left time: 546.7607s\n",
      "\titers: 200, epoch: 4 | loss: 0.1352073\n",
      "\tspeed: 0.0182s/iter; left time: 173.1652s\n",
      "\titers: 300, epoch: 4 | loss: 0.1411973\n",
      "\tspeed: 0.0182s/iter; left time: 171.1044s\n",
      "\titers: 400, epoch: 4 | loss: 0.1697282\n",
      "\tspeed: 0.0182s/iter; left time: 169.5168s\n",
      "\titers: 500, epoch: 4 | loss: 0.0927050\n",
      "\tspeed: 0.0182s/iter; left time: 167.4862s\n",
      "Epoch: 4 cost time: 10.662667036056519\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1389273 Vali Loss: 0.0384533 Test Loss: 0.1248211\n",
      "Validation loss decreased (0.041203 --> 0.038453).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1008001\n",
      "\tspeed: 0.0560s/iter; left time: 505.1199s\n",
      "\titers: 200, epoch: 5 | loss: 0.1308750\n",
      "\tspeed: 0.0183s/iter; left time: 163.2790s\n",
      "\titers: 300, epoch: 5 | loss: 0.1068856\n",
      "\tspeed: 0.0183s/iter; left time: 161.4229s\n",
      "\titers: 400, epoch: 5 | loss: 0.1131738\n",
      "\tspeed: 0.0183s/iter; left time: 159.7989s\n",
      "\titers: 500, epoch: 5 | loss: 0.0717618\n",
      "\tspeed: 0.0183s/iter; left time: 157.8891s\n",
      "Epoch: 5 cost time: 10.738748788833618\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1323583 Vali Loss: 0.0376206 Test Loss: 0.1269149\n",
      "Validation loss decreased (0.038453 --> 0.037621).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1632042\n",
      "\tspeed: 0.0575s/iter; left time: 485.7453s\n",
      "\titers: 200, epoch: 6 | loss: 0.1068073\n",
      "\tspeed: 0.0183s/iter; left time: 153.0845s\n",
      "\titers: 300, epoch: 6 | loss: 0.2113397\n",
      "\tspeed: 0.0183s/iter; left time: 151.1871s\n",
      "\titers: 400, epoch: 6 | loss: 0.1107387\n",
      "\tspeed: 0.0183s/iter; left time: 149.4178s\n",
      "\titers: 500, epoch: 6 | loss: 0.1167599\n",
      "\tspeed: 0.0183s/iter; left time: 147.5590s\n",
      "Epoch: 6 cost time: 10.758628368377686\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1287186 Vali Loss: 0.0375797 Test Loss: 0.1266044\n",
      "Validation loss decreased (0.037621 --> 0.037580).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1238780\n",
      "\tspeed: 0.0617s/iter; left time: 486.2009s\n",
      "\titers: 200, epoch: 7 | loss: 0.1068896\n",
      "\tspeed: 0.0198s/iter; left time: 154.0622s\n",
      "\titers: 300, epoch: 7 | loss: 0.1458076\n",
      "\tspeed: 0.0183s/iter; left time: 140.7225s\n",
      "\titers: 400, epoch: 7 | loss: 0.1221422\n",
      "\tspeed: 0.0184s/iter; left time: 139.8128s\n",
      "\titers: 500, epoch: 7 | loss: 0.0889262\n",
      "\tspeed: 0.0184s/iter; left time: 137.5455s\n",
      "Epoch: 7 cost time: 11.16085171699524\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1286428 Vali Loss: 0.0375248 Test Loss: 0.1260683\n",
      "Validation loss decreased (0.037580 --> 0.037525).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1094912\n",
      "\tspeed: 0.0565s/iter; left time: 412.8795s\n",
      "\titers: 200, epoch: 8 | loss: 0.0991819\n",
      "\tspeed: 0.0184s/iter; left time: 132.3943s\n",
      "\titers: 300, epoch: 8 | loss: 0.1341850\n",
      "\tspeed: 0.0184s/iter; left time: 130.6105s\n",
      "\titers: 400, epoch: 8 | loss: 0.1574862\n",
      "\tspeed: 0.0183s/iter; left time: 128.5812s\n",
      "\titers: 500, epoch: 8 | loss: 0.1078135\n",
      "\tspeed: 0.0183s/iter; left time: 126.7123s\n",
      "Epoch: 8 cost time: 10.758219718933105\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1276205 Vali Loss: 0.0374951 Test Loss: 0.1265586\n",
      "Validation loss decreased (0.037525 --> 0.037495).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1345530\n",
      "\tspeed: 0.0555s/iter; left time: 374.1753s\n",
      "\titers: 200, epoch: 9 | loss: 0.1701355\n",
      "\tspeed: 0.0183s/iter; left time: 121.6919s\n",
      "\titers: 300, epoch: 9 | loss: 0.1660757\n",
      "\tspeed: 0.0183s/iter; left time: 119.7592s\n",
      "\titers: 400, epoch: 9 | loss: 0.2267914\n",
      "\tspeed: 0.0183s/iter; left time: 117.8966s\n",
      "\titers: 500, epoch: 9 | loss: 0.0914125\n",
      "\tspeed: 0.0183s/iter; left time: 116.1973s\n",
      "Epoch: 9 cost time: 10.741132497787476\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1275924 Vali Loss: 0.0375651 Test Loss: 0.1264918\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1320315\n",
      "\tspeed: 0.0567s/iter; left time: 349.8703s\n",
      "\titers: 200, epoch: 10 | loss: 0.1074320\n",
      "\tspeed: 0.0184s/iter; left time: 111.5362s\n",
      "\titers: 300, epoch: 10 | loss: 0.1262881\n",
      "\tspeed: 0.0184s/iter; left time: 109.7384s\n",
      "\titers: 400, epoch: 10 | loss: 0.1170432\n",
      "\tspeed: 0.0184s/iter; left time: 107.8640s\n",
      "\titers: 500, epoch: 10 | loss: 0.1459234\n",
      "\tspeed: 0.0183s/iter; left time: 105.8577s\n",
      "Epoch: 10 cost time: 10.737431049346924\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1270538 Vali Loss: 0.0372558 Test Loss: 0.1263026\n",
      "Validation loss decreased (0.037495 --> 0.037256).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1405462\n",
      "\tspeed: 0.0559s/iter; left time: 313.0922s\n",
      "\titers: 200, epoch: 11 | loss: 0.1231383\n",
      "\tspeed: 0.0183s/iter; left time: 100.7084s\n",
      "\titers: 300, epoch: 11 | loss: 0.1248415\n",
      "\tspeed: 0.0183s/iter; left time: 98.8910s\n",
      "\titers: 400, epoch: 11 | loss: 0.1385459\n",
      "\tspeed: 0.0184s/iter; left time: 97.3379s\n",
      "\titers: 500, epoch: 11 | loss: 0.1068804\n",
      "\tspeed: 0.0184s/iter; left time: 95.4418s\n",
      "Epoch: 11 cost time: 10.755859136581421\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1275050 Vali Loss: 0.0372520 Test Loss: 0.1262792\n",
      "Validation loss decreased (0.037256 --> 0.037252).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1180813\n",
      "\tspeed: 0.0605s/iter; left time: 304.3307s\n",
      "\titers: 200, epoch: 12 | loss: 0.1363812\n",
      "\tspeed: 0.0212s/iter; left time: 104.3885s\n",
      "\titers: 300, epoch: 12 | loss: 0.1083949\n",
      "\tspeed: 0.0211s/iter; left time: 102.1393s\n",
      "\titers: 400, epoch: 12 | loss: 0.0979449\n",
      "\tspeed: 0.0211s/iter; left time: 99.9010s\n",
      "\titers: 500, epoch: 12 | loss: 0.1293892\n",
      "\tspeed: 0.0212s/iter; left time: 98.3552s\n",
      "Epoch: 12 cost time: 12.382421493530273\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1270305 Vali Loss: 0.0372749 Test Loss: 0.1262751\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1076863\n",
      "\tspeed: 0.0604s/iter; left time: 269.2761s\n",
      "\titers: 200, epoch: 13 | loss: 0.0892995\n",
      "\tspeed: 0.0211s/iter; left time: 92.0183s\n",
      "\titers: 300, epoch: 13 | loss: 0.1081605\n",
      "\tspeed: 0.0211s/iter; left time: 89.8865s\n",
      "\titers: 400, epoch: 13 | loss: 0.0759180\n",
      "\tspeed: 0.0211s/iter; left time: 87.9048s\n",
      "\titers: 500, epoch: 13 | loss: 0.1010875\n",
      "\tspeed: 0.0211s/iter; left time: 85.6117s\n",
      "Epoch: 13 cost time: 12.362730741500854\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1264350 Vali Loss: 0.0374249 Test Loss: 0.1263012\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.0805550\n",
      "\tspeed: 0.0627s/iter; left time: 244.0920s\n",
      "\titers: 200, epoch: 14 | loss: 0.0965289\n",
      "\tspeed: 0.0212s/iter; left time: 80.2055s\n",
      "\titers: 300, epoch: 14 | loss: 0.1239678\n",
      "\tspeed: 0.0212s/iter; left time: 78.0799s\n",
      "\titers: 400, epoch: 14 | loss: 0.1377636\n",
      "\tspeed: 0.0211s/iter; left time: 75.9428s\n",
      "\titers: 500, epoch: 14 | loss: 0.1081927\n",
      "\tspeed: 0.0211s/iter; left time: 73.8334s\n",
      "Epoch: 14 cost time: 12.3464834690094\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1268316 Vali Loss: 0.0373815 Test Loss: 0.1263072\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1264682114124298, mae:0.22387194633483887\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2027539\n",
      "\tspeed: 0.0341s/iter; left time: 385.5615s\n",
      "\titers: 200, epoch: 1 | loss: 0.1669776\n",
      "\tspeed: 0.0220s/iter; left time: 246.5837s\n",
      "\titers: 300, epoch: 1 | loss: 0.1981617\n",
      "\tspeed: 0.0220s/iter; left time: 244.5813s\n",
      "\titers: 400, epoch: 1 | loss: 0.2945663\n",
      "\tspeed: 0.0220s/iter; left time: 241.9595s\n",
      "\titers: 500, epoch: 1 | loss: 0.2654594\n",
      "\tspeed: 0.0221s/iter; left time: 240.8377s\n",
      "Epoch: 1 cost time: 13.819376707077026\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2067640 Vali Loss: 0.0437287 Test Loss: 0.1357310\n",
      "Validation loss decreased (inf --> 0.043729).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2346632\n",
      "\tspeed: 0.0635s/iter; left time: 681.6570s\n",
      "\titers: 200, epoch: 2 | loss: 0.1298181\n",
      "\tspeed: 0.0221s/iter; left time: 234.7835s\n",
      "\titers: 300, epoch: 2 | loss: 0.1294292\n",
      "\tspeed: 0.0221s/iter; left time: 232.5997s\n",
      "\titers: 400, epoch: 2 | loss: 0.1555942\n",
      "\tspeed: 0.0221s/iter; left time: 230.2525s\n",
      "\titers: 500, epoch: 2 | loss: 0.1474744\n",
      "\tspeed: 0.0221s/iter; left time: 228.2132s\n",
      "Epoch: 2 cost time: 12.929855108261108\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1592953 Vali Loss: 0.0386249 Test Loss: 0.1238144\n",
      "Validation loss decreased (0.043729 --> 0.038625).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2576421\n",
      "\tspeed: 0.0641s/iter; left time: 650.8554s\n",
      "\titers: 200, epoch: 3 | loss: 0.1106855\n",
      "\tspeed: 0.0221s/iter; left time: 222.1966s\n",
      "\titers: 300, epoch: 3 | loss: 0.1619062\n",
      "\tspeed: 0.0221s/iter; left time: 219.9812s\n",
      "\titers: 400, epoch: 3 | loss: 0.1001397\n",
      "\tspeed: 0.0221s/iter; left time: 217.6827s\n",
      "\titers: 500, epoch: 3 | loss: 0.0994980\n",
      "\tspeed: 0.0221s/iter; left time: 215.7020s\n",
      "Epoch: 3 cost time: 12.896713733673096\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1308025 Vali Loss: 0.0353702 Test Loss: 0.1231166\n",
      "Validation loss decreased (0.038625 --> 0.035370).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1022077\n",
      "\tspeed: 0.0661s/iter; left time: 634.1011s\n",
      "\titers: 200, epoch: 4 | loss: 0.0759279\n",
      "\tspeed: 0.0222s/iter; left time: 210.5675s\n",
      "\titers: 300, epoch: 4 | loss: 0.1092489\n",
      "\tspeed: 0.0222s/iter; left time: 208.5418s\n",
      "\titers: 400, epoch: 4 | loss: 0.0982543\n",
      "\tspeed: 0.0222s/iter; left time: 206.2838s\n",
      "\titers: 500, epoch: 4 | loss: 0.1520389\n",
      "\tspeed: 0.0222s/iter; left time: 204.0266s\n",
      "Epoch: 4 cost time: 13.118576526641846\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1136707 Vali Loss: 0.0346266 Test Loss: 0.1269748\n",
      "Validation loss decreased (0.035370 --> 0.034627).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1267427\n",
      "\tspeed: 0.0638s/iter; left time: 575.6996s\n",
      "\titers: 200, epoch: 5 | loss: 0.1023141\n",
      "\tspeed: 0.0220s/iter; left time: 196.5982s\n",
      "\titers: 300, epoch: 5 | loss: 0.0798481\n",
      "\tspeed: 0.0220s/iter; left time: 194.3547s\n",
      "\titers: 400, epoch: 5 | loss: 0.0946030\n",
      "\tspeed: 0.0220s/iter; left time: 191.9657s\n",
      "\titers: 500, epoch: 5 | loss: 0.0963447\n",
      "\tspeed: 0.0220s/iter; left time: 189.7018s\n",
      "Epoch: 5 cost time: 12.883232593536377\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1071186 Vali Loss: 0.0336836 Test Loss: 0.1273362\n",
      "Validation loss decreased (0.034627 --> 0.033684).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0972172\n",
      "\tspeed: 0.0665s/iter; left time: 562.0284s\n",
      "\titers: 200, epoch: 6 | loss: 0.1180001\n",
      "\tspeed: 0.0220s/iter; left time: 183.4606s\n",
      "\titers: 300, epoch: 6 | loss: 0.0808669\n",
      "\tspeed: 0.0220s/iter; left time: 181.3414s\n",
      "\titers: 400, epoch: 6 | loss: 0.0732322\n",
      "\tspeed: 0.0220s/iter; left time: 179.0073s\n",
      "\titers: 500, epoch: 6 | loss: 0.1540477\n",
      "\tspeed: 0.0220s/iter; left time: 176.8477s\n",
      "Epoch: 6 cost time: 12.840306758880615\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1037475 Vali Loss: 0.0350687 Test Loss: 0.1272723\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1238752\n",
      "\tspeed: 0.0644s/iter; left time: 507.7554s\n",
      "\titers: 200, epoch: 7 | loss: 0.1158283\n",
      "\tspeed: 0.0229s/iter; left time: 177.8985s\n",
      "\titers: 300, epoch: 7 | loss: 0.0841666\n",
      "\tspeed: 0.0223s/iter; left time: 171.6560s\n",
      "\titers: 400, epoch: 7 | loss: 0.0927563\n",
      "\tspeed: 0.0221s/iter; left time: 167.7603s\n",
      "\titers: 500, epoch: 7 | loss: 0.1020523\n",
      "\tspeed: 0.0221s/iter; left time: 165.5209s\n",
      "Epoch: 7 cost time: 13.180501461029053\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1008883 Vali Loss: 0.0350252 Test Loss: 0.1266070\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1005726\n",
      "\tspeed: 0.0642s/iter; left time: 469.0356s\n",
      "\titers: 200, epoch: 8 | loss: 0.2125474\n",
      "\tspeed: 0.0240s/iter; left time: 173.1768s\n",
      "\titers: 300, epoch: 8 | loss: 0.0797290\n",
      "\tspeed: 0.0240s/iter; left time: 170.6309s\n",
      "\titers: 400, epoch: 8 | loss: 0.0906385\n",
      "\tspeed: 0.0240s/iter; left time: 168.2739s\n",
      "\titers: 500, epoch: 8 | loss: 0.0828229\n",
      "\tspeed: 0.0230s/iter; left time: 158.7859s\n",
      "Epoch: 8 cost time: 13.750190019607544\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1002590 Vali Loss: 0.0339656 Test Loss: 0.1274215\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12748165428638458, mae:0.22400608658790588\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1785491\n",
      "\tspeed: 0.0331s/iter; left time: 373.8398s\n",
      "\titers: 200, epoch: 1 | loss: 0.1034653\n",
      "\tspeed: 0.0213s/iter; left time: 238.1668s\n",
      "\titers: 300, epoch: 1 | loss: 0.1610785\n",
      "\tspeed: 0.0213s/iter; left time: 235.9638s\n",
      "\titers: 400, epoch: 1 | loss: 0.1331869\n",
      "\tspeed: 0.0213s/iter; left time: 234.1018s\n",
      "\titers: 500, epoch: 1 | loss: 0.1715312\n",
      "\tspeed: 0.0213s/iter; left time: 232.0509s\n",
      "Epoch: 1 cost time: 13.361382007598877\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1773544 Vali Loss: 0.0365672 Test Loss: 0.1138351\n",
      "Validation loss decreased (inf --> 0.036567).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1486347\n",
      "\tspeed: 0.0620s/iter; left time: 665.7714s\n",
      "\titers: 200, epoch: 2 | loss: 0.1087361\n",
      "\tspeed: 0.0213s/iter; left time: 226.5467s\n",
      "\titers: 300, epoch: 2 | loss: 0.1416785\n",
      "\tspeed: 0.0213s/iter; left time: 224.3477s\n",
      "\titers: 400, epoch: 2 | loss: 0.1260082\n",
      "\tspeed: 0.0213s/iter; left time: 222.2356s\n",
      "\titers: 500, epoch: 2 | loss: 0.0900498\n",
      "\tspeed: 0.0213s/iter; left time: 220.2472s\n",
      "Epoch: 2 cost time: 12.46543836593628\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1427053 Vali Loss: 0.0326686 Test Loss: 0.1104448\n",
      "Validation loss decreased (0.036567 --> 0.032669).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1181588\n",
      "\tspeed: 0.0628s/iter; left time: 638.0728s\n",
      "\titers: 200, epoch: 3 | loss: 0.1013483\n",
      "\tspeed: 0.0214s/iter; left time: 215.1717s\n",
      "\titers: 300, epoch: 3 | loss: 0.1209200\n",
      "\tspeed: 0.0214s/iter; left time: 213.1776s\n",
      "\titers: 400, epoch: 3 | loss: 0.0976466\n",
      "\tspeed: 0.0214s/iter; left time: 210.9697s\n",
      "\titers: 500, epoch: 3 | loss: 0.1044774\n",
      "\tspeed: 0.0214s/iter; left time: 208.8784s\n",
      "Epoch: 3 cost time: 12.50110149383545\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1186728 Vali Loss: 0.0319777 Test Loss: 0.1122661\n",
      "Validation loss decreased (0.032669 --> 0.031978).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0921805\n",
      "\tspeed: 0.0616s/iter; left time: 590.6133s\n",
      "\titers: 200, epoch: 4 | loss: 0.1141695\n",
      "\tspeed: 0.0214s/iter; left time: 203.3073s\n",
      "\titers: 300, epoch: 4 | loss: 0.1044969\n",
      "\tspeed: 0.0214s/iter; left time: 201.0543s\n",
      "\titers: 400, epoch: 4 | loss: 0.1281468\n",
      "\tspeed: 0.0214s/iter; left time: 198.6277s\n",
      "\titers: 500, epoch: 4 | loss: 0.0988808\n",
      "\tspeed: 0.0214s/iter; left time: 196.5577s\n",
      "Epoch: 4 cost time: 12.500771045684814\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1057914 Vali Loss: 0.0311259 Test Loss: 0.1089387\n",
      "Validation loss decreased (0.031978 --> 0.031126).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1255900\n",
      "\tspeed: 0.0619s/iter; left time: 558.7027s\n",
      "\titers: 200, epoch: 5 | loss: 0.0753093\n",
      "\tspeed: 0.0214s/iter; left time: 190.9318s\n",
      "\titers: 300, epoch: 5 | loss: 0.0820977\n",
      "\tspeed: 0.0214s/iter; left time: 188.7971s\n",
      "\titers: 400, epoch: 5 | loss: 0.1364666\n",
      "\tspeed: 0.0214s/iter; left time: 186.6594s\n",
      "\titers: 500, epoch: 5 | loss: 0.0913140\n",
      "\tspeed: 0.0214s/iter; left time: 184.2951s\n",
      "Epoch: 5 cost time: 12.529300689697266\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0997650 Vali Loss: 0.0314607 Test Loss: 0.1088687\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0785641\n",
      "\tspeed: 0.0609s/iter; left time: 514.3819s\n",
      "\titers: 200, epoch: 6 | loss: 0.1225024\n",
      "\tspeed: 0.0213s/iter; left time: 178.1248s\n",
      "\titers: 300, epoch: 6 | loss: 0.1325319\n",
      "\tspeed: 0.0213s/iter; left time: 176.0816s\n",
      "\titers: 400, epoch: 6 | loss: 0.0782834\n",
      "\tspeed: 0.0213s/iter; left time: 173.9651s\n",
      "\titers: 500, epoch: 6 | loss: 0.0707568\n",
      "\tspeed: 0.0213s/iter; left time: 171.6843s\n",
      "Epoch: 6 cost time: 12.468377113342285\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0953235 Vali Loss: 0.0311220 Test Loss: 0.1090435\n",
      "Validation loss decreased (0.031126 --> 0.031122).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0702489\n",
      "\tspeed: 0.0614s/iter; left time: 483.8419s\n",
      "\titers: 200, epoch: 7 | loss: 0.1165190\n",
      "\tspeed: 0.0214s/iter; left time: 166.1771s\n",
      "\titers: 300, epoch: 7 | loss: 0.1115409\n",
      "\tspeed: 0.0213s/iter; left time: 163.7164s\n",
      "\titers: 400, epoch: 7 | loss: 0.0763219\n",
      "\tspeed: 0.0214s/iter; left time: 161.8868s\n",
      "\titers: 500, epoch: 7 | loss: 0.1219211\n",
      "\tspeed: 0.0213s/iter; left time: 159.4680s\n",
      "Epoch: 7 cost time: 12.47177505493164\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0939907 Vali Loss: 0.0313113 Test Loss: 0.1090514\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0761810\n",
      "\tspeed: 0.0622s/iter; left time: 454.6373s\n",
      "\titers: 200, epoch: 8 | loss: 0.0958735\n",
      "\tspeed: 0.0214s/iter; left time: 154.1427s\n",
      "\titers: 300, epoch: 8 | loss: 0.0982624\n",
      "\tspeed: 0.0214s/iter; left time: 152.0230s\n",
      "\titers: 400, epoch: 8 | loss: 0.0802097\n",
      "\tspeed: 0.0214s/iter; left time: 149.9926s\n",
      "\titers: 500, epoch: 8 | loss: 0.1419071\n",
      "\tspeed: 0.0214s/iter; left time: 147.6354s\n",
      "Epoch: 8 cost time: 12.501315832138062\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0933016 Vali Loss: 0.0310123 Test Loss: 0.1087838\n",
      "Validation loss decreased (0.031122 --> 0.031012).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0783801\n",
      "\tspeed: 0.0620s/iter; left time: 418.0212s\n",
      "\titers: 200, epoch: 9 | loss: 0.1293915\n",
      "\tspeed: 0.0213s/iter; left time: 141.3672s\n",
      "\titers: 300, epoch: 9 | loss: 0.1090923\n",
      "\tspeed: 0.0213s/iter; left time: 139.1032s\n",
      "\titers: 400, epoch: 9 | loss: 0.0697182\n",
      "\tspeed: 0.0213s/iter; left time: 137.0775s\n",
      "\titers: 500, epoch: 9 | loss: 0.0602542\n",
      "\tspeed: 0.0213s/iter; left time: 135.0803s\n",
      "Epoch: 9 cost time: 12.459821224212646\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0938148 Vali Loss: 0.0310205 Test Loss: 0.1087866\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0679607\n",
      "\tspeed: 0.0610s/iter; left time: 376.3541s\n",
      "\titers: 200, epoch: 10 | loss: 0.0885144\n",
      "\tspeed: 0.0213s/iter; left time: 129.2535s\n",
      "\titers: 300, epoch: 10 | loss: 0.0781095\n",
      "\tspeed: 0.0213s/iter; left time: 127.0385s\n",
      "\titers: 400, epoch: 10 | loss: 0.0953951\n",
      "\tspeed: 0.0213s/iter; left time: 124.9052s\n",
      "\titers: 500, epoch: 10 | loss: 0.1224032\n",
      "\tspeed: 0.0213s/iter; left time: 122.7091s\n",
      "Epoch: 10 cost time: 12.414680480957031\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0919785 Vali Loss: 0.0310270 Test Loss: 0.1089423\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0876282\n",
      "\tspeed: 0.0608s/iter; left time: 340.2920s\n",
      "\titers: 200, epoch: 11 | loss: 0.0823253\n",
      "\tspeed: 0.0214s/iter; left time: 117.5756s\n",
      "\titers: 300, epoch: 11 | loss: 0.0980454\n",
      "\tspeed: 0.0214s/iter; left time: 115.4640s\n",
      "\titers: 400, epoch: 11 | loss: 0.0720253\n",
      "\tspeed: 0.0214s/iter; left time: 113.3886s\n",
      "\titers: 500, epoch: 11 | loss: 0.1342674\n",
      "\tspeed: 0.0214s/iter; left time: 111.1949s\n",
      "Epoch: 11 cost time: 12.49315595626831\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0928769 Vali Loss: 0.0309564 Test Loss: 0.1089035\n",
      "Validation loss decreased (0.031012 --> 0.030956).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.0873149\n",
      "\tspeed: 0.0636s/iter; left time: 320.1410s\n",
      "\titers: 200, epoch: 12 | loss: 0.0897143\n",
      "\tspeed: 0.0233s/iter; left time: 114.7311s\n",
      "\titers: 300, epoch: 12 | loss: 0.0802560\n",
      "\tspeed: 0.0233s/iter; left time: 112.3522s\n",
      "\titers: 400, epoch: 12 | loss: 0.1330236\n",
      "\tspeed: 0.0232s/iter; left time: 109.9764s\n",
      "\titers: 500, epoch: 12 | loss: 0.1009890\n",
      "\tspeed: 0.0232s/iter; left time: 107.6539s\n",
      "Epoch: 12 cost time: 13.572937250137329\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0916396 Vali Loss: 0.0309637 Test Loss: 0.1088975\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.0757189\n",
      "\tspeed: 0.0641s/iter; left time: 285.9039s\n",
      "\titers: 200, epoch: 13 | loss: 0.1029050\n",
      "\tspeed: 0.0214s/iter; left time: 93.1421s\n",
      "\titers: 300, epoch: 13 | loss: 0.1308431\n",
      "\tspeed: 0.0214s/iter; left time: 90.9784s\n",
      "\titers: 400, epoch: 13 | loss: 0.0862136\n",
      "\tspeed: 0.0213s/iter; left time: 88.7805s\n",
      "\titers: 500, epoch: 13 | loss: 0.0903103\n",
      "\tspeed: 0.0212s/iter; left time: 86.1885s\n",
      "Epoch: 13 cost time: 12.438134908676147\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0925450 Vali Loss: 0.0310434 Test Loss: 0.1089011\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.0980130\n",
      "\tspeed: 0.0599s/iter; left time: 232.9698s\n",
      "\titers: 200, epoch: 14 | loss: 0.0712179\n",
      "\tspeed: 0.0213s/iter; left time: 80.9185s\n",
      "\titers: 300, epoch: 14 | loss: 0.0745564\n",
      "\tspeed: 0.0213s/iter; left time: 78.7702s\n",
      "\titers: 400, epoch: 14 | loss: 0.0895295\n",
      "\tspeed: 0.0213s/iter; left time: 76.6383s\n",
      "\titers: 500, epoch: 14 | loss: 0.1133360\n",
      "\tspeed: 0.0213s/iter; left time: 74.4832s\n",
      "Epoch: 14 cost time: 12.46585464477539\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.0920662 Vali Loss: 0.0309956 Test Loss: 0.1088896\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1090446412563324, mae:0.20337125658988953\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2186458\n",
      "\tspeed: 0.0344s/iter; left time: 388.5830s\n",
      "\titers: 200, epoch: 1 | loss: 0.2041420\n",
      "\tspeed: 0.0221s/iter; left time: 247.4437s\n",
      "\titers: 300, epoch: 1 | loss: 0.1998271\n",
      "\tspeed: 0.0221s/iter; left time: 245.3798s\n",
      "\titers: 400, epoch: 1 | loss: 0.2263469\n",
      "\tspeed: 0.0222s/iter; left time: 243.7532s\n",
      "\titers: 500, epoch: 1 | loss: 0.2081954\n",
      "\tspeed: 0.0207s/iter; left time: 225.9471s\n",
      "Epoch: 1 cost time: 13.596477508544922\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2079525 Vali Loss: 0.0443054 Test Loss: 0.1338999\n",
      "Validation loss decreased (inf --> 0.044305).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0878384\n",
      "\tspeed: 0.0619s/iter; left time: 664.7139s\n",
      "\titers: 200, epoch: 2 | loss: 0.2626744\n",
      "\tspeed: 0.0222s/iter; left time: 235.8705s\n",
      "\titers: 300, epoch: 2 | loss: 0.1360231\n",
      "\tspeed: 0.0221s/iter; left time: 233.1601s\n",
      "\titers: 400, epoch: 2 | loss: 0.1155632\n",
      "\tspeed: 0.0222s/iter; left time: 231.1270s\n",
      "\titers: 500, epoch: 2 | loss: 0.1218925\n",
      "\tspeed: 0.0221s/iter; left time: 228.5851s\n",
      "Epoch: 2 cost time: 12.948352813720703\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1629091 Vali Loss: 0.0399663 Test Loss: 0.1289942\n",
      "Validation loss decreased (0.044305 --> 0.039966).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0891363\n",
      "\tspeed: 0.0611s/iter; left time: 620.5207s\n",
      "\titers: 200, epoch: 3 | loss: 0.1080725\n",
      "\tspeed: 0.0202s/iter; left time: 202.9316s\n",
      "\titers: 300, epoch: 3 | loss: 0.1031212\n",
      "\tspeed: 0.0202s/iter; left time: 200.7370s\n",
      "\titers: 400, epoch: 3 | loss: 0.1273677\n",
      "\tspeed: 0.0201s/iter; left time: 198.4379s\n",
      "\titers: 500, epoch: 3 | loss: 0.1084824\n",
      "\tspeed: 0.0201s/iter; left time: 196.6093s\n",
      "Epoch: 3 cost time: 11.797197341918945\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1317193 Vali Loss: 0.0394654 Test Loss: 0.1277618\n",
      "Validation loss decreased (0.039966 --> 0.039465).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0673430\n",
      "\tspeed: 0.0593s/iter; left time: 568.9870s\n",
      "\titers: 200, epoch: 4 | loss: 0.1248085\n",
      "\tspeed: 0.0200s/iter; left time: 189.8777s\n",
      "\titers: 300, epoch: 4 | loss: 0.0912658\n",
      "\tspeed: 0.0199s/iter; left time: 187.1494s\n",
      "\titers: 400, epoch: 4 | loss: 0.1342447\n",
      "\tspeed: 0.0199s/iter; left time: 185.1886s\n",
      "\titers: 500, epoch: 4 | loss: 0.0975383\n",
      "\tspeed: 0.0200s/iter; left time: 183.4233s\n",
      "Epoch: 4 cost time: 11.685548067092896\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1173056 Vali Loss: 0.0376338 Test Loss: 0.1263050\n",
      "Validation loss decreased (0.039465 --> 0.037634).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1109312\n",
      "\tspeed: 0.0595s/iter; left time: 537.0247s\n",
      "\titers: 200, epoch: 5 | loss: 0.1237783\n",
      "\tspeed: 0.0200s/iter; left time: 178.3113s\n",
      "\titers: 300, epoch: 5 | loss: 0.1165981\n",
      "\tspeed: 0.0200s/iter; left time: 176.4705s\n",
      "\titers: 400, epoch: 5 | loss: 0.0904403\n",
      "\tspeed: 0.0200s/iter; left time: 174.5137s\n",
      "\titers: 500, epoch: 5 | loss: 0.1135031\n",
      "\tspeed: 0.0200s/iter; left time: 172.3141s\n",
      "Epoch: 5 cost time: 11.701964378356934\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1088367 Vali Loss: 0.0365250 Test Loss: 0.1262030\n",
      "Validation loss decreased (0.037634 --> 0.036525).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1269113\n",
      "\tspeed: 0.0628s/iter; left time: 530.4114s\n",
      "\titers: 200, epoch: 6 | loss: 0.1076501\n",
      "\tspeed: 0.0201s/iter; left time: 167.9921s\n",
      "\titers: 300, epoch: 6 | loss: 0.1019956\n",
      "\tspeed: 0.0201s/iter; left time: 165.9485s\n",
      "\titers: 400, epoch: 6 | loss: 0.0877422\n",
      "\tspeed: 0.0201s/iter; left time: 163.7794s\n",
      "\titers: 500, epoch: 6 | loss: 0.1660132\n",
      "\tspeed: 0.0201s/iter; left time: 161.9634s\n",
      "Epoch: 6 cost time: 11.798437118530273\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1055819 Vali Loss: 0.0368312 Test Loss: 0.1275319\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1239618\n",
      "\tspeed: 0.0625s/iter; left time: 492.8356s\n",
      "\titers: 200, epoch: 7 | loss: 0.1024051\n",
      "\tspeed: 0.0201s/iter; left time: 156.0298s\n",
      "\titers: 300, epoch: 7 | loss: 0.1231804\n",
      "\tspeed: 0.0200s/iter; left time: 153.9803s\n",
      "\titers: 400, epoch: 7 | loss: 0.0943301\n",
      "\tspeed: 0.0200s/iter; left time: 151.8990s\n",
      "\titers: 500, epoch: 7 | loss: 0.0661685\n",
      "\tspeed: 0.0200s/iter; left time: 149.8228s\n",
      "Epoch: 7 cost time: 11.688949346542358\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1039574 Vali Loss: 0.0366394 Test Loss: 0.1269912\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1205875\n",
      "\tspeed: 0.0618s/iter; left time: 452.1735s\n",
      "\titers: 200, epoch: 8 | loss: 0.1459271\n",
      "\tspeed: 0.0222s/iter; left time: 160.2718s\n",
      "\titers: 300, epoch: 8 | loss: 0.1205403\n",
      "\tspeed: 0.0221s/iter; left time: 157.3392s\n",
      "\titers: 400, epoch: 8 | loss: 0.0813425\n",
      "\tspeed: 0.0221s/iter; left time: 155.2078s\n",
      "\titers: 500, epoch: 8 | loss: 0.1189137\n",
      "\tspeed: 0.0221s/iter; left time: 152.8789s\n",
      "Epoch: 8 cost time: 12.918823003768921\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1026915 Vali Loss: 0.0364686 Test Loss: 0.1263820\n",
      "Validation loss decreased (0.036525 --> 0.036469).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1197486\n",
      "\tspeed: 0.0640s/iter; left time: 431.3513s\n",
      "\titers: 200, epoch: 9 | loss: 0.0918249\n",
      "\tspeed: 0.0201s/iter; left time: 133.5486s\n",
      "\titers: 300, epoch: 9 | loss: 0.1258810\n",
      "\tspeed: 0.0201s/iter; left time: 131.2370s\n",
      "\titers: 400, epoch: 9 | loss: 0.0980082\n",
      "\tspeed: 0.0201s/iter; left time: 129.3801s\n",
      "\titers: 500, epoch: 9 | loss: 0.0667927\n",
      "\tspeed: 0.0201s/iter; left time: 127.1785s\n",
      "Epoch: 9 cost time: 11.766298532485962\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1020714 Vali Loss: 0.0364871 Test Loss: 0.1269563\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1081315\n",
      "\tspeed: 0.0592s/iter; left time: 365.3864s\n",
      "\titers: 200, epoch: 10 | loss: 0.0703289\n",
      "\tspeed: 0.0200s/iter; left time: 121.2370s\n",
      "\titers: 300, epoch: 10 | loss: 0.1466069\n",
      "\tspeed: 0.0199s/iter; left time: 119.1067s\n",
      "\titers: 400, epoch: 10 | loss: 0.1373107\n",
      "\tspeed: 0.0200s/iter; left time: 117.1758s\n",
      "\titers: 500, epoch: 10 | loss: 0.1043450\n",
      "\tspeed: 0.0199s/iter; left time: 115.0932s\n",
      "Epoch: 10 cost time: 11.693408012390137\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1018137 Vali Loss: 0.0365492 Test Loss: 0.1266269\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0926427\n",
      "\tspeed: 0.0614s/iter; left time: 343.8704s\n",
      "\titers: 200, epoch: 11 | loss: 0.0865288\n",
      "\tspeed: 0.0230s/iter; left time: 126.7153s\n",
      "\titers: 300, epoch: 11 | loss: 0.0943823\n",
      "\tspeed: 0.0221s/iter; left time: 119.5037s\n",
      "\titers: 400, epoch: 11 | loss: 0.1200916\n",
      "\tspeed: 0.0199s/iter; left time: 105.6824s\n",
      "\titers: 500, epoch: 11 | loss: 0.0841507\n",
      "\tspeed: 0.0199s/iter; left time: 103.7302s\n",
      "Epoch: 11 cost time: 12.488039493560791\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1023736 Vali Loss: 0.0365195 Test Loss: 0.1265664\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12653130292892456, mae:0.22853916883468628\n",
      ">>> LR=1e-4,DO=0.2,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1838763\n",
      "\tspeed: 0.0222s/iter; left time: 250.4996s\n",
      "\titers: 200, epoch: 1 | loss: 0.2262921\n",
      "\tspeed: 0.0106s/iter; left time: 118.6945s\n",
      "\titers: 300, epoch: 1 | loss: 0.2421962\n",
      "\tspeed: 0.0106s/iter; left time: 117.8111s\n",
      "\titers: 400, epoch: 1 | loss: 0.2350823\n",
      "\tspeed: 0.0106s/iter; left time: 116.4880s\n",
      "\titers: 500, epoch: 1 | loss: 0.1849077\n",
      "\tspeed: 0.0106s/iter; left time: 115.5713s\n",
      "Epoch: 1 cost time: 7.2485411167144775\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2469439 Vali Loss: 0.0468240 Test Loss: 0.1477441\n",
      "Validation loss decreased (inf --> 0.046824).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3650033\n",
      "\tspeed: 0.0367s/iter; left time: 393.3535s\n",
      "\titers: 200, epoch: 2 | loss: 0.3034966\n",
      "\tspeed: 0.0106s/iter; left time: 113.0625s\n",
      "\titers: 300, epoch: 2 | loss: 0.2237196\n",
      "\tspeed: 0.0106s/iter; left time: 111.9491s\n",
      "\titers: 400, epoch: 2 | loss: 0.1715376\n",
      "\tspeed: 0.0106s/iter; left time: 111.0286s\n",
      "\titers: 500, epoch: 2 | loss: 0.2009435\n",
      "\tspeed: 0.0106s/iter; left time: 109.4956s\n",
      "Epoch: 2 cost time: 6.380062103271484\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2086817 Vali Loss: 0.0431858 Test Loss: 0.1302046\n",
      "Validation loss decreased (0.046824 --> 0.043186).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1407596\n",
      "\tspeed: 0.0386s/iter; left time: 391.8592s\n",
      "\titers: 200, epoch: 3 | loss: 0.2522303\n",
      "\tspeed: 0.0118s/iter; left time: 119.0943s\n",
      "\titers: 300, epoch: 3 | loss: 0.1577499\n",
      "\tspeed: 0.0118s/iter; left time: 117.9305s\n",
      "\titers: 400, epoch: 3 | loss: 0.1961393\n",
      "\tspeed: 0.0119s/iter; left time: 116.9755s\n",
      "\titers: 500, epoch: 3 | loss: 0.2381822\n",
      "\tspeed: 0.0118s/iter; left time: 115.3959s\n",
      "Epoch: 3 cost time: 7.074812889099121\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1940981 Vali Loss: 0.0392426 Test Loss: 0.1196013\n",
      "Validation loss decreased (0.043186 --> 0.039243).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1993661\n",
      "\tspeed: 0.0391s/iter; left time: 374.6525s\n",
      "\titers: 200, epoch: 4 | loss: 0.1229510\n",
      "\tspeed: 0.0106s/iter; left time: 100.8878s\n",
      "\titers: 300, epoch: 4 | loss: 0.1835459\n",
      "\tspeed: 0.0107s/iter; left time: 100.2252s\n",
      "\titers: 400, epoch: 4 | loss: 0.1944800\n",
      "\tspeed: 0.0106s/iter; left time: 98.3593s\n",
      "\titers: 500, epoch: 4 | loss: 0.1841007\n",
      "\tspeed: 0.0106s/iter; left time: 97.0314s\n",
      "Epoch: 4 cost time: 6.459578275680542\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1866241 Vali Loss: 0.0382622 Test Loss: 0.1176750\n",
      "Validation loss decreased (0.039243 --> 0.038262).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1781220\n",
      "\tspeed: 0.0363s/iter; left time: 327.8166s\n",
      "\titers: 200, epoch: 5 | loss: 0.1857679\n",
      "\tspeed: 0.0105s/iter; left time: 93.4360s\n",
      "\titers: 300, epoch: 5 | loss: 0.1451971\n",
      "\tspeed: 0.0105s/iter; left time: 92.1939s\n",
      "\titers: 400, epoch: 5 | loss: 0.1592865\n",
      "\tspeed: 0.0104s/iter; left time: 91.0742s\n",
      "\titers: 500, epoch: 5 | loss: 0.2001340\n",
      "\tspeed: 0.0105s/iter; left time: 90.2665s\n",
      "Epoch: 5 cost time: 6.260727643966675\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1833853 Vali Loss: 0.0377534 Test Loss: 0.1163326\n",
      "Validation loss decreased (0.038262 --> 0.037753).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1641723\n",
      "\tspeed: 0.0381s/iter; left time: 321.7358s\n",
      "\titers: 200, epoch: 6 | loss: 0.1214992\n",
      "\tspeed: 0.0106s/iter; left time: 88.1058s\n",
      "\titers: 300, epoch: 6 | loss: 0.1937904\n",
      "\tspeed: 0.0105s/iter; left time: 86.9854s\n",
      "\titers: 400, epoch: 6 | loss: 0.2075890\n",
      "\tspeed: 0.0106s/iter; left time: 86.2214s\n",
      "\titers: 500, epoch: 6 | loss: 0.1625378\n",
      "\tspeed: 0.0106s/iter; left time: 85.1962s\n",
      "Epoch: 6 cost time: 6.295174837112427\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1814093 Vali Loss: 0.0374731 Test Loss: 0.1154826\n",
      "Validation loss decreased (0.037753 --> 0.037473).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2088200\n",
      "\tspeed: 0.0373s/iter; left time: 294.0798s\n",
      "\titers: 200, epoch: 7 | loss: 0.2539992\n",
      "\tspeed: 0.0106s/iter; left time: 82.3514s\n",
      "\titers: 300, epoch: 7 | loss: 0.1518456\n",
      "\tspeed: 0.0105s/iter; left time: 80.6905s\n",
      "\titers: 400, epoch: 7 | loss: 0.1531211\n",
      "\tspeed: 0.0105s/iter; left time: 79.6147s\n",
      "\titers: 500, epoch: 7 | loss: 0.1590254\n",
      "\tspeed: 0.0105s/iter; left time: 78.7829s\n",
      "Epoch: 7 cost time: 6.292436599731445\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1806771 Vali Loss: 0.0374486 Test Loss: 0.1151799\n",
      "Validation loss decreased (0.037473 --> 0.037449).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1841307\n",
      "\tspeed: 0.0375s/iter; left time: 274.0773s\n",
      "\titers: 200, epoch: 8 | loss: 0.1829584\n",
      "\tspeed: 0.0105s/iter; left time: 76.0000s\n",
      "\titers: 300, epoch: 8 | loss: 0.2311469\n",
      "\tspeed: 0.0105s/iter; left time: 74.8968s\n",
      "\titers: 400, epoch: 8 | loss: 0.2344930\n",
      "\tspeed: 0.0105s/iter; left time: 73.8518s\n",
      "\titers: 500, epoch: 8 | loss: 0.1260647\n",
      "\tspeed: 0.0105s/iter; left time: 72.6312s\n",
      "Epoch: 8 cost time: 6.350559234619141\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1801370 Vali Loss: 0.0372930 Test Loss: 0.1151874\n",
      "Validation loss decreased (0.037449 --> 0.037293).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2357312\n",
      "\tspeed: 0.0369s/iter; left time: 248.6211s\n",
      "\titers: 200, epoch: 9 | loss: 0.1606665\n",
      "\tspeed: 0.0106s/iter; left time: 70.4337s\n",
      "\titers: 300, epoch: 9 | loss: 0.0794805\n",
      "\tspeed: 0.0106s/iter; left time: 69.5638s\n",
      "\titers: 400, epoch: 9 | loss: 0.1917628\n",
      "\tspeed: 0.0106s/iter; left time: 68.5037s\n",
      "\titers: 500, epoch: 9 | loss: 0.1591353\n",
      "\tspeed: 0.0106s/iter; left time: 67.2944s\n",
      "Epoch: 9 cost time: 6.3471996784210205\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1792545 Vali Loss: 0.0375814 Test Loss: 0.1152591\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.2054558\n",
      "\tspeed: 0.0372s/iter; left time: 229.2767s\n",
      "\titers: 200, epoch: 10 | loss: 0.1715946\n",
      "\tspeed: 0.0106s/iter; left time: 64.5890s\n",
      "\titers: 300, epoch: 10 | loss: 0.1450879\n",
      "\tspeed: 0.0106s/iter; left time: 63.3431s\n",
      "\titers: 400, epoch: 10 | loss: 0.1791696\n",
      "\tspeed: 0.0106s/iter; left time: 62.4282s\n",
      "\titers: 500, epoch: 10 | loss: 0.2627719\n",
      "\tspeed: 0.0106s/iter; left time: 61.3526s\n",
      "Epoch: 10 cost time: 6.446164131164551\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1789110 Vali Loss: 0.0374061 Test Loss: 0.1154338\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2018587\n",
      "\tspeed: 0.0360s/iter; left time: 201.8990s\n",
      "\titers: 200, epoch: 11 | loss: 0.1902418\n",
      "\tspeed: 0.0105s/iter; left time: 58.0135s\n",
      "\titers: 300, epoch: 11 | loss: 0.1466813\n",
      "\tspeed: 0.0105s/iter; left time: 56.8456s\n",
      "\titers: 400, epoch: 11 | loss: 0.2677454\n",
      "\tspeed: 0.0105s/iter; left time: 55.8593s\n",
      "\titers: 500, epoch: 11 | loss: 0.1518935\n",
      "\tspeed: 0.0105s/iter; left time: 54.7653s\n",
      "Epoch: 11 cost time: 6.289125442504883\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1808354 Vali Loss: 0.0372992 Test Loss: 0.1152959\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1153513640165329, mae:0.206453338265419\n",
      ">>> LR=1e-4,DO=0.2,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1828886\n",
      "\tspeed: 0.0233s/iter; left time: 263.2814s\n",
      "\titers: 200, epoch: 1 | loss: 0.2457823\n",
      "\tspeed: 0.0116s/iter; left time: 129.5680s\n",
      "\titers: 300, epoch: 1 | loss: 0.3082028\n",
      "\tspeed: 0.0115s/iter; left time: 127.9223s\n",
      "\titers: 400, epoch: 1 | loss: 0.2259460\n",
      "\tspeed: 0.0115s/iter; left time: 126.9674s\n",
      "\titers: 500, epoch: 1 | loss: 0.2219390\n",
      "\tspeed: 0.0116s/iter; left time: 126.2234s\n",
      "Epoch: 1 cost time: 7.804235219955444\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2199660 Vali Loss: 0.0366850 Test Loss: 0.1126287\n",
      "Validation loss decreased (inf --> 0.036685).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1593059\n",
      "\tspeed: 0.0370s/iter; left time: 397.0967s\n",
      "\titers: 200, epoch: 2 | loss: 0.2054645\n",
      "\tspeed: 0.0103s/iter; left time: 109.4674s\n",
      "\titers: 300, epoch: 2 | loss: 0.1543722\n",
      "\tspeed: 0.0103s/iter; left time: 108.3013s\n",
      "\titers: 400, epoch: 2 | loss: 0.1701510\n",
      "\tspeed: 0.0103s/iter; left time: 107.4670s\n",
      "\titers: 500, epoch: 2 | loss: 0.2060413\n",
      "\tspeed: 0.0103s/iter; left time: 106.1873s\n",
      "Epoch: 2 cost time: 6.178025484085083\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1892942 Vali Loss: 0.0348876 Test Loss: 0.1103104\n",
      "Validation loss decreased (0.036685 --> 0.034888).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1005346\n",
      "\tspeed: 0.0373s/iter; left time: 379.0005s\n",
      "\titers: 200, epoch: 3 | loss: 0.1503472\n",
      "\tspeed: 0.0104s/iter; left time: 104.1321s\n",
      "\titers: 300, epoch: 3 | loss: 0.3203785\n",
      "\tspeed: 0.0103s/iter; left time: 102.9715s\n",
      "\titers: 400, epoch: 3 | loss: 0.1883775\n",
      "\tspeed: 0.0103s/iter; left time: 101.8913s\n",
      "\titers: 500, epoch: 3 | loss: 0.1290856\n",
      "\tspeed: 0.0103s/iter; left time: 100.4712s\n",
      "Epoch: 3 cost time: 6.207367181777954\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1770942 Vali Loss: 0.0338217 Test Loss: 0.1093030\n",
      "Validation loss decreased (0.034888 --> 0.033822).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1441337\n",
      "\tspeed: 0.0372s/iter; left time: 356.9251s\n",
      "\titers: 200, epoch: 4 | loss: 0.1593233\n",
      "\tspeed: 0.0103s/iter; left time: 98.1835s\n",
      "\titers: 300, epoch: 4 | loss: 0.1120121\n",
      "\tspeed: 0.0103s/iter; left time: 97.0027s\n",
      "\titers: 400, epoch: 4 | loss: 0.1324253\n",
      "\tspeed: 0.0103s/iter; left time: 96.0166s\n",
      "\titers: 500, epoch: 4 | loss: 0.2130708\n",
      "\tspeed: 0.0103s/iter; left time: 94.5907s\n",
      "Epoch: 4 cost time: 6.218059778213501\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1718756 Vali Loss: 0.0328530 Test Loss: 0.1078141\n",
      "Validation loss decreased (0.033822 --> 0.032853).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1337461\n",
      "\tspeed: 0.0372s/iter; left time: 335.9640s\n",
      "\titers: 200, epoch: 5 | loss: 0.2215776\n",
      "\tspeed: 0.0110s/iter; left time: 98.2454s\n",
      "\titers: 300, epoch: 5 | loss: 0.1527465\n",
      "\tspeed: 0.0103s/iter; left time: 91.2822s\n",
      "\titers: 400, epoch: 5 | loss: 0.1537911\n",
      "\tspeed: 0.0104s/iter; left time: 90.3574s\n",
      "\titers: 500, epoch: 5 | loss: 0.1676764\n",
      "\tspeed: 0.0103s/iter; left time: 89.1868s\n",
      "Epoch: 5 cost time: 6.4047791957855225\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1691386 Vali Loss: 0.0326039 Test Loss: 0.1077918\n",
      "Validation loss decreased (0.032853 --> 0.032604).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1157265\n",
      "\tspeed: 0.0376s/iter; left time: 317.9457s\n",
      "\titers: 200, epoch: 6 | loss: 0.2231238\n",
      "\tspeed: 0.0103s/iter; left time: 86.2264s\n",
      "\titers: 300, epoch: 6 | loss: 0.1714498\n",
      "\tspeed: 0.0103s/iter; left time: 85.2704s\n",
      "\titers: 400, epoch: 6 | loss: 0.1467870\n",
      "\tspeed: 0.0103s/iter; left time: 84.2614s\n",
      "\titers: 500, epoch: 6 | loss: 0.1083220\n",
      "\tspeed: 0.0103s/iter; left time: 83.2712s\n",
      "Epoch: 6 cost time: 6.191277027130127\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1677612 Vali Loss: 0.0324452 Test Loss: 0.1069785\n",
      "Validation loss decreased (0.032604 --> 0.032445).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1656626\n",
      "\tspeed: 0.0366s/iter; left time: 288.1499s\n",
      "\titers: 200, epoch: 7 | loss: 0.1971352\n",
      "\tspeed: 0.0103s/iter; left time: 79.8674s\n",
      "\titers: 300, epoch: 7 | loss: 0.2850979\n",
      "\tspeed: 0.0103s/iter; left time: 79.0563s\n",
      "\titers: 400, epoch: 7 | loss: 0.2115485\n",
      "\tspeed: 0.0103s/iter; left time: 77.8262s\n",
      "\titers: 500, epoch: 7 | loss: 0.1697742\n",
      "\tspeed: 0.0103s/iter; left time: 76.7433s\n",
      "Epoch: 7 cost time: 6.166748046875\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1670734 Vali Loss: 0.0323769 Test Loss: 0.1075213\n",
      "Validation loss decreased (0.032445 --> 0.032377).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1333192\n",
      "\tspeed: 0.0374s/iter; left time: 273.0782s\n",
      "\titers: 200, epoch: 8 | loss: 0.1212558\n",
      "\tspeed: 0.0104s/iter; left time: 74.8812s\n",
      "\titers: 300, epoch: 8 | loss: 0.1591988\n",
      "\tspeed: 0.0104s/iter; left time: 73.6088s\n",
      "\titers: 400, epoch: 8 | loss: 0.1547630\n",
      "\tspeed: 0.0104s/iter; left time: 72.6936s\n",
      "\titers: 500, epoch: 8 | loss: 0.1005236\n",
      "\tspeed: 0.0104s/iter; left time: 71.6409s\n",
      "Epoch: 8 cost time: 6.208264350891113\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1678574 Vali Loss: 0.0322266 Test Loss: 0.1071254\n",
      "Validation loss decreased (0.032377 --> 0.032227).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1429574\n",
      "\tspeed: 0.0371s/iter; left time: 250.1331s\n",
      "\titers: 200, epoch: 9 | loss: 0.1759467\n",
      "\tspeed: 0.0104s/iter; left time: 68.7700s\n",
      "\titers: 300, epoch: 9 | loss: 0.1407218\n",
      "\tspeed: 0.0104s/iter; left time: 67.7576s\n",
      "\titers: 400, epoch: 9 | loss: 0.2024473\n",
      "\tspeed: 0.0103s/iter; left time: 66.5546s\n",
      "\titers: 500, epoch: 9 | loss: 0.1664674\n",
      "\tspeed: 0.0103s/iter; left time: 65.6058s\n",
      "Epoch: 9 cost time: 6.219608545303345\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1662162 Vali Loss: 0.0323011 Test Loss: 0.1072334\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1816216\n",
      "\tspeed: 0.0360s/iter; left time: 222.1664s\n",
      "\titers: 200, epoch: 10 | loss: 0.1361496\n",
      "\tspeed: 0.0104s/iter; left time: 63.0954s\n",
      "\titers: 300, epoch: 10 | loss: 0.1309139\n",
      "\tspeed: 0.0104s/iter; left time: 61.8348s\n",
      "\titers: 400, epoch: 10 | loss: 0.1110898\n",
      "\tspeed: 0.0104s/iter; left time: 60.9238s\n",
      "\titers: 500, epoch: 10 | loss: 0.1107252\n",
      "\tspeed: 0.0104s/iter; left time: 59.9082s\n",
      "Epoch: 10 cost time: 6.213876962661743\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1679023 Vali Loss: 0.0323042 Test Loss: 0.1072503\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1647340\n",
      "\tspeed: 0.0356s/iter; left time: 199.6754s\n",
      "\titers: 200, epoch: 11 | loss: 0.1793726\n",
      "\tspeed: 0.0103s/iter; left time: 56.7992s\n",
      "\titers: 300, epoch: 11 | loss: 0.1983196\n",
      "\tspeed: 0.0103s/iter; left time: 55.8906s\n",
      "\titers: 400, epoch: 11 | loss: 0.1548596\n",
      "\tspeed: 0.0103s/iter; left time: 54.7482s\n",
      "\titers: 500, epoch: 11 | loss: 0.1399179\n",
      "\tspeed: 0.0103s/iter; left time: 53.5098s\n",
      "Epoch: 11 cost time: 6.145546913146973\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1657922 Vali Loss: 0.0321813 Test Loss: 0.1072216\n",
      "Validation loss decreased (0.032227 --> 0.032181).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1445267\n",
      "\tspeed: 0.0362s/iter; left time: 182.1525s\n",
      "\titers: 200, epoch: 12 | loss: 0.1725959\n",
      "\tspeed: 0.0103s/iter; left time: 50.9845s\n",
      "\titers: 300, epoch: 12 | loss: 0.1363489\n",
      "\tspeed: 0.0103s/iter; left time: 49.6942s\n",
      "\titers: 400, epoch: 12 | loss: 0.2557544\n",
      "\tspeed: 0.0103s/iter; left time: 48.6767s\n",
      "\titers: 500, epoch: 12 | loss: 0.1478703\n",
      "\tspeed: 0.0103s/iter; left time: 47.7447s\n",
      "Epoch: 12 cost time: 6.214647054672241\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1666862 Vali Loss: 0.0322796 Test Loss: 0.1072213\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.2649360\n",
      "\tspeed: 0.0354s/iter; left time: 158.0790s\n",
      "\titers: 200, epoch: 13 | loss: 0.2243658\n",
      "\tspeed: 0.0102s/iter; left time: 44.6372s\n",
      "\titers: 300, epoch: 13 | loss: 0.1305198\n",
      "\tspeed: 0.0102s/iter; left time: 43.5969s\n",
      "\titers: 400, epoch: 13 | loss: 0.1820780\n",
      "\tspeed: 0.0102s/iter; left time: 42.5733s\n",
      "\titers: 500, epoch: 13 | loss: 0.1925435\n",
      "\tspeed: 0.0102s/iter; left time: 41.5580s\n",
      "Epoch: 13 cost time: 6.112804889678955\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1665053 Vali Loss: 0.0321626 Test Loss: 0.1072109\n",
      "Validation loss decreased (0.032181 --> 0.032163).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1634994\n",
      "\tspeed: 0.0360s/iter; left time: 140.0623s\n",
      "\titers: 200, epoch: 14 | loss: 0.1243813\n",
      "\tspeed: 0.0102s/iter; left time: 38.7174s\n",
      "\titers: 300, epoch: 14 | loss: 0.1844851\n",
      "\tspeed: 0.0102s/iter; left time: 37.7263s\n",
      "\titers: 400, epoch: 14 | loss: 0.1706056\n",
      "\tspeed: 0.0102s/iter; left time: 36.7342s\n",
      "\titers: 500, epoch: 14 | loss: 0.1824636\n",
      "\tspeed: 0.0102s/iter; left time: 35.6889s\n",
      "Epoch: 14 cost time: 6.122521638870239\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1669251 Vali Loss: 0.0322283 Test Loss: 0.1072074\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1038841\n",
      "\tspeed: 0.0376s/iter; left time: 124.7426s\n",
      "\titers: 200, epoch: 15 | loss: 0.1738171\n",
      "\tspeed: 0.0103s/iter; left time: 33.3287s\n",
      "\titers: 300, epoch: 15 | loss: 0.1616683\n",
      "\tspeed: 0.0103s/iter; left time: 32.2772s\n",
      "\titers: 400, epoch: 15 | loss: 0.1510081\n",
      "\tspeed: 0.0103s/iter; left time: 31.1481s\n",
      "\titers: 500, epoch: 15 | loss: 0.1392745\n",
      "\tspeed: 0.0103s/iter; left time: 30.1695s\n",
      "Epoch: 15 cost time: 6.190774440765381\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1655932 Vali Loss: 0.0321591 Test Loss: 0.1072100\n",
      "Validation loss decreased (0.032163 --> 0.032159).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.2543693\n",
      "\tspeed: 0.0372s/iter; left time: 102.3199s\n",
      "\titers: 200, epoch: 16 | loss: 0.1520774\n",
      "\tspeed: 0.0103s/iter; left time: 27.3810s\n",
      "\titers: 300, epoch: 16 | loss: 0.2085544\n",
      "\tspeed: 0.0103s/iter; left time: 26.2609s\n",
      "\titers: 400, epoch: 16 | loss: 0.1595235\n",
      "\tspeed: 0.0103s/iter; left time: 25.2975s\n",
      "\titers: 500, epoch: 16 | loss: 0.2832646\n",
      "\tspeed: 0.0103s/iter; left time: 24.2661s\n",
      "Epoch: 16 cost time: 6.1795878410339355\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1666242 Vali Loss: 0.0321436 Test Loss: 0.1072102\n",
      "Validation loss decreased (0.032159 --> 0.032144).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "\titers: 100, epoch: 17 | loss: 0.1185087\n",
      "\tspeed: 0.0368s/iter; left time: 80.2188s\n",
      "\titers: 200, epoch: 17 | loss: 0.1497921\n",
      "\tspeed: 0.0104s/iter; left time: 21.5941s\n",
      "\titers: 300, epoch: 17 | loss: 0.1590874\n",
      "\tspeed: 0.0104s/iter; left time: 20.5105s\n",
      "\titers: 400, epoch: 17 | loss: 0.1416201\n",
      "\tspeed: 0.0104s/iter; left time: 19.4950s\n",
      "\titers: 500, epoch: 17 | loss: 0.1077986\n",
      "\tspeed: 0.0104s/iter; left time: 18.4916s\n",
      "Epoch: 17 cost time: 6.239096403121948\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1670342 Vali Loss: 0.0322758 Test Loss: 0.1072100\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.2016373\n",
      "\tspeed: 0.0356s/iter; left time: 57.3499s\n",
      "\titers: 200, epoch: 18 | loss: 0.1689910\n",
      "\tspeed: 0.0103s/iter; left time: 15.6018s\n",
      "\titers: 300, epoch: 18 | loss: 0.1362761\n",
      "\tspeed: 0.0103s/iter; left time: 14.5280s\n",
      "\titers: 400, epoch: 18 | loss: 0.2473616\n",
      "\tspeed: 0.0103s/iter; left time: 13.5283s\n",
      "\titers: 500, epoch: 18 | loss: 0.1303532\n",
      "\tspeed: 0.0103s/iter; left time: 12.4929s\n",
      "Epoch: 18 cost time: 6.161825656890869\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.1664147 Vali Loss: 0.0322466 Test Loss: 0.1072100\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "\titers: 100, epoch: 19 | loss: 0.1495986\n",
      "\tspeed: 0.0358s/iter; left time: 37.2607s\n",
      "\titers: 200, epoch: 19 | loss: 0.1992128\n",
      "\tspeed: 0.0104s/iter; left time: 9.7462s\n",
      "\titers: 300, epoch: 19 | loss: 0.1432451\n",
      "\tspeed: 0.0104s/iter; left time: 8.7084s\n",
      "\titers: 400, epoch: 19 | loss: 0.1850358\n",
      "\tspeed: 0.0103s/iter; left time: 7.6608s\n",
      "\titers: 500, epoch: 19 | loss: 0.1601970\n",
      "\tspeed: 0.0104s/iter; left time: 6.6377s\n",
      "Epoch: 19 cost time: 6.194748163223267\n",
      "Epoch: 19, Steps: 570 | Train Loss: 0.1664159 Vali Loss: 0.0322582 Test Loss: 0.1072100\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1073606014251709, mae:0.19618366658687592\n",
      ">>> LR=1e-4,DO=0.2,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2698248\n",
      "\tspeed: 0.0235s/iter; left time: 265.9032s\n",
      "\titers: 200, epoch: 1 | loss: 0.1963461\n",
      "\tspeed: 0.0117s/iter; left time: 130.4981s\n",
      "\titers: 300, epoch: 1 | loss: 0.1730838\n",
      "\tspeed: 0.0116s/iter; left time: 128.8122s\n",
      "\titers: 400, epoch: 1 | loss: 0.1825804\n",
      "\tspeed: 0.0116s/iter; left time: 127.5995s\n",
      "\titers: 500, epoch: 1 | loss: 0.2201471\n",
      "\tspeed: 0.0116s/iter; left time: 126.6893s\n",
      "Epoch: 1 cost time: 7.867076396942139\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2553634 Vali Loss: 0.0487838 Test Loss: 0.1375273\n",
      "Validation loss decreased (inf --> 0.048784).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2197944\n",
      "\tspeed: 0.0373s/iter; left time: 400.0725s\n",
      "\titers: 200, epoch: 2 | loss: 0.2865478\n",
      "\tspeed: 0.0103s/iter; left time: 109.8099s\n",
      "\titers: 300, epoch: 2 | loss: 0.2022398\n",
      "\tspeed: 0.0103s/iter; left time: 108.7823s\n",
      "\titers: 400, epoch: 2 | loss: 0.2947821\n",
      "\tspeed: 0.0104s/iter; left time: 108.0542s\n",
      "\titers: 500, epoch: 2 | loss: 0.2457532\n",
      "\tspeed: 0.0103s/iter; left time: 106.8363s\n",
      "Epoch: 2 cost time: 6.185974836349487\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2137631 Vali Loss: 0.0405234 Test Loss: 0.1260065\n",
      "Validation loss decreased (0.048784 --> 0.040523).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1453066\n",
      "\tspeed: 0.0362s/iter; left time: 367.4504s\n",
      "\titers: 200, epoch: 3 | loss: 0.1868069\n",
      "\tspeed: 0.0103s/iter; left time: 103.7717s\n",
      "\titers: 300, epoch: 3 | loss: 0.2248434\n",
      "\tspeed: 0.0103s/iter; left time: 102.6675s\n",
      "\titers: 400, epoch: 3 | loss: 0.1697673\n",
      "\tspeed: 0.0103s/iter; left time: 101.4865s\n",
      "\titers: 500, epoch: 3 | loss: 0.1538604\n",
      "\tspeed: 0.0103s/iter; left time: 100.4576s\n",
      "Epoch: 3 cost time: 6.176154375076294\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2001636 Vali Loss: 0.0395559 Test Loss: 0.1234830\n",
      "Validation loss decreased (0.040523 --> 0.039556).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1186689\n",
      "\tspeed: 0.0360s/iter; left time: 345.3126s\n",
      "\titers: 200, epoch: 4 | loss: 0.1220308\n",
      "\tspeed: 0.0103s/iter; left time: 97.7848s\n",
      "\titers: 300, epoch: 4 | loss: 0.1631663\n",
      "\tspeed: 0.0103s/iter; left time: 96.7646s\n",
      "\titers: 400, epoch: 4 | loss: 0.2533978\n",
      "\tspeed: 0.0103s/iter; left time: 95.7789s\n",
      "\titers: 500, epoch: 4 | loss: 0.1756260\n",
      "\tspeed: 0.0103s/iter; left time: 94.7160s\n",
      "Epoch: 4 cost time: 6.166089773178101\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1933635 Vali Loss: 0.0392864 Test Loss: 0.1252249\n",
      "Validation loss decreased (0.039556 --> 0.039286).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1849314\n",
      "\tspeed: 0.0377s/iter; left time: 340.0861s\n",
      "\titers: 200, epoch: 5 | loss: 0.1326260\n",
      "\tspeed: 0.0103s/iter; left time: 91.8090s\n",
      "\titers: 300, epoch: 5 | loss: 0.2090940\n",
      "\tspeed: 0.0103s/iter; left time: 90.8862s\n",
      "\titers: 400, epoch: 5 | loss: 0.1819342\n",
      "\tspeed: 0.0103s/iter; left time: 89.8603s\n",
      "\titers: 500, epoch: 5 | loss: 0.2528400\n",
      "\tspeed: 0.0103s/iter; left time: 88.7903s\n",
      "Epoch: 5 cost time: 6.191557168960571\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1899995 Vali Loss: 0.0387793 Test Loss: 0.1243352\n",
      "Validation loss decreased (0.039286 --> 0.038779).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1329355\n",
      "\tspeed: 0.0366s/iter; left time: 309.5955s\n",
      "\titers: 200, epoch: 6 | loss: 0.1990060\n",
      "\tspeed: 0.0103s/iter; left time: 86.2847s\n",
      "\titers: 300, epoch: 6 | loss: 0.1675856\n",
      "\tspeed: 0.0103s/iter; left time: 85.2305s\n",
      "\titers: 400, epoch: 6 | loss: 0.1494978\n",
      "\tspeed: 0.0103s/iter; left time: 84.1955s\n",
      "\titers: 500, epoch: 6 | loss: 0.1950927\n",
      "\tspeed: 0.0103s/iter; left time: 83.2082s\n",
      "Epoch: 6 cost time: 6.195836782455444\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1885858 Vali Loss: 0.0383679 Test Loss: 0.1227845\n",
      "Validation loss decreased (0.038779 --> 0.038368).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1334369\n",
      "\tspeed: 0.0383s/iter; left time: 301.8590s\n",
      "\titers: 200, epoch: 7 | loss: 0.1708986\n",
      "\tspeed: 0.0104s/iter; left time: 80.8702s\n",
      "\titers: 300, epoch: 7 | loss: 0.1579319\n",
      "\tspeed: 0.0104s/iter; left time: 79.6527s\n",
      "\titers: 400, epoch: 7 | loss: 0.1086935\n",
      "\tspeed: 0.0104s/iter; left time: 78.4813s\n",
      "\titers: 500, epoch: 7 | loss: 0.2146642\n",
      "\tspeed: 0.0104s/iter; left time: 77.4886s\n",
      "Epoch: 7 cost time: 6.218514442443848\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1858311 Vali Loss: 0.0383617 Test Loss: 0.1226782\n",
      "Validation loss decreased (0.038368 --> 0.038362).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1608976\n",
      "\tspeed: 0.0377s/iter; left time: 275.6333s\n",
      "\titers: 200, epoch: 8 | loss: 0.1631071\n",
      "\tspeed: 0.0117s/iter; left time: 84.1102s\n",
      "\titers: 300, epoch: 8 | loss: 0.2231359\n",
      "\tspeed: 0.0116s/iter; left time: 82.4029s\n",
      "\titers: 400, epoch: 8 | loss: 0.2170218\n",
      "\tspeed: 0.0113s/iter; left time: 79.2691s\n",
      "\titers: 500, epoch: 8 | loss: 0.1870292\n",
      "\tspeed: 0.0103s/iter; left time: 70.9133s\n",
      "Epoch: 8 cost time: 6.6744866371154785\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1859616 Vali Loss: 0.0384163 Test Loss: 0.1231131\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1614467\n",
      "\tspeed: 0.0378s/iter; left time: 254.6681s\n",
      "\titers: 200, epoch: 9 | loss: 0.2182490\n",
      "\tspeed: 0.0103s/iter; left time: 68.2997s\n",
      "\titers: 300, epoch: 9 | loss: 0.1493307\n",
      "\tspeed: 0.0103s/iter; left time: 67.3449s\n",
      "\titers: 400, epoch: 9 | loss: 0.1383214\n",
      "\tspeed: 0.0103s/iter; left time: 66.3577s\n",
      "\titers: 500, epoch: 9 | loss: 0.1916260\n",
      "\tspeed: 0.0103s/iter; left time: 65.3322s\n",
      "Epoch: 9 cost time: 6.186264276504517\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1843808 Vali Loss: 0.0384711 Test Loss: 0.1228933\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1567998\n",
      "\tspeed: 0.0360s/iter; left time: 222.1473s\n",
      "\titers: 200, epoch: 10 | loss: 0.2485952\n",
      "\tspeed: 0.0103s/iter; left time: 62.4432s\n",
      "\titers: 300, epoch: 10 | loss: 0.2517125\n",
      "\tspeed: 0.0103s/iter; left time: 61.2843s\n",
      "\titers: 400, epoch: 10 | loss: 0.2100986\n",
      "\tspeed: 0.0103s/iter; left time: 60.3639s\n",
      "\titers: 500, epoch: 10 | loss: 0.2018932\n",
      "\tspeed: 0.0103s/iter; left time: 59.2533s\n",
      "Epoch: 10 cost time: 6.180964946746826\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1854664 Vali Loss: 0.0385537 Test Loss: 0.1228057\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12285853922367096, mae:0.21647882461547852\n",
      ">>> LR=1e-4,DO=0.2,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2728188\n",
      "\tspeed: 0.0235s/iter; left time: 265.8643s\n",
      "\titers: 200, epoch: 1 | loss: 0.2574952\n",
      "\tspeed: 0.0121s/iter; left time: 135.0741s\n",
      "\titers: 300, epoch: 1 | loss: 0.1812697\n",
      "\tspeed: 0.0129s/iter; left time: 143.0089s\n",
      "\titers: 400, epoch: 1 | loss: 0.1532026\n",
      "\tspeed: 0.0129s/iter; left time: 141.8252s\n",
      "\titers: 500, epoch: 1 | loss: 0.2401675\n",
      "\tspeed: 0.0129s/iter; left time: 141.0127s\n",
      "Epoch: 1 cost time: 8.387934684753418\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2418825 Vali Loss: 0.0426715 Test Loss: 0.1281548\n",
      "Validation loss decreased (inf --> 0.042671).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2165265\n",
      "\tspeed: 0.0419s/iter; left time: 449.7705s\n",
      "\titers: 200, epoch: 2 | loss: 0.1754232\n",
      "\tspeed: 0.0118s/iter; left time: 125.6789s\n",
      "\titers: 300, epoch: 2 | loss: 0.1547919\n",
      "\tspeed: 0.0118s/iter; left time: 124.2874s\n",
      "\titers: 400, epoch: 2 | loss: 0.1172236\n",
      "\tspeed: 0.0118s/iter; left time: 123.2965s\n",
      "\titers: 500, epoch: 2 | loss: 0.2130183\n",
      "\tspeed: 0.0118s/iter; left time: 122.0062s\n",
      "Epoch: 2 cost time: 7.033994913101196\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2053043 Vali Loss: 0.0398624 Test Loss: 0.1240785\n",
      "Validation loss decreased (0.042671 --> 0.039862).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2351242\n",
      "\tspeed: 0.0394s/iter; left time: 400.6150s\n",
      "\titers: 200, epoch: 3 | loss: 0.2170653\n",
      "\tspeed: 0.0119s/iter; left time: 119.8015s\n",
      "\titers: 300, epoch: 3 | loss: 0.1421339\n",
      "\tspeed: 0.0119s/iter; left time: 118.6679s\n",
      "\titers: 400, epoch: 3 | loss: 0.1911538\n",
      "\tspeed: 0.0119s/iter; left time: 117.5824s\n",
      "\titers: 500, epoch: 3 | loss: 0.1371060\n",
      "\tspeed: 0.0119s/iter; left time: 116.1668s\n",
      "Epoch: 3 cost time: 7.10011100769043\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1833844 Vali Loss: 0.0381102 Test Loss: 0.1208010\n",
      "Validation loss decreased (0.039862 --> 0.038110).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1335539\n",
      "\tspeed: 0.0390s/iter; left time: 373.6727s\n",
      "\titers: 200, epoch: 4 | loss: 0.2521101\n",
      "\tspeed: 0.0119s/iter; left time: 112.7409s\n",
      "\titers: 300, epoch: 4 | loss: 0.1180318\n",
      "\tspeed: 0.0119s/iter; left time: 111.2884s\n",
      "\titers: 400, epoch: 4 | loss: 0.1190270\n",
      "\tspeed: 0.0119s/iter; left time: 110.1127s\n",
      "\titers: 500, epoch: 4 | loss: 0.1296462\n",
      "\tspeed: 0.0119s/iter; left time: 108.9469s\n",
      "Epoch: 4 cost time: 7.074910640716553\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1727422 Vali Loss: 0.0359602 Test Loss: 0.1186741\n",
      "Validation loss decreased (0.038110 --> 0.035960).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1812001\n",
      "\tspeed: 0.0389s/iter; left time: 350.6290s\n",
      "\titers: 200, epoch: 5 | loss: 0.1499797\n",
      "\tspeed: 0.0119s/iter; left time: 105.8755s\n",
      "\titers: 300, epoch: 5 | loss: 0.2162277\n",
      "\tspeed: 0.0119s/iter; left time: 104.8781s\n",
      "\titers: 400, epoch: 5 | loss: 0.1553775\n",
      "\tspeed: 0.0119s/iter; left time: 103.5422s\n",
      "\titers: 500, epoch: 5 | loss: 0.1368521\n",
      "\tspeed: 0.0119s/iter; left time: 102.3341s\n",
      "Epoch: 5 cost time: 7.094435691833496\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1688252 Vali Loss: 0.0367247 Test Loss: 0.1190778\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1485026\n",
      "\tspeed: 0.0393s/iter; left time: 332.0156s\n",
      "\titers: 200, epoch: 6 | loss: 0.0967241\n",
      "\tspeed: 0.0119s/iter; left time: 99.4085s\n",
      "\titers: 300, epoch: 6 | loss: 0.2068383\n",
      "\tspeed: 0.0119s/iter; left time: 98.0356s\n",
      "\titers: 400, epoch: 6 | loss: 0.2037544\n",
      "\tspeed: 0.0119s/iter; left time: 96.6778s\n",
      "\titers: 500, epoch: 6 | loss: 0.1254910\n",
      "\tspeed: 0.0119s/iter; left time: 95.6761s\n",
      "Epoch: 6 cost time: 7.0502238273620605\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1657459 Vali Loss: 0.0353277 Test Loss: 0.1163611\n",
      "Validation loss decreased (0.035960 --> 0.035328).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1641413\n",
      "\tspeed: 0.0399s/iter; left time: 314.7786s\n",
      "\titers: 200, epoch: 7 | loss: 0.1582499\n",
      "\tspeed: 0.0119s/iter; left time: 92.2538s\n",
      "\titers: 300, epoch: 7 | loss: 0.1567524\n",
      "\tspeed: 0.0119s/iter; left time: 91.3198s\n",
      "\titers: 400, epoch: 7 | loss: 0.1162649\n",
      "\tspeed: 0.0119s/iter; left time: 90.0086s\n",
      "\titers: 500, epoch: 7 | loss: 0.1505215\n",
      "\tspeed: 0.0119s/iter; left time: 88.6570s\n",
      "Epoch: 7 cost time: 7.076368808746338\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1633433 Vali Loss: 0.0350584 Test Loss: 0.1166410\n",
      "Validation loss decreased (0.035328 --> 0.035058).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1034093\n",
      "\tspeed: 0.0413s/iter; left time: 302.1956s\n",
      "\titers: 200, epoch: 8 | loss: 0.1616497\n",
      "\tspeed: 0.0130s/iter; left time: 93.3826s\n",
      "\titers: 300, epoch: 8 | loss: 0.1574582\n",
      "\tspeed: 0.0130s/iter; left time: 92.3965s\n",
      "\titers: 400, epoch: 8 | loss: 0.1227569\n",
      "\tspeed: 0.0129s/iter; left time: 90.5872s\n",
      "\titers: 500, epoch: 8 | loss: 0.1155061\n",
      "\tspeed: 0.0129s/iter; left time: 89.4104s\n",
      "Epoch: 8 cost time: 7.688216924667358\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1651215 Vali Loss: 0.0351653 Test Loss: 0.1164627\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1255578\n",
      "\tspeed: 0.0394s/iter; left time: 265.5216s\n",
      "\titers: 200, epoch: 9 | loss: 0.1620957\n",
      "\tspeed: 0.0119s/iter; left time: 78.7473s\n",
      "\titers: 300, epoch: 9 | loss: 0.1645151\n",
      "\tspeed: 0.0119s/iter; left time: 77.5623s\n",
      "\titers: 400, epoch: 9 | loss: 0.1184326\n",
      "\tspeed: 0.0119s/iter; left time: 76.3395s\n",
      "\titers: 500, epoch: 9 | loss: 0.2043733\n",
      "\tspeed: 0.0119s/iter; left time: 75.1463s\n",
      "Epoch: 9 cost time: 7.0228376388549805\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1640864 Vali Loss: 0.0350849 Test Loss: 0.1165322\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1445729\n",
      "\tspeed: 0.0395s/iter; left time: 243.8942s\n",
      "\titers: 200, epoch: 10 | loss: 0.1848298\n",
      "\tspeed: 0.0119s/iter; left time: 72.2299s\n",
      "\titers: 300, epoch: 10 | loss: 0.1493964\n",
      "\tspeed: 0.0119s/iter; left time: 70.9479s\n",
      "\titers: 400, epoch: 10 | loss: 0.1604544\n",
      "\tspeed: 0.0119s/iter; left time: 69.7554s\n",
      "\titers: 500, epoch: 10 | loss: 0.1954258\n",
      "\tspeed: 0.0119s/iter; left time: 68.5214s\n",
      "Epoch: 10 cost time: 7.033463954925537\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1638730 Vali Loss: 0.0352154 Test Loss: 0.1164913\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11679396778345108, mae:0.21177786588668823\n",
      ">>> LR=1e-4,DO=0.2,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1363186\n",
      "\tspeed: 0.0248s/iter; left time: 280.1847s\n",
      "\titers: 200, epoch: 1 | loss: 0.3413582\n",
      "\tspeed: 0.0126s/iter; left time: 140.5833s\n",
      "\titers: 300, epoch: 1 | loss: 0.1783569\n",
      "\tspeed: 0.0125s/iter; left time: 138.9512s\n",
      "\titers: 400, epoch: 1 | loss: 0.2139740\n",
      "\tspeed: 0.0125s/iter; left time: 137.9136s\n",
      "\titers: 500, epoch: 1 | loss: 0.1596255\n",
      "\tspeed: 0.0126s/iter; left time: 137.3551s\n",
      "Epoch: 1 cost time: 8.430464506149292\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2173056 Vali Loss: 0.0368995 Test Loss: 0.1138321\n",
      "Validation loss decreased (inf --> 0.036899).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1699772\n",
      "\tspeed: 0.0408s/iter; left time: 438.1796s\n",
      "\titers: 200, epoch: 2 | loss: 0.1333708\n",
      "\tspeed: 0.0115s/iter; left time: 121.9103s\n",
      "\titers: 300, epoch: 2 | loss: 0.1933986\n",
      "\tspeed: 0.0115s/iter; left time: 120.8703s\n",
      "\titers: 400, epoch: 2 | loss: 0.2743535\n",
      "\tspeed: 0.0115s/iter; left time: 119.8692s\n",
      "\titers: 500, epoch: 2 | loss: 0.2752552\n",
      "\tspeed: 0.0115s/iter; left time: 118.5199s\n",
      "Epoch: 2 cost time: 6.876612901687622\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1849333 Vali Loss: 0.0353342 Test Loss: 0.1119078\n",
      "Validation loss decreased (0.036899 --> 0.035334).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2045713\n",
      "\tspeed: 0.0408s/iter; left time: 414.6648s\n",
      "\titers: 200, epoch: 3 | loss: 0.1044277\n",
      "\tspeed: 0.0115s/iter; left time: 115.6846s\n",
      "\titers: 300, epoch: 3 | loss: 0.1900565\n",
      "\tspeed: 0.0115s/iter; left time: 114.2630s\n",
      "\titers: 400, epoch: 3 | loss: 0.1648793\n",
      "\tspeed: 0.0114s/iter; left time: 112.8879s\n",
      "\titers: 500, epoch: 3 | loss: 0.2183758\n",
      "\tspeed: 0.0114s/iter; left time: 111.7607s\n",
      "Epoch: 3 cost time: 6.912928581237793\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1719539 Vali Loss: 0.0331079 Test Loss: 0.1087243\n",
      "Validation loss decreased (0.035334 --> 0.033108).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1481705\n",
      "\tspeed: 0.0386s/iter; left time: 369.9515s\n",
      "\titers: 200, epoch: 4 | loss: 0.1115734\n",
      "\tspeed: 0.0115s/iter; left time: 109.0901s\n",
      "\titers: 300, epoch: 4 | loss: 0.0927960\n",
      "\tspeed: 0.0115s/iter; left time: 107.8642s\n",
      "\titers: 400, epoch: 4 | loss: 0.1426336\n",
      "\tspeed: 0.0115s/iter; left time: 106.8366s\n",
      "\titers: 500, epoch: 4 | loss: 0.1575047\n",
      "\tspeed: 0.0115s/iter; left time: 105.6598s\n",
      "Epoch: 4 cost time: 6.875374794006348\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1625956 Vali Loss: 0.0322053 Test Loss: 0.1091411\n",
      "Validation loss decreased (0.033108 --> 0.032205).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1714803\n",
      "\tspeed: 0.0395s/iter; left time: 356.7159s\n",
      "\titers: 200, epoch: 5 | loss: 0.1156754\n",
      "\tspeed: 0.0115s/iter; left time: 102.4437s\n",
      "\titers: 300, epoch: 5 | loss: 0.2285974\n",
      "\tspeed: 0.0115s/iter; left time: 101.1725s\n",
      "\titers: 400, epoch: 5 | loss: 0.1490882\n",
      "\tspeed: 0.0115s/iter; left time: 99.9374s\n",
      "\titers: 500, epoch: 5 | loss: 0.1352341\n",
      "\tspeed: 0.0115s/iter; left time: 98.8161s\n",
      "Epoch: 5 cost time: 6.856023073196411\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1591634 Vali Loss: 0.0319482 Test Loss: 0.1095468\n",
      "Validation loss decreased (0.032205 --> 0.031948).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2010585\n",
      "\tspeed: 0.0386s/iter; left time: 326.3037s\n",
      "\titers: 200, epoch: 6 | loss: 0.1728979\n",
      "\tspeed: 0.0114s/iter; left time: 95.4584s\n",
      "\titers: 300, epoch: 6 | loss: 0.1127171\n",
      "\tspeed: 0.0114s/iter; left time: 94.3862s\n",
      "\titers: 400, epoch: 6 | loss: 0.1233015\n",
      "\tspeed: 0.0114s/iter; left time: 93.1818s\n",
      "\titers: 500, epoch: 6 | loss: 0.1220856\n",
      "\tspeed: 0.0114s/iter; left time: 91.9578s\n",
      "Epoch: 6 cost time: 6.83613395690918\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1572244 Vali Loss: 0.0312679 Test Loss: 0.1084600\n",
      "Validation loss decreased (0.031948 --> 0.031268).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1036759\n",
      "\tspeed: 0.0382s/iter; left time: 301.1584s\n",
      "\titers: 200, epoch: 7 | loss: 0.1983109\n",
      "\tspeed: 0.0115s/iter; left time: 89.3234s\n",
      "\titers: 300, epoch: 7 | loss: 0.1038504\n",
      "\tspeed: 0.0115s/iter; left time: 88.0879s\n",
      "\titers: 400, epoch: 7 | loss: 0.1253054\n",
      "\tspeed: 0.0115s/iter; left time: 86.9304s\n",
      "\titers: 500, epoch: 7 | loss: 0.1448579\n",
      "\tspeed: 0.0115s/iter; left time: 85.6584s\n",
      "Epoch: 7 cost time: 6.857322692871094\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1564264 Vali Loss: 0.0313811 Test Loss: 0.1085556\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1076777\n",
      "\tspeed: 0.0389s/iter; left time: 284.0689s\n",
      "\titers: 200, epoch: 8 | loss: 0.1857146\n",
      "\tspeed: 0.0114s/iter; left time: 82.3839s\n",
      "\titers: 300, epoch: 8 | loss: 0.1267212\n",
      "\tspeed: 0.0114s/iter; left time: 81.1951s\n",
      "\titers: 400, epoch: 8 | loss: 0.1391076\n",
      "\tspeed: 0.0114s/iter; left time: 80.1334s\n",
      "\titers: 500, epoch: 8 | loss: 0.1148637\n",
      "\tspeed: 0.0114s/iter; left time: 78.9707s\n",
      "Epoch: 8 cost time: 6.79107141494751\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1560815 Vali Loss: 0.0314133 Test Loss: 0.1088604\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2353387\n",
      "\tspeed: 0.0389s/iter; left time: 262.0769s\n",
      "\titers: 200, epoch: 9 | loss: 0.1030874\n",
      "\tspeed: 0.0114s/iter; left time: 75.9976s\n",
      "\titers: 300, epoch: 9 | loss: 0.1714372\n",
      "\tspeed: 0.0114s/iter; left time: 74.8217s\n",
      "\titers: 400, epoch: 9 | loss: 0.1623469\n",
      "\tspeed: 0.0114s/iter; left time: 73.6649s\n",
      "\titers: 500, epoch: 9 | loss: 0.2412311\n",
      "\tspeed: 0.0114s/iter; left time: 72.5506s\n",
      "Epoch: 9 cost time: 6.815186500549316\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1555682 Vali Loss: 0.0312154 Test Loss: 0.1087304\n",
      "Validation loss decreased (0.031268 --> 0.031215).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1525932\n",
      "\tspeed: 0.0398s/iter; left time: 245.4679s\n",
      "\titers: 200, epoch: 10 | loss: 0.1397904\n",
      "\tspeed: 0.0115s/iter; left time: 69.5891s\n",
      "\titers: 300, epoch: 10 | loss: 0.1151299\n",
      "\tspeed: 0.0114s/iter; left time: 68.3488s\n",
      "\titers: 400, epoch: 10 | loss: 0.1559325\n",
      "\tspeed: 0.0115s/iter; left time: 67.3305s\n",
      "\titers: 500, epoch: 10 | loss: 0.1672229\n",
      "\tspeed: 0.0115s/iter; left time: 66.1249s\n",
      "Epoch: 10 cost time: 6.832020282745361\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1566576 Vali Loss: 0.0312890 Test Loss: 0.1087427\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0999604\n",
      "\tspeed: 0.0377s/iter; left time: 211.0216s\n",
      "\titers: 200, epoch: 11 | loss: 0.1711862\n",
      "\tspeed: 0.0114s/iter; left time: 62.8356s\n",
      "\titers: 300, epoch: 11 | loss: 0.1427087\n",
      "\tspeed: 0.0114s/iter; left time: 61.6016s\n",
      "\titers: 400, epoch: 11 | loss: 0.1879228\n",
      "\tspeed: 0.0115s/iter; left time: 60.7664s\n",
      "\titers: 500, epoch: 11 | loss: 0.2188693\n",
      "\tspeed: 0.0115s/iter; left time: 59.8659s\n",
      "Epoch: 11 cost time: 6.840463161468506\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1565516 Vali Loss: 0.0312251 Test Loss: 0.1087238\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1394170\n",
      "\tspeed: 0.0388s/iter; left time: 195.1785s\n",
      "\titers: 200, epoch: 12 | loss: 0.1381668\n",
      "\tspeed: 0.0124s/iter; left time: 61.0061s\n",
      "\titers: 300, epoch: 12 | loss: 0.1242190\n",
      "\tspeed: 0.0115s/iter; left time: 55.6905s\n",
      "\titers: 400, epoch: 12 | loss: 0.2307628\n",
      "\tspeed: 0.0115s/iter; left time: 54.4305s\n",
      "\titers: 500, epoch: 12 | loss: 0.1490344\n",
      "\tspeed: 0.0115s/iter; left time: 53.3822s\n",
      "Epoch: 12 cost time: 7.027728080749512\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1558459 Vali Loss: 0.0314087 Test Loss: 0.1087496\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10887391120195389, mae:0.198089137673378\n",
      ">>> LR=1e-4,DO=0.2,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3059065\n",
      "\tspeed: 0.0240s/iter; left time: 270.7626s\n",
      "\titers: 200, epoch: 1 | loss: 0.2737502\n",
      "\tspeed: 0.0121s/iter; left time: 135.3325s\n",
      "\titers: 300, epoch: 1 | loss: 0.3231884\n",
      "\tspeed: 0.0120s/iter; left time: 133.6449s\n",
      "\titers: 400, epoch: 1 | loss: 0.2475859\n",
      "\tspeed: 0.0120s/iter; left time: 132.5518s\n",
      "\titers: 500, epoch: 1 | loss: 0.2284089\n",
      "\tspeed: 0.0121s/iter; left time: 131.4501s\n",
      "Epoch: 1 cost time: 8.120875358581543\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2450399 Vali Loss: 0.0431757 Test Loss: 0.1317969\n",
      "Validation loss decreased (inf --> 0.043176).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2676797\n",
      "\tspeed: 0.0399s/iter; left time: 427.8134s\n",
      "\titers: 200, epoch: 2 | loss: 0.2882361\n",
      "\tspeed: 0.0121s/iter; left time: 128.5443s\n",
      "\titers: 300, epoch: 2 | loss: 0.1485207\n",
      "\tspeed: 0.0121s/iter; left time: 127.1323s\n",
      "\titers: 400, epoch: 2 | loss: 0.1523848\n",
      "\tspeed: 0.0120s/iter; left time: 125.2999s\n",
      "\titers: 500, epoch: 2 | loss: 0.1721504\n",
      "\tspeed: 0.0108s/iter; left time: 112.0127s\n",
      "Epoch: 2 cost time: 6.976976156234741\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2084066 Vali Loss: 0.0420639 Test Loss: 0.1244295\n",
      "Validation loss decreased (0.043176 --> 0.042064).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1705528\n",
      "\tspeed: 0.0374s/iter; left time: 379.9855s\n",
      "\titers: 200, epoch: 3 | loss: 0.1598030\n",
      "\tspeed: 0.0109s/iter; left time: 109.4104s\n",
      "\titers: 300, epoch: 3 | loss: 0.2289789\n",
      "\tspeed: 0.0109s/iter; left time: 108.3635s\n",
      "\titers: 400, epoch: 3 | loss: 0.1601818\n",
      "\tspeed: 0.0109s/iter; left time: 107.4204s\n",
      "\titers: 500, epoch: 3 | loss: 0.1804475\n",
      "\tspeed: 0.0109s/iter; left time: 106.2611s\n",
      "Epoch: 3 cost time: 6.505877256393433\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1902824 Vali Loss: 0.0413035 Test Loss: 0.1218363\n",
      "Validation loss decreased (0.042064 --> 0.041304).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1751935\n",
      "\tspeed: 0.0377s/iter; left time: 361.1652s\n",
      "\titers: 200, epoch: 4 | loss: 0.1332253\n",
      "\tspeed: 0.0109s/iter; left time: 103.8486s\n",
      "\titers: 300, epoch: 4 | loss: 0.1604505\n",
      "\tspeed: 0.0110s/iter; left time: 102.9555s\n",
      "\titers: 400, epoch: 4 | loss: 0.2998212\n",
      "\tspeed: 0.0109s/iter; left time: 101.4034s\n",
      "\titers: 500, epoch: 4 | loss: 0.1957961\n",
      "\tspeed: 0.0109s/iter; left time: 100.5824s\n",
      "Epoch: 4 cost time: 6.561441421508789\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1811429 Vali Loss: 0.0391628 Test Loss: 0.1194913\n",
      "Validation loss decreased (0.041304 --> 0.039163).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1672255\n",
      "\tspeed: 0.0385s/iter; left time: 347.4179s\n",
      "\titers: 200, epoch: 5 | loss: 0.1672589\n",
      "\tspeed: 0.0109s/iter; left time: 96.9606s\n",
      "\titers: 300, epoch: 5 | loss: 0.2072868\n",
      "\tspeed: 0.0109s/iter; left time: 96.0347s\n",
      "\titers: 400, epoch: 5 | loss: 0.1369065\n",
      "\tspeed: 0.0109s/iter; left time: 94.8146s\n",
      "\titers: 500, epoch: 5 | loss: 0.2729971\n",
      "\tspeed: 0.0109s/iter; left time: 93.9972s\n",
      "Epoch: 5 cost time: 6.486450433731079\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1767657 Vali Loss: 0.0388981 Test Loss: 0.1188247\n",
      "Validation loss decreased (0.039163 --> 0.038898).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1614633\n",
      "\tspeed: 0.0373s/iter; left time: 315.3352s\n",
      "\titers: 200, epoch: 6 | loss: 0.1716885\n",
      "\tspeed: 0.0108s/iter; left time: 90.5097s\n",
      "\titers: 300, epoch: 6 | loss: 0.1799382\n",
      "\tspeed: 0.0108s/iter; left time: 89.4641s\n",
      "\titers: 400, epoch: 6 | loss: 0.1472095\n",
      "\tspeed: 0.0108s/iter; left time: 88.3888s\n",
      "\titers: 500, epoch: 6 | loss: 0.1638059\n",
      "\tspeed: 0.0108s/iter; left time: 87.2997s\n",
      "Epoch: 6 cost time: 6.5075364112854\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1739943 Vali Loss: 0.0392144 Test Loss: 0.1204562\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1445032\n",
      "\tspeed: 0.0387s/iter; left time: 304.8591s\n",
      "\titers: 200, epoch: 7 | loss: 0.1458511\n",
      "\tspeed: 0.0109s/iter; left time: 84.9106s\n",
      "\titers: 300, epoch: 7 | loss: 0.1075231\n",
      "\tspeed: 0.0109s/iter; left time: 83.7758s\n",
      "\titers: 400, epoch: 7 | loss: 0.1690814\n",
      "\tspeed: 0.0109s/iter; left time: 82.7612s\n",
      "\titers: 500, epoch: 7 | loss: 0.1312335\n",
      "\tspeed: 0.0109s/iter; left time: 81.6405s\n",
      "Epoch: 7 cost time: 6.521836042404175\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1724856 Vali Loss: 0.0389544 Test Loss: 0.1188632\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1433409\n",
      "\tspeed: 0.0369s/iter; left time: 269.4769s\n",
      "\titers: 200, epoch: 8 | loss: 0.2169636\n",
      "\tspeed: 0.0109s/iter; left time: 78.3633s\n",
      "\titers: 300, epoch: 8 | loss: 0.2177719\n",
      "\tspeed: 0.0109s/iter; left time: 77.3332s\n",
      "\titers: 400, epoch: 8 | loss: 0.1571627\n",
      "\tspeed: 0.0109s/iter; left time: 76.0941s\n",
      "\titers: 500, epoch: 8 | loss: 0.1202934\n",
      "\tspeed: 0.0109s/iter; left time: 75.1974s\n",
      "Epoch: 8 cost time: 6.474441051483154\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1722129 Vali Loss: 0.0385712 Test Loss: 0.1187824\n",
      "Validation loss decreased (0.038898 --> 0.038571).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1691125\n",
      "\tspeed: 0.0388s/iter; left time: 261.5789s\n",
      "\titers: 200, epoch: 9 | loss: 0.1330729\n",
      "\tspeed: 0.0121s/iter; left time: 80.1124s\n",
      "\titers: 300, epoch: 9 | loss: 0.1934624\n",
      "\tspeed: 0.0121s/iter; left time: 78.9661s\n",
      "\titers: 400, epoch: 9 | loss: 0.2186877\n",
      "\tspeed: 0.0119s/iter; left time: 76.9068s\n",
      "\titers: 500, epoch: 9 | loss: 0.1144264\n",
      "\tspeed: 0.0108s/iter; left time: 68.7142s\n",
      "Epoch: 9 cost time: 6.9772162437438965\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1712640 Vali Loss: 0.0386326 Test Loss: 0.1186360\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1872510\n",
      "\tspeed: 0.0363s/iter; left time: 223.8906s\n",
      "\titers: 200, epoch: 10 | loss: 0.1101950\n",
      "\tspeed: 0.0108s/iter; left time: 65.7720s\n",
      "\titers: 300, epoch: 10 | loss: 0.1981407\n",
      "\tspeed: 0.0108s/iter; left time: 64.7277s\n",
      "\titers: 400, epoch: 10 | loss: 0.1394067\n",
      "\tspeed: 0.0108s/iter; left time: 63.6745s\n",
      "\titers: 500, epoch: 10 | loss: 0.2065775\n",
      "\tspeed: 0.0108s/iter; left time: 62.4906s\n",
      "Epoch: 10 cost time: 6.459103107452393\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1707153 Vali Loss: 0.0387613 Test Loss: 0.1187380\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1338069\n",
      "\tspeed: 0.0374s/iter; left time: 209.3431s\n",
      "\titers: 200, epoch: 11 | loss: 0.1755780\n",
      "\tspeed: 0.0121s/iter; left time: 66.2994s\n",
      "\titers: 300, epoch: 11 | loss: 0.1517908\n",
      "\tspeed: 0.0121s/iter; left time: 65.2348s\n",
      "\titers: 400, epoch: 11 | loss: 0.1131015\n",
      "\tspeed: 0.0118s/iter; left time: 62.5603s\n",
      "\titers: 500, epoch: 11 | loss: 0.1341830\n",
      "\tspeed: 0.0109s/iter; left time: 56.7165s\n",
      "Epoch: 11 cost time: 6.933863162994385\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1710720 Vali Loss: 0.0387002 Test Loss: 0.1187326\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11894439905881882, mae:0.2141129970550537\n",
      ">>> LR=1e-4,DO=0.2,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2167726\n",
      "\tspeed: 0.0283s/iter; left time: 319.8734s\n",
      "\titers: 200, epoch: 1 | loss: 0.1834397\n",
      "\tspeed: 0.0159s/iter; left time: 178.3737s\n",
      "\titers: 300, epoch: 1 | loss: 0.3097599\n",
      "\tspeed: 0.0145s/iter; left time: 161.4795s\n",
      "\titers: 400, epoch: 1 | loss: 0.2252580\n",
      "\tspeed: 0.0146s/iter; left time: 160.2860s\n",
      "\titers: 500, epoch: 1 | loss: 0.1791959\n",
      "\tspeed: 0.0146s/iter; left time: 159.0785s\n",
      "Epoch: 1 cost time: 9.870149850845337\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2450417 Vali Loss: 0.0441765 Test Loss: 0.1331939\n",
      "Validation loss decreased (inf --> 0.044177).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2415560\n",
      "\tspeed: 0.0511s/iter; left time: 548.7174s\n",
      "\titers: 200, epoch: 2 | loss: 0.1884318\n",
      "\tspeed: 0.0168s/iter; left time: 178.8155s\n",
      "\titers: 300, epoch: 2 | loss: 0.1717824\n",
      "\tspeed: 0.0167s/iter; left time: 176.3399s\n",
      "\titers: 400, epoch: 2 | loss: 0.1287038\n",
      "\tspeed: 0.0168s/iter; left time: 175.5854s\n",
      "\titers: 500, epoch: 2 | loss: 0.2091839\n",
      "\tspeed: 0.0168s/iter; left time: 173.2230s\n",
      "Epoch: 2 cost time: 9.860525131225586\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2067083 Vali Loss: 0.0420786 Test Loss: 0.1339201\n",
      "Validation loss decreased (0.044177 --> 0.042079).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3394342\n",
      "\tspeed: 0.0492s/iter; left time: 499.9028s\n",
      "\titers: 200, epoch: 3 | loss: 0.2081882\n",
      "\tspeed: 0.0152s/iter; left time: 152.8488s\n",
      "\titers: 300, epoch: 3 | loss: 0.1111782\n",
      "\tspeed: 0.0145s/iter; left time: 144.4224s\n",
      "\titers: 400, epoch: 3 | loss: 0.1616367\n",
      "\tspeed: 0.0145s/iter; left time: 143.1332s\n",
      "\titers: 500, epoch: 3 | loss: 0.1385147\n",
      "\tspeed: 0.0145s/iter; left time: 141.3561s\n",
      "Epoch: 3 cost time: 8.810574769973755\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1899784 Vali Loss: 0.0385048 Test Loss: 0.1220661\n",
      "Validation loss decreased (0.042079 --> 0.038505).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1740040\n",
      "\tspeed: 0.0472s/iter; left time: 453.1233s\n",
      "\titers: 200, epoch: 4 | loss: 0.1524063\n",
      "\tspeed: 0.0145s/iter; left time: 137.8705s\n",
      "\titers: 300, epoch: 4 | loss: 0.1879634\n",
      "\tspeed: 0.0145s/iter; left time: 136.5785s\n",
      "\titers: 400, epoch: 4 | loss: 0.1962401\n",
      "\tspeed: 0.0145s/iter; left time: 134.8697s\n",
      "\titers: 500, epoch: 4 | loss: 0.1578258\n",
      "\tspeed: 0.0145s/iter; left time: 133.5183s\n",
      "Epoch: 4 cost time: 8.57285213470459\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1817282 Vali Loss: 0.0374422 Test Loss: 0.1222099\n",
      "Validation loss decreased (0.038505 --> 0.037442).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1531262\n",
      "\tspeed: 0.0467s/iter; left time: 421.2899s\n",
      "\titers: 200, epoch: 5 | loss: 0.1575074\n",
      "\tspeed: 0.0145s/iter; left time: 129.5178s\n",
      "\titers: 300, epoch: 5 | loss: 0.1539699\n",
      "\tspeed: 0.0145s/iter; left time: 128.2318s\n",
      "\titers: 400, epoch: 5 | loss: 0.2075742\n",
      "\tspeed: 0.0145s/iter; left time: 126.3961s\n",
      "\titers: 500, epoch: 5 | loss: 0.2027942\n",
      "\tspeed: 0.0145s/iter; left time: 125.2037s\n",
      "Epoch: 5 cost time: 8.587470769882202\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1787642 Vali Loss: 0.0370470 Test Loss: 0.1194206\n",
      "Validation loss decreased (0.037442 --> 0.037047).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1353499\n",
      "\tspeed: 0.0458s/iter; left time: 387.2726s\n",
      "\titers: 200, epoch: 6 | loss: 0.1671976\n",
      "\tspeed: 0.0146s/iter; left time: 121.7180s\n",
      "\titers: 300, epoch: 6 | loss: 0.1244141\n",
      "\tspeed: 0.0146s/iter; left time: 120.4045s\n",
      "\titers: 400, epoch: 6 | loss: 0.1210807\n",
      "\tspeed: 0.0146s/iter; left time: 118.7803s\n",
      "\titers: 500, epoch: 6 | loss: 0.1896802\n",
      "\tspeed: 0.0146s/iter; left time: 117.3584s\n",
      "Epoch: 6 cost time: 8.608338117599487\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1749954 Vali Loss: 0.0370123 Test Loss: 0.1199432\n",
      "Validation loss decreased (0.037047 --> 0.037012).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1805391\n",
      "\tspeed: 0.0491s/iter; left time: 386.7613s\n",
      "\titers: 200, epoch: 7 | loss: 0.1321836\n",
      "\tspeed: 0.0147s/iter; left time: 114.6823s\n",
      "\titers: 300, epoch: 7 | loss: 0.1360212\n",
      "\tspeed: 0.0147s/iter; left time: 112.9716s\n",
      "\titers: 400, epoch: 7 | loss: 0.1530548\n",
      "\tspeed: 0.0147s/iter; left time: 111.1774s\n",
      "\titers: 500, epoch: 7 | loss: 0.2867018\n",
      "\tspeed: 0.0147s/iter; left time: 109.9165s\n",
      "Epoch: 7 cost time: 8.696028470993042\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1749006 Vali Loss: 0.0368639 Test Loss: 0.1194540\n",
      "Validation loss decreased (0.037012 --> 0.036864).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2613420\n",
      "\tspeed: 0.0457s/iter; left time: 333.9591s\n",
      "\titers: 200, epoch: 8 | loss: 0.2187726\n",
      "\tspeed: 0.0145s/iter; left time: 104.9099s\n",
      "\titers: 300, epoch: 8 | loss: 0.2201245\n",
      "\tspeed: 0.0146s/iter; left time: 103.4960s\n",
      "\titers: 400, epoch: 8 | loss: 0.1328292\n",
      "\tspeed: 0.0145s/iter; left time: 101.7845s\n",
      "\titers: 500, epoch: 8 | loss: 0.1874684\n",
      "\tspeed: 0.0145s/iter; left time: 100.3716s\n",
      "Epoch: 8 cost time: 8.55875015258789\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1744350 Vali Loss: 0.0369365 Test Loss: 0.1196728\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2474883\n",
      "\tspeed: 0.0472s/iter; left time: 318.1204s\n",
      "\titers: 200, epoch: 9 | loss: 0.2378532\n",
      "\tspeed: 0.0145s/iter; left time: 96.3916s\n",
      "\titers: 300, epoch: 9 | loss: 0.2754472\n",
      "\tspeed: 0.0145s/iter; left time: 94.9865s\n",
      "\titers: 400, epoch: 9 | loss: 0.1970205\n",
      "\tspeed: 0.0145s/iter; left time: 93.6815s\n",
      "\titers: 500, epoch: 9 | loss: 0.1458042\n",
      "\tspeed: 0.0145s/iter; left time: 91.9050s\n",
      "Epoch: 9 cost time: 8.51830005645752\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1737375 Vali Loss: 0.0369257 Test Loss: 0.1192469\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1998157\n",
      "\tspeed: 0.0453s/iter; left time: 279.4586s\n",
      "\titers: 200, epoch: 10 | loss: 0.1477786\n",
      "\tspeed: 0.0146s/iter; left time: 88.3686s\n",
      "\titers: 300, epoch: 10 | loss: 0.1360759\n",
      "\tspeed: 0.0145s/iter; left time: 86.8517s\n",
      "\titers: 400, epoch: 10 | loss: 0.1441279\n",
      "\tspeed: 0.0145s/iter; left time: 85.0904s\n",
      "\titers: 500, epoch: 10 | loss: 0.2008854\n",
      "\tspeed: 0.0146s/iter; left time: 83.9743s\n",
      "Epoch: 10 cost time: 8.557718276977539\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1744781 Vali Loss: 0.0366690 Test Loss: 0.1191785\n",
      "Validation loss decreased (0.036864 --> 0.036669).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1552098\n",
      "\tspeed: 0.0463s/iter; left time: 259.5450s\n",
      "\titers: 200, epoch: 11 | loss: 0.1430847\n",
      "\tspeed: 0.0144s/iter; left time: 79.3599s\n",
      "\titers: 300, epoch: 11 | loss: 0.1563972\n",
      "\tspeed: 0.0144s/iter; left time: 77.9159s\n",
      "\titers: 400, epoch: 11 | loss: 0.1357811\n",
      "\tspeed: 0.0145s/iter; left time: 76.9779s\n",
      "\titers: 500, epoch: 11 | loss: 0.1447430\n",
      "\tspeed: 0.0146s/iter; left time: 75.7060s\n",
      "Epoch: 11 cost time: 8.55006718635559\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1743035 Vali Loss: 0.0367894 Test Loss: 0.1191359\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1334599\n",
      "\tspeed: 0.0481s/iter; left time: 241.9582s\n",
      "\titers: 200, epoch: 12 | loss: 0.1164096\n",
      "\tspeed: 0.0147s/iter; left time: 72.2696s\n",
      "\titers: 300, epoch: 12 | loss: 0.2074345\n",
      "\tspeed: 0.0147s/iter; left time: 70.8111s\n",
      "\titers: 400, epoch: 12 | loss: 0.1870444\n",
      "\tspeed: 0.0146s/iter; left time: 69.2633s\n",
      "\titers: 500, epoch: 12 | loss: 0.1454222\n",
      "\tspeed: 0.0146s/iter; left time: 67.5228s\n",
      "Epoch: 12 cost time: 8.636688232421875\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1735718 Vali Loss: 0.0368066 Test Loss: 0.1192624\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1866234\n",
      "\tspeed: 0.0485s/iter; left time: 216.3967s\n",
      "\titers: 200, epoch: 13 | loss: 0.2737667\n",
      "\tspeed: 0.0163s/iter; left time: 71.2683s\n",
      "\titers: 300, epoch: 13 | loss: 0.2283096\n",
      "\tspeed: 0.0163s/iter; left time: 69.5812s\n",
      "\titers: 400, epoch: 13 | loss: 0.1208643\n",
      "\tspeed: 0.0163s/iter; left time: 67.7476s\n",
      "\titers: 500, epoch: 13 | loss: 0.1841916\n",
      "\tspeed: 0.0163s/iter; left time: 66.0849s\n",
      "Epoch: 13 cost time: 9.614113569259644\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1736273 Vali Loss: 0.0367905 Test Loss: 0.1192441\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11934473365545273, mae:0.2106429487466812\n",
      ">>> LR=1e-4,DO=0.2,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3230739\n",
      "\tspeed: 0.0268s/iter; left time: 302.4545s\n",
      "\titers: 200, epoch: 1 | loss: 0.2487925\n",
      "\tspeed: 0.0144s/iter; left time: 161.1509s\n",
      "\titers: 300, epoch: 1 | loss: 0.1291303\n",
      "\tspeed: 0.0143s/iter; left time: 159.2251s\n",
      "\titers: 400, epoch: 1 | loss: 0.3135703\n",
      "\tspeed: 0.0144s/iter; left time: 158.1962s\n",
      "\titers: 500, epoch: 1 | loss: 0.1859436\n",
      "\tspeed: 0.0144s/iter; left time: 156.8208s\n",
      "Epoch: 1 cost time: 9.483754634857178\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2187694 Vali Loss: 0.0367812 Test Loss: 0.1119486\n",
      "Validation loss decreased (inf --> 0.036781).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1615953\n",
      "\tspeed: 0.0460s/iter; left time: 493.9548s\n",
      "\titers: 200, epoch: 2 | loss: 0.2700863\n",
      "\tspeed: 0.0165s/iter; left time: 175.1453s\n",
      "\titers: 300, epoch: 2 | loss: 0.1883779\n",
      "\tspeed: 0.0165s/iter; left time: 173.5154s\n",
      "\titers: 400, epoch: 2 | loss: 0.1708111\n",
      "\tspeed: 0.0165s/iter; left time: 171.8406s\n",
      "\titers: 500, epoch: 2 | loss: 0.1733276\n",
      "\tspeed: 0.0165s/iter; left time: 170.1287s\n",
      "Epoch: 2 cost time: 9.493303775787354\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1877026 Vali Loss: 0.0355086 Test Loss: 0.1129241\n",
      "Validation loss decreased (0.036781 --> 0.035509).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1765935\n",
      "\tspeed: 0.0490s/iter; left time: 497.6342s\n",
      "\titers: 200, epoch: 3 | loss: 0.0953627\n",
      "\tspeed: 0.0164s/iter; left time: 165.1556s\n",
      "\titers: 300, epoch: 3 | loss: 0.2513504\n",
      "\tspeed: 0.0164s/iter; left time: 163.4051s\n",
      "\titers: 400, epoch: 3 | loss: 0.1355092\n",
      "\tspeed: 0.0164s/iter; left time: 161.7642s\n",
      "\titers: 500, epoch: 3 | loss: 0.1496388\n",
      "\tspeed: 0.0164s/iter; left time: 159.9796s\n",
      "Epoch: 3 cost time: 9.321431159973145\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1742259 Vali Loss: 0.0344777 Test Loss: 0.1114840\n",
      "Validation loss decreased (0.035509 --> 0.034478).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1729999\n",
      "\tspeed: 0.0501s/iter; left time: 480.6183s\n",
      "\titers: 200, epoch: 4 | loss: 0.1659377\n",
      "\tspeed: 0.0164s/iter; left time: 155.4486s\n",
      "\titers: 300, epoch: 4 | loss: 0.2141808\n",
      "\tspeed: 0.0164s/iter; left time: 153.7234s\n",
      "\titers: 400, epoch: 4 | loss: 0.1064999\n",
      "\tspeed: 0.0163s/iter; left time: 151.3961s\n",
      "\titers: 500, epoch: 4 | loss: 0.1109399\n",
      "\tspeed: 0.0163s/iter; left time: 150.0431s\n",
      "Epoch: 4 cost time: 9.585179805755615\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1698102 Vali Loss: 0.0334893 Test Loss: 0.1089157\n",
      "Validation loss decreased (0.034478 --> 0.033489).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0924286\n",
      "\tspeed: 0.0493s/iter; left time: 445.0573s\n",
      "\titers: 200, epoch: 5 | loss: 0.1825116\n",
      "\tspeed: 0.0164s/iter; left time: 146.3284s\n",
      "\titers: 300, epoch: 5 | loss: 0.2019356\n",
      "\tspeed: 0.0164s/iter; left time: 144.3953s\n",
      "\titers: 400, epoch: 5 | loss: 0.1343013\n",
      "\tspeed: 0.0164s/iter; left time: 143.2492s\n",
      "\titers: 500, epoch: 5 | loss: 0.1753863\n",
      "\tspeed: 0.0165s/iter; left time: 142.2478s\n",
      "Epoch: 5 cost time: 9.661692380905151\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1663471 Vali Loss: 0.0334223 Test Loss: 0.1087708\n",
      "Validation loss decreased (0.033489 --> 0.033422).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1363288\n",
      "\tspeed: 0.0495s/iter; left time: 418.4244s\n",
      "\titers: 200, epoch: 6 | loss: 0.3008175\n",
      "\tspeed: 0.0160s/iter; left time: 133.6828s\n",
      "\titers: 300, epoch: 6 | loss: 0.2603256\n",
      "\tspeed: 0.0161s/iter; left time: 132.6049s\n",
      "\titers: 400, epoch: 6 | loss: 0.1751878\n",
      "\tspeed: 0.0160s/iter; left time: 130.6256s\n",
      "\titers: 500, epoch: 6 | loss: 0.1767675\n",
      "\tspeed: 0.0160s/iter; left time: 129.1416s\n",
      "Epoch: 6 cost time: 9.459484815597534\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1655399 Vali Loss: 0.0328816 Test Loss: 0.1075305\n",
      "Validation loss decreased (0.033422 --> 0.032882).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1137792\n",
      "\tspeed: 0.0471s/iter; left time: 371.1622s\n",
      "\titers: 200, epoch: 7 | loss: 0.1645022\n",
      "\tspeed: 0.0143s/iter; left time: 111.1763s\n",
      "\titers: 300, epoch: 7 | loss: 0.2490925\n",
      "\tspeed: 0.0143s/iter; left time: 109.6960s\n",
      "\titers: 400, epoch: 7 | loss: 0.1427208\n",
      "\tspeed: 0.0143s/iter; left time: 108.0402s\n",
      "\titers: 500, epoch: 7 | loss: 0.2128087\n",
      "\tspeed: 0.0143s/iter; left time: 106.7018s\n",
      "Epoch: 7 cost time: 8.433992147445679\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1638563 Vali Loss: 0.0328501 Test Loss: 0.1075453\n",
      "Validation loss decreased (0.032882 --> 0.032850).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1055184\n",
      "\tspeed: 0.0460s/iter; left time: 336.2463s\n",
      "\titers: 200, epoch: 8 | loss: 0.1524740\n",
      "\tspeed: 0.0142s/iter; left time: 102.5927s\n",
      "\titers: 300, epoch: 8 | loss: 0.2451741\n",
      "\tspeed: 0.0142s/iter; left time: 101.1899s\n",
      "\titers: 400, epoch: 8 | loss: 0.1768970\n",
      "\tspeed: 0.0142s/iter; left time: 99.4525s\n",
      "\titers: 500, epoch: 8 | loss: 0.1730819\n",
      "\tspeed: 0.0143s/iter; left time: 98.7556s\n",
      "Epoch: 8 cost time: 8.386746168136597\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1632265 Vali Loss: 0.0329301 Test Loss: 0.1077431\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1219473\n",
      "\tspeed: 0.0479s/iter; left time: 322.7229s\n",
      "\titers: 200, epoch: 9 | loss: 0.1291988\n",
      "\tspeed: 0.0142s/iter; left time: 94.6083s\n",
      "\titers: 300, epoch: 9 | loss: 0.1806177\n",
      "\tspeed: 0.0142s/iter; left time: 93.1927s\n",
      "\titers: 400, epoch: 9 | loss: 0.1606346\n",
      "\tspeed: 0.0142s/iter; left time: 91.7371s\n",
      "\titers: 500, epoch: 9 | loss: 0.1853399\n",
      "\tspeed: 0.0142s/iter; left time: 90.1899s\n",
      "Epoch: 9 cost time: 8.416950941085815\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1631130 Vali Loss: 0.0327687 Test Loss: 0.1077360\n",
      "Validation loss decreased (0.032850 --> 0.032769).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1848225\n",
      "\tspeed: 0.0481s/iter; left time: 296.7070s\n",
      "\titers: 200, epoch: 10 | loss: 0.1568534\n",
      "\tspeed: 0.0143s/iter; left time: 86.5389s\n",
      "\titers: 300, epoch: 10 | loss: 0.1562719\n",
      "\tspeed: 0.0142s/iter; left time: 85.0617s\n",
      "\titers: 400, epoch: 10 | loss: 0.1687929\n",
      "\tspeed: 0.0142s/iter; left time: 83.6076s\n",
      "\titers: 500, epoch: 10 | loss: 0.2149202\n",
      "\tspeed: 0.0143s/iter; left time: 82.3483s\n",
      "Epoch: 10 cost time: 8.439673662185669\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1628169 Vali Loss: 0.0327760 Test Loss: 0.1076413\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2316905\n",
      "\tspeed: 0.0468s/iter; left time: 261.9121s\n",
      "\titers: 200, epoch: 11 | loss: 0.1681755\n",
      "\tspeed: 0.0141s/iter; left time: 77.8225s\n",
      "\titers: 300, epoch: 11 | loss: 0.1939025\n",
      "\tspeed: 0.0142s/iter; left time: 76.4648s\n",
      "\titers: 400, epoch: 11 | loss: 0.1152691\n",
      "\tspeed: 0.0142s/iter; left time: 75.0431s\n",
      "\titers: 500, epoch: 11 | loss: 0.2666770\n",
      "\tspeed: 0.0142s/iter; left time: 73.6009s\n",
      "Epoch: 11 cost time: 8.339339017868042\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1618295 Vali Loss: 0.0327559 Test Loss: 0.1076812\n",
      "Validation loss decreased (0.032769 --> 0.032756).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.2332382\n",
      "\tspeed: 0.0459s/iter; left time: 231.0152s\n",
      "\titers: 200, epoch: 12 | loss: 0.1990643\n",
      "\tspeed: 0.0143s/iter; left time: 70.4871s\n",
      "\titers: 300, epoch: 12 | loss: 0.1215841\n",
      "\tspeed: 0.0143s/iter; left time: 68.9968s\n",
      "\titers: 400, epoch: 12 | loss: 0.1708272\n",
      "\tspeed: 0.0143s/iter; left time: 67.5711s\n",
      "\titers: 500, epoch: 12 | loss: 0.1868755\n",
      "\tspeed: 0.0143s/iter; left time: 66.1807s\n",
      "Epoch: 12 cost time: 8.447917461395264\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1638072 Vali Loss: 0.0328175 Test Loss: 0.1076672\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1394779\n",
      "\tspeed: 0.0458s/iter; left time: 204.1421s\n",
      "\titers: 200, epoch: 13 | loss: 0.1236927\n",
      "\tspeed: 0.0142s/iter; left time: 61.8675s\n",
      "\titers: 300, epoch: 13 | loss: 0.1323943\n",
      "\tspeed: 0.0142s/iter; left time: 60.3749s\n",
      "\titers: 400, epoch: 13 | loss: 0.0949029\n",
      "\tspeed: 0.0141s/iter; left time: 58.7910s\n",
      "\titers: 500, epoch: 13 | loss: 0.1751146\n",
      "\tspeed: 0.0141s/iter; left time: 57.3542s\n",
      "Epoch: 13 cost time: 8.324563264846802\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1628508 Vali Loss: 0.0326996 Test Loss: 0.1076649\n",
      "Validation loss decreased (0.032756 --> 0.032700).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1346692\n",
      "\tspeed: 0.0456s/iter; left time: 177.4224s\n",
      "\titers: 200, epoch: 14 | loss: 0.2459736\n",
      "\tspeed: 0.0142s/iter; left time: 53.8837s\n",
      "\titers: 300, epoch: 14 | loss: 0.1126859\n",
      "\tspeed: 0.0142s/iter; left time: 52.4176s\n",
      "\titers: 400, epoch: 14 | loss: 0.1289571\n",
      "\tspeed: 0.0142s/iter; left time: 50.9620s\n",
      "\titers: 500, epoch: 14 | loss: 0.1478224\n",
      "\tspeed: 0.0142s/iter; left time: 49.7121s\n",
      "Epoch: 14 cost time: 8.409562826156616\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1616575 Vali Loss: 0.0327439 Test Loss: 0.1076713\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1395341\n",
      "\tspeed: 0.0465s/iter; left time: 154.5477s\n",
      "\titers: 200, epoch: 15 | loss: 0.1093530\n",
      "\tspeed: 0.0143s/iter; left time: 46.0819s\n",
      "\titers: 300, epoch: 15 | loss: 0.1623834\n",
      "\tspeed: 0.0143s/iter; left time: 44.5513s\n",
      "\titers: 400, epoch: 15 | loss: 0.1505805\n",
      "\tspeed: 0.0143s/iter; left time: 43.2208s\n",
      "\titers: 500, epoch: 15 | loss: 0.2240412\n",
      "\tspeed: 0.0143s/iter; left time: 41.7750s\n",
      "Epoch: 15 cost time: 8.42960810661316\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1623262 Vali Loss: 0.0327374 Test Loss: 0.1076703\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.1810603\n",
      "\tspeed: 0.0461s/iter; left time: 126.7784s\n",
      "\titers: 200, epoch: 16 | loss: 0.1773026\n",
      "\tspeed: 0.0143s/iter; left time: 37.8312s\n",
      "\titers: 300, epoch: 16 | loss: 0.1407213\n",
      "\tspeed: 0.0143s/iter; left time: 36.3939s\n",
      "\titers: 400, epoch: 16 | loss: 0.1876066\n",
      "\tspeed: 0.0143s/iter; left time: 34.9389s\n",
      "\titers: 500, epoch: 16 | loss: 0.1328565\n",
      "\tspeed: 0.0143s/iter; left time: 33.5363s\n",
      "Epoch: 16 cost time: 8.433439016342163\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1619266 Vali Loss: 0.0328557 Test Loss: 0.1076714\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1078135147690773, mae:0.1963559091091156\n",
      ">>> LR=1e-4,DO=0.2,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1469629\n",
      "\tspeed: 0.0264s/iter; left time: 298.1687s\n",
      "\titers: 200, epoch: 1 | loss: 0.3179166\n",
      "\tspeed: 0.0145s/iter; left time: 162.4787s\n",
      "\titers: 300, epoch: 1 | loss: 0.3197975\n",
      "\tspeed: 0.0145s/iter; left time: 160.6179s\n",
      "\titers: 400, epoch: 1 | loss: 0.2389847\n",
      "\tspeed: 0.0145s/iter; left time: 159.9809s\n",
      "\titers: 500, epoch: 1 | loss: 0.2648219\n",
      "\tspeed: 0.0145s/iter; left time: 158.3036s\n",
      "Epoch: 1 cost time: 9.517126560211182\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2509745 Vali Loss: 0.0440646 Test Loss: 0.1338914\n",
      "Validation loss decreased (inf --> 0.044065).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1855400\n",
      "\tspeed: 0.0479s/iter; left time: 513.5786s\n",
      "\titers: 200, epoch: 2 | loss: 0.1298666\n",
      "\tspeed: 0.0144s/iter; left time: 153.3624s\n",
      "\titers: 300, epoch: 2 | loss: 0.1377157\n",
      "\tspeed: 0.0144s/iter; left time: 151.9331s\n",
      "\titers: 400, epoch: 2 | loss: 0.1488576\n",
      "\tspeed: 0.0144s/iter; left time: 150.3182s\n",
      "\titers: 500, epoch: 2 | loss: 0.2571372\n",
      "\tspeed: 0.0144s/iter; left time: 148.8761s\n",
      "Epoch: 2 cost time: 8.566364049911499\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2121049 Vali Loss: 0.0422762 Test Loss: 0.1261149\n",
      "Validation loss decreased (0.044065 --> 0.042276).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2350141\n",
      "\tspeed: 0.0488s/iter; left time: 495.5341s\n",
      "\titers: 200, epoch: 3 | loss: 0.2209245\n",
      "\tspeed: 0.0143s/iter; left time: 143.7416s\n",
      "\titers: 300, epoch: 3 | loss: 0.1703092\n",
      "\tspeed: 0.0143s/iter; left time: 142.4378s\n",
      "\titers: 400, epoch: 3 | loss: 0.1649446\n",
      "\tspeed: 0.0143s/iter; left time: 140.7350s\n",
      "\titers: 500, epoch: 3 | loss: 0.2347749\n",
      "\tspeed: 0.0143s/iter; left time: 139.5791s\n",
      "Epoch: 3 cost time: 8.501073598861694\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1959081 Vali Loss: 0.0407369 Test Loss: 0.1245840\n",
      "Validation loss decreased (0.042276 --> 0.040737).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2508076\n",
      "\tspeed: 0.0496s/iter; left time: 476.0394s\n",
      "\titers: 200, epoch: 4 | loss: 0.1658720\n",
      "\tspeed: 0.0162s/iter; left time: 153.3938s\n",
      "\titers: 300, epoch: 4 | loss: 0.2019032\n",
      "\tspeed: 0.0160s/iter; left time: 149.8702s\n",
      "\titers: 400, epoch: 4 | loss: 0.1830573\n",
      "\tspeed: 0.0150s/iter; left time: 139.3838s\n",
      "\titers: 500, epoch: 4 | loss: 0.1332845\n",
      "\tspeed: 0.0162s/iter; left time: 148.5317s\n",
      "Epoch: 4 cost time: 9.441164016723633\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1896349 Vali Loss: 0.0393675 Test Loss: 0.1208681\n",
      "Validation loss decreased (0.040737 --> 0.039367).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1568950\n",
      "\tspeed: 0.0486s/iter; left time: 438.2920s\n",
      "\titers: 200, epoch: 5 | loss: 0.1988941\n",
      "\tspeed: 0.0143s/iter; left time: 127.4450s\n",
      "\titers: 300, epoch: 5 | loss: 0.1730237\n",
      "\tspeed: 0.0143s/iter; left time: 126.0138s\n",
      "\titers: 400, epoch: 5 | loss: 0.2516692\n",
      "\tspeed: 0.0143s/iter; left time: 124.5407s\n",
      "\titers: 500, epoch: 5 | loss: 0.1377376\n",
      "\tspeed: 0.0143s/iter; left time: 123.1296s\n",
      "Epoch: 5 cost time: 8.469141006469727\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1855682 Vali Loss: 0.0391606 Test Loss: 0.1201594\n",
      "Validation loss decreased (0.039367 --> 0.039161).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2225715\n",
      "\tspeed: 0.0474s/iter; left time: 400.4604s\n",
      "\titers: 200, epoch: 6 | loss: 0.1699176\n",
      "\tspeed: 0.0143s/iter; left time: 119.1986s\n",
      "\titers: 300, epoch: 6 | loss: 0.1154262\n",
      "\tspeed: 0.0143s/iter; left time: 117.7507s\n",
      "\titers: 400, epoch: 6 | loss: 0.1992546\n",
      "\tspeed: 0.0143s/iter; left time: 116.3268s\n",
      "\titers: 500, epoch: 6 | loss: 0.1731847\n",
      "\tspeed: 0.0143s/iter; left time: 114.8606s\n",
      "Epoch: 6 cost time: 8.4952712059021\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1828942 Vali Loss: 0.0389261 Test Loss: 0.1218258\n",
      "Validation loss decreased (0.039161 --> 0.038926).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2352667\n",
      "\tspeed: 0.0488s/iter; left time: 384.3643s\n",
      "\titers: 200, epoch: 7 | loss: 0.1074341\n",
      "\tspeed: 0.0161s/iter; left time: 125.5220s\n",
      "\titers: 300, epoch: 7 | loss: 0.1429074\n",
      "\tspeed: 0.0161s/iter; left time: 123.7020s\n",
      "\titers: 400, epoch: 7 | loss: 0.2743106\n",
      "\tspeed: 0.0161s/iter; left time: 121.9266s\n",
      "\titers: 500, epoch: 7 | loss: 0.1668637\n",
      "\tspeed: 0.0153s/iter; left time: 114.4301s\n",
      "Epoch: 7 cost time: 9.293813705444336\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1840404 Vali Loss: 0.0387647 Test Loss: 0.1200844\n",
      "Validation loss decreased (0.038926 --> 0.038765).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1498211\n",
      "\tspeed: 0.0479s/iter; left time: 350.0392s\n",
      "\titers: 200, epoch: 8 | loss: 0.1567590\n",
      "\tspeed: 0.0143s/iter; left time: 103.1245s\n",
      "\titers: 300, epoch: 8 | loss: 0.1566342\n",
      "\tspeed: 0.0144s/iter; left time: 102.2557s\n",
      "\titers: 400, epoch: 8 | loss: 0.1933138\n",
      "\tspeed: 0.0143s/iter; left time: 100.3813s\n",
      "\titers: 500, epoch: 8 | loss: 0.1202289\n",
      "\tspeed: 0.0144s/iter; left time: 99.1789s\n",
      "Epoch: 8 cost time: 8.61384630203247\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1832267 Vali Loss: 0.0390567 Test Loss: 0.1209443\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.2395315\n",
      "\tspeed: 0.0492s/iter; left time: 331.6256s\n",
      "\titers: 200, epoch: 9 | loss: 0.1514681\n",
      "\tspeed: 0.0144s/iter; left time: 95.7583s\n",
      "\titers: 300, epoch: 9 | loss: 0.1267509\n",
      "\tspeed: 0.0144s/iter; left time: 94.1088s\n",
      "\titers: 400, epoch: 9 | loss: 0.1650192\n",
      "\tspeed: 0.0143s/iter; left time: 92.3014s\n",
      "\titers: 500, epoch: 9 | loss: 0.1709631\n",
      "\tspeed: 0.0144s/iter; left time: 91.0858s\n",
      "Epoch: 9 cost time: 8.537040948867798\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1813043 Vali Loss: 0.0388237 Test Loss: 0.1210111\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.2333269\n",
      "\tspeed: 0.0469s/iter; left time: 289.7228s\n",
      "\titers: 200, epoch: 10 | loss: 0.2065901\n",
      "\tspeed: 0.0144s/iter; left time: 87.7156s\n",
      "\titers: 300, epoch: 10 | loss: 0.1792526\n",
      "\tspeed: 0.0144s/iter; left time: 85.7174s\n",
      "\titers: 400, epoch: 10 | loss: 0.2072501\n",
      "\tspeed: 0.0144s/iter; left time: 84.3475s\n",
      "\titers: 500, epoch: 10 | loss: 0.1652863\n",
      "\tspeed: 0.0144s/iter; left time: 82.8374s\n",
      "Epoch: 10 cost time: 8.4965980052948\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1810540 Vali Loss: 0.0388431 Test Loss: 0.1207669\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12025460600852966, mae:0.21314102411270142\n",
      ">>> LR=1e-4,DO=0.2,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2807164\n",
      "\tspeed: 0.0288s/iter; left time: 325.3063s\n",
      "\titers: 200, epoch: 1 | loss: 0.1994449\n",
      "\tspeed: 0.0170s/iter; left time: 190.0256s\n",
      "\titers: 300, epoch: 1 | loss: 0.2542409\n",
      "\tspeed: 0.0169s/iter; left time: 188.1471s\n",
      "\titers: 400, epoch: 1 | loss: 0.2241962\n",
      "\tspeed: 0.0170s/iter; left time: 186.7053s\n",
      "\titers: 500, epoch: 1 | loss: 0.1999758\n",
      "\tspeed: 0.0170s/iter; left time: 185.0748s\n",
      "Epoch: 1 cost time: 10.910790920257568\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2381237 Vali Loss: 0.0437958 Test Loss: 0.1303702\n",
      "Validation loss decreased (inf --> 0.043796).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2267788\n",
      "\tspeed: 0.0528s/iter; left time: 566.5389s\n",
      "\titers: 200, epoch: 2 | loss: 0.2498884\n",
      "\tspeed: 0.0169s/iter; left time: 179.6097s\n",
      "\titers: 300, epoch: 2 | loss: 0.2195622\n",
      "\tspeed: 0.0169s/iter; left time: 177.8815s\n",
      "\titers: 400, epoch: 2 | loss: 0.1399293\n",
      "\tspeed: 0.0169s/iter; left time: 176.2882s\n",
      "\titers: 500, epoch: 2 | loss: 0.1576113\n",
      "\tspeed: 0.0169s/iter; left time: 174.6298s\n",
      "Epoch: 2 cost time: 9.95423436164856\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1979245 Vali Loss: 0.0395067 Test Loss: 0.1271103\n",
      "Validation loss decreased (0.043796 --> 0.039507).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1486230\n",
      "\tspeed: 0.0520s/iter; left time: 528.0409s\n",
      "\titers: 200, epoch: 3 | loss: 0.1651161\n",
      "\tspeed: 0.0169s/iter; left time: 170.5032s\n",
      "\titers: 300, epoch: 3 | loss: 0.2907280\n",
      "\tspeed: 0.0170s/iter; left time: 168.8497s\n",
      "\titers: 400, epoch: 3 | loss: 0.1437851\n",
      "\tspeed: 0.0169s/iter; left time: 166.7123s\n",
      "\titers: 500, epoch: 3 | loss: 0.1431609\n",
      "\tspeed: 0.0169s/iter; left time: 164.9023s\n",
      "Epoch: 3 cost time: 9.929981231689453\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1768802 Vali Loss: 0.0373004 Test Loss: 0.1236406\n",
      "Validation loss decreased (0.039507 --> 0.037300).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1289243\n",
      "\tspeed: 0.0530s/iter; left time: 508.7331s\n",
      "\titers: 200, epoch: 4 | loss: 0.1872966\n",
      "\tspeed: 0.0170s/iter; left time: 160.9338s\n",
      "\titers: 300, epoch: 4 | loss: 0.1971416\n",
      "\tspeed: 0.0169s/iter; left time: 159.1409s\n",
      "\titers: 400, epoch: 4 | loss: 0.2076066\n",
      "\tspeed: 0.0170s/iter; left time: 157.5485s\n",
      "\titers: 500, epoch: 4 | loss: 0.1533278\n",
      "\tspeed: 0.0170s/iter; left time: 155.8126s\n",
      "Epoch: 4 cost time: 9.97232460975647\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1660419 Vali Loss: 0.0349997 Test Loss: 0.1204328\n",
      "Validation loss decreased (0.037300 --> 0.035000).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1321922\n",
      "\tspeed: 0.0510s/iter; left time: 460.2842s\n",
      "\titers: 200, epoch: 5 | loss: 0.2033115\n",
      "\tspeed: 0.0170s/iter; left time: 151.2562s\n",
      "\titers: 300, epoch: 5 | loss: 0.1826844\n",
      "\tspeed: 0.0169s/iter; left time: 149.2893s\n",
      "\titers: 400, epoch: 5 | loss: 0.1735277\n",
      "\tspeed: 0.0169s/iter; left time: 147.6575s\n",
      "\titers: 500, epoch: 5 | loss: 0.1227321\n",
      "\tspeed: 0.0169s/iter; left time: 145.9401s\n",
      "Epoch: 5 cost time: 9.965690851211548\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1602384 Vali Loss: 0.0347689 Test Loss: 0.1204603\n",
      "Validation loss decreased (0.035000 --> 0.034769).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1950388\n",
      "\tspeed: 0.0540s/iter; left time: 456.7097s\n",
      "\titers: 200, epoch: 6 | loss: 0.1227389\n",
      "\tspeed: 0.0169s/iter; left time: 141.0803s\n",
      "\titers: 300, epoch: 6 | loss: 0.1735075\n",
      "\tspeed: 0.0169s/iter; left time: 139.3627s\n",
      "\titers: 400, epoch: 6 | loss: 0.2130548\n",
      "\tspeed: 0.0169s/iter; left time: 137.6771s\n",
      "\titers: 500, epoch: 6 | loss: 0.1318509\n",
      "\tspeed: 0.0169s/iter; left time: 135.9820s\n",
      "Epoch: 6 cost time: 10.116937637329102\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1573654 Vali Loss: 0.0346455 Test Loss: 0.1200864\n",
      "Validation loss decreased (0.034769 --> 0.034646).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1442911\n",
      "\tspeed: 0.0521s/iter; left time: 410.2196s\n",
      "\titers: 200, epoch: 7 | loss: 0.1986550\n",
      "\tspeed: 0.0170s/iter; left time: 132.0262s\n",
      "\titers: 300, epoch: 7 | loss: 0.1928933\n",
      "\tspeed: 0.0170s/iter; left time: 130.3276s\n",
      "\titers: 400, epoch: 7 | loss: 0.1173649\n",
      "\tspeed: 0.0170s/iter; left time: 128.6015s\n",
      "\titers: 500, epoch: 7 | loss: 0.1246245\n",
      "\tspeed: 0.0183s/iter; left time: 136.6695s\n",
      "Epoch: 7 cost time: 10.265721082687378\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1556588 Vali Loss: 0.0342328 Test Loss: 0.1201242\n",
      "Validation loss decreased (0.034646 --> 0.034233).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1714974\n",
      "\tspeed: 0.0535s/iter; left time: 390.9720s\n",
      "\titers: 200, epoch: 8 | loss: 0.1074537\n",
      "\tspeed: 0.0169s/iter; left time: 121.8017s\n",
      "\titers: 300, epoch: 8 | loss: 0.1497885\n",
      "\tspeed: 0.0169s/iter; left time: 120.1036s\n",
      "\titers: 400, epoch: 8 | loss: 0.0966494\n",
      "\tspeed: 0.0169s/iter; left time: 118.5242s\n",
      "\titers: 500, epoch: 8 | loss: 0.1098924\n",
      "\tspeed: 0.0169s/iter; left time: 116.8423s\n",
      "Epoch: 8 cost time: 9.961990594863892\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1553105 Vali Loss: 0.0344104 Test Loss: 0.1205098\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1266415\n",
      "\tspeed: 0.0508s/iter; left time: 342.1345s\n",
      "\titers: 200, epoch: 9 | loss: 0.0931378\n",
      "\tspeed: 0.0169s/iter; left time: 112.4913s\n",
      "\titers: 300, epoch: 9 | loss: 0.1561387\n",
      "\tspeed: 0.0169s/iter; left time: 110.8588s\n",
      "\titers: 400, epoch: 9 | loss: 0.1585056\n",
      "\tspeed: 0.0169s/iter; left time: 109.0994s\n",
      "\titers: 500, epoch: 9 | loss: 0.2204621\n",
      "\tspeed: 0.0169s/iter; left time: 107.4323s\n",
      "Epoch: 9 cost time: 9.992456436157227\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1537810 Vali Loss: 0.0342483 Test Loss: 0.1203181\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.2070161\n",
      "\tspeed: 0.0502s/iter; left time: 310.0184s\n",
      "\titers: 200, epoch: 10 | loss: 0.0768331\n",
      "\tspeed: 0.0169s/iter; left time: 102.6042s\n",
      "\titers: 300, epoch: 10 | loss: 0.2067846\n",
      "\tspeed: 0.0169s/iter; left time: 100.9767s\n",
      "\titers: 400, epoch: 10 | loss: 0.1695709\n",
      "\tspeed: 0.0169s/iter; left time: 99.2124s\n",
      "\titers: 500, epoch: 10 | loss: 0.1195202\n",
      "\tspeed: 0.0169s/iter; left time: 97.5734s\n",
      "Epoch: 10 cost time: 9.928817749023438\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1544903 Vali Loss: 0.0344964 Test Loss: 0.1203680\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12027682363986969, mae:0.21584366261959076\n",
      ">>> LR=1e-4,DO=0.2,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1919311\n",
      "\tspeed: 0.0301s/iter; left time: 339.6413s\n",
      "\titers: 200, epoch: 1 | loss: 0.1355707\n",
      "\tspeed: 0.0179s/iter; left time: 200.0537s\n",
      "\titers: 300, epoch: 1 | loss: 0.2324904\n",
      "\tspeed: 0.0178s/iter; left time: 198.0977s\n",
      "\titers: 400, epoch: 1 | loss: 0.2188273\n",
      "\tspeed: 0.0179s/iter; left time: 196.8822s\n",
      "\titers: 500, epoch: 1 | loss: 0.1164604\n",
      "\tspeed: 0.0179s/iter; left time: 195.0143s\n",
      "Epoch: 1 cost time: 11.414002418518066\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2136093 Vali Loss: 0.0361288 Test Loss: 0.1138234\n",
      "Validation loss decreased (inf --> 0.036129).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1510792\n",
      "\tspeed: 0.0533s/iter; left time: 572.1839s\n",
      "\titers: 200, epoch: 2 | loss: 0.1406453\n",
      "\tspeed: 0.0179s/iter; left time: 190.6888s\n",
      "\titers: 300, epoch: 2 | loss: 0.2300675\n",
      "\tspeed: 0.0179s/iter; left time: 188.5501s\n",
      "\titers: 400, epoch: 2 | loss: 0.2397196\n",
      "\tspeed: 0.0179s/iter; left time: 186.3122s\n",
      "\titers: 500, epoch: 2 | loss: 0.1692325\n",
      "\tspeed: 0.0179s/iter; left time: 184.9823s\n",
      "Epoch: 2 cost time: 10.522971630096436\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1819060 Vali Loss: 0.0354373 Test Loss: 0.1129405\n",
      "Validation loss decreased (0.036129 --> 0.035437).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1561161\n",
      "\tspeed: 0.0536s/iter; left time: 544.9832s\n",
      "\titers: 200, epoch: 3 | loss: 0.1524493\n",
      "\tspeed: 0.0178s/iter; left time: 179.5554s\n",
      "\titers: 300, epoch: 3 | loss: 0.1533091\n",
      "\tspeed: 0.0176s/iter; left time: 175.3029s\n",
      "\titers: 400, epoch: 3 | loss: 0.2105260\n",
      "\tspeed: 0.0164s/iter; left time: 161.3254s\n",
      "\titers: 500, epoch: 3 | loss: 0.1116992\n",
      "\tspeed: 0.0164s/iter; left time: 159.7108s\n",
      "Epoch: 3 cost time: 10.05886721611023\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1655874 Vali Loss: 0.0329602 Test Loss: 0.1078570\n",
      "Validation loss decreased (0.035437 --> 0.032960).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1438631\n",
      "\tspeed: 0.0524s/iter; left time: 502.4034s\n",
      "\titers: 200, epoch: 4 | loss: 0.2944773\n",
      "\tspeed: 0.0179s/iter; left time: 169.7283s\n",
      "\titers: 300, epoch: 4 | loss: 0.1522587\n",
      "\tspeed: 0.0179s/iter; left time: 167.8379s\n",
      "\titers: 400, epoch: 4 | loss: 0.1090287\n",
      "\tspeed: 0.0179s/iter; left time: 166.3079s\n",
      "\titers: 500, epoch: 4 | loss: 0.1948645\n",
      "\tspeed: 0.0179s/iter; left time: 164.2635s\n",
      "Epoch: 4 cost time: 10.495243787765503\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1562246 Vali Loss: 0.0324491 Test Loss: 0.1078768\n",
      "Validation loss decreased (0.032960 --> 0.032449).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1842611\n",
      "\tspeed: 0.0516s/iter; left time: 465.5602s\n",
      "\titers: 200, epoch: 5 | loss: 0.1855548\n",
      "\tspeed: 0.0163s/iter; left time: 145.8201s\n",
      "\titers: 300, epoch: 5 | loss: 0.1641506\n",
      "\tspeed: 0.0163s/iter; left time: 144.2058s\n",
      "\titers: 400, epoch: 5 | loss: 0.1029566\n",
      "\tspeed: 0.0163s/iter; left time: 142.4226s\n",
      "\titers: 500, epoch: 5 | loss: 0.2050424\n",
      "\tspeed: 0.0164s/iter; left time: 140.9929s\n",
      "Epoch: 5 cost time: 9.624077320098877\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1526626 Vali Loss: 0.0320908 Test Loss: 0.1077455\n",
      "Validation loss decreased (0.032449 --> 0.032091).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1627000\n",
      "\tspeed: 0.0502s/iter; left time: 424.3409s\n",
      "\titers: 200, epoch: 6 | loss: 0.1896338\n",
      "\tspeed: 0.0163s/iter; left time: 136.4839s\n",
      "\titers: 300, epoch: 6 | loss: 0.2693917\n",
      "\tspeed: 0.0163s/iter; left time: 134.7372s\n",
      "\titers: 400, epoch: 6 | loss: 0.1517547\n",
      "\tspeed: 0.0163s/iter; left time: 133.0457s\n",
      "\titers: 500, epoch: 6 | loss: 0.1533870\n",
      "\tspeed: 0.0163s/iter; left time: 131.3931s\n",
      "Epoch: 6 cost time: 9.62017273902893\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1504282 Vali Loss: 0.0319907 Test Loss: 0.1079596\n",
      "Validation loss decreased (0.032091 --> 0.031991).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2064373\n",
      "\tspeed: 0.0498s/iter; left time: 392.2732s\n",
      "\titers: 200, epoch: 7 | loss: 0.0981622\n",
      "\tspeed: 0.0163s/iter; left time: 127.2129s\n",
      "\titers: 300, epoch: 7 | loss: 0.1423584\n",
      "\tspeed: 0.0163s/iter; left time: 125.5667s\n",
      "\titers: 400, epoch: 7 | loss: 0.2041747\n",
      "\tspeed: 0.0163s/iter; left time: 123.9185s\n",
      "\titers: 500, epoch: 7 | loss: 0.2211102\n",
      "\tspeed: 0.0163s/iter; left time: 122.2910s\n",
      "Epoch: 7 cost time: 9.635476112365723\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1502212 Vali Loss: 0.0320323 Test Loss: 0.1082623\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0899850\n",
      "\tspeed: 0.0503s/iter; left time: 367.9645s\n",
      "\titers: 200, epoch: 8 | loss: 0.2159691\n",
      "\tspeed: 0.0177s/iter; left time: 127.5912s\n",
      "\titers: 300, epoch: 8 | loss: 0.1730623\n",
      "\tspeed: 0.0164s/iter; left time: 116.2789s\n",
      "\titers: 400, epoch: 8 | loss: 0.1780045\n",
      "\tspeed: 0.0163s/iter; left time: 114.6275s\n",
      "\titers: 500, epoch: 8 | loss: 0.1362632\n",
      "\tspeed: 0.0164s/iter; left time: 112.9955s\n",
      "Epoch: 8 cost time: 9.895041465759277\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1481123 Vali Loss: 0.0319850 Test Loss: 0.1078942\n",
      "Validation loss decreased (0.031991 --> 0.031985).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0998281\n",
      "\tspeed: 0.0506s/iter; left time: 341.0699s\n",
      "\titers: 200, epoch: 9 | loss: 0.1439245\n",
      "\tspeed: 0.0164s/iter; left time: 108.6062s\n",
      "\titers: 300, epoch: 9 | loss: 0.0975575\n",
      "\tspeed: 0.0163s/iter; left time: 106.8479s\n",
      "\titers: 400, epoch: 9 | loss: 0.1241299\n",
      "\tspeed: 0.0171s/iter; left time: 110.2855s\n",
      "\titers: 500, epoch: 9 | loss: 0.0834725\n",
      "\tspeed: 0.0178s/iter; left time: 112.8096s\n",
      "Epoch: 9 cost time: 10.002873420715332\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1485585 Vali Loss: 0.0319350 Test Loss: 0.1079367\n",
      "Validation loss decreased (0.031985 --> 0.031935).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1683648\n",
      "\tspeed: 0.0539s/iter; left time: 332.8455s\n",
      "\titers: 200, epoch: 10 | loss: 0.1398911\n",
      "\tspeed: 0.0178s/iter; left time: 108.2462s\n",
      "\titers: 300, epoch: 10 | loss: 0.1231320\n",
      "\tspeed: 0.0178s/iter; left time: 106.5095s\n",
      "\titers: 400, epoch: 10 | loss: 0.1850924\n",
      "\tspeed: 0.0178s/iter; left time: 104.6480s\n",
      "\titers: 500, epoch: 10 | loss: 0.1255039\n",
      "\tspeed: 0.0179s/iter; left time: 103.1369s\n",
      "Epoch: 10 cost time: 10.477241039276123\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1485522 Vali Loss: 0.0318961 Test Loss: 0.1078511\n",
      "Validation loss decreased (0.031935 --> 0.031896).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0992068\n",
      "\tspeed: 0.0527s/iter; left time: 294.9624s\n",
      "\titers: 200, epoch: 11 | loss: 0.1115770\n",
      "\tspeed: 0.0179s/iter; left time: 98.2264s\n",
      "\titers: 300, epoch: 11 | loss: 0.0927474\n",
      "\tspeed: 0.0179s/iter; left time: 96.4192s\n",
      "\titers: 400, epoch: 11 | loss: 0.1755264\n",
      "\tspeed: 0.0178s/iter; left time: 94.6187s\n",
      "\titers: 500, epoch: 11 | loss: 0.1436180\n",
      "\tspeed: 0.0179s/iter; left time: 92.9226s\n",
      "Epoch: 11 cost time: 10.512802600860596\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1481259 Vali Loss: 0.0320139 Test Loss: 0.1077969\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.0950868\n",
      "\tspeed: 0.0510s/iter; left time: 256.6987s\n",
      "\titers: 200, epoch: 12 | loss: 0.1472794\n",
      "\tspeed: 0.0163s/iter; left time: 80.5165s\n",
      "\titers: 300, epoch: 12 | loss: 0.0909319\n",
      "\tspeed: 0.0163s/iter; left time: 78.9336s\n",
      "\titers: 400, epoch: 12 | loss: 0.1409163\n",
      "\tspeed: 0.0163s/iter; left time: 77.2005s\n",
      "\titers: 500, epoch: 12 | loss: 0.2169520\n",
      "\tspeed: 0.0163s/iter; left time: 75.6086s\n",
      "Epoch: 12 cost time: 9.611430406570435\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1475296 Vali Loss: 0.0319398 Test Loss: 0.1078073\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.2090934\n",
      "\tspeed: 0.0509s/iter; left time: 227.2496s\n",
      "\titers: 200, epoch: 13 | loss: 0.2313940\n",
      "\tspeed: 0.0164s/iter; left time: 71.3290s\n",
      "\titers: 300, epoch: 13 | loss: 0.1662891\n",
      "\tspeed: 0.0164s/iter; left time: 69.6843s\n",
      "\titers: 400, epoch: 13 | loss: 0.1259979\n",
      "\tspeed: 0.0163s/iter; left time: 67.9890s\n",
      "\titers: 500, epoch: 13 | loss: 0.1475657\n",
      "\tspeed: 0.0164s/iter; left time: 66.4284s\n",
      "Epoch: 13 cost time: 9.595389127731323\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1476226 Vali Loss: 0.0320234 Test Loss: 0.1078198\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1079949215054512, mae:0.19775967299938202\n",
      ">>> LR=1e-4,DO=0.2,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2413167\n",
      "\tspeed: 0.0293s/iter; left time: 331.0887s\n",
      "\titers: 200, epoch: 1 | loss: 0.2447659\n",
      "\tspeed: 0.0172s/iter; left time: 192.3798s\n",
      "\titers: 300, epoch: 1 | loss: 0.1951623\n",
      "\tspeed: 0.0172s/iter; left time: 190.4222s\n",
      "\titers: 400, epoch: 1 | loss: 0.2647790\n",
      "\tspeed: 0.0172s/iter; left time: 189.3569s\n",
      "\titers: 500, epoch: 1 | loss: 0.4042602\n",
      "\tspeed: 0.0172s/iter; left time: 187.5016s\n",
      "Epoch: 1 cost time: 11.060932636260986\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2468356 Vali Loss: 0.0431872 Test Loss: 0.1299762\n",
      "Validation loss decreased (inf --> 0.043187).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2797507\n",
      "\tspeed: 0.0520s/iter; left time: 558.2171s\n",
      "\titers: 200, epoch: 2 | loss: 0.1393885\n",
      "\tspeed: 0.0167s/iter; left time: 177.9228s\n",
      "\titers: 300, epoch: 2 | loss: 0.2722968\n",
      "\tspeed: 0.0156s/iter; left time: 164.4728s\n",
      "\titers: 400, epoch: 2 | loss: 0.1652651\n",
      "\tspeed: 0.0161s/iter; left time: 167.4754s\n",
      "\titers: 500, epoch: 2 | loss: 0.2828748\n",
      "\tspeed: 0.0172s/iter; left time: 177.2619s\n",
      "Epoch: 2 cost time: 9.781877040863037\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2045503 Vali Loss: 0.0416179 Test Loss: 0.1277904\n",
      "Validation loss decreased (0.043187 --> 0.041618).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2099430\n",
      "\tspeed: 0.0497s/iter; left time: 504.5190s\n",
      "\titers: 200, epoch: 3 | loss: 0.1105419\n",
      "\tspeed: 0.0178s/iter; left time: 178.9055s\n",
      "\titers: 300, epoch: 3 | loss: 0.1900432\n",
      "\tspeed: 0.0178s/iter; left time: 177.0201s\n",
      "\titers: 400, epoch: 3 | loss: 0.1622627\n",
      "\tspeed: 0.0178s/iter; left time: 175.3483s\n",
      "\titers: 500, epoch: 3 | loss: 0.1956563\n",
      "\tspeed: 0.0178s/iter; left time: 173.6739s\n",
      "Epoch: 3 cost time: 10.173734903335571\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1829635 Vali Loss: 0.0392807 Test Loss: 0.1231934\n",
      "Validation loss decreased (0.041618 --> 0.039281).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1368286\n",
      "\tspeed: 0.0491s/iter; left time: 470.9119s\n",
      "\titers: 200, epoch: 4 | loss: 0.2423485\n",
      "\tspeed: 0.0156s/iter; left time: 147.8103s\n",
      "\titers: 300, epoch: 4 | loss: 0.1873889\n",
      "\tspeed: 0.0156s/iter; left time: 146.2722s\n",
      "\titers: 400, epoch: 4 | loss: 0.1467016\n",
      "\tspeed: 0.0155s/iter; left time: 144.4240s\n",
      "\titers: 500, epoch: 4 | loss: 0.2073897\n",
      "\tspeed: 0.0156s/iter; left time: 143.2898s\n",
      "Epoch: 4 cost time: 9.18679165840149\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1739222 Vali Loss: 0.0375154 Test Loss: 0.1216830\n",
      "Validation loss decreased (0.039281 --> 0.037515).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1482170\n",
      "\tspeed: 0.0493s/iter; left time: 444.8909s\n",
      "\titers: 200, epoch: 5 | loss: 0.1361156\n",
      "\tspeed: 0.0154s/iter; left time: 137.7712s\n",
      "\titers: 300, epoch: 5 | loss: 0.1386197\n",
      "\tspeed: 0.0155s/iter; left time: 136.4560s\n",
      "\titers: 400, epoch: 5 | loss: 0.1360067\n",
      "\tspeed: 0.0154s/iter; left time: 134.7255s\n",
      "\titers: 500, epoch: 5 | loss: 0.1387597\n",
      "\tspeed: 0.0155s/iter; left time: 133.3229s\n",
      "Epoch: 5 cost time: 9.134907484054565\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1657920 Vali Loss: 0.0373320 Test Loss: 0.1218878\n",
      "Validation loss decreased (0.037515 --> 0.037332).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1826727\n",
      "\tspeed: 0.0505s/iter; left time: 426.7671s\n",
      "\titers: 200, epoch: 6 | loss: 0.1267354\n",
      "\tspeed: 0.0172s/iter; left time: 143.4061s\n",
      "\titers: 300, epoch: 6 | loss: 0.0930751\n",
      "\tspeed: 0.0169s/iter; left time: 139.3659s\n",
      "\titers: 400, epoch: 6 | loss: 0.1173369\n",
      "\tspeed: 0.0156s/iter; left time: 126.7661s\n",
      "\titers: 500, epoch: 6 | loss: 0.1261780\n",
      "\tspeed: 0.0156s/iter; left time: 125.2633s\n",
      "Epoch: 6 cost time: 9.617348670959473\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1635939 Vali Loss: 0.0366563 Test Loss: 0.1212485\n",
      "Validation loss decreased (0.037332 --> 0.036656).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1912948\n",
      "\tspeed: 0.0481s/iter; left time: 379.2485s\n",
      "\titers: 200, epoch: 7 | loss: 0.1365837\n",
      "\tspeed: 0.0155s/iter; left time: 120.7377s\n",
      "\titers: 300, epoch: 7 | loss: 0.1388618\n",
      "\tspeed: 0.0155s/iter; left time: 119.2989s\n",
      "\titers: 400, epoch: 7 | loss: 0.1891867\n",
      "\tspeed: 0.0155s/iter; left time: 117.5702s\n",
      "\titers: 500, epoch: 7 | loss: 0.1679867\n",
      "\tspeed: 0.0155s/iter; left time: 116.0261s\n",
      "Epoch: 7 cost time: 9.156214714050293\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1611788 Vali Loss: 0.0365083 Test Loss: 0.1210062\n",
      "Validation loss decreased (0.036656 --> 0.036508).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1519459\n",
      "\tspeed: 0.0488s/iter; left time: 356.5490s\n",
      "\titers: 200, epoch: 8 | loss: 0.1881487\n",
      "\tspeed: 0.0154s/iter; left time: 111.2559s\n",
      "\titers: 300, epoch: 8 | loss: 0.1068528\n",
      "\tspeed: 0.0154s/iter; left time: 109.6970s\n",
      "\titers: 400, epoch: 8 | loss: 0.1606441\n",
      "\tspeed: 0.0154s/iter; left time: 108.1538s\n",
      "\titers: 500, epoch: 8 | loss: 0.1078089\n",
      "\tspeed: 0.0154s/iter; left time: 106.5966s\n",
      "Epoch: 8 cost time: 9.117467641830444\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1593887 Vali Loss: 0.0366137 Test Loss: 0.1212772\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1538752\n",
      "\tspeed: 0.0493s/iter; left time: 332.2466s\n",
      "\titers: 200, epoch: 9 | loss: 0.1486090\n",
      "\tspeed: 0.0171s/iter; left time: 113.8541s\n",
      "\titers: 300, epoch: 9 | loss: 0.1573620\n",
      "\tspeed: 0.0171s/iter; left time: 112.0975s\n",
      "\titers: 400, epoch: 9 | loss: 0.1237305\n",
      "\tspeed: 0.0171s/iter; left time: 110.4213s\n",
      "\titers: 500, epoch: 9 | loss: 0.1568079\n",
      "\tspeed: 0.0172s/iter; left time: 108.8352s\n",
      "Epoch: 9 cost time: 10.060559034347534\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1593239 Vali Loss: 0.0365885 Test Loss: 0.1213836\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1925525\n",
      "\tspeed: 0.0496s/iter; left time: 306.2649s\n",
      "\titers: 200, epoch: 10 | loss: 0.1895633\n",
      "\tspeed: 0.0155s/iter; left time: 94.1317s\n",
      "\titers: 300, epoch: 10 | loss: 0.1333423\n",
      "\tspeed: 0.0155s/iter; left time: 92.7845s\n",
      "\titers: 400, epoch: 10 | loss: 0.1853685\n",
      "\tspeed: 0.0155s/iter; left time: 91.2803s\n",
      "\titers: 500, epoch: 10 | loss: 0.2689129\n",
      "\tspeed: 0.0156s/iter; left time: 89.7984s\n",
      "Epoch: 10 cost time: 9.23764967918396\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1621295 Vali Loss: 0.0365173 Test Loss: 0.1213550\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1211564838886261, mae:0.2156672477722168\n",
      ">>> LR=1e-4,DO=0.2,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1701213\n",
      "\tspeed: 0.0301s/iter; left time: 340.4649s\n",
      "\titers: 200, epoch: 1 | loss: 0.2276553\n",
      "\tspeed: 0.0186s/iter; left time: 208.2304s\n",
      "\titers: 300, epoch: 1 | loss: 0.3742180\n",
      "\tspeed: 0.0186s/iter; left time: 206.3400s\n",
      "\titers: 400, epoch: 1 | loss: 0.2055091\n",
      "\tspeed: 0.0186s/iter; left time: 204.8428s\n",
      "\titers: 500, epoch: 1 | loss: 0.1715035\n",
      "\tspeed: 0.0186s/iter; left time: 203.1586s\n",
      "Epoch: 1 cost time: 11.813156604766846\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2427779 Vali Loss: 0.0438735 Test Loss: 0.1348092\n",
      "Validation loss decreased (inf --> 0.043874).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3773701\n",
      "\tspeed: 0.0588s/iter; left time: 630.5824s\n",
      "\titers: 200, epoch: 2 | loss: 0.1882428\n",
      "\tspeed: 0.0185s/iter; left time: 196.7245s\n",
      "\titers: 300, epoch: 2 | loss: 0.1453577\n",
      "\tspeed: 0.0185s/iter; left time: 194.6598s\n",
      "\titers: 400, epoch: 2 | loss: 0.1704081\n",
      "\tspeed: 0.0185s/iter; left time: 192.8801s\n",
      "\titers: 500, epoch: 2 | loss: 0.1764275\n",
      "\tspeed: 0.0185s/iter; left time: 190.9878s\n",
      "Epoch: 2 cost time: 10.838157415390015\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2056473 Vali Loss: 0.0413137 Test Loss: 0.1240067\n",
      "Validation loss decreased (0.043874 --> 0.041314).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1649174\n",
      "\tspeed: 0.0556s/iter; left time: 565.2800s\n",
      "\titers: 200, epoch: 3 | loss: 0.1766003\n",
      "\tspeed: 0.0186s/iter; left time: 186.8933s\n",
      "\titers: 300, epoch: 3 | loss: 0.1858291\n",
      "\tspeed: 0.0186s/iter; left time: 185.3019s\n",
      "\titers: 400, epoch: 3 | loss: 0.1261468\n",
      "\tspeed: 0.0186s/iter; left time: 183.5429s\n",
      "\titers: 500, epoch: 3 | loss: 0.1542312\n",
      "\tspeed: 0.0186s/iter; left time: 181.4823s\n",
      "Epoch: 3 cost time: 10.89339566230774\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1867160 Vali Loss: 0.0395584 Test Loss: 0.1199562\n",
      "Validation loss decreased (0.041314 --> 0.039558).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2248079\n",
      "\tspeed: 0.0598s/iter; left time: 573.9904s\n",
      "\titers: 200, epoch: 4 | loss: 0.2410038\n",
      "\tspeed: 0.0186s/iter; left time: 176.0732s\n",
      "\titers: 300, epoch: 4 | loss: 0.2181978\n",
      "\tspeed: 0.0186s/iter; left time: 174.6877s\n",
      "\titers: 400, epoch: 4 | loss: 0.1244966\n",
      "\tspeed: 0.0184s/iter; left time: 171.3728s\n",
      "\titers: 500, epoch: 4 | loss: 0.1746538\n",
      "\tspeed: 0.0184s/iter; left time: 168.9589s\n",
      "Epoch: 4 cost time: 10.85622501373291\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1779401 Vali Loss: 0.0380415 Test Loss: 0.1196130\n",
      "Validation loss decreased (0.039558 --> 0.038041).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1850338\n",
      "\tspeed: 0.0603s/iter; left time: 544.1521s\n",
      "\titers: 200, epoch: 5 | loss: 0.1880086\n",
      "\tspeed: 0.0200s/iter; left time: 178.8183s\n",
      "\titers: 300, epoch: 5 | loss: 0.1586958\n",
      "\tspeed: 0.0187s/iter; left time: 164.6201s\n",
      "\titers: 400, epoch: 5 | loss: 0.1390885\n",
      "\tspeed: 0.0187s/iter; left time: 162.7366s\n",
      "\titers: 500, epoch: 5 | loss: 0.1527496\n",
      "\tspeed: 0.0187s/iter; left time: 160.8822s\n",
      "Epoch: 5 cost time: 11.293629884719849\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1739779 Vali Loss: 0.0368697 Test Loss: 0.1180260\n",
      "Validation loss decreased (0.038041 --> 0.036870).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0908256\n",
      "\tspeed: 0.0579s/iter; left time: 489.2901s\n",
      "\titers: 200, epoch: 6 | loss: 0.1706789\n",
      "\tspeed: 0.0185s/iter; left time: 154.8766s\n",
      "\titers: 300, epoch: 6 | loss: 0.1332409\n",
      "\tspeed: 0.0185s/iter; left time: 152.9559s\n",
      "\titers: 400, epoch: 6 | loss: 0.1949337\n",
      "\tspeed: 0.0185s/iter; left time: 151.0792s\n",
      "\titers: 500, epoch: 6 | loss: 0.1837593\n",
      "\tspeed: 0.0186s/iter; left time: 149.4221s\n",
      "Epoch: 6 cost time: 10.896344661712646\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1713157 Vali Loss: 0.0367580 Test Loss: 0.1180002\n",
      "Validation loss decreased (0.036870 --> 0.036758).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1275187\n",
      "\tspeed: 0.0571s/iter; left time: 450.2435s\n",
      "\titers: 200, epoch: 7 | loss: 0.1628376\n",
      "\tspeed: 0.0185s/iter; left time: 144.0787s\n",
      "\titers: 300, epoch: 7 | loss: 0.1614579\n",
      "\tspeed: 0.0185s/iter; left time: 142.1528s\n",
      "\titers: 400, epoch: 7 | loss: 0.1849743\n",
      "\tspeed: 0.0185s/iter; left time: 140.2877s\n",
      "\titers: 500, epoch: 7 | loss: 0.2650965\n",
      "\tspeed: 0.0185s/iter; left time: 138.3964s\n",
      "Epoch: 7 cost time: 10.866853952407837\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1693896 Vali Loss: 0.0368110 Test Loss: 0.1179816\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1658965\n",
      "\tspeed: 0.0560s/iter; left time: 409.6529s\n",
      "\titers: 200, epoch: 8 | loss: 0.1343313\n",
      "\tspeed: 0.0186s/iter; left time: 133.9893s\n",
      "\titers: 300, epoch: 8 | loss: 0.1295584\n",
      "\tspeed: 0.0186s/iter; left time: 132.1996s\n",
      "\titers: 400, epoch: 8 | loss: 0.2085418\n",
      "\tspeed: 0.0186s/iter; left time: 130.2382s\n",
      "\titers: 500, epoch: 8 | loss: 0.1509864\n",
      "\tspeed: 0.0186s/iter; left time: 128.3357s\n",
      "Epoch: 8 cost time: 10.851993560791016\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1687659 Vali Loss: 0.0365846 Test Loss: 0.1179681\n",
      "Validation loss decreased (0.036758 --> 0.036585).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0972329\n",
      "\tspeed: 0.0586s/iter; left time: 395.0272s\n",
      "\titers: 200, epoch: 9 | loss: 0.2183301\n",
      "\tspeed: 0.0184s/iter; left time: 122.4898s\n",
      "\titers: 300, epoch: 9 | loss: 0.0979860\n",
      "\tspeed: 0.0184s/iter; left time: 120.6687s\n",
      "\titers: 400, epoch: 9 | loss: 0.1908693\n",
      "\tspeed: 0.0184s/iter; left time: 118.7823s\n",
      "\titers: 500, epoch: 9 | loss: 0.0836755\n",
      "\tspeed: 0.0184s/iter; left time: 116.8667s\n",
      "Epoch: 9 cost time: 10.819642305374146\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1694459 Vali Loss: 0.0367797 Test Loss: 0.1178907\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1670914\n",
      "\tspeed: 0.0583s/iter; left time: 359.5532s\n",
      "\titers: 200, epoch: 10 | loss: 0.2010027\n",
      "\tspeed: 0.0186s/iter; left time: 112.8550s\n",
      "\titers: 300, epoch: 10 | loss: 0.1051348\n",
      "\tspeed: 0.0185s/iter; left time: 110.4709s\n",
      "\titers: 400, epoch: 10 | loss: 0.1393016\n",
      "\tspeed: 0.0184s/iter; left time: 108.0709s\n",
      "\titers: 500, epoch: 10 | loss: 0.1491832\n",
      "\tspeed: 0.0184s/iter; left time: 106.3504s\n",
      "Epoch: 10 cost time: 10.801588535308838\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1686444 Vali Loss: 0.0364459 Test Loss: 0.1173587\n",
      "Validation loss decreased (0.036585 --> 0.036446).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1405297\n",
      "\tspeed: 0.0594s/iter; left time: 332.7217s\n",
      "\titers: 200, epoch: 11 | loss: 0.2319062\n",
      "\tspeed: 0.0199s/iter; left time: 109.6291s\n",
      "\titers: 300, epoch: 11 | loss: 0.1606440\n",
      "\tspeed: 0.0187s/iter; left time: 100.8728s\n",
      "\titers: 400, epoch: 11 | loss: 0.1128259\n",
      "\tspeed: 0.0187s/iter; left time: 98.9822s\n",
      "\titers: 500, epoch: 11 | loss: 0.2395503\n",
      "\tspeed: 0.0187s/iter; left time: 97.0600s\n",
      "Epoch: 11 cost time: 11.279836654663086\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1688100 Vali Loss: 0.0364996 Test Loss: 0.1175109\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1831477\n",
      "\tspeed: 0.0562s/iter; left time: 282.7731s\n",
      "\titers: 200, epoch: 12 | loss: 0.2048599\n",
      "\tspeed: 0.0187s/iter; left time: 92.0266s\n",
      "\titers: 300, epoch: 12 | loss: 0.0970611\n",
      "\tspeed: 0.0186s/iter; left time: 89.8085s\n",
      "\titers: 400, epoch: 12 | loss: 0.1759743\n",
      "\tspeed: 0.0186s/iter; left time: 87.9130s\n",
      "\titers: 500, epoch: 12 | loss: 0.1466150\n",
      "\tspeed: 0.0186s/iter; left time: 86.0830s\n",
      "Epoch: 12 cost time: 10.907198190689087\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1692958 Vali Loss: 0.0363186 Test Loss: 0.1174844\n",
      "Validation loss decreased (0.036446 --> 0.036319).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1935764\n",
      "\tspeed: 0.0577s/iter; left time: 257.3975s\n",
      "\titers: 200, epoch: 13 | loss: 0.1432235\n",
      "\tspeed: 0.0185s/iter; left time: 80.4647s\n",
      "\titers: 300, epoch: 13 | loss: 0.1678448\n",
      "\tspeed: 0.0184s/iter; left time: 78.5767s\n",
      "\titers: 400, epoch: 13 | loss: 0.1233341\n",
      "\tspeed: 0.0184s/iter; left time: 76.7274s\n",
      "\titers: 500, epoch: 13 | loss: 0.2573188\n",
      "\tspeed: 0.0184s/iter; left time: 74.8615s\n",
      "Epoch: 13 cost time: 10.814025402069092\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1691465 Vali Loss: 0.0364455 Test Loss: 0.1174650\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1777302\n",
      "\tspeed: 0.0588s/iter; left time: 228.7296s\n",
      "\titers: 200, epoch: 14 | loss: 0.1110812\n",
      "\tspeed: 0.0186s/iter; left time: 70.4957s\n",
      "\titers: 300, epoch: 14 | loss: 0.1335861\n",
      "\tspeed: 0.0186s/iter; left time: 68.6335s\n",
      "\titers: 400, epoch: 14 | loss: 0.1736268\n",
      "\tspeed: 0.0186s/iter; left time: 66.7892s\n",
      "\titers: 500, epoch: 14 | loss: 0.2093794\n",
      "\tspeed: 0.0186s/iter; left time: 64.9429s\n",
      "Epoch: 14 cost time: 10.895894289016724\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1693446 Vali Loss: 0.0364211 Test Loss: 0.1174515\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1223397\n",
      "\tspeed: 0.0560s/iter; left time: 186.1326s\n",
      "\titers: 200, epoch: 15 | loss: 0.1927694\n",
      "\tspeed: 0.0186s/iter; left time: 59.7678s\n",
      "\titers: 300, epoch: 15 | loss: 0.1334334\n",
      "\tspeed: 0.0186s/iter; left time: 57.9306s\n",
      "\titers: 400, epoch: 15 | loss: 0.2074654\n",
      "\tspeed: 0.0186s/iter; left time: 56.0416s\n",
      "\titers: 500, epoch: 15 | loss: 0.1467664\n",
      "\tspeed: 0.0185s/iter; left time: 54.1423s\n",
      "Epoch: 15 cost time: 10.868999719619751\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1696263 Vali Loss: 0.0364447 Test Loss: 0.1174427\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11763672530651093, mae:0.2096700668334961\n",
      ">>> LR=1e-4,DO=0.2,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4402836\n",
      "\tspeed: 0.0299s/iter; left time: 338.0881s\n",
      "\titers: 200, epoch: 1 | loss: 0.2280317\n",
      "\tspeed: 0.0183s/iter; left time: 204.7724s\n",
      "\titers: 300, epoch: 1 | loss: 0.2163184\n",
      "\tspeed: 0.0183s/iter; left time: 202.9987s\n",
      "\titers: 400, epoch: 1 | loss: 0.1716065\n",
      "\tspeed: 0.0183s/iter; left time: 201.6183s\n",
      "\titers: 500, epoch: 1 | loss: 0.2263470\n",
      "\tspeed: 0.0183s/iter; left time: 199.7056s\n",
      "Epoch: 1 cost time: 11.646839380264282\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2177778 Vali Loss: 0.0359990 Test Loss: 0.1144829\n",
      "Validation loss decreased (inf --> 0.035999).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1530267\n",
      "\tspeed: 0.0573s/iter; left time: 614.7974s\n",
      "\titers: 200, epoch: 2 | loss: 0.2354390\n",
      "\tspeed: 0.0183s/iter; left time: 194.6309s\n",
      "\titers: 300, epoch: 2 | loss: 0.2974493\n",
      "\tspeed: 0.0183s/iter; left time: 192.3844s\n",
      "\titers: 400, epoch: 2 | loss: 0.1610678\n",
      "\tspeed: 0.0184s/iter; left time: 191.4642s\n",
      "\titers: 500, epoch: 2 | loss: 0.1318150\n",
      "\tspeed: 0.0184s/iter; left time: 189.7632s\n",
      "Epoch: 2 cost time: 10.765905141830444\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1872759 Vali Loss: 0.0355785 Test Loss: 0.1103872\n",
      "Validation loss decreased (0.035999 --> 0.035578).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1258722\n",
      "\tspeed: 0.0560s/iter; left time: 569.4283s\n",
      "\titers: 200, epoch: 3 | loss: 0.1638420\n",
      "\tspeed: 0.0184s/iter; left time: 184.7245s\n",
      "\titers: 300, epoch: 3 | loss: 0.1729170\n",
      "\tspeed: 0.0183s/iter; left time: 182.6766s\n",
      "\titers: 400, epoch: 3 | loss: 0.1569908\n",
      "\tspeed: 0.0183s/iter; left time: 180.6873s\n",
      "\titers: 500, epoch: 3 | loss: 0.1315092\n",
      "\tspeed: 0.0183s/iter; left time: 178.9412s\n",
      "Epoch: 3 cost time: 10.763192653656006\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1725716 Vali Loss: 0.0344700 Test Loss: 0.1102411\n",
      "Validation loss decreased (0.035578 --> 0.034470).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1933473\n",
      "\tspeed: 0.0554s/iter; left time: 531.2877s\n",
      "\titers: 200, epoch: 4 | loss: 0.1183194\n",
      "\tspeed: 0.0183s/iter; left time: 173.5738s\n",
      "\titers: 300, epoch: 4 | loss: 0.3026279\n",
      "\tspeed: 0.0183s/iter; left time: 171.7115s\n",
      "\titers: 400, epoch: 4 | loss: 0.1547152\n",
      "\tspeed: 0.0183s/iter; left time: 169.7462s\n",
      "\titers: 500, epoch: 4 | loss: 0.1303916\n",
      "\tspeed: 0.0183s/iter; left time: 167.9024s\n",
      "Epoch: 4 cost time: 10.70569658279419\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1665681 Vali Loss: 0.0332996 Test Loss: 0.1072504\n",
      "Validation loss decreased (0.034470 --> 0.033300).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1343402\n",
      "\tspeed: 0.0557s/iter; left time: 502.5663s\n",
      "\titers: 200, epoch: 5 | loss: 0.1484613\n",
      "\tspeed: 0.0183s/iter; left time: 163.4106s\n",
      "\titers: 300, epoch: 5 | loss: 0.1597691\n",
      "\tspeed: 0.0183s/iter; left time: 161.6312s\n",
      "\titers: 400, epoch: 5 | loss: 0.1628501\n",
      "\tspeed: 0.0183s/iter; left time: 159.7070s\n",
      "\titers: 500, epoch: 5 | loss: 0.1543639\n",
      "\tspeed: 0.0183s/iter; left time: 157.9101s\n",
      "Epoch: 5 cost time: 10.750510692596436\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1636487 Vali Loss: 0.0330104 Test Loss: 0.1068674\n",
      "Validation loss decreased (0.033300 --> 0.033010).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2362644\n",
      "\tspeed: 0.0574s/iter; left time: 485.1516s\n",
      "\titers: 200, epoch: 6 | loss: 0.1262991\n",
      "\tspeed: 0.0184s/iter; left time: 153.5183s\n",
      "\titers: 300, epoch: 6 | loss: 0.1783826\n",
      "\tspeed: 0.0184s/iter; left time: 151.4388s\n",
      "\titers: 400, epoch: 6 | loss: 0.1358697\n",
      "\tspeed: 0.0184s/iter; left time: 149.6255s\n",
      "\titers: 500, epoch: 6 | loss: 0.0978378\n",
      "\tspeed: 0.0184s/iter; left time: 147.9861s\n",
      "Epoch: 6 cost time: 10.745052576065063\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1615609 Vali Loss: 0.0325446 Test Loss: 0.1063081\n",
      "Validation loss decreased (0.033010 --> 0.032545).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1607695\n",
      "\tspeed: 0.0562s/iter; left time: 442.6905s\n",
      "\titers: 200, epoch: 7 | loss: 0.2219531\n",
      "\tspeed: 0.0184s/iter; left time: 143.0789s\n",
      "\titers: 300, epoch: 7 | loss: 0.1253432\n",
      "\tspeed: 0.0184s/iter; left time: 141.2029s\n",
      "\titers: 400, epoch: 7 | loss: 0.1608795\n",
      "\tspeed: 0.0184s/iter; left time: 139.1591s\n",
      "\titers: 500, epoch: 7 | loss: 0.1382012\n",
      "\tspeed: 0.0184s/iter; left time: 137.4268s\n",
      "Epoch: 7 cost time: 10.771886110305786\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1585845 Vali Loss: 0.0325666 Test Loss: 0.1066777\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1129051\n",
      "\tspeed: 0.0552s/iter; left time: 403.9246s\n",
      "\titers: 200, epoch: 8 | loss: 0.1330525\n",
      "\tspeed: 0.0183s/iter; left time: 131.6300s\n",
      "\titers: 300, epoch: 8 | loss: 0.1465790\n",
      "\tspeed: 0.0182s/iter; left time: 129.5306s\n",
      "\titers: 400, epoch: 8 | loss: 0.1551302\n",
      "\tspeed: 0.0182s/iter; left time: 127.7641s\n",
      "\titers: 500, epoch: 8 | loss: 0.2000866\n",
      "\tspeed: 0.0182s/iter; left time: 125.8231s\n",
      "Epoch: 8 cost time: 10.683382511138916\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1580343 Vali Loss: 0.0326724 Test Loss: 0.1066297\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0877338\n",
      "\tspeed: 0.0564s/iter; left time: 380.5114s\n",
      "\titers: 200, epoch: 9 | loss: 0.1193765\n",
      "\tspeed: 0.0184s/iter; left time: 122.0577s\n",
      "\titers: 300, epoch: 9 | loss: 0.1743827\n",
      "\tspeed: 0.0184s/iter; left time: 120.1061s\n",
      "\titers: 400, epoch: 9 | loss: 0.1482083\n",
      "\tspeed: 0.0184s/iter; left time: 118.2930s\n",
      "\titers: 500, epoch: 9 | loss: 0.1290707\n",
      "\tspeed: 0.0184s/iter; left time: 116.3956s\n",
      "Epoch: 9 cost time: 10.76368761062622\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1584717 Vali Loss: 0.0326634 Test Loss: 0.1063355\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10645223408937454, mae:0.19559837877750397\n",
      ">>> LR=1e-4,DO=0.2,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2651801\n",
      "\tspeed: 0.0326s/iter; left time: 367.9997s\n",
      "\titers: 200, epoch: 1 | loss: 0.2061377\n",
      "\tspeed: 0.0204s/iter; left time: 228.9651s\n",
      "\titers: 300, epoch: 1 | loss: 0.2616118\n",
      "\tspeed: 0.0205s/iter; left time: 227.4625s\n",
      "\titers: 400, epoch: 1 | loss: 0.2890184\n",
      "\tspeed: 0.0205s/iter; left time: 225.8515s\n",
      "\titers: 500, epoch: 1 | loss: 0.3381493\n",
      "\tspeed: 0.0204s/iter; left time: 222.8007s\n",
      "Epoch: 1 cost time: 12.934062004089355\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2511811 Vali Loss: 0.0428458 Test Loss: 0.1402139\n",
      "Validation loss decreased (inf --> 0.042846).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2722202\n",
      "\tspeed: 0.0599s/iter; left time: 643.2503s\n",
      "\titers: 200, epoch: 2 | loss: 0.3023338\n",
      "\tspeed: 0.0205s/iter; left time: 217.8314s\n",
      "\titers: 300, epoch: 2 | loss: 0.4220283\n",
      "\tspeed: 0.0205s/iter; left time: 215.5290s\n",
      "\titers: 400, epoch: 2 | loss: 0.1778596\n",
      "\tspeed: 0.0206s/iter; left time: 214.4352s\n",
      "\titers: 500, epoch: 2 | loss: 0.1238796\n",
      "\tspeed: 0.0205s/iter; left time: 211.4107s\n",
      "Epoch: 2 cost time: 12.004593133926392\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2115823 Vali Loss: 0.0441549 Test Loss: 0.1311648\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1402862\n",
      "\tspeed: 0.0565s/iter; left time: 574.1456s\n",
      "\titers: 200, epoch: 3 | loss: 0.1880560\n",
      "\tspeed: 0.0182s/iter; left time: 183.2780s\n",
      "\titers: 300, epoch: 3 | loss: 0.2121368\n",
      "\tspeed: 0.0182s/iter; left time: 181.3041s\n",
      "\titers: 400, epoch: 3 | loss: 0.2370778\n",
      "\tspeed: 0.0182s/iter; left time: 179.4372s\n",
      "\titers: 500, epoch: 3 | loss: 0.1820183\n",
      "\tspeed: 0.0182s/iter; left time: 177.5720s\n",
      "Epoch: 3 cost time: 10.6332368850708\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1936986 Vali Loss: 0.0395063 Test Loss: 0.1265240\n",
      "Validation loss decreased (0.042846 --> 0.039506).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1352281\n",
      "\tspeed: 0.0599s/iter; left time: 574.5997s\n",
      "\titers: 200, epoch: 4 | loss: 0.1846642\n",
      "\tspeed: 0.0205s/iter; left time: 194.4604s\n",
      "\titers: 300, epoch: 4 | loss: 0.1713757\n",
      "\tspeed: 0.0205s/iter; left time: 192.3996s\n",
      "\titers: 400, epoch: 4 | loss: 0.2349405\n",
      "\tspeed: 0.0205s/iter; left time: 190.3966s\n",
      "\titers: 500, epoch: 4 | loss: 0.1164267\n",
      "\tspeed: 0.0205s/iter; left time: 188.2475s\n",
      "Epoch: 4 cost time: 11.988919496536255\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1848535 Vali Loss: 0.0393417 Test Loss: 0.1241873\n",
      "Validation loss decreased (0.039506 --> 0.039342).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1429969\n",
      "\tspeed: 0.0594s/iter; left time: 535.6586s\n",
      "\titers: 200, epoch: 5 | loss: 0.1879042\n",
      "\tspeed: 0.0205s/iter; left time: 182.6063s\n",
      "\titers: 300, epoch: 5 | loss: 0.1457430\n",
      "\tspeed: 0.0205s/iter; left time: 180.4921s\n",
      "\titers: 400, epoch: 5 | loss: 0.1728651\n",
      "\tspeed: 0.0205s/iter; left time: 178.4266s\n",
      "\titers: 500, epoch: 5 | loss: 0.0938167\n",
      "\tspeed: 0.0205s/iter; left time: 176.6964s\n",
      "Epoch: 5 cost time: 11.960446119308472\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1798460 Vali Loss: 0.0385588 Test Loss: 0.1223332\n",
      "Validation loss decreased (0.039342 --> 0.038559).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2298993\n",
      "\tspeed: 0.0583s/iter; left time: 492.8683s\n",
      "\titers: 200, epoch: 6 | loss: 0.1317866\n",
      "\tspeed: 0.0183s/iter; left time: 153.0079s\n",
      "\titers: 300, epoch: 6 | loss: 0.3391422\n",
      "\tspeed: 0.0183s/iter; left time: 151.1662s\n",
      "\titers: 400, epoch: 6 | loss: 0.1500954\n",
      "\tspeed: 0.0183s/iter; left time: 149.0775s\n",
      "\titers: 500, epoch: 6 | loss: 0.1802107\n",
      "\tspeed: 0.0183s/iter; left time: 147.1616s\n",
      "Epoch: 6 cost time: 10.745839595794678\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1778498 Vali Loss: 0.0386358 Test Loss: 0.1219048\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1515136\n",
      "\tspeed: 0.0562s/iter; left time: 442.8441s\n",
      "\titers: 200, epoch: 7 | loss: 0.1496806\n",
      "\tspeed: 0.0182s/iter; left time: 141.8847s\n",
      "\titers: 300, epoch: 7 | loss: 0.1845922\n",
      "\tspeed: 0.0182s/iter; left time: 139.9789s\n",
      "\titers: 400, epoch: 7 | loss: 0.1701262\n",
      "\tspeed: 0.0182s/iter; left time: 138.1601s\n",
      "\titers: 500, epoch: 7 | loss: 0.1152266\n",
      "\tspeed: 0.0182s/iter; left time: 136.3441s\n",
      "Epoch: 7 cost time: 10.707399368286133\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1774723 Vali Loss: 0.0386863 Test Loss: 0.1224025\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1423626\n",
      "\tspeed: 0.0555s/iter; left time: 406.0390s\n",
      "\titers: 200, epoch: 8 | loss: 0.1177387\n",
      "\tspeed: 0.0183s/iter; left time: 132.1528s\n",
      "\titers: 300, epoch: 8 | loss: 0.1614120\n",
      "\tspeed: 0.0183s/iter; left time: 130.3085s\n",
      "\titers: 400, epoch: 8 | loss: 0.2179567\n",
      "\tspeed: 0.0183s/iter; left time: 128.4633s\n",
      "\titers: 500, epoch: 8 | loss: 0.1504788\n",
      "\tspeed: 0.0183s/iter; left time: 126.7031s\n",
      "Epoch: 8 cost time: 10.732959270477295\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1768888 Vali Loss: 0.0385472 Test Loss: 0.1227081\n",
      "Validation loss decreased (0.038559 --> 0.038547).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1685435\n",
      "\tspeed: 0.0578s/iter; left time: 389.4792s\n",
      "\titers: 200, epoch: 9 | loss: 0.2040238\n",
      "\tspeed: 0.0205s/iter; left time: 135.9405s\n",
      "\titers: 300, epoch: 9 | loss: 0.2436899\n",
      "\tspeed: 0.0205s/iter; left time: 133.8646s\n",
      "\titers: 400, epoch: 9 | loss: 0.3090192\n",
      "\tspeed: 0.0205s/iter; left time: 131.8185s\n",
      "\titers: 500, epoch: 9 | loss: 0.1248585\n",
      "\tspeed: 0.0205s/iter; left time: 129.7333s\n",
      "Epoch: 9 cost time: 11.9623441696167\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1764126 Vali Loss: 0.0386066 Test Loss: 0.1222442\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1617981\n",
      "\tspeed: 0.0580s/iter; left time: 358.1148s\n",
      "\titers: 200, epoch: 10 | loss: 0.1409273\n",
      "\tspeed: 0.0182s/iter; left time: 110.7477s\n",
      "\titers: 300, epoch: 10 | loss: 0.1977402\n",
      "\tspeed: 0.0182s/iter; left time: 108.9508s\n",
      "\titers: 400, epoch: 10 | loss: 0.1650602\n",
      "\tspeed: 0.0182s/iter; left time: 106.9719s\n",
      "\titers: 500, epoch: 10 | loss: 0.1825350\n",
      "\tspeed: 0.0182s/iter; left time: 105.1330s\n",
      "Epoch: 10 cost time: 10.688281774520874\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1764346 Vali Loss: 0.0383638 Test Loss: 0.1220553\n",
      "Validation loss decreased (0.038547 --> 0.038364).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1726952\n",
      "\tspeed: 0.0561s/iter; left time: 313.9756s\n",
      "\titers: 200, epoch: 11 | loss: 0.1705813\n",
      "\tspeed: 0.0183s/iter; left time: 100.5829s\n",
      "\titers: 300, epoch: 11 | loss: 0.1822181\n",
      "\tspeed: 0.0183s/iter; left time: 98.6064s\n",
      "\titers: 400, epoch: 11 | loss: 0.1932990\n",
      "\tspeed: 0.0184s/iter; left time: 97.6707s\n",
      "\titers: 500, epoch: 11 | loss: 0.1583101\n",
      "\tspeed: 0.0183s/iter; left time: 95.0362s\n",
      "Epoch: 11 cost time: 10.706536531448364\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1773822 Vali Loss: 0.0383823 Test Loss: 0.1220864\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1290665\n",
      "\tspeed: 0.0568s/iter; left time: 285.6260s\n",
      "\titers: 200, epoch: 12 | loss: 0.2061146\n",
      "\tspeed: 0.0184s/iter; left time: 90.4989s\n",
      "\titers: 300, epoch: 12 | loss: 0.1524406\n",
      "\tspeed: 0.0183s/iter; left time: 88.5019s\n",
      "\titers: 400, epoch: 12 | loss: 0.1417967\n",
      "\tspeed: 0.0183s/iter; left time: 86.6533s\n",
      "\titers: 500, epoch: 12 | loss: 0.2068213\n",
      "\tspeed: 0.0184s/iter; left time: 84.9833s\n",
      "Epoch: 12 cost time: 10.755274772644043\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1763787 Vali Loss: 0.0384361 Test Loss: 0.1220857\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1596949\n",
      "\tspeed: 0.0561s/iter; left time: 250.4742s\n",
      "\titers: 200, epoch: 13 | loss: 0.1017286\n",
      "\tspeed: 0.0182s/iter; left time: 79.5848s\n",
      "\titers: 300, epoch: 13 | loss: 0.1444184\n",
      "\tspeed: 0.0183s/iter; left time: 77.7806s\n",
      "\titers: 400, epoch: 13 | loss: 0.1134202\n",
      "\tspeed: 0.0183s/iter; left time: 75.9491s\n",
      "\titers: 500, epoch: 13 | loss: 0.1189065\n",
      "\tspeed: 0.0183s/iter; left time: 74.1617s\n",
      "Epoch: 13 cost time: 10.688488483428955\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1760794 Vali Loss: 0.0385588 Test Loss: 0.1220563\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12221644818782806, mae:0.21517717838287354\n",
      ">>> LR=1e-4,DO=0.2,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2293160\n",
      "\tspeed: 0.0339s/iter; left time: 382.6815s\n",
      "\titers: 200, epoch: 1 | loss: 0.2001203\n",
      "\tspeed: 0.0221s/iter; left time: 247.5792s\n",
      "\titers: 300, epoch: 1 | loss: 0.2317519\n",
      "\tspeed: 0.0221s/iter; left time: 244.9824s\n",
      "\titers: 400, epoch: 1 | loss: 0.3238974\n",
      "\tspeed: 0.0221s/iter; left time: 242.5891s\n",
      "\titers: 500, epoch: 1 | loss: 0.2662609\n",
      "\tspeed: 0.0221s/iter; left time: 240.4858s\n",
      "Epoch: 1 cost time: 13.812785148620605\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2369264 Vali Loss: 0.0448870 Test Loss: 0.1381774\n",
      "Validation loss decreased (inf --> 0.044887).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2949812\n",
      "\tspeed: 0.0650s/iter; left time: 697.2384s\n",
      "\titers: 200, epoch: 2 | loss: 0.1773820\n",
      "\tspeed: 0.0220s/iter; left time: 234.2295s\n",
      "\titers: 300, epoch: 2 | loss: 0.1580946\n",
      "\tspeed: 0.0220s/iter; left time: 231.2782s\n",
      "\titers: 400, epoch: 2 | loss: 0.1641154\n",
      "\tspeed: 0.0220s/iter; left time: 229.7652s\n",
      "\titers: 500, epoch: 2 | loss: 0.1876259\n",
      "\tspeed: 0.0220s/iter; left time: 227.4769s\n",
      "Epoch: 2 cost time: 12.834163665771484\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1964609 Vali Loss: 0.0406355 Test Loss: 0.1257435\n",
      "Validation loss decreased (0.044887 --> 0.040636).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3679785\n",
      "\tspeed: 0.0626s/iter; left time: 635.6897s\n",
      "\titers: 200, epoch: 3 | loss: 0.1269350\n",
      "\tspeed: 0.0220s/iter; left time: 221.7177s\n",
      "\titers: 300, epoch: 3 | loss: 0.2260499\n",
      "\tspeed: 0.0220s/iter; left time: 218.9736s\n",
      "\titers: 400, epoch: 3 | loss: 0.1312096\n",
      "\tspeed: 0.0220s/iter; left time: 216.8935s\n",
      "\titers: 500, epoch: 3 | loss: 0.1155650\n",
      "\tspeed: 0.0220s/iter; left time: 214.7667s\n",
      "Epoch: 3 cost time: 12.838762521743774\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1737390 Vali Loss: 0.0379185 Test Loss: 0.1199889\n",
      "Validation loss decreased (0.040636 --> 0.037919).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1278060\n",
      "\tspeed: 0.0656s/iter; left time: 629.0245s\n",
      "\titers: 200, epoch: 4 | loss: 0.1287254\n",
      "\tspeed: 0.0222s/iter; left time: 210.7320s\n",
      "\titers: 300, epoch: 4 | loss: 0.1430790\n",
      "\tspeed: 0.0222s/iter; left time: 208.3661s\n",
      "\titers: 400, epoch: 4 | loss: 0.1266261\n",
      "\tspeed: 0.0222s/iter; left time: 205.9158s\n",
      "\titers: 500, epoch: 4 | loss: 0.1892242\n",
      "\tspeed: 0.0220s/iter; left time: 202.6529s\n",
      "Epoch: 4 cost time: 12.958061218261719\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1611997 Vali Loss: 0.0359299 Test Loss: 0.1203324\n",
      "Validation loss decreased (0.037919 --> 0.035930).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1713608\n",
      "\tspeed: 0.0647s/iter; left time: 583.2723s\n",
      "\titers: 200, epoch: 5 | loss: 0.1498552\n",
      "\tspeed: 0.0221s/iter; left time: 197.4818s\n",
      "\titers: 300, epoch: 5 | loss: 0.1025852\n",
      "\tspeed: 0.0222s/iter; left time: 195.5518s\n",
      "\titers: 400, epoch: 5 | loss: 0.1424865\n",
      "\tspeed: 0.0221s/iter; left time: 192.7884s\n",
      "\titers: 500, epoch: 5 | loss: 0.1350143\n",
      "\tspeed: 0.0221s/iter; left time: 190.7290s\n",
      "Epoch: 5 cost time: 12.903506755828857\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1554374 Vali Loss: 0.0349023 Test Loss: 0.1193037\n",
      "Validation loss decreased (0.035930 --> 0.034902).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1344327\n",
      "\tspeed: 0.0649s/iter; left time: 548.1738s\n",
      "\titers: 200, epoch: 6 | loss: 0.1777411\n",
      "\tspeed: 0.0222s/iter; left time: 185.4393s\n",
      "\titers: 300, epoch: 6 | loss: 0.1242370\n",
      "\tspeed: 0.0222s/iter; left time: 182.8892s\n",
      "\titers: 400, epoch: 6 | loss: 0.1273502\n",
      "\tspeed: 0.0221s/iter; left time: 180.3197s\n",
      "\titers: 500, epoch: 6 | loss: 0.2024987\n",
      "\tspeed: 0.0221s/iter; left time: 178.3256s\n",
      "Epoch: 6 cost time: 12.968868970870972\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1519231 Vali Loss: 0.0356104 Test Loss: 0.1197865\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1739931\n",
      "\tspeed: 0.0634s/iter; left time: 499.2696s\n",
      "\titers: 200, epoch: 7 | loss: 0.1780449\n",
      "\tspeed: 0.0221s/iter; left time: 171.5851s\n",
      "\titers: 300, epoch: 7 | loss: 0.1181497\n",
      "\tspeed: 0.0221s/iter; left time: 169.4671s\n",
      "\titers: 400, epoch: 7 | loss: 0.1467361\n",
      "\tspeed: 0.0221s/iter; left time: 167.4043s\n",
      "\titers: 500, epoch: 7 | loss: 0.1584349\n",
      "\tspeed: 0.0221s/iter; left time: 165.1126s\n",
      "Epoch: 7 cost time: 12.876832246780396\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1489323 Vali Loss: 0.0355754 Test Loss: 0.1189778\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1233920\n",
      "\tspeed: 0.0630s/iter; left time: 460.6843s\n",
      "\titers: 200, epoch: 8 | loss: 0.3557206\n",
      "\tspeed: 0.0220s/iter; left time: 158.6489s\n",
      "\titers: 300, epoch: 8 | loss: 0.1108116\n",
      "\tspeed: 0.0220s/iter; left time: 156.3286s\n",
      "\titers: 400, epoch: 8 | loss: 0.1410093\n",
      "\tspeed: 0.0220s/iter; left time: 154.2158s\n",
      "\titers: 500, epoch: 8 | loss: 0.1274756\n",
      "\tspeed: 0.0220s/iter; left time: 152.2821s\n",
      "Epoch: 8 cost time: 12.837616443634033\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1493372 Vali Loss: 0.0346900 Test Loss: 0.1187895\n",
      "Validation loss decreased (0.034902 --> 0.034690).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1598254\n",
      "\tspeed: 0.0649s/iter; left time: 437.5378s\n",
      "\titers: 200, epoch: 9 | loss: 0.1528068\n",
      "\tspeed: 0.0221s/iter; left time: 146.7674s\n",
      "\titers: 300, epoch: 9 | loss: 0.1569958\n",
      "\tspeed: 0.0221s/iter; left time: 144.4968s\n",
      "\titers: 400, epoch: 9 | loss: 0.1132833\n",
      "\tspeed: 0.0221s/iter; left time: 142.3231s\n",
      "\titers: 500, epoch: 9 | loss: 0.1267483\n",
      "\tspeed: 0.0221s/iter; left time: 140.0813s\n",
      "Epoch: 9 cost time: 12.903800964355469\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1495864 Vali Loss: 0.0347729 Test Loss: 0.1186510\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1693633\n",
      "\tspeed: 0.0629s/iter; left time: 387.9503s\n",
      "\titers: 200, epoch: 10 | loss: 0.1689127\n",
      "\tspeed: 0.0222s/iter; left time: 134.5608s\n",
      "\titers: 300, epoch: 10 | loss: 0.1406300\n",
      "\tspeed: 0.0222s/iter; left time: 132.3967s\n",
      "\titers: 400, epoch: 10 | loss: 0.2072676\n",
      "\tspeed: 0.0222s/iter; left time: 130.3322s\n",
      "\titers: 500, epoch: 10 | loss: 0.1296462\n",
      "\tspeed: 0.0222s/iter; left time: 128.0927s\n",
      "Epoch: 10 cost time: 12.957226753234863\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1483664 Vali Loss: 0.0349972 Test Loss: 0.1186653\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1339197\n",
      "\tspeed: 0.0632s/iter; left time: 354.1658s\n",
      "\titers: 200, epoch: 11 | loss: 0.1176230\n",
      "\tspeed: 0.0221s/iter; left time: 121.5845s\n",
      "\titers: 300, epoch: 11 | loss: 0.0925952\n",
      "\tspeed: 0.0221s/iter; left time: 119.3717s\n",
      "\titers: 400, epoch: 11 | loss: 0.1302447\n",
      "\tspeed: 0.0221s/iter; left time: 116.9320s\n",
      "\titers: 500, epoch: 11 | loss: 0.1341548\n",
      "\tspeed: 0.0221s/iter; left time: 114.9034s\n",
      "Epoch: 11 cost time: 12.886634588241577\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1501437 Vali Loss: 0.0350755 Test Loss: 0.1187449\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11892738938331604, mae:0.21354109048843384\n",
      ">>> LR=1e-4,DO=0.2,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2284682\n",
      "\tspeed: 0.0333s/iter; left time: 376.0734s\n",
      "\titers: 200, epoch: 1 | loss: 0.1140975\n",
      "\tspeed: 0.0214s/iter; left time: 239.3476s\n",
      "\titers: 300, epoch: 1 | loss: 0.1872586\n",
      "\tspeed: 0.0214s/iter; left time: 237.1982s\n",
      "\titers: 400, epoch: 1 | loss: 0.1804277\n",
      "\tspeed: 0.0214s/iter; left time: 235.0842s\n",
      "\titers: 500, epoch: 1 | loss: 0.1918750\n",
      "\tspeed: 0.0214s/iter; left time: 233.0636s\n",
      "Epoch: 1 cost time: 13.431069135665894\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2124422 Vali Loss: 0.0370007 Test Loss: 0.1144546\n",
      "Validation loss decreased (inf --> 0.037001).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1790348\n",
      "\tspeed: 0.0651s/iter; left time: 699.0435s\n",
      "\titers: 200, epoch: 2 | loss: 0.1375379\n",
      "\tspeed: 0.0214s/iter; left time: 227.1964s\n",
      "\titers: 300, epoch: 2 | loss: 0.2338322\n",
      "\tspeed: 0.0214s/iter; left time: 225.1415s\n",
      "\titers: 400, epoch: 2 | loss: 0.1661720\n",
      "\tspeed: 0.0214s/iter; left time: 223.1238s\n",
      "\titers: 500, epoch: 2 | loss: 0.1245318\n",
      "\tspeed: 0.0214s/iter; left time: 221.2083s\n",
      "Epoch: 2 cost time: 12.517714500427246\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1824991 Vali Loss: 0.0336115 Test Loss: 0.1110842\n",
      "Validation loss decreased (0.037001 --> 0.033611).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1640447\n",
      "\tspeed: 0.0618s/iter; left time: 628.2373s\n",
      "\titers: 200, epoch: 3 | loss: 0.1281942\n",
      "\tspeed: 0.0214s/iter; left time: 215.4208s\n",
      "\titers: 300, epoch: 3 | loss: 0.1569435\n",
      "\tspeed: 0.0214s/iter; left time: 213.2377s\n",
      "\titers: 400, epoch: 3 | loss: 0.1140294\n",
      "\tspeed: 0.0214s/iter; left time: 211.2747s\n",
      "\titers: 500, epoch: 3 | loss: 0.1418954\n",
      "\tspeed: 0.0214s/iter; left time: 208.8870s\n",
      "Epoch: 3 cost time: 12.497617959976196\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1628980 Vali Loss: 0.0330244 Test Loss: 0.1112351\n",
      "Validation loss decreased (0.033611 --> 0.033024).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1311267\n",
      "\tspeed: 0.0608s/iter; left time: 582.6913s\n",
      "\titers: 200, epoch: 4 | loss: 0.1572121\n",
      "\tspeed: 0.0213s/iter; left time: 202.2775s\n",
      "\titers: 300, epoch: 4 | loss: 0.1599234\n",
      "\tspeed: 0.0213s/iter; left time: 200.0617s\n",
      "\titers: 400, epoch: 4 | loss: 0.1857014\n",
      "\tspeed: 0.0213s/iter; left time: 197.9664s\n",
      "\titers: 500, epoch: 4 | loss: 0.1329582\n",
      "\tspeed: 0.0213s/iter; left time: 195.8449s\n",
      "Epoch: 4 cost time: 12.45613980293274\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1534347 Vali Loss: 0.0313272 Test Loss: 0.1073021\n",
      "Validation loss decreased (0.033024 --> 0.031327).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1965039\n",
      "\tspeed: 0.0614s/iter; left time: 553.8475s\n",
      "\titers: 200, epoch: 5 | loss: 0.1252093\n",
      "\tspeed: 0.0213s/iter; left time: 190.2806s\n",
      "\titers: 300, epoch: 5 | loss: 0.1540219\n",
      "\tspeed: 0.0213s/iter; left time: 188.2721s\n",
      "\titers: 400, epoch: 5 | loss: 0.1737693\n",
      "\tspeed: 0.0214s/iter; left time: 186.2445s\n",
      "\titers: 500, epoch: 5 | loss: 0.1343280\n",
      "\tspeed: 0.0214s/iter; left time: 184.0884s\n",
      "Epoch: 5 cost time: 12.472914218902588\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1480673 Vali Loss: 0.0314623 Test Loss: 0.1069397\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1305945\n",
      "\tspeed: 0.0612s/iter; left time: 516.9196s\n",
      "\titers: 200, epoch: 6 | loss: 0.1698483\n",
      "\tspeed: 0.0214s/iter; left time: 178.8318s\n",
      "\titers: 300, epoch: 6 | loss: 0.2030552\n",
      "\tspeed: 0.0214s/iter; left time: 176.6051s\n",
      "\titers: 400, epoch: 6 | loss: 0.1135494\n",
      "\tspeed: 0.0214s/iter; left time: 174.5068s\n",
      "\titers: 500, epoch: 6 | loss: 0.0988165\n",
      "\tspeed: 0.0214s/iter; left time: 172.5971s\n",
      "Epoch: 6 cost time: 12.503016948699951\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1447756 Vali Loss: 0.0306504 Test Loss: 0.1057928\n",
      "Validation loss decreased (0.031327 --> 0.030650).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1269063\n",
      "\tspeed: 0.0623s/iter; left time: 491.2659s\n",
      "\titers: 200, epoch: 7 | loss: 0.1763606\n",
      "\tspeed: 0.0214s/iter; left time: 166.4906s\n",
      "\titers: 300, epoch: 7 | loss: 0.1897619\n",
      "\tspeed: 0.0214s/iter; left time: 164.3624s\n",
      "\titers: 400, epoch: 7 | loss: 0.1130605\n",
      "\tspeed: 0.0214s/iter; left time: 162.2003s\n",
      "\titers: 500, epoch: 7 | loss: 0.1624167\n",
      "\tspeed: 0.0214s/iter; left time: 160.0491s\n",
      "Epoch: 7 cost time: 12.51346755027771\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1435283 Vali Loss: 0.0310711 Test Loss: 0.1057043\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1158329\n",
      "\tspeed: 0.0603s/iter; left time: 441.1756s\n",
      "\titers: 200, epoch: 8 | loss: 0.1492592\n",
      "\tspeed: 0.0213s/iter; left time: 153.9061s\n",
      "\titers: 300, epoch: 8 | loss: 0.1385576\n",
      "\tspeed: 0.0214s/iter; left time: 151.9016s\n",
      "\titers: 400, epoch: 8 | loss: 0.1291806\n",
      "\tspeed: 0.0213s/iter; left time: 149.6708s\n",
      "\titers: 500, epoch: 8 | loss: 0.2251658\n",
      "\tspeed: 0.0214s/iter; left time: 147.6340s\n",
      "Epoch: 8 cost time: 12.441862106323242\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1441585 Vali Loss: 0.0307371 Test Loss: 0.1055154\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1272317\n",
      "\tspeed: 0.0652s/iter; left time: 439.3553s\n",
      "\titers: 200, epoch: 9 | loss: 0.1994877\n",
      "\tspeed: 0.0233s/iter; left time: 154.4846s\n",
      "\titers: 300, epoch: 9 | loss: 0.1821720\n",
      "\tspeed: 0.0233s/iter; left time: 152.1146s\n",
      "\titers: 400, epoch: 9 | loss: 0.1311061\n",
      "\tspeed: 0.0233s/iter; left time: 149.9678s\n",
      "\titers: 500, epoch: 9 | loss: 0.0953144\n",
      "\tspeed: 0.0232s/iter; left time: 146.9556s\n",
      "Epoch: 9 cost time: 13.498720407485962\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1427759 Vali Loss: 0.0307497 Test Loss: 0.1055081\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10591603815555573, mae:0.1953425258398056\n",
      ">>> LR=1e-4,DO=0.2,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2562765\n",
      "\tspeed: 0.0321s/iter; left time: 362.2165s\n",
      "\titers: 200, epoch: 1 | loss: 0.2157764\n",
      "\tspeed: 0.0203s/iter; left time: 226.9246s\n",
      "\titers: 300, epoch: 1 | loss: 0.2481079\n",
      "\tspeed: 0.0202s/iter; left time: 223.7747s\n",
      "\titers: 400, epoch: 1 | loss: 0.2475539\n",
      "\tspeed: 0.0202s/iter; left time: 222.1250s\n",
      "\titers: 500, epoch: 1 | loss: 0.2619237\n",
      "\tspeed: 0.0219s/iter; left time: 239.1631s\n",
      "Epoch: 1 cost time: 13.065285444259644\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2410248 Vali Loss: 0.0453150 Test Loss: 0.1347040\n",
      "Validation loss decreased (inf --> 0.045315).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1065180\n",
      "\tspeed: 0.0639s/iter; left time: 685.8050s\n",
      "\titers: 200, epoch: 2 | loss: 0.3174434\n",
      "\tspeed: 0.0202s/iter; left time: 214.5798s\n",
      "\titers: 300, epoch: 2 | loss: 0.1810112\n",
      "\tspeed: 0.0202s/iter; left time: 212.8601s\n",
      "\titers: 400, epoch: 2 | loss: 0.1442247\n",
      "\tspeed: 0.0202s/iter; left time: 210.3418s\n",
      "\titers: 500, epoch: 2 | loss: 0.1641787\n",
      "\tspeed: 0.0202s/iter; left time: 208.3410s\n",
      "Epoch: 2 cost time: 11.827003002166748\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2043236 Vali Loss: 0.0404540 Test Loss: 0.1295821\n",
      "Validation loss decreased (0.045315 --> 0.040454).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1248439\n",
      "\tspeed: 0.0594s/iter; left time: 603.6768s\n",
      "\titers: 200, epoch: 3 | loss: 0.1446200\n",
      "\tspeed: 0.0201s/iter; left time: 202.2818s\n",
      "\titers: 300, epoch: 3 | loss: 0.1604957\n",
      "\tspeed: 0.0201s/iter; left time: 199.9701s\n",
      "\titers: 400, epoch: 3 | loss: 0.1915502\n",
      "\tspeed: 0.0201s/iter; left time: 198.1422s\n",
      "\titers: 500, epoch: 3 | loss: 0.1480977\n",
      "\tspeed: 0.0201s/iter; left time: 195.9640s\n",
      "Epoch: 3 cost time: 11.748139381408691\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1780425 Vali Loss: 0.0381560 Test Loss: 0.1222825\n",
      "Validation loss decreased (0.040454 --> 0.038156).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1330593\n",
      "\tspeed: 0.0599s/iter; left time: 574.5599s\n",
      "\titers: 200, epoch: 4 | loss: 0.2623751\n",
      "\tspeed: 0.0202s/iter; left time: 191.8159s\n",
      "\titers: 300, epoch: 4 | loss: 0.1235647\n",
      "\tspeed: 0.0202s/iter; left time: 189.6706s\n",
      "\titers: 400, epoch: 4 | loss: 0.1763392\n",
      "\tspeed: 0.0202s/iter; left time: 187.4778s\n",
      "\titers: 500, epoch: 4 | loss: 0.1472546\n",
      "\tspeed: 0.0202s/iter; left time: 185.5195s\n",
      "Epoch: 4 cost time: 11.80426836013794\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1684250 Vali Loss: 0.0369960 Test Loss: 0.1230981\n",
      "Validation loss decreased (0.038156 --> 0.036996).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1510783\n",
      "\tspeed: 0.0605s/iter; left time: 545.4131s\n",
      "\titers: 200, epoch: 5 | loss: 0.2297237\n",
      "\tspeed: 0.0201s/iter; left time: 179.5251s\n",
      "\titers: 300, epoch: 5 | loss: 0.1699297\n",
      "\tspeed: 0.0201s/iter; left time: 177.6847s\n",
      "\titers: 400, epoch: 5 | loss: 0.1289299\n",
      "\tspeed: 0.0201s/iter; left time: 175.2476s\n",
      "\titers: 500, epoch: 5 | loss: 0.1482836\n",
      "\tspeed: 0.0201s/iter; left time: 173.2976s\n",
      "Epoch: 5 cost time: 11.748889446258545\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1612471 Vali Loss: 0.0358364 Test Loss: 0.1210869\n",
      "Validation loss decreased (0.036996 --> 0.035836).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1806330\n",
      "\tspeed: 0.0603s/iter; left time: 509.4864s\n",
      "\titers: 200, epoch: 6 | loss: 0.1296096\n",
      "\tspeed: 0.0202s/iter; left time: 168.3258s\n",
      "\titers: 300, epoch: 6 | loss: 0.1711587\n",
      "\tspeed: 0.0202s/iter; left time: 166.3499s\n",
      "\titers: 400, epoch: 6 | loss: 0.1098421\n",
      "\tspeed: 0.0202s/iter; left time: 164.2502s\n",
      "\titers: 500, epoch: 6 | loss: 0.2536576\n",
      "\tspeed: 0.0202s/iter; left time: 162.2799s\n",
      "Epoch: 6 cost time: 11.78755784034729\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1581290 Vali Loss: 0.0355851 Test Loss: 0.1205030\n",
      "Validation loss decreased (0.035836 --> 0.035585).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1820057\n",
      "\tspeed: 0.0612s/iter; left time: 482.6886s\n",
      "\titers: 200, epoch: 7 | loss: 0.1551099\n",
      "\tspeed: 0.0222s/iter; left time: 172.7862s\n",
      "\titers: 300, epoch: 7 | loss: 0.1663122\n",
      "\tspeed: 0.0218s/iter; left time: 167.4448s\n",
      "\titers: 400, epoch: 7 | loss: 0.1852769\n",
      "\tspeed: 0.0202s/iter; left time: 153.2577s\n",
      "\titers: 500, epoch: 7 | loss: 0.1291391\n",
      "\tspeed: 0.0202s/iter; left time: 150.9397s\n",
      "Epoch: 7 cost time: 12.379070520401001\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1576374 Vali Loss: 0.0353794 Test Loss: 0.1205120\n",
      "Validation loss decreased (0.035585 --> 0.035379).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2096474\n",
      "\tspeed: 0.0610s/iter; left time: 445.9116s\n",
      "\titers: 200, epoch: 8 | loss: 0.2134181\n",
      "\tspeed: 0.0202s/iter; left time: 145.6592s\n",
      "\titers: 300, epoch: 8 | loss: 0.1830942\n",
      "\tspeed: 0.0202s/iter; left time: 143.6053s\n",
      "\titers: 400, epoch: 8 | loss: 0.1176649\n",
      "\tspeed: 0.0202s/iter; left time: 141.4985s\n",
      "\titers: 500, epoch: 8 | loss: 0.1991496\n",
      "\tspeed: 0.0202s/iter; left time: 139.5702s\n",
      "Epoch: 8 cost time: 11.812223196029663\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1570029 Vali Loss: 0.0353128 Test Loss: 0.1201875\n",
      "Validation loss decreased (0.035379 --> 0.035313).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1775825\n",
      "\tspeed: 0.0601s/iter; left time: 405.1292s\n",
      "\titers: 200, epoch: 9 | loss: 0.1360510\n",
      "\tspeed: 0.0202s/iter; left time: 133.8809s\n",
      "\titers: 300, epoch: 9 | loss: 0.2114218\n",
      "\tspeed: 0.0201s/iter; left time: 131.7804s\n",
      "\titers: 400, epoch: 9 | loss: 0.1251161\n",
      "\tspeed: 0.0201s/iter; left time: 129.7640s\n",
      "\titers: 500, epoch: 9 | loss: 0.0949787\n",
      "\tspeed: 0.0202s/iter; left time: 128.1710s\n",
      "Epoch: 9 cost time: 11.772069454193115\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1550673 Vali Loss: 0.0352326 Test Loss: 0.1203308\n",
      "Validation loss decreased (0.035313 --> 0.035233).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1661598\n",
      "\tspeed: 0.0594s/iter; left time: 366.5107s\n",
      "\titers: 200, epoch: 10 | loss: 0.1141818\n",
      "\tspeed: 0.0201s/iter; left time: 122.0412s\n",
      "\titers: 300, epoch: 10 | loss: 0.2880810\n",
      "\tspeed: 0.0201s/iter; left time: 119.8168s\n",
      "\titers: 400, epoch: 10 | loss: 0.1969699\n",
      "\tspeed: 0.0201s/iter; left time: 117.9208s\n",
      "\titers: 500, epoch: 10 | loss: 0.1559758\n",
      "\tspeed: 0.0201s/iter; left time: 115.7311s\n",
      "Epoch: 10 cost time: 11.731609344482422\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1543263 Vali Loss: 0.0353549 Test Loss: 0.1202883\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1343611\n",
      "\tspeed: 0.0585s/iter; left time: 327.6299s\n",
      "\titers: 200, epoch: 11 | loss: 0.1319569\n",
      "\tspeed: 0.0202s/iter; left time: 110.9076s\n",
      "\titers: 300, epoch: 11 | loss: 0.1341392\n",
      "\tspeed: 0.0202s/iter; left time: 108.8608s\n",
      "\titers: 400, epoch: 11 | loss: 0.2018586\n",
      "\tspeed: 0.0202s/iter; left time: 106.8393s\n",
      "\titers: 500, epoch: 11 | loss: 0.1080743\n",
      "\tspeed: 0.0202s/iter; left time: 104.8047s\n",
      "Epoch: 11 cost time: 11.767860174179077\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1560429 Vali Loss: 0.0352690 Test Loss: 0.1202708\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1515093\n",
      "\tspeed: 0.0598s/iter; left time: 301.0899s\n",
      "\titers: 200, epoch: 12 | loss: 0.1632299\n",
      "\tspeed: 0.0222s/iter; left time: 109.3757s\n",
      "\titers: 300, epoch: 12 | loss: 0.2150737\n",
      "\tspeed: 0.0222s/iter; left time: 107.1041s\n",
      "\titers: 400, epoch: 12 | loss: 0.1359158\n",
      "\tspeed: 0.0222s/iter; left time: 104.9406s\n",
      "\titers: 500, epoch: 12 | loss: 0.2216803\n",
      "\tspeed: 0.0222s/iter; left time: 102.6924s\n",
      "Epoch: 12 cost time: 12.934865474700928\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1539562 Vali Loss: 0.0352557 Test Loss: 0.1203630\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12048591673374176, mae:0.21578265726566315\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/tune.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077a135",
   "metadata": {},
   "source": [
    "## df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9521465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pandas as pd, pathlib, textwrap\n",
    "\n",
    "LOG = pathlib.Path('tuning_saved.log')          #  adjust if needed\n",
    "ESC = re.compile(r'\\x1b\\[[0-9;]*m')             # strip ANSI colours\n",
    "\n",
    "# patterns ------------------------------------------------------------\n",
    "start_re   = re.compile(r'start training.*?_pl(\\d+)_dm(\\d+).*?_el(\\d+)_')\n",
    "lr_re      = re.compile(r'Learning Rate:\\s*([0-9.eE+-]+)')\n",
    "do_re      = re.compile(r'Dropout:\\s*([0-9.eE+-]+)')\n",
    "msemae_re  = re.compile(r'mse:\\s*([0-9.eE+-]+).*?mae:\\s*([0-9.eE+-]+)',\n",
    "                        re.I)\n",
    "\n",
    "rows, cur = [], {}\n",
    "\n",
    "with LOG.open() as fh:\n",
    "    for raw in fh:\n",
    "        line = ESC.sub('', raw).strip()\n",
    "\n",
    "        # -------- start-training line  (captures EL / DM / PL) ----------\n",
    "        m = start_re.search(line)\n",
    "        if m:\n",
    "            cur.update({'PL': float(m.group(1)),\n",
    "                        'DM': float(m.group(2)),\n",
    "                        'EL': float(m.group(3))})\n",
    "            continue\n",
    "\n",
    "        # -------- Run-parameter lines  (captures LR / DO) ---------------\n",
    "        m = lr_re.search(line)\n",
    "        if m:\n",
    "            cur['LR'] = float(m.group(1));  continue\n",
    "\n",
    "        m = do_re.search(line)\n",
    "        if m:\n",
    "            cur['DO'] = float(m.group(1));  continue\n",
    "\n",
    "        # -------- result line  (captures mse / mae  row) ---------------\n",
    "        m = msemae_re.search(line)\n",
    "        if m and {'LR','DO','EL','DM','PL'}.issubset(cur):\n",
    "            rows.append({**cur,\n",
    "                         'mse': float(m.group(1)),\n",
    "                         'mae': float(m.group(2))})\n",
    "            cur = {}                                    # reset for next run\n",
    "\n",
    "# ------------------------------ dataframe -----------------------------\n",
    "df = pd.DataFrame(rows)\n",
    "if df.empty:\n",
    "    raise RuntimeError(textwrap.dedent(f\"\"\"\n",
    "         No rows parsed - check that {LOG} really contains the patterns\n",
    "       start training, Learning Rate:, Dropout: and mse:.\n",
    "    \"\"\"))\n",
    "\n",
    "df = df[['LR','DO','EL','DM','PL','mse','mae']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91726c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>DO</th>\n",
       "      <th>EL</th>\n",
       "      <th>DM</th>\n",
       "      <th>PL</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.104427</td>\n",
       "      <td>0.196775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.105916</td>\n",
       "      <td>0.195343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.106452</td>\n",
       "      <td>0.195598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.107361</td>\n",
       "      <td>0.196184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.107814</td>\n",
       "      <td>0.196356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.107995</td>\n",
       "      <td>0.197760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.108420</td>\n",
       "      <td>0.199717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.108478</td>\n",
       "      <td>0.199562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.108874</td>\n",
       "      <td>0.198089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.109037</td>\n",
       "      <td>0.199166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.109045</td>\n",
       "      <td>0.203371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.109061</td>\n",
       "      <td>0.200739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.109823</td>\n",
       "      <td>0.200003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.202134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.110498</td>\n",
       "      <td>0.204203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.110835</td>\n",
       "      <td>0.203020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.204743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.111351</td>\n",
       "      <td>0.202390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.112056</td>\n",
       "      <td>0.205305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.112172</td>\n",
       "      <td>0.205155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.112203</td>\n",
       "      <td>0.204322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.112304</td>\n",
       "      <td>0.202860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.113960</td>\n",
       "      <td>0.208746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.113961</td>\n",
       "      <td>0.208240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.114574</td>\n",
       "      <td>0.208252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.115351</td>\n",
       "      <td>0.206453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.116421</td>\n",
       "      <td>0.209235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.116557</td>\n",
       "      <td>0.208305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.116677</td>\n",
       "      <td>0.207318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.116695</td>\n",
       "      <td>0.216090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.116794</td>\n",
       "      <td>0.211778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.117637</td>\n",
       "      <td>0.209670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.118456</td>\n",
       "      <td>0.217072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.118772</td>\n",
       "      <td>0.217095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.118927</td>\n",
       "      <td>0.213541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.118944</td>\n",
       "      <td>0.214113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.119116</td>\n",
       "      <td>0.213711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.119195</td>\n",
       "      <td>0.217824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.119345</td>\n",
       "      <td>0.210643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120113</td>\n",
       "      <td>0.214763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120255</td>\n",
       "      <td>0.213141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120277</td>\n",
       "      <td>0.215844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120486</td>\n",
       "      <td>0.215783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120676</td>\n",
       "      <td>0.216683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120806</td>\n",
       "      <td>0.215986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.120820</td>\n",
       "      <td>0.212849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.121156</td>\n",
       "      <td>0.215667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.121746</td>\n",
       "      <td>0.217245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.122092</td>\n",
       "      <td>0.219716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.122216</td>\n",
       "      <td>0.215177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.122256</td>\n",
       "      <td>0.218939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.122859</td>\n",
       "      <td>0.216479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.123691</td>\n",
       "      <td>0.220861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.124008</td>\n",
       "      <td>0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.124041</td>\n",
       "      <td>0.220039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.124362</td>\n",
       "      <td>0.218177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.124657</td>\n",
       "      <td>0.217874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.126468</td>\n",
       "      <td>0.223872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.126531</td>\n",
       "      <td>0.228539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.126796</td>\n",
       "      <td>0.221878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.127482</td>\n",
       "      <td>0.224006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.127708</td>\n",
       "      <td>0.220844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.127813</td>\n",
       "      <td>0.224185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.128108</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.128520</td>\n",
       "      <td>0.223004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.128667</td>\n",
       "      <td>0.231366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.128718</td>\n",
       "      <td>0.228094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.130162</td>\n",
       "      <td>0.227396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.130441</td>\n",
       "      <td>0.232814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.130452</td>\n",
       "      <td>0.232187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.131002</td>\n",
       "      <td>0.224135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.226018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.132062</td>\n",
       "      <td>0.231527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.132675</td>\n",
       "      <td>0.230896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.132955</td>\n",
       "      <td>0.233626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.133099</td>\n",
       "      <td>0.233109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.133304</td>\n",
       "      <td>0.231443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.133459</td>\n",
       "      <td>0.232994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.134030</td>\n",
       "      <td>0.231216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.134525</td>\n",
       "      <td>0.233683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.135127</td>\n",
       "      <td>0.232807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.135435</td>\n",
       "      <td>0.231636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.135650</td>\n",
       "      <td>0.234883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.135652</td>\n",
       "      <td>0.234210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.136472</td>\n",
       "      <td>0.232906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.136822</td>\n",
       "      <td>0.230131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.239360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137111</td>\n",
       "      <td>0.231023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137322</td>\n",
       "      <td>0.237498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137509</td>\n",
       "      <td>0.230977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137812</td>\n",
       "      <td>0.241425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137907</td>\n",
       "      <td>0.241079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.137917</td>\n",
       "      <td>0.230432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.138090</td>\n",
       "      <td>0.234682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.138220</td>\n",
       "      <td>0.239147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.138552</td>\n",
       "      <td>0.235961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.241204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.139073</td>\n",
       "      <td>0.231680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.139619</td>\n",
       "      <td>0.244022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.140585</td>\n",
       "      <td>0.244191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.140597</td>\n",
       "      <td>0.241085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.140623</td>\n",
       "      <td>0.230752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>0.245103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.142155</td>\n",
       "      <td>0.244538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.142589</td>\n",
       "      <td>0.241565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.142780</td>\n",
       "      <td>0.239206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.142811</td>\n",
       "      <td>0.244204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.143093</td>\n",
       "      <td>0.240851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.143351</td>\n",
       "      <td>0.240250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.143602</td>\n",
       "      <td>0.243967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.144553</td>\n",
       "      <td>0.238607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.146461</td>\n",
       "      <td>0.246714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.147015</td>\n",
       "      <td>0.238889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.147643</td>\n",
       "      <td>0.244725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.147890</td>\n",
       "      <td>0.244020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.149250</td>\n",
       "      <td>0.244295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.149746</td>\n",
       "      <td>0.242802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.149871</td>\n",
       "      <td>0.246455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.150909</td>\n",
       "      <td>0.248287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.152691</td>\n",
       "      <td>0.245261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.153724</td>\n",
       "      <td>0.251459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.154063</td>\n",
       "      <td>0.252839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.154111</td>\n",
       "      <td>0.252454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.154389</td>\n",
       "      <td>0.248131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.154593</td>\n",
       "      <td>0.252041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.154820</td>\n",
       "      <td>0.252011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.154941</td>\n",
       "      <td>0.251267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.155051</td>\n",
       "      <td>0.252460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.155166</td>\n",
       "      <td>0.244105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.155404</td>\n",
       "      <td>0.249838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.155627</td>\n",
       "      <td>0.250330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.155812</td>\n",
       "      <td>0.252482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.155846</td>\n",
       "      <td>0.253959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.156141</td>\n",
       "      <td>0.253127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.156274</td>\n",
       "      <td>0.254190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.156617</td>\n",
       "      <td>0.254265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.157289</td>\n",
       "      <td>0.254738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.158426</td>\n",
       "      <td>0.258602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.158538</td>\n",
       "      <td>0.253393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.159279</td>\n",
       "      <td>0.263933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.159889</td>\n",
       "      <td>0.255709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.160060</td>\n",
       "      <td>0.258478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.160820</td>\n",
       "      <td>0.255091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.161433</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.162313</td>\n",
       "      <td>0.257030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.162558</td>\n",
       "      <td>0.257950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.163181</td>\n",
       "      <td>0.260688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.163577</td>\n",
       "      <td>0.258739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.164099</td>\n",
       "      <td>0.260617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.164423</td>\n",
       "      <td>0.262524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.164715</td>\n",
       "      <td>0.259517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.165778</td>\n",
       "      <td>0.261321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.165839</td>\n",
       "      <td>0.265451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.167284</td>\n",
       "      <td>0.260146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>0.261163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.167823</td>\n",
       "      <td>0.265283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.168547</td>\n",
       "      <td>0.270375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.169305</td>\n",
       "      <td>0.265010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.172032</td>\n",
       "      <td>0.270293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.175663</td>\n",
       "      <td>0.269662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.175911</td>\n",
       "      <td>0.274322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.176130</td>\n",
       "      <td>0.267167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LR   DO   EL     DM    PL       mse       mae\n",
       "0    0.0005  0.2  2.0  256.0  24.0  0.104427  0.196775\n",
       "1    0.0001  0.2  4.0  512.0  24.0  0.105916  0.195343\n",
       "2    0.0001  0.2  4.0  256.0  24.0  0.106452  0.195598\n",
       "3    0.0001  0.2  2.0  256.0  24.0  0.107361  0.196184\n",
       "4    0.0001  0.2  3.0  256.0  24.0  0.107814  0.196356\n",
       "5    0.0001  0.2  3.0  512.0  24.0  0.107995  0.197760\n",
       "6    0.0001  0.1  2.0  512.0  24.0  0.108420  0.199717\n",
       "7    0.0001  0.1  2.0  256.0  24.0  0.108478  0.199562\n",
       "8    0.0001  0.2  2.0  512.0  24.0  0.108874  0.198089\n",
       "9    0.0005  0.2  3.0  256.0  24.0  0.109037  0.199166\n",
       "10   0.0001  0.1  4.0  512.0  24.0  0.109045  0.203371\n",
       "11   0.0001  0.1  4.0  256.0  24.0  0.109061  0.200739\n",
       "12   0.0001  0.1  3.0  256.0  24.0  0.109823  0.200003\n",
       "13   0.0010  0.1  2.0  256.0  24.0  0.110150  0.202134\n",
       "14   0.0001  0.1  3.0  512.0  24.0  0.110498  0.204203\n",
       "15   0.0005  0.2  3.0  512.0  24.0  0.110835  0.203020\n",
       "16   0.0001  0.0  2.0  256.0  24.0  0.111300  0.204743\n",
       "17   0.0005  0.2  2.0  512.0  24.0  0.111351  0.202390\n",
       "18   0.0001  0.0  3.0  512.0  24.0  0.112056  0.205305\n",
       "19   0.0005  0.1  2.0  256.0  24.0  0.112172  0.205155\n",
       "20   0.0010  0.2  2.0  256.0  24.0  0.112203  0.204322\n",
       "21   0.0005  0.2  4.0  256.0  24.0  0.112304  0.202860\n",
       "22   0.0001  0.0  4.0  256.0  24.0  0.113960  0.208746\n",
       "23   0.0001  0.0  3.0  256.0  24.0  0.113961  0.208240\n",
       "24   0.0005  0.1  2.0  512.0  24.0  0.114574  0.208252\n",
       "25   0.0001  0.2  2.0  256.0  24.0  0.115351  0.206453\n",
       "26   0.0010  0.0  2.0  256.0  24.0  0.116421  0.209235\n",
       "27   0.0005  0.1  4.0  256.0  24.0  0.116557  0.208305\n",
       "28   0.0005  0.1  3.0  256.0  24.0  0.116677  0.207318\n",
       "29   0.0005  0.0  3.0  256.0  24.0  0.116695  0.216090\n",
       "30   0.0001  0.2  2.0  512.0  24.0  0.116794  0.211778\n",
       "31   0.0001  0.2  4.0  256.0  24.0  0.117637  0.209670\n",
       "32   0.0001  0.1  3.0  512.0  24.0  0.118456  0.217072\n",
       "33   0.0001  0.0  2.0  512.0  24.0  0.118772  0.217095\n",
       "34   0.0001  0.2  4.0  512.0  24.0  0.118927  0.213541\n",
       "35   0.0001  0.2  2.0  512.0  24.0  0.118944  0.214113\n",
       "36   0.0001  0.1  2.0  256.0  24.0  0.119116  0.213711\n",
       "37   0.0001  0.1  2.0  512.0  24.0  0.119195  0.217824\n",
       "38   0.0001  0.2  3.0  256.0  24.0  0.119345  0.210643\n",
       "39   0.0005  0.2  2.0  512.0  24.0  0.120113  0.214763\n",
       "40   0.0001  0.2  3.0  256.0  24.0  0.120255  0.213141\n",
       "41   0.0001  0.2  3.0  512.0  24.0  0.120277  0.215844\n",
       "42   0.0001  0.2  4.0  512.0  24.0  0.120486  0.215783\n",
       "43   0.0005  0.0  2.0  256.0  24.0  0.120676  0.216683\n",
       "44   0.0001  0.1  4.0  256.0  24.0  0.120806  0.215986\n",
       "45   0.0005  0.0  4.0  256.0  24.0  0.120820  0.212849\n",
       "46   0.0001  0.2  3.0  512.0  24.0  0.121156  0.215667\n",
       "47   0.0001  0.1  3.0  256.0  24.0  0.121746  0.217245\n",
       "48   0.0001  0.0  4.0  512.0  24.0  0.122092  0.219716\n",
       "49   0.0001  0.2  4.0  256.0  24.0  0.122216  0.215177\n",
       "50   0.0001  0.1  3.0  256.0  24.0  0.122256  0.218939\n",
       "51   0.0001  0.2  2.0  256.0  24.0  0.122859  0.216479\n",
       "52   0.0005  0.2  3.0  256.0  24.0  0.123691  0.220861\n",
       "53   0.0001  0.1  2.0  512.0  24.0  0.124008  0.221900\n",
       "54   0.0001  0.1  2.0  256.0  24.0  0.124041  0.220039\n",
       "55   0.0005  0.2  3.0  256.0  24.0  0.124362  0.218177\n",
       "56   0.0005  0.0  2.0  512.0  24.0  0.124657  0.217874\n",
       "57   0.0001  0.1  4.0  256.0  24.0  0.126468  0.223872\n",
       "58   0.0001  0.1  4.0  512.0  24.0  0.126531  0.228539\n",
       "59   0.0005  0.2  2.0  256.0  24.0  0.126796  0.221878\n",
       "60   0.0001  0.1  4.0  512.0  24.0  0.127482  0.224006\n",
       "61   0.0005  0.2  2.0  256.0  24.0  0.127708  0.220844\n",
       "62   0.0005  0.2  4.0  256.0  24.0  0.127813  0.224185\n",
       "63   0.0005  0.2  3.0  512.0  24.0  0.128108  0.220500\n",
       "64   0.0005  0.2  4.0  256.0  24.0  0.128520  0.223004\n",
       "65   0.0001  0.0  3.0  512.0  24.0  0.128667  0.231366\n",
       "66   0.0005  0.1  2.0  512.0  24.0  0.128718  0.228094\n",
       "67   0.0005  0.1  4.0  512.0  24.0  0.130162  0.227396\n",
       "68   0.0005  0.0  4.0  512.0  24.0  0.130441  0.232814\n",
       "69   0.0001  0.1  3.0  512.0  24.0  0.130452  0.232187\n",
       "70   0.0010  0.1  2.0  256.0  24.0  0.131002  0.224135\n",
       "71   0.0005  0.1  3.0  512.0  24.0  0.131790  0.226018\n",
       "72   0.0005  0.0  3.0  512.0  24.0  0.132062  0.231527\n",
       "73   0.0005  0.1  3.0  256.0  24.0  0.132675  0.230896\n",
       "74   0.0005  0.1  2.0  256.0  24.0  0.132955  0.233626\n",
       "75   0.0001  0.0  2.0  256.0  24.0  0.133099  0.233109\n",
       "76   0.0005  0.2  2.0  512.0  24.0  0.133304  0.231443\n",
       "77   0.0001  0.0  2.0  512.0  24.0  0.133459  0.232994\n",
       "78   0.0005  0.1  2.0  512.0  24.0  0.134030  0.231216\n",
       "79   0.0001  0.0  4.0  256.0  24.0  0.134525  0.233683\n",
       "80   0.0005  0.1  2.0  256.0  24.0  0.135127  0.232807\n",
       "81   0.0010  0.0  2.0  512.0  24.0  0.135435  0.231636\n",
       "82   0.0005  0.0  2.0  256.0  24.0  0.135650  0.234883\n",
       "83   0.0005  0.1  4.0  256.0  24.0  0.135652  0.234210\n",
       "84   0.0010  0.2  2.0  256.0  24.0  0.136472  0.232906\n",
       "85   0.0010  0.2  2.0  256.0  24.0  0.136822  0.230131\n",
       "86   0.0005  0.0  4.0  256.0  24.0  0.137107  0.239360\n",
       "87   0.0005  0.1  4.0  256.0  24.0  0.137111  0.231023\n",
       "88   0.0001  0.0  2.0  256.0  24.0  0.137322  0.237498\n",
       "89   0.0010  0.1  3.0  256.0  24.0  0.137509  0.230977\n",
       "90   0.0005  0.0  3.0  256.0  24.0  0.137812  0.241425\n",
       "91   0.0001  0.0  3.0  256.0  24.0  0.137907  0.241079\n",
       "92   0.0005  0.2  4.0  512.0  24.0  0.137917  0.230432\n",
       "93   0.0005  0.1  3.0  256.0  24.0  0.138090  0.234682\n",
       "94   0.0001  0.0  4.0  256.0  24.0  0.138220  0.239147\n",
       "95   0.0005  0.0  2.0  512.0  24.0  0.138552  0.235961\n",
       "96   0.0001  0.0  4.0  512.0  24.0  0.138739  0.241204\n",
       "97   0.0005  0.2  3.0  512.0  24.0  0.139073  0.231680\n",
       "98   0.0005  0.0  3.0  512.0  24.0  0.139619  0.244022\n",
       "99   0.0001  0.0  3.0  512.0  24.0  0.140585  0.244191\n",
       "100  0.0010  0.0  2.0  256.0  24.0  0.140597  0.241085\n",
       "101  0.0010  0.1  3.0  256.0  24.0  0.140623  0.230752\n",
       "102  0.0001  0.0  4.0  512.0  24.0  0.141908  0.245103\n",
       "103  0.0001  0.0  2.0  512.0  24.0  0.142155  0.244538\n",
       "104  0.0005  0.0  3.0  256.0  24.0  0.142589  0.241565\n",
       "105  0.0001  0.0  3.0  256.0  24.0  0.142780  0.239206\n",
       "106  0.0005  0.0  2.0  512.0  24.0  0.142811  0.244204\n",
       "107  0.0005  0.0  2.0  256.0  24.0  0.143093  0.240851\n",
       "108  0.0010  0.1  4.0  256.0  24.0  0.143351  0.240250\n",
       "109  0.0005  0.0  4.0  256.0  24.0  0.143602  0.243967\n",
       "110  0.0005  0.0  4.0  512.0  24.0  0.144553  0.238607\n",
       "111  0.0005  0.1  4.0  512.0  24.0  0.146461  0.246714\n",
       "112  0.0005  0.2  4.0  512.0  24.0  0.147015  0.238889\n",
       "113  0.0010  0.2  3.0  256.0  24.0  0.147643  0.244725\n",
       "114  0.0005  0.0  3.0  512.0  24.0  0.147890  0.244020\n",
       "115  0.0010  0.1  4.0  256.0  24.0  0.149250  0.244295\n",
       "116  0.0010  0.2  3.0  256.0  24.0  0.149746  0.242802\n",
       "117  0.0010  0.1  2.0  256.0  24.0  0.149871  0.246455\n",
       "118  0.0010  0.1  4.0  512.0  24.0  0.150909  0.248287\n",
       "119  0.0010  0.2  4.0  256.0  24.0  0.152691  0.245261\n",
       "120  0.0010  0.1  3.0  256.0  24.0  0.153724  0.251459\n",
       "121  0.0010  0.0  3.0  512.0  24.0  0.154063  0.252839\n",
       "122  0.0010  0.0  4.0  256.0  24.0  0.154111  0.252454\n",
       "123  0.0010  0.0  2.0  512.0  24.0  0.154389  0.248131\n",
       "124  0.0010  0.0  3.0  256.0  24.0  0.154593  0.252041\n",
       "125  0.0010  0.1  4.0  512.0  24.0  0.154820  0.252011\n",
       "126  0.0010  0.0  3.0  256.0  24.0  0.154941  0.251267\n",
       "127  0.0005  0.1  4.0  512.0  24.0  0.155051  0.252460\n",
       "128  0.0005  0.1  3.0  512.0  24.0  0.155166  0.244105\n",
       "129  0.0005  0.2  4.0  512.0  24.0  0.155404  0.249838\n",
       "130  0.0010  0.2  3.0  256.0  24.0  0.155627  0.250330\n",
       "131  0.0010  0.2  4.0  256.0  24.0  0.155812  0.252482\n",
       "132  0.0010  0.2  4.0  512.0  24.0  0.155846  0.253959\n",
       "133  0.0010  0.2  4.0  256.0  24.0  0.156141  0.253127\n",
       "134  0.0010  0.1  3.0  512.0  24.0  0.156274  0.254190\n",
       "135  0.0010  0.1  4.0  512.0  24.0  0.156617  0.254265\n",
       "136  0.0010  0.0  4.0  256.0  24.0  0.157289  0.254738\n",
       "137  0.0010  0.0  4.0  512.0  24.0  0.158426  0.258602\n",
       "138  0.0010  0.2  3.0  512.0  24.0  0.158538  0.253393\n",
       "139  0.0010  0.0  3.0  256.0  24.0  0.159279  0.263933\n",
       "140  0.0010  0.2  3.0  512.0  24.0  0.159889  0.255709\n",
       "141  0.0010  0.0  3.0  512.0  24.0  0.160060  0.258478\n",
       "142  0.0010  0.2  4.0  512.0  24.0  0.160820  0.255091\n",
       "143  0.0010  0.1  4.0  256.0  24.0  0.161433  0.261800\n",
       "144  0.0010  0.1  3.0  512.0  24.0  0.162313  0.257030\n",
       "145  0.0010  0.0  4.0  256.0  24.0  0.162558  0.257950\n",
       "146  0.0010  0.0  2.0  256.0  24.0  0.163181  0.260688\n",
       "147  0.0010  0.1  2.0  512.0  24.0  0.163577  0.258739\n",
       "148  0.0010  0.2  4.0  512.0  24.0  0.164099  0.260617\n",
       "149  0.0010  0.0  4.0  512.0  24.0  0.164423  0.262524\n",
       "150  0.0010  0.1  3.0  512.0  24.0  0.164715  0.259517\n",
       "151  0.0010  0.2  3.0  512.0  24.0  0.165778  0.261321\n",
       "152  0.0005  0.1  3.0  512.0  24.0  0.165839  0.265451\n",
       "153  0.0010  0.2  2.0  512.0  24.0  0.167284  0.260146\n",
       "154  0.0010  0.1  2.0  512.0  24.0  0.167818  0.261163\n",
       "155  0.0010  0.1  2.0  512.0  24.0  0.167823  0.265283\n",
       "156  0.0010  0.0  2.0  512.0  24.0  0.168547  0.270375\n",
       "157  0.0010  0.0  4.0  512.0  24.0  0.169305  0.265010\n",
       "158  0.0010  0.2  2.0  512.0  24.0  0.172032  0.270293\n",
       "159  0.0010  0.0  3.0  512.0  24.0  0.175663  0.269662\n",
       "160  0.0005  0.0  4.0  512.0  24.0  0.175911  0.274322\n",
       "161  0.0010  0.2  2.0  512.0  24.0  0.176130  0.267167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "display(df.sort_values(['mse','mae']).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bc76d",
   "metadata": {},
   "source": [
    "## winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dad3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_penetration,wind_penetration,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['total_load', 'solar_penetration', 'wind_penetration']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['total_load', 'solar_penetration', 'wind_penetration']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['total_load', 'solar_penetration', 'wind_penetration']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3291382\n",
      "\tspeed: 0.0258s/iter; left time: 1174.4750s\n",
      "\titers: 200, epoch: 1 | loss: 0.1490098\n",
      "\tspeed: 0.0141s/iter; left time: 641.4461s\n",
      "\titers: 300, epoch: 1 | loss: 0.1037220\n",
      "\tspeed: 0.0141s/iter; left time: 638.3797s\n",
      "\titers: 400, epoch: 1 | loss: 0.6137409\n",
      "\tspeed: 0.0141s/iter; left time: 637.4973s\n",
      "\titers: 500, epoch: 1 | loss: 0.0901251\n",
      "\tspeed: 0.0142s/iter; left time: 637.7237s\n",
      "\titers: 600, epoch: 1 | loss: 0.1678524\n",
      "\tspeed: 0.0141s/iter; left time: 634.2431s\n",
      "\titers: 700, epoch: 1 | loss: 0.2328449\n",
      "\tspeed: 0.0142s/iter; left time: 635.4142s\n",
      "\titers: 800, epoch: 1 | loss: 0.9505996\n",
      "\tspeed: 0.0141s/iter; left time: 633.1431s\n",
      "\titers: 900, epoch: 1 | loss: 0.4708728\n",
      "\tspeed: 0.0141s/iter; left time: 629.5445s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1070232\n",
      "\tspeed: 0.0142s/iter; left time: 630.6288s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1077124\n",
      "\tspeed: 0.0142s/iter; left time: 632.0337s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1455047\n",
      "\tspeed: 0.0142s/iter; left time: 629.9187s\n",
      "\titers: 1300, epoch: 1 | loss: 0.3675669\n",
      "\tspeed: 0.0142s/iter; left time: 630.2625s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0668229\n",
      "\tspeed: 0.0142s/iter; left time: 627.2830s\n",
      "\titers: 1500, epoch: 1 | loss: 0.2910351\n",
      "\tspeed: 0.0142s/iter; left time: 625.7754s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2438120\n",
      "\tspeed: 0.0142s/iter; left time: 624.3622s\n",
      "\titers: 1700, epoch: 1 | loss: 0.5840662\n",
      "\tspeed: 0.0142s/iter; left time: 622.5429s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0609143\n",
      "\tspeed: 0.0142s/iter; left time: 622.1949s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2436295\n",
      "\tspeed: 0.0142s/iter; left time: 620.9883s\n",
      "\titers: 2000, epoch: 1 | loss: 0.8889745\n",
      "\tspeed: 0.0142s/iter; left time: 618.6075s\n",
      "\titers: 2100, epoch: 1 | loss: 0.3528905\n",
      "\tspeed: 0.0142s/iter; left time: 617.7524s\n",
      "\titers: 2200, epoch: 1 | loss: 0.5312578\n",
      "\tspeed: 0.0142s/iter; left time: 615.9247s\n",
      "\titers: 2300, epoch: 1 | loss: 0.2991142\n",
      "\tspeed: 0.0142s/iter; left time: 613.7821s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1761147\n",
      "\tspeed: 0.0142s/iter; left time: 612.4969s\n",
      "\titers: 2500, epoch: 1 | loss: 0.6227214\n",
      "\tspeed: 0.0142s/iter; left time: 611.3196s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1113367\n",
      "\tspeed: 0.0142s/iter; left time: 608.8931s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2326176\n",
      "\tspeed: 0.0142s/iter; left time: 607.6941s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2833475\n",
      "\tspeed: 0.0142s/iter; left time: 605.9866s\n",
      "\titers: 2900, epoch: 1 | loss: 0.5292840\n",
      "\tspeed: 0.0142s/iter; left time: 603.8865s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1477914\n",
      "\tspeed: 0.0142s/iter; left time: 603.0066s\n",
      "\titers: 3100, epoch: 1 | loss: 0.2786612\n",
      "\tspeed: 0.0142s/iter; left time: 600.9509s\n",
      "\titers: 3200, epoch: 1 | loss: 0.5202354\n",
      "\tspeed: 0.0141s/iter; left time: 598.1792s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0784023\n",
      "\tspeed: 0.0141s/iter; left time: 597.4133s\n",
      "\titers: 3400, epoch: 1 | loss: 0.5335225\n",
      "\tspeed: 0.0141s/iter; left time: 595.7472s\n",
      "\titers: 3500, epoch: 1 | loss: 0.3169599\n",
      "\tspeed: 0.0141s/iter; left time: 594.2211s\n",
      "\titers: 3600, epoch: 1 | loss: 0.2002701\n",
      "\tspeed: 0.0141s/iter; left time: 592.4460s\n",
      "\titers: 3700, epoch: 1 | loss: 0.3039925\n",
      "\tspeed: 0.0141s/iter; left time: 591.4419s\n",
      "\titers: 3800, epoch: 1 | loss: 0.4044019\n",
      "\tspeed: 0.0141s/iter; left time: 590.4910s\n",
      "\titers: 3900, epoch: 1 | loss: 1.2051228\n",
      "\tspeed: 0.0141s/iter; left time: 588.2561s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2754239\n",
      "\tspeed: 0.0141s/iter; left time: 586.9841s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0910320\n",
      "\tspeed: 0.0141s/iter; left time: 585.3550s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1616185\n",
      "\tspeed: 0.0141s/iter; left time: 583.9347s\n",
      "\titers: 4300, epoch: 1 | loss: 0.3379201\n",
      "\tspeed: 0.0142s/iter; left time: 583.8218s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1321377\n",
      "\tspeed: 0.0142s/iter; left time: 583.3085s\n",
      "\titers: 4500, epoch: 1 | loss: 0.2418478\n",
      "\tspeed: 0.0142s/iter; left time: 584.8699s\n",
      "Epoch: 1 cost time: 65.7528703212738\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.3375847 Vali Loss: 0.0673943 Test Loss: 0.1938943\n",
      "Validation loss decreased (inf --> 0.067394).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.0597807\n",
      "\tspeed: 0.1289s/iter; left time: 5273.3486s\n",
      "\titers: 200, epoch: 2 | loss: 0.0674525\n",
      "\tspeed: 0.0143s/iter; left time: 584.3905s\n",
      "\titers: 300, epoch: 2 | loss: 0.2022534\n",
      "\tspeed: 0.0143s/iter; left time: 582.2192s\n",
      "\titers: 400, epoch: 2 | loss: 0.2076694\n",
      "\tspeed: 0.0143s/iter; left time: 579.5762s\n",
      "\titers: 500, epoch: 2 | loss: 0.3929858\n",
      "\tspeed: 0.0142s/iter; left time: 575.5860s\n",
      "\titers: 600, epoch: 2 | loss: 0.1899005\n",
      "\tspeed: 0.0142s/iter; left time: 574.2900s\n",
      "\titers: 700, epoch: 2 | loss: 0.0708140\n",
      "\tspeed: 0.0142s/iter; left time: 572.8415s\n",
      "\titers: 800, epoch: 2 | loss: 0.1211045\n",
      "\tspeed: 0.0142s/iter; left time: 571.3921s\n",
      "\titers: 900, epoch: 2 | loss: 0.1128002\n",
      "\tspeed: 0.0142s/iter; left time: 570.3654s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1623647\n",
      "\tspeed: 0.0142s/iter; left time: 569.5325s\n",
      "\titers: 1100, epoch: 2 | loss: 1.0472305\n",
      "\tspeed: 0.0142s/iter; left time: 567.9795s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1858845\n",
      "\tspeed: 0.0143s/iter; left time: 567.4134s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1096548\n",
      "\tspeed: 0.0143s/iter; left time: 566.4274s\n",
      "\titers: 1400, epoch: 2 | loss: 0.2056573\n",
      "\tspeed: 0.0143s/iter; left time: 565.6236s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0521696\n",
      "\tspeed: 0.0143s/iter; left time: 563.3122s\n",
      "\titers: 1600, epoch: 2 | loss: 0.2417170\n",
      "\tspeed: 0.0143s/iter; left time: 561.7880s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3463925\n",
      "\tspeed: 0.0143s/iter; left time: 560.4335s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0709882\n",
      "\tspeed: 0.0143s/iter; left time: 558.6621s\n",
      "\titers: 1900, epoch: 2 | loss: 0.2344091\n",
      "\tspeed: 0.0142s/iter; left time: 556.1139s\n",
      "\titers: 2000, epoch: 2 | loss: 0.2023625\n",
      "\tspeed: 0.0142s/iter; left time: 553.1473s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0897627\n",
      "\tspeed: 0.0141s/iter; left time: 550.1172s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1765037\n",
      "\tspeed: 0.0141s/iter; left time: 548.5941s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1266783\n",
      "\tspeed: 0.0141s/iter; left time: 546.9397s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2086416\n",
      "\tspeed: 0.0141s/iter; left time: 545.7719s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1249593\n",
      "\tspeed: 0.0141s/iter; left time: 544.3031s\n",
      "\titers: 2600, epoch: 2 | loss: 0.4156908\n",
      "\tspeed: 0.0141s/iter; left time: 542.7432s\n",
      "\titers: 2700, epoch: 2 | loss: 0.2025797\n",
      "\tspeed: 0.0142s/iter; left time: 544.3198s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1333885\n",
      "\tspeed: 0.0142s/iter; left time: 543.5516s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1613431\n",
      "\tspeed: 0.0142s/iter; left time: 542.5865s\n",
      "\titers: 3000, epoch: 2 | loss: 0.2645910\n",
      "\tspeed: 0.0142s/iter; left time: 540.2414s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1267328\n",
      "\tspeed: 0.0142s/iter; left time: 539.2842s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2745427\n",
      "\tspeed: 0.0142s/iter; left time: 538.2281s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1312595\n",
      "\tspeed: 0.0142s/iter; left time: 536.8351s\n",
      "\titers: 3400, epoch: 2 | loss: 0.2082453\n",
      "\tspeed: 0.0142s/iter; left time: 534.9103s\n",
      "\titers: 3500, epoch: 2 | loss: 0.2919978\n",
      "\tspeed: 0.0142s/iter; left time: 533.8599s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1987554\n",
      "\tspeed: 0.0142s/iter; left time: 531.9197s\n",
      "\titers: 3700, epoch: 2 | loss: 0.5037205\n",
      "\tspeed: 0.0142s/iter; left time: 530.4334s\n",
      "\titers: 3800, epoch: 2 | loss: 0.3194287\n",
      "\tspeed: 0.0142s/iter; left time: 529.9148s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2176579\n",
      "\tspeed: 0.0142s/iter; left time: 528.5927s\n",
      "\titers: 4000, epoch: 2 | loss: 0.5338027\n",
      "\tspeed: 0.0143s/iter; left time: 527.9583s\n",
      "\titers: 4100, epoch: 2 | loss: 0.5264261\n",
      "\tspeed: 0.0142s/iter; left time: 525.4718s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1686651\n",
      "\tspeed: 0.0142s/iter; left time: 523.9317s\n",
      "\titers: 4300, epoch: 2 | loss: 0.2446898\n",
      "\tspeed: 0.0142s/iter; left time: 522.2543s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1807210\n",
      "\tspeed: 0.0142s/iter; left time: 521.3835s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0969841\n",
      "\tspeed: 0.0142s/iter; left time: 519.9396s\n",
      "Epoch: 2 cost time: 65.13763117790222\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.3050980 Vali Loss: 0.0621823 Test Loss: 0.1692979\n",
      "Validation loss decreased (0.067394 --> 0.062182).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4909893\n",
      "\tspeed: 0.1363s/iter; left time: 4952.8743s\n",
      "\titers: 200, epoch: 3 | loss: 0.0633782\n",
      "\tspeed: 0.0143s/iter; left time: 517.2623s\n",
      "\titers: 300, epoch: 3 | loss: 0.3294092\n",
      "\tspeed: 0.0143s/iter; left time: 515.7294s\n",
      "\titers: 400, epoch: 3 | loss: 0.1717898\n",
      "\tspeed: 0.0142s/iter; left time: 513.5465s\n",
      "\titers: 500, epoch: 3 | loss: 0.2593009\n",
      "\tspeed: 0.0143s/iter; left time: 512.2365s\n",
      "\titers: 600, epoch: 3 | loss: 0.2297810\n",
      "\tspeed: 0.0143s/iter; left time: 510.8033s\n",
      "\titers: 700, epoch: 3 | loss: 1.1807755\n",
      "\tspeed: 0.0143s/iter; left time: 509.6187s\n",
      "\titers: 800, epoch: 3 | loss: 0.2283778\n",
      "\tspeed: 0.0143s/iter; left time: 509.5378s\n",
      "\titers: 900, epoch: 3 | loss: 0.1637796\n",
      "\tspeed: 0.0143s/iter; left time: 507.9234s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3263541\n",
      "\tspeed: 0.0143s/iter; left time: 506.5940s\n",
      "\titers: 1100, epoch: 3 | loss: 0.2311409\n",
      "\tspeed: 0.0143s/iter; left time: 505.2538s\n",
      "\titers: 1200, epoch: 3 | loss: 0.2775754\n",
      "\tspeed: 0.0143s/iter; left time: 503.7485s\n",
      "\titers: 1300, epoch: 3 | loss: 0.4343381\n",
      "\tspeed: 0.0143s/iter; left time: 502.0255s\n",
      "\titers: 1400, epoch: 3 | loss: 0.2543454\n",
      "\tspeed: 0.0143s/iter; left time: 500.7970s\n",
      "\titers: 1500, epoch: 3 | loss: 0.3323680\n",
      "\tspeed: 0.0143s/iter; left time: 499.4813s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1564339\n",
      "\tspeed: 0.0143s/iter; left time: 498.0498s\n",
      "\titers: 1700, epoch: 3 | loss: 1.2027413\n",
      "\tspeed: 0.0143s/iter; left time: 496.7325s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1120338\n",
      "\tspeed: 0.0143s/iter; left time: 495.8892s\n",
      "\titers: 1900, epoch: 3 | loss: 0.8186440\n",
      "\tspeed: 0.0143s/iter; left time: 493.8228s\n",
      "\titers: 2000, epoch: 3 | loss: 1.1645291\n",
      "\tspeed: 0.0143s/iter; left time: 492.9134s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1419883\n",
      "\tspeed: 0.0143s/iter; left time: 491.3760s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0934008\n",
      "\tspeed: 0.0143s/iter; left time: 490.1147s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1385167\n",
      "\tspeed: 0.0143s/iter; left time: 488.2893s\n",
      "\titers: 2400, epoch: 3 | loss: 0.3057636\n",
      "\tspeed: 0.0143s/iter; left time: 486.9193s\n",
      "\titers: 2500, epoch: 3 | loss: 0.2764452\n",
      "\tspeed: 0.0143s/iter; left time: 485.2117s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2090632\n",
      "\tspeed: 0.0143s/iter; left time: 483.9174s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0952461\n",
      "\tspeed: 0.0143s/iter; left time: 482.4280s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1890287\n",
      "\tspeed: 0.0143s/iter; left time: 481.2260s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1632638\n",
      "\tspeed: 0.0143s/iter; left time: 479.5869s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0667046\n",
      "\tspeed: 0.0143s/iter; left time: 478.0105s\n",
      "\titers: 3100, epoch: 3 | loss: 0.3519943\n",
      "\tspeed: 0.0143s/iter; left time: 476.5312s\n",
      "\titers: 3200, epoch: 3 | loss: 0.9953343\n",
      "\tspeed: 0.0143s/iter; left time: 475.0930s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1052945\n",
      "\tspeed: 0.0143s/iter; left time: 474.3655s\n",
      "\titers: 3400, epoch: 3 | loss: 0.4149146\n",
      "\tspeed: 0.0143s/iter; left time: 472.3526s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0706064\n",
      "\tspeed: 0.0143s/iter; left time: 471.2139s\n",
      "\titers: 3600, epoch: 3 | loss: 0.3543812\n",
      "\tspeed: 0.0143s/iter; left time: 469.5097s\n",
      "\titers: 3700, epoch: 3 | loss: 0.5902951\n",
      "\tspeed: 0.0143s/iter; left time: 468.2425s\n",
      "\titers: 3800, epoch: 3 | loss: 0.2843511\n",
      "\tspeed: 0.0143s/iter; left time: 466.6585s\n",
      "\titers: 3900, epoch: 3 | loss: 0.4159548\n",
      "\tspeed: 0.0143s/iter; left time: 465.5536s\n",
      "\titers: 4000, epoch: 3 | loss: 0.3127344\n",
      "\tspeed: 0.0143s/iter; left time: 463.7403s\n",
      "\titers: 4100, epoch: 3 | loss: 0.4677638\n",
      "\tspeed: 0.0143s/iter; left time: 462.3509s\n",
      "\titers: 4200, epoch: 3 | loss: 0.3427523\n",
      "\tspeed: 0.0143s/iter; left time: 460.3135s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0447169\n",
      "\tspeed: 0.0142s/iter; left time: 457.9139s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1924510\n",
      "\tspeed: 0.0143s/iter; left time: 457.9270s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0855219\n",
      "\tspeed: 0.0143s/iter; left time: 456.5411s\n",
      "Epoch: 3 cost time: 65.42073655128479\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.2783006 Vali Loss: 0.0581749 Test Loss: 0.1676416\n",
      "Validation loss decreased (0.062182 --> 0.058175).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2911586\n",
      "\tspeed: 0.1428s/iter; left time: 4538.5896s\n",
      "\titers: 200, epoch: 4 | loss: 0.4999390\n",
      "\tspeed: 0.0142s/iter; left time: 449.9161s\n",
      "\titers: 300, epoch: 4 | loss: 0.1218522\n",
      "\tspeed: 0.0147s/iter; left time: 465.3318s\n",
      "\titers: 400, epoch: 4 | loss: 0.3585121\n",
      "\tspeed: 0.0157s/iter; left time: 495.1100s\n",
      "\titers: 500, epoch: 4 | loss: 0.3138391\n",
      "\tspeed: 0.0157s/iter; left time: 494.1803s\n",
      "\titers: 600, epoch: 4 | loss: 0.1456127\n",
      "\tspeed: 0.0162s/iter; left time: 505.5629s\n",
      "\titers: 700, epoch: 4 | loss: 0.2381327\n",
      "\tspeed: 0.0163s/iter; left time: 507.4400s\n",
      "\titers: 800, epoch: 4 | loss: 0.3562238\n",
      "\tspeed: 0.0163s/iter; left time: 505.8839s\n",
      "\titers: 900, epoch: 4 | loss: 0.2505225\n",
      "\tspeed: 0.0163s/iter; left time: 504.0209s\n",
      "\titers: 1000, epoch: 4 | loss: 0.4611588\n",
      "\tspeed: 0.0163s/iter; left time: 502.4295s\n",
      "\titers: 1100, epoch: 4 | loss: 0.3360014\n",
      "\tspeed: 0.0163s/iter; left time: 500.8093s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1464740\n",
      "\tspeed: 0.0163s/iter; left time: 499.6131s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1873611\n",
      "\tspeed: 0.0163s/iter; left time: 499.1548s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0668399\n",
      "\tspeed: 0.0163s/iter; left time: 497.6212s\n",
      "\titers: 1500, epoch: 4 | loss: 0.3520324\n",
      "\tspeed: 0.0163s/iter; left time: 495.8140s\n",
      "\titers: 1600, epoch: 4 | loss: 0.2570711\n",
      "\tspeed: 0.0163s/iter; left time: 493.8782s\n",
      "\titers: 1700, epoch: 4 | loss: 0.2630900\n",
      "\tspeed: 0.0163s/iter; left time: 492.5589s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0978833\n",
      "\tspeed: 0.0163s/iter; left time: 491.3764s\n",
      "\titers: 1900, epoch: 4 | loss: 0.5283551\n",
      "\tspeed: 0.0163s/iter; left time: 489.1509s\n",
      "\titers: 2000, epoch: 4 | loss: 0.3346965\n",
      "\tspeed: 0.0163s/iter; left time: 487.9625s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0292819\n",
      "\tspeed: 0.0163s/iter; left time: 485.9401s\n",
      "\titers: 2200, epoch: 4 | loss: 0.3551190\n",
      "\tspeed: 0.0160s/iter; left time: 476.2115s\n",
      "\titers: 2300, epoch: 4 | loss: 0.5371008\n",
      "\tspeed: 0.0142s/iter; left time: 420.0532s\n",
      "\titers: 2400, epoch: 4 | loss: 0.3664227\n",
      "\tspeed: 0.0142s/iter; left time: 418.3475s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0450820\n",
      "\tspeed: 0.0142s/iter; left time: 418.5828s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1271425\n",
      "\tspeed: 0.0143s/iter; left time: 417.3503s\n",
      "\titers: 2700, epoch: 4 | loss: 1.2215331\n",
      "\tspeed: 0.0143s/iter; left time: 415.9550s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0366771\n",
      "\tspeed: 0.0142s/iter; left time: 414.3747s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0575548\n",
      "\tspeed: 0.0143s/iter; left time: 413.2044s\n",
      "\titers: 3000, epoch: 4 | loss: 0.7131411\n",
      "\tspeed: 0.0142s/iter; left time: 411.4833s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1818416\n",
      "\tspeed: 0.0142s/iter; left time: 410.1623s\n",
      "\titers: 3200, epoch: 4 | loss: 0.5432249\n",
      "\tspeed: 0.0142s/iter; left time: 408.7656s\n",
      "\titers: 3300, epoch: 4 | loss: 0.2772226\n",
      "\tspeed: 0.0143s/iter; left time: 407.8852s\n",
      "\titers: 3400, epoch: 4 | loss: 0.2363740\n",
      "\tspeed: 0.0144s/iter; left time: 410.1181s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1059153\n",
      "\tspeed: 0.0152s/iter; left time: 431.8653s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1376586\n",
      "\tspeed: 0.0158s/iter; left time: 445.5598s\n",
      "\titers: 3700, epoch: 4 | loss: 0.3188474\n",
      "\tspeed: 0.0158s/iter; left time: 443.9311s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1126788\n",
      "\tspeed: 0.0158s/iter; left time: 442.3804s\n",
      "\titers: 3900, epoch: 4 | loss: 0.3460814\n",
      "\tspeed: 0.0157s/iter; left time: 440.7053s\n",
      "\titers: 4000, epoch: 4 | loss: 0.2076877\n",
      "\tspeed: 0.0157s/iter; left time: 439.0913s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0878901\n",
      "\tspeed: 0.0157s/iter; left time: 437.5962s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1440676\n",
      "\tspeed: 0.0158s/iter; left time: 436.0708s\n",
      "\titers: 4300, epoch: 4 | loss: 0.5823860\n",
      "\tspeed: 0.0158s/iter; left time: 434.4893s\n",
      "\titers: 4400, epoch: 4 | loss: 0.2575873\n",
      "\tspeed: 0.0157s/iter; left time: 432.7185s\n",
      "\titers: 4500, epoch: 4 | loss: 0.2091081\n",
      "\tspeed: 0.0157s/iter; left time: 431.2587s\n",
      "Epoch: 4 cost time: 70.6815857887268\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.2639951 Vali Loss: 0.0546164 Test Loss: 0.1577426\n",
      "Validation loss decreased (0.058175 --> 0.054616).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0829182\n",
      "\tspeed: 0.1301s/iter; left time: 3543.0460s\n",
      "\titers: 200, epoch: 5 | loss: 0.1418929\n",
      "\tspeed: 0.0143s/iter; left time: 387.1728s\n",
      "\titers: 300, epoch: 5 | loss: 0.1275262\n",
      "\tspeed: 0.0143s/iter; left time: 385.6973s\n",
      "\titers: 400, epoch: 5 | loss: 0.0834998\n",
      "\tspeed: 0.0143s/iter; left time: 384.1468s\n",
      "\titers: 500, epoch: 5 | loss: 0.5865388\n",
      "\tspeed: 0.0143s/iter; left time: 382.8294s\n",
      "\titers: 600, epoch: 5 | loss: 0.0913362\n",
      "\tspeed: 0.0143s/iter; left time: 381.9568s\n",
      "\titers: 700, epoch: 5 | loss: 0.0520629\n",
      "\tspeed: 0.0143s/iter; left time: 380.0594s\n",
      "\titers: 800, epoch: 5 | loss: 0.2017144\n",
      "\tspeed: 0.0143s/iter; left time: 379.0554s\n",
      "\titers: 900, epoch: 5 | loss: 0.2669882\n",
      "\tspeed: 0.0143s/iter; left time: 377.1042s\n",
      "\titers: 1000, epoch: 5 | loss: 0.2945513\n",
      "\tspeed: 0.0143s/iter; left time: 375.7424s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0937312\n",
      "\tspeed: 0.0143s/iter; left time: 373.9756s\n",
      "\titers: 1200, epoch: 5 | loss: 0.2905221\n",
      "\tspeed: 0.0143s/iter; left time: 373.2118s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1973864\n",
      "\tspeed: 0.0143s/iter; left time: 371.0800s\n",
      "\titers: 1400, epoch: 5 | loss: 0.2814027\n",
      "\tspeed: 0.0142s/iter; left time: 369.2427s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1941628\n",
      "\tspeed: 0.0143s/iter; left time: 368.0944s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1104811\n",
      "\tspeed: 0.0142s/iter; left time: 366.5429s\n",
      "\titers: 1700, epoch: 5 | loss: 0.5862591\n",
      "\tspeed: 0.0143s/iter; left time: 365.4009s\n",
      "\titers: 1800, epoch: 5 | loss: 0.2298188\n",
      "\tspeed: 0.0143s/iter; left time: 364.1773s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0613005\n",
      "\tspeed: 0.0142s/iter; left time: 362.2580s\n",
      "\titers: 2000, epoch: 5 | loss: 0.8852897\n",
      "\tspeed: 0.0142s/iter; left time: 360.6451s\n",
      "\titers: 2100, epoch: 5 | loss: 0.2192393\n",
      "\tspeed: 0.0142s/iter; left time: 359.1570s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0718171\n",
      "\tspeed: 0.0142s/iter; left time: 357.3582s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0854106\n",
      "\tspeed: 0.0142s/iter; left time: 355.7924s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1704925\n",
      "\tspeed: 0.0142s/iter; left time: 354.3703s\n",
      "\titers: 2500, epoch: 5 | loss: 0.2084619\n",
      "\tspeed: 0.0142s/iter; left time: 352.8855s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1793004\n",
      "\tspeed: 0.0142s/iter; left time: 351.8260s\n",
      "\titers: 2700, epoch: 5 | loss: 0.2876534\n",
      "\tspeed: 0.0142s/iter; left time: 350.1198s\n",
      "\titers: 2800, epoch: 5 | loss: 0.2868614\n",
      "\tspeed: 0.0142s/iter; left time: 348.6948s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1154773\n",
      "\tspeed: 0.0142s/iter; left time: 347.2531s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1621205\n",
      "\tspeed: 0.0143s/iter; left time: 346.8972s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1881648\n",
      "\tspeed: 0.0143s/iter; left time: 345.6368s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0339701\n",
      "\tspeed: 0.0142s/iter; left time: 343.8270s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1365678\n",
      "\tspeed: 0.0143s/iter; left time: 342.6101s\n",
      "\titers: 3400, epoch: 5 | loss: 0.3781883\n",
      "\tspeed: 0.0142s/iter; left time: 341.0130s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1444806\n",
      "\tspeed: 0.0142s/iter; left time: 339.5466s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1765105\n",
      "\tspeed: 0.0142s/iter; left time: 338.1039s\n",
      "\titers: 3700, epoch: 5 | loss: 0.1531853\n",
      "\tspeed: 0.0143s/iter; left time: 336.8639s\n",
      "\titers: 3800, epoch: 5 | loss: 0.2852480\n",
      "\tspeed: 0.0143s/iter; left time: 335.3877s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1752526\n",
      "\tspeed: 0.0142s/iter; left time: 333.8051s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1431749\n",
      "\tspeed: 0.0143s/iter; left time: 332.5839s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1160231\n",
      "\tspeed: 0.0143s/iter; left time: 331.2112s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1041800\n",
      "\tspeed: 0.0143s/iter; left time: 329.7027s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1034875\n",
      "\tspeed: 0.0143s/iter; left time: 328.2989s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0402852\n",
      "\tspeed: 0.0143s/iter; left time: 327.1430s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1131040\n",
      "\tspeed: 0.0143s/iter; left time: 325.7533s\n",
      "Epoch: 5 cost time: 65.2471342086792\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.2575099 Vali Loss: 0.0543610 Test Loss: 0.1549808\n",
      "Validation loss decreased (0.054616 --> 0.054361).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0963573\n",
      "\tspeed: 0.1378s/iter; left time: 3124.3830s\n",
      "\titers: 200, epoch: 6 | loss: 0.3405622\n",
      "\tspeed: 0.0143s/iter; left time: 323.2639s\n",
      "\titers: 300, epoch: 6 | loss: 0.1865671\n",
      "\tspeed: 0.0143s/iter; left time: 322.3323s\n",
      "\titers: 400, epoch: 6 | loss: 0.2731573\n",
      "\tspeed: 0.0143s/iter; left time: 319.8380s\n",
      "\titers: 500, epoch: 6 | loss: 0.4088089\n",
      "\tspeed: 0.0143s/iter; left time: 318.3146s\n",
      "\titers: 600, epoch: 6 | loss: 0.4475451\n",
      "\tspeed: 0.0143s/iter; left time: 316.7005s\n",
      "\titers: 700, epoch: 6 | loss: 0.0774577\n",
      "\tspeed: 0.0143s/iter; left time: 315.9336s\n",
      "\titers: 800, epoch: 6 | loss: 0.1205065\n",
      "\tspeed: 0.0143s/iter; left time: 313.9611s\n",
      "\titers: 900, epoch: 6 | loss: 0.1261158\n",
      "\tspeed: 0.0143s/iter; left time: 312.3822s\n",
      "\titers: 1000, epoch: 6 | loss: 0.3775113\n",
      "\tspeed: 0.0143s/iter; left time: 310.9109s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0540762\n",
      "\tspeed: 0.0142s/iter; left time: 308.8655s\n",
      "\titers: 1200, epoch: 6 | loss: 0.2346160\n",
      "\tspeed: 0.0143s/iter; left time: 307.6195s\n",
      "\titers: 1300, epoch: 6 | loss: 0.2658291\n",
      "\tspeed: 0.0142s/iter; left time: 305.9479s\n",
      "\titers: 1400, epoch: 6 | loss: 0.2773753\n",
      "\tspeed: 0.0142s/iter; left time: 304.5471s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1435795\n",
      "\tspeed: 0.0143s/iter; left time: 303.1961s\n",
      "\titers: 1600, epoch: 6 | loss: 0.4485857\n",
      "\tspeed: 0.0142s/iter; left time: 301.6396s\n",
      "\titers: 1700, epoch: 6 | loss: 0.2636383\n",
      "\tspeed: 0.0142s/iter; left time: 300.0889s\n",
      "\titers: 1800, epoch: 6 | loss: 0.4362047\n",
      "\tspeed: 0.0142s/iter; left time: 298.6295s\n",
      "\titers: 1900, epoch: 6 | loss: 0.3951306\n",
      "\tspeed: 0.0142s/iter; left time: 297.2652s\n",
      "\titers: 2000, epoch: 6 | loss: 0.6590569\n",
      "\tspeed: 0.0143s/iter; left time: 296.0745s\n",
      "\titers: 2100, epoch: 6 | loss: 0.3607806\n",
      "\tspeed: 0.0143s/iter; left time: 294.7725s\n",
      "\titers: 2200, epoch: 6 | loss: 0.2990990\n",
      "\tspeed: 0.0143s/iter; left time: 293.2912s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1099658\n",
      "\tspeed: 0.0142s/iter; left time: 291.5619s\n",
      "\titers: 2400, epoch: 6 | loss: 0.2147509\n",
      "\tspeed: 0.0142s/iter; left time: 290.0504s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1226614\n",
      "\tspeed: 0.0142s/iter; left time: 288.6405s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1159119\n",
      "\tspeed: 0.0142s/iter; left time: 287.0853s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1199892\n",
      "\tspeed: 0.0142s/iter; left time: 285.7711s\n",
      "\titers: 2800, epoch: 6 | loss: 0.2258014\n",
      "\tspeed: 0.0142s/iter; left time: 284.2539s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2200111\n",
      "\tspeed: 0.0142s/iter; left time: 282.8707s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0500991\n",
      "\tspeed: 0.0142s/iter; left time: 281.5345s\n",
      "\titers: 3100, epoch: 6 | loss: 0.2976670\n",
      "\tspeed: 0.0142s/iter; left time: 280.0324s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1158589\n",
      "\tspeed: 0.0142s/iter; left time: 278.6491s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1545437\n",
      "\tspeed: 0.0142s/iter; left time: 277.3520s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0562676\n",
      "\tspeed: 0.0142s/iter; left time: 275.7589s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0401151\n",
      "\tspeed: 0.0142s/iter; left time: 274.2869s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1709601\n",
      "\tspeed: 0.0142s/iter; left time: 272.8099s\n",
      "\titers: 3700, epoch: 6 | loss: 0.3243546\n",
      "\tspeed: 0.0142s/iter; left time: 271.5377s\n",
      "\titers: 3800, epoch: 6 | loss: 1.0127968\n",
      "\tspeed: 0.0142s/iter; left time: 270.3280s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0907378\n",
      "\tspeed: 0.0142s/iter; left time: 268.8303s\n",
      "\titers: 4000, epoch: 6 | loss: 0.4097501\n",
      "\tspeed: 0.0142s/iter; left time: 267.3278s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0467648\n",
      "\tspeed: 0.0142s/iter; left time: 265.8960s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1517370\n",
      "\tspeed: 0.0142s/iter; left time: 264.4074s\n",
      "\titers: 4300, epoch: 6 | loss: 0.4287617\n",
      "\tspeed: 0.0142s/iter; left time: 262.8208s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1835717\n",
      "\tspeed: 0.0142s/iter; left time: 261.5032s\n",
      "\titers: 4500, epoch: 6 | loss: 0.3481352\n",
      "\tspeed: 0.0143s/iter; left time: 260.8902s\n",
      "Epoch: 6 cost time: 65.22627401351929\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.2527961 Vali Loss: 0.0522597 Test Loss: 0.1519772\n",
      "Validation loss decreased (0.054361 --> 0.052260).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2113188\n",
      "\tspeed: 0.1338s/iter; left time: 2423.7789s\n",
      "\titers: 200, epoch: 7 | loss: 0.3359199\n",
      "\tspeed: 0.0143s/iter; left time: 256.8936s\n",
      "\titers: 300, epoch: 7 | loss: 0.4571475\n",
      "\tspeed: 0.0143s/iter; left time: 255.8759s\n",
      "\titers: 400, epoch: 7 | loss: 0.2552275\n",
      "\tspeed: 0.0142s/iter; left time: 253.6274s\n",
      "\titers: 500, epoch: 7 | loss: 0.1054062\n",
      "\tspeed: 0.0145s/iter; left time: 257.0864s\n",
      "\titers: 600, epoch: 7 | loss: 0.1625092\n",
      "\tspeed: 0.0157s/iter; left time: 277.3865s\n",
      "\titers: 700, epoch: 7 | loss: 0.0538092\n",
      "\tspeed: 0.0157s/iter; left time: 275.2091s\n",
      "\titers: 800, epoch: 7 | loss: 0.1083405\n",
      "\tspeed: 0.0157s/iter; left time: 273.7750s\n",
      "\titers: 900, epoch: 7 | loss: 0.6536971\n",
      "\tspeed: 0.0157s/iter; left time: 272.6283s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0566030\n",
      "\tspeed: 0.0158s/iter; left time: 271.5844s\n",
      "\titers: 1100, epoch: 7 | loss: 0.8014874\n",
      "\tspeed: 0.0157s/iter; left time: 269.5827s\n",
      "\titers: 1200, epoch: 7 | loss: 0.4344320\n",
      "\tspeed: 0.0157s/iter; left time: 267.5677s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1740578\n",
      "\tspeed: 0.0157s/iter; left time: 266.2793s\n",
      "\titers: 1400, epoch: 7 | loss: 0.3211494\n",
      "\tspeed: 0.0157s/iter; left time: 264.7593s\n",
      "\titers: 1500, epoch: 7 | loss: 0.2775639\n",
      "\tspeed: 0.0157s/iter; left time: 263.2239s\n",
      "\titers: 1600, epoch: 7 | loss: 0.2180572\n",
      "\tspeed: 0.0157s/iter; left time: 261.3191s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0896768\n",
      "\tspeed: 0.0157s/iter; left time: 259.8793s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0903889\n",
      "\tspeed: 0.0150s/iter; left time: 246.1591s\n",
      "\titers: 1900, epoch: 7 | loss: 0.1306486\n",
      "\tspeed: 0.0142s/iter; left time: 232.1125s\n",
      "\titers: 2000, epoch: 7 | loss: 0.1427216\n",
      "\tspeed: 0.0142s/iter; left time: 230.5814s\n",
      "\titers: 2100, epoch: 7 | loss: 0.3563719\n",
      "\tspeed: 0.0142s/iter; left time: 229.2436s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1195960\n",
      "\tspeed: 0.0142s/iter; left time: 227.7322s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0714326\n",
      "\tspeed: 0.0142s/iter; left time: 226.4010s\n",
      "\titers: 2400, epoch: 7 | loss: 0.2129743\n",
      "\tspeed: 0.0142s/iter; left time: 224.9772s\n",
      "\titers: 2500, epoch: 7 | loss: 0.2988663\n",
      "\tspeed: 0.0142s/iter; left time: 223.4964s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1389491\n",
      "\tspeed: 0.0142s/iter; left time: 222.1840s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0590624\n",
      "\tspeed: 0.0156s/iter; left time: 241.5728s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1708938\n",
      "\tspeed: 0.0157s/iter; left time: 242.3622s\n",
      "\titers: 2900, epoch: 7 | loss: 0.2344916\n",
      "\tspeed: 0.0157s/iter; left time: 240.7622s\n",
      "\titers: 3000, epoch: 7 | loss: 0.1976618\n",
      "\tspeed: 0.0157s/iter; left time: 239.2030s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0297178\n",
      "\tspeed: 0.0157s/iter; left time: 237.7264s\n",
      "\titers: 3200, epoch: 7 | loss: 0.6209712\n",
      "\tspeed: 0.0157s/iter; left time: 236.0930s\n",
      "\titers: 3300, epoch: 7 | loss: 0.1106067\n",
      "\tspeed: 0.0152s/iter; left time: 226.8153s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0948443\n",
      "\tspeed: 0.0142s/iter; left time: 210.7005s\n",
      "\titers: 3500, epoch: 7 | loss: 0.3069868\n",
      "\tspeed: 0.0142s/iter; left time: 209.4759s\n",
      "\titers: 3600, epoch: 7 | loss: 0.2621201\n",
      "\tspeed: 0.0142s/iter; left time: 207.9391s\n",
      "\titers: 3700, epoch: 7 | loss: 0.4557164\n",
      "\tspeed: 0.0142s/iter; left time: 206.5665s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1233699\n",
      "\tspeed: 0.0142s/iter; left time: 205.4340s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0604380\n",
      "\tspeed: 0.0143s/iter; left time: 204.0986s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1932746\n",
      "\tspeed: 0.0142s/iter; left time: 202.6071s\n",
      "\titers: 4100, epoch: 7 | loss: 0.3051146\n",
      "\tspeed: 0.0142s/iter; left time: 201.1541s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0371534\n",
      "\tspeed: 0.0142s/iter; left time: 199.6620s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0363916\n",
      "\tspeed: 0.0142s/iter; left time: 198.2247s\n",
      "\titers: 4400, epoch: 7 | loss: 0.1334946\n",
      "\tspeed: 0.0142s/iter; left time: 196.8269s\n",
      "\titers: 4500, epoch: 7 | loss: 0.1401726\n",
      "\tspeed: 0.0142s/iter; left time: 195.4165s\n",
      "Epoch: 7 cost time: 68.02103877067566\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.2493945 Vali Loss: 0.0525729 Test Loss: 0.1533197\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1326552\n",
      "\tspeed: 0.1328s/iter; left time: 1802.1308s\n",
      "\titers: 200, epoch: 8 | loss: 0.1113854\n",
      "\tspeed: 0.0158s/iter; left time: 212.3267s\n",
      "\titers: 300, epoch: 8 | loss: 0.4363588\n",
      "\tspeed: 0.0157s/iter; left time: 210.3549s\n",
      "\titers: 400, epoch: 8 | loss: 0.0893780\n",
      "\tspeed: 0.0157s/iter; left time: 208.5367s\n",
      "\titers: 500, epoch: 8 | loss: 0.3738170\n",
      "\tspeed: 0.0157s/iter; left time: 207.2570s\n",
      "\titers: 600, epoch: 8 | loss: 0.4516700\n",
      "\tspeed: 0.0158s/iter; left time: 205.8609s\n",
      "\titers: 700, epoch: 8 | loss: 0.5182699\n",
      "\tspeed: 0.0157s/iter; left time: 203.8498s\n",
      "\titers: 800, epoch: 8 | loss: 0.2421166\n",
      "\tspeed: 0.0157s/iter; left time: 202.3668s\n",
      "\titers: 900, epoch: 8 | loss: 0.2400156\n",
      "\tspeed: 0.0157s/iter; left time: 200.7609s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1976524\n",
      "\tspeed: 0.0157s/iter; left time: 199.3754s\n",
      "\titers: 1100, epoch: 8 | loss: 0.2925050\n",
      "\tspeed: 0.0157s/iter; left time: 197.6971s\n",
      "\titers: 1200, epoch: 8 | loss: 0.4107059\n",
      "\tspeed: 0.0157s/iter; left time: 196.0946s\n",
      "\titers: 1300, epoch: 8 | loss: 1.3378042\n",
      "\tspeed: 0.0157s/iter; left time: 194.3743s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0624989\n",
      "\tspeed: 0.0157s/iter; left time: 192.7789s\n",
      "\titers: 1500, epoch: 8 | loss: 0.1547224\n",
      "\tspeed: 0.0157s/iter; left time: 191.4381s\n",
      "\titers: 1600, epoch: 8 | loss: 0.2005935\n",
      "\tspeed: 0.0147s/iter; left time: 177.9197s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0877384\n",
      "\tspeed: 0.0143s/iter; left time: 170.6570s\n",
      "\titers: 1800, epoch: 8 | loss: 0.3641151\n",
      "\tspeed: 0.0153s/iter; left time: 181.0441s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1956605\n",
      "\tspeed: 0.0157s/iter; left time: 185.0998s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1289739\n",
      "\tspeed: 0.0154s/iter; left time: 179.8868s\n",
      "\titers: 2100, epoch: 8 | loss: 0.1046009\n",
      "\tspeed: 0.0143s/iter; left time: 164.9985s\n",
      "\titers: 2200, epoch: 8 | loss: 0.1539181\n",
      "\tspeed: 0.0143s/iter; left time: 163.4811s\n",
      "\titers: 2300, epoch: 8 | loss: 0.2792454\n",
      "\tspeed: 0.0143s/iter; left time: 162.0851s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1248742\n",
      "\tspeed: 0.0143s/iter; left time: 160.6335s\n",
      "\titers: 2500, epoch: 8 | loss: 0.2453257\n",
      "\tspeed: 0.0143s/iter; left time: 159.1723s\n",
      "\titers: 2600, epoch: 8 | loss: 0.1182591\n",
      "\tspeed: 0.0143s/iter; left time: 157.7637s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1530906\n",
      "\tspeed: 0.0141s/iter; left time: 155.0637s\n",
      "\titers: 2800, epoch: 8 | loss: 0.1004607\n",
      "\tspeed: 0.0141s/iter; left time: 153.4969s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1684385\n",
      "\tspeed: 0.0141s/iter; left time: 152.0926s\n",
      "\titers: 3000, epoch: 8 | loss: 0.5727048\n",
      "\tspeed: 0.0141s/iter; left time: 150.6360s\n",
      "\titers: 3100, epoch: 8 | loss: 0.9061102\n",
      "\tspeed: 0.0141s/iter; left time: 149.2538s\n",
      "\titers: 3200, epoch: 8 | loss: 0.3121394\n",
      "\tspeed: 0.0141s/iter; left time: 147.7947s\n",
      "\titers: 3300, epoch: 8 | loss: 0.2114329\n",
      "\tspeed: 0.0141s/iter; left time: 146.4225s\n",
      "\titers: 3400, epoch: 8 | loss: 0.2175026\n",
      "\tspeed: 0.0141s/iter; left time: 145.0338s\n",
      "\titers: 3500, epoch: 8 | loss: 0.6193746\n",
      "\tspeed: 0.0141s/iter; left time: 143.6272s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0517359\n",
      "\tspeed: 0.0141s/iter; left time: 142.2169s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2895639\n",
      "\tspeed: 0.0141s/iter; left time: 140.8049s\n",
      "\titers: 3800, epoch: 8 | loss: 0.3608701\n",
      "\tspeed: 0.0141s/iter; left time: 139.4003s\n",
      "\titers: 3900, epoch: 8 | loss: 0.2145395\n",
      "\tspeed: 0.0141s/iter; left time: 137.9097s\n",
      "\titers: 4000, epoch: 8 | loss: 0.2880820\n",
      "\tspeed: 0.0141s/iter; left time: 136.5345s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0801134\n",
      "\tspeed: 0.0141s/iter; left time: 135.1222s\n",
      "\titers: 4200, epoch: 8 | loss: 0.1573067\n",
      "\tspeed: 0.0141s/iter; left time: 133.7584s\n",
      "\titers: 4300, epoch: 8 | loss: 0.4154709\n",
      "\tspeed: 0.0141s/iter; left time: 132.2250s\n",
      "\titers: 4400, epoch: 8 | loss: 0.4047716\n",
      "\tspeed: 0.0141s/iter; left time: 130.8263s\n",
      "\titers: 4500, epoch: 8 | loss: 0.7002885\n",
      "\tspeed: 0.0141s/iter; left time: 129.3879s\n",
      "Epoch: 8 cost time: 67.63555121421814\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.2504234 Vali Loss: 0.0526646 Test Loss: 0.1530095\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1595867\n",
      "\tspeed: 0.1295s/iter; left time: 1166.5756s\n",
      "\titers: 200, epoch: 9 | loss: 0.3530492\n",
      "\tspeed: 0.0143s/iter; left time: 127.4392s\n",
      "\titers: 300, epoch: 9 | loss: 0.0728085\n",
      "\tspeed: 0.0143s/iter; left time: 125.8283s\n",
      "\titers: 400, epoch: 9 | loss: 0.2416262\n",
      "\tspeed: 0.0143s/iter; left time: 124.3216s\n",
      "\titers: 500, epoch: 9 | loss: 0.1263596\n",
      "\tspeed: 0.0143s/iter; left time: 122.9453s\n",
      "\titers: 600, epoch: 9 | loss: 0.0385061\n",
      "\tspeed: 0.0143s/iter; left time: 121.5129s\n",
      "\titers: 700, epoch: 9 | loss: 0.3870395\n",
      "\tspeed: 0.0143s/iter; left time: 120.0614s\n",
      "\titers: 800, epoch: 9 | loss: 0.0621153\n",
      "\tspeed: 0.0143s/iter; left time: 118.5348s\n",
      "\titers: 900, epoch: 9 | loss: 0.1663207\n",
      "\tspeed: 0.0143s/iter; left time: 117.0702s\n",
      "\titers: 1000, epoch: 9 | loss: 0.3750638\n",
      "\tspeed: 0.0155s/iter; left time: 125.8704s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0924963\n",
      "\tspeed: 0.0157s/iter; left time: 125.9574s\n",
      "\titers: 1200, epoch: 9 | loss: 0.4830179\n",
      "\tspeed: 0.0157s/iter; left time: 124.3685s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0652306\n",
      "\tspeed: 0.0157s/iter; left time: 122.8370s\n",
      "\titers: 1400, epoch: 9 | loss: 0.4466890\n",
      "\tspeed: 0.0157s/iter; left time: 121.1111s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1428217\n",
      "\tspeed: 0.0155s/iter; left time: 118.1460s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0843109\n",
      "\tspeed: 0.0142s/iter; left time: 106.8085s\n",
      "\titers: 1700, epoch: 9 | loss: 0.1532654\n",
      "\tspeed: 0.0142s/iter; left time: 105.2911s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0794844\n",
      "\tspeed: 0.0142s/iter; left time: 103.9485s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0962915\n",
      "\tspeed: 0.0142s/iter; left time: 102.7203s\n",
      "\titers: 2000, epoch: 9 | loss: 0.1820645\n",
      "\tspeed: 0.0142s/iter; left time: 100.9324s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0725456\n",
      "\tspeed: 0.0142s/iter; left time: 99.3150s\n",
      "\titers: 2200, epoch: 9 | loss: 0.2221693\n",
      "\tspeed: 0.0142s/iter; left time: 97.9045s\n",
      "\titers: 2300, epoch: 9 | loss: 0.4822427\n",
      "\tspeed: 0.0142s/iter; left time: 96.4496s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0554838\n",
      "\tspeed: 0.0142s/iter; left time: 95.0462s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0531694\n",
      "\tspeed: 0.0142s/iter; left time: 93.9362s\n",
      "\titers: 2600, epoch: 9 | loss: 0.2746172\n",
      "\tspeed: 0.0142s/iter; left time: 92.5188s\n",
      "\titers: 2700, epoch: 9 | loss: 0.1303993\n",
      "\tspeed: 0.0142s/iter; left time: 90.9976s\n",
      "\titers: 2800, epoch: 9 | loss: 0.4128854\n",
      "\tspeed: 0.0142s/iter; left time: 89.6142s\n",
      "\titers: 2900, epoch: 9 | loss: 0.2014362\n",
      "\tspeed: 0.0142s/iter; left time: 88.1957s\n",
      "\titers: 3000, epoch: 9 | loss: 0.2514510\n",
      "\tspeed: 0.0142s/iter; left time: 86.8349s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0706444\n",
      "\tspeed: 0.0142s/iter; left time: 85.4517s\n",
      "\titers: 3200, epoch: 9 | loss: 0.1360653\n",
      "\tspeed: 0.0142s/iter; left time: 84.0371s\n",
      "\titers: 3300, epoch: 9 | loss: 0.5869950\n",
      "\tspeed: 0.0142s/iter; left time: 82.6081s\n",
      "\titers: 3400, epoch: 9 | loss: 0.6108298\n",
      "\tspeed: 0.0142s/iter; left time: 81.0474s\n",
      "\titers: 3500, epoch: 9 | loss: 0.1883400\n",
      "\tspeed: 0.0142s/iter; left time: 79.6053s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0698287\n",
      "\tspeed: 0.0142s/iter; left time: 78.1006s\n",
      "\titers: 3700, epoch: 9 | loss: 0.1925623\n",
      "\tspeed: 0.0141s/iter; left time: 76.4293s\n",
      "\titers: 3800, epoch: 9 | loss: 0.3135700\n",
      "\tspeed: 0.0141s/iter; left time: 75.1141s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0955295\n",
      "\tspeed: 0.0141s/iter; left time: 73.6453s\n",
      "\titers: 4000, epoch: 9 | loss: 0.1519378\n",
      "\tspeed: 0.0141s/iter; left time: 72.1989s\n",
      "\titers: 4100, epoch: 9 | loss: 0.1359244\n",
      "\tspeed: 0.0141s/iter; left time: 70.7113s\n",
      "\titers: 4200, epoch: 9 | loss: 0.1317504\n",
      "\tspeed: 0.0141s/iter; left time: 69.2871s\n",
      "\titers: 4300, epoch: 9 | loss: 0.1020665\n",
      "\tspeed: 0.0141s/iter; left time: 67.8977s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0766850\n",
      "\tspeed: 0.0141s/iter; left time: 66.5523s\n",
      "\titers: 4500, epoch: 9 | loss: 0.1877633\n",
      "\tspeed: 0.0141s/iter; left time: 65.0985s\n",
      "Epoch: 9 cost time: 65.94082927703857\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.2475977 Vali Loss: 0.0523881 Test Loss: 0.1529920\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['total_load', 'solar_penetration', 'wind_penetration']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15200412273406982, mae:0.2476680874824524\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash scripts/TimeXer.sh \\\n",
    "     --features MS \\\n",
    "     --predictor solar_penetration,wind_penetration,total_load \\\n",
    "     --enc_in 4  --dec_in 4  --c_out 1 \\\n",
    "     --dropout 0.2  --learning_rate 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc5655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       total_load          \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3937238\n",
      "\tspeed: 0.0256s/iter; left time: 1161.4764s\n",
      "\titers: 200, epoch: 1 | loss: 0.2211885\n",
      "\tspeed: 0.0141s/iter; left time: 640.5980s\n",
      "\titers: 300, epoch: 1 | loss: 0.0974906\n",
      "\tspeed: 0.0144s/iter; left time: 649.7062s\n",
      "\titers: 400, epoch: 1 | loss: 0.3731572\n",
      "\tspeed: 0.0156s/iter; left time: 705.0323s\n",
      "\titers: 500, epoch: 1 | loss: 0.0808192\n",
      "\tspeed: 0.0156s/iter; left time: 703.2882s\n",
      "\titers: 600, epoch: 1 | loss: 0.1748722\n",
      "\tspeed: 0.0156s/iter; left time: 701.8358s\n",
      "\titers: 700, epoch: 1 | loss: 0.1925261\n",
      "\tspeed: 0.0156s/iter; left time: 700.4299s\n",
      "\titers: 800, epoch: 1 | loss: 0.6776096\n",
      "\tspeed: 0.0142s/iter; left time: 635.5049s\n",
      "\titers: 900, epoch: 1 | loss: 0.4568157\n",
      "\tspeed: 0.0141s/iter; left time: 630.0284s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0857605\n",
      "\tspeed: 0.0141s/iter; left time: 628.6983s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0655505\n",
      "\tspeed: 0.0141s/iter; left time: 625.6989s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1716764\n",
      "\tspeed: 0.0141s/iter; left time: 624.2162s\n",
      "\titers: 1300, epoch: 1 | loss: 0.3708221\n",
      "\tspeed: 0.0141s/iter; left time: 623.5983s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0571158\n",
      "\tspeed: 0.0141s/iter; left time: 621.4232s\n",
      "\titers: 1500, epoch: 1 | loss: 0.2492069\n",
      "\tspeed: 0.0141s/iter; left time: 620.4677s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2295239\n",
      "\tspeed: 0.0141s/iter; left time: 618.6635s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4100257\n",
      "\tspeed: 0.0141s/iter; left time: 617.1769s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1021772\n",
      "\tspeed: 0.0141s/iter; left time: 615.8636s\n",
      "\titers: 1900, epoch: 1 | loss: 0.3098867\n",
      "\tspeed: 0.0141s/iter; left time: 614.4877s\n",
      "\titers: 2000, epoch: 1 | loss: 0.7882435\n",
      "\tspeed: 0.0141s/iter; left time: 612.9945s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1910979\n",
      "\tspeed: 0.0141s/iter; left time: 611.6719s\n",
      "\titers: 2200, epoch: 1 | loss: 0.5234938\n",
      "\tspeed: 0.0141s/iter; left time: 610.2233s\n",
      "\titers: 2300, epoch: 1 | loss: 0.2088784\n",
      "\tspeed: 0.0141s/iter; left time: 609.0683s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1711332\n",
      "\tspeed: 0.0141s/iter; left time: 607.1109s\n",
      "\titers: 2500, epoch: 1 | loss: 0.6244156\n",
      "\tspeed: 0.0141s/iter; left time: 605.8612s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0848932\n",
      "\tspeed: 0.0141s/iter; left time: 605.1592s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2835775\n",
      "\tspeed: 0.0141s/iter; left time: 603.0884s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2885987\n",
      "\tspeed: 0.0141s/iter; left time: 602.5033s\n",
      "\titers: 2900, epoch: 1 | loss: 0.4694354\n",
      "\tspeed: 0.0141s/iter; left time: 600.6982s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1837000\n",
      "\tspeed: 0.0141s/iter; left time: 598.9098s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1423325\n",
      "\tspeed: 0.0141s/iter; left time: 597.8892s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2809777\n",
      "\tspeed: 0.0141s/iter; left time: 596.6275s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0560180\n",
      "\tspeed: 0.0141s/iter; left time: 594.5702s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4974792\n",
      "\tspeed: 0.0141s/iter; left time: 593.3340s\n",
      "\titers: 3500, epoch: 1 | loss: 0.3582458\n",
      "\tspeed: 0.0141s/iter; left time: 591.8361s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1837243\n",
      "\tspeed: 0.0141s/iter; left time: 590.5301s\n",
      "\titers: 3700, epoch: 1 | loss: 0.3009663\n",
      "\tspeed: 0.0141s/iter; left time: 589.2148s\n",
      "\titers: 3800, epoch: 1 | loss: 0.3312285\n",
      "\tspeed: 0.0141s/iter; left time: 587.3925s\n",
      "\titers: 3900, epoch: 1 | loss: 0.8456334\n",
      "\tspeed: 0.0141s/iter; left time: 586.1249s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2644045\n",
      "\tspeed: 0.0141s/iter; left time: 585.0885s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0952836\n",
      "\tspeed: 0.0141s/iter; left time: 583.4649s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1163210\n",
      "\tspeed: 0.0141s/iter; left time: 582.0887s\n",
      "\titers: 4300, epoch: 1 | loss: 0.3398380\n",
      "\tspeed: 0.0141s/iter; left time: 580.6317s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1051230\n",
      "\tspeed: 0.0141s/iter; left time: 579.3009s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1957417\n",
      "\tspeed: 0.0147s/iter; left time: 603.2078s\n",
      "Epoch: 1 cost time: 66.05369234085083\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.3218442 Vali Loss: 0.0613904 Test Loss: 0.1981160\n",
      "Validation loss decreased (inf --> 0.061390).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.0886572\n",
      "\tspeed: 0.1344s/iter; left time: 5495.7587s\n",
      "\titers: 200, epoch: 2 | loss: 0.1051599\n",
      "\tspeed: 0.0141s/iter; left time: 576.2880s\n",
      "\titers: 300, epoch: 2 | loss: 0.1354137\n",
      "\tspeed: 0.0141s/iter; left time: 573.8929s\n",
      "\titers: 400, epoch: 2 | loss: 0.3516432\n",
      "\tspeed: 0.0141s/iter; left time: 572.0544s\n",
      "\titers: 500, epoch: 2 | loss: 0.2956066\n",
      "\tspeed: 0.0141s/iter; left time: 570.4909s\n",
      "\titers: 600, epoch: 2 | loss: 0.1722189\n",
      "\tspeed: 0.0141s/iter; left time: 569.9102s\n",
      "\titers: 700, epoch: 2 | loss: 0.1058796\n",
      "\tspeed: 0.0141s/iter; left time: 568.0542s\n",
      "\titers: 800, epoch: 2 | loss: 0.1169791\n",
      "\tspeed: 0.0141s/iter; left time: 566.4268s\n",
      "\titers: 900, epoch: 2 | loss: 0.0656988\n",
      "\tspeed: 0.0141s/iter; left time: 564.7408s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1678888\n",
      "\tspeed: 0.0141s/iter; left time: 563.5487s\n",
      "\titers: 1100, epoch: 2 | loss: 0.5977303\n",
      "\tspeed: 0.0149s/iter; left time: 592.6978s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2456419\n",
      "\tspeed: 0.0156s/iter; left time: 619.6988s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1665373\n",
      "\tspeed: 0.0156s/iter; left time: 617.9895s\n",
      "\titers: 1400, epoch: 2 | loss: 0.3216705\n",
      "\tspeed: 0.0156s/iter; left time: 616.3040s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0513116\n",
      "\tspeed: 0.0156s/iter; left time: 614.8130s\n",
      "\titers: 1600, epoch: 2 | loss: 0.5017438\n",
      "\tspeed: 0.0156s/iter; left time: 613.0638s\n",
      "\titers: 1700, epoch: 2 | loss: 0.6098756\n",
      "\tspeed: 0.0156s/iter; left time: 611.8212s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1393766\n",
      "\tspeed: 0.0156s/iter; left time: 611.2405s\n",
      "\titers: 1900, epoch: 2 | loss: 0.2743874\n",
      "\tspeed: 0.0156s/iter; left time: 610.6078s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1706437\n",
      "\tspeed: 0.0156s/iter; left time: 608.6958s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1121742\n",
      "\tspeed: 0.0156s/iter; left time: 607.2022s\n",
      "\titers: 2200, epoch: 2 | loss: 0.2459603\n",
      "\tspeed: 0.0156s/iter; left time: 605.7007s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1464717\n",
      "\tspeed: 0.0156s/iter; left time: 604.0963s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1899845\n",
      "\tspeed: 0.0156s/iter; left time: 602.3478s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1126133\n",
      "\tspeed: 0.0156s/iter; left time: 600.9402s\n",
      "\titers: 2600, epoch: 2 | loss: 1.3365586\n",
      "\tspeed: 0.0156s/iter; left time: 599.3857s\n",
      "\titers: 2700, epoch: 2 | loss: 0.2151973\n",
      "\tspeed: 0.0156s/iter; left time: 597.9980s\n",
      "\titers: 2800, epoch: 2 | loss: 0.2158315\n",
      "\tspeed: 0.0156s/iter; left time: 596.2254s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1721942\n",
      "\tspeed: 0.0156s/iter; left time: 594.7802s\n",
      "\titers: 3000, epoch: 2 | loss: 0.4064582\n",
      "\tspeed: 0.0156s/iter; left time: 592.9955s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1187558\n",
      "\tspeed: 0.0156s/iter; left time: 591.3762s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2704747\n",
      "\tspeed: 0.0156s/iter; left time: 589.9150s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0638485\n",
      "\tspeed: 0.0156s/iter; left time: 588.0924s\n",
      "\titers: 3400, epoch: 2 | loss: 0.3714327\n",
      "\tspeed: 0.0156s/iter; left time: 586.5904s\n",
      "\titers: 3500, epoch: 2 | loss: 0.3458272\n",
      "\tspeed: 0.0156s/iter; left time: 584.9859s\n",
      "\titers: 3600, epoch: 2 | loss: 0.2526027\n",
      "\tspeed: 0.0156s/iter; left time: 583.6537s\n",
      "\titers: 3700, epoch: 2 | loss: 0.5279131\n",
      "\tspeed: 0.0156s/iter; left time: 581.9569s\n",
      "\titers: 3800, epoch: 2 | loss: 0.2403472\n",
      "\tspeed: 0.0156s/iter; left time: 580.5438s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1897175\n",
      "\tspeed: 0.0156s/iter; left time: 579.0465s\n",
      "\titers: 4000, epoch: 2 | loss: 0.4684605\n",
      "\tspeed: 0.0156s/iter; left time: 577.4270s\n",
      "\titers: 4100, epoch: 2 | loss: 0.4591953\n",
      "\tspeed: 0.0156s/iter; left time: 575.8218s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1515115\n",
      "\tspeed: 0.0156s/iter; left time: 574.2877s\n",
      "\titers: 4300, epoch: 2 | loss: 0.2142822\n",
      "\tspeed: 0.0156s/iter; left time: 572.5631s\n",
      "\titers: 4400, epoch: 2 | loss: 0.2413058\n",
      "\tspeed: 0.0156s/iter; left time: 571.0835s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1620038\n",
      "\tspeed: 0.0156s/iter; left time: 569.6231s\n",
      "Epoch: 2 cost time: 69.81865763664246\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.3130835 Vali Loss: 0.0604125 Test Loss: 0.1864917\n",
      "Validation loss decreased (0.061390 --> 0.060413).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4148970\n",
      "\tspeed: 0.1408s/iter; left time: 5118.1603s\n",
      "\titers: 200, epoch: 3 | loss: 0.0731344\n",
      "\tspeed: 0.0142s/iter; left time: 513.5270s\n",
      "\titers: 300, epoch: 3 | loss: 0.3756325\n",
      "\tspeed: 0.0141s/iter; left time: 511.3671s\n",
      "\titers: 400, epoch: 3 | loss: 0.2101761\n",
      "\tspeed: 0.0142s/iter; left time: 510.3369s\n",
      "\titers: 500, epoch: 3 | loss: 0.4138548\n",
      "\tspeed: 0.0142s/iter; left time: 508.6022s\n",
      "\titers: 600, epoch: 3 | loss: 0.1911284\n",
      "\tspeed: 0.0141s/iter; left time: 507.0961s\n",
      "\titers: 700, epoch: 3 | loss: 1.0696054\n",
      "\tspeed: 0.0141s/iter; left time: 505.6930s\n",
      "\titers: 800, epoch: 3 | loss: 0.2562020\n",
      "\tspeed: 0.0141s/iter; left time: 504.0824s\n",
      "\titers: 900, epoch: 3 | loss: 0.1621725\n",
      "\tspeed: 0.0155s/iter; left time: 549.7420s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3749084\n",
      "\tspeed: 0.0156s/iter; left time: 553.9423s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1332163\n",
      "\tspeed: 0.0156s/iter; left time: 552.3698s\n",
      "\titers: 1200, epoch: 3 | loss: 0.3157081\n",
      "\tspeed: 0.0156s/iter; left time: 550.7006s\n",
      "\titers: 1300, epoch: 3 | loss: 0.3776181\n",
      "\tspeed: 0.0156s/iter; left time: 549.2675s\n",
      "\titers: 1400, epoch: 3 | loss: 0.2447106\n",
      "\tspeed: 0.0156s/iter; left time: 547.6180s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2980069\n",
      "\tspeed: 0.0156s/iter; left time: 546.1646s\n",
      "\titers: 1600, epoch: 3 | loss: 0.2165819\n",
      "\tspeed: 0.0156s/iter; left time: 544.4670s\n",
      "\titers: 1700, epoch: 3 | loss: 1.0325658\n",
      "\tspeed: 0.0156s/iter; left time: 543.0621s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1084550\n",
      "\tspeed: 0.0156s/iter; left time: 541.3656s\n",
      "\titers: 1900, epoch: 3 | loss: 0.6338503\n",
      "\tspeed: 0.0156s/iter; left time: 539.7152s\n",
      "\titers: 2000, epoch: 3 | loss: 1.5903376\n",
      "\tspeed: 0.0156s/iter; left time: 538.3418s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1080404\n",
      "\tspeed: 0.0156s/iter; left time: 536.5067s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1159033\n",
      "\tspeed: 0.0156s/iter; left time: 535.1128s\n",
      "\titers: 2300, epoch: 3 | loss: 0.2216500\n",
      "\tspeed: 0.0156s/iter; left time: 533.4959s\n",
      "\titers: 2400, epoch: 3 | loss: 0.2534862\n",
      "\tspeed: 0.0156s/iter; left time: 531.9541s\n",
      "\titers: 2500, epoch: 3 | loss: 0.2425389\n",
      "\tspeed: 0.0156s/iter; left time: 530.3933s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1461578\n",
      "\tspeed: 0.0156s/iter; left time: 528.8775s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1443017\n",
      "\tspeed: 0.0156s/iter; left time: 527.3424s\n",
      "\titers: 2800, epoch: 3 | loss: 0.2303417\n",
      "\tspeed: 0.0156s/iter; left time: 525.6845s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1591731\n",
      "\tspeed: 0.0156s/iter; left time: 524.1014s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0631804\n",
      "\tspeed: 0.0156s/iter; left time: 522.4085s\n",
      "\titers: 3100, epoch: 3 | loss: 0.3590523\n",
      "\tspeed: 0.0156s/iter; left time: 520.8389s\n",
      "\titers: 3200, epoch: 3 | loss: 0.5994760\n",
      "\tspeed: 0.0156s/iter; left time: 519.4324s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1818077\n",
      "\tspeed: 0.0156s/iter; left time: 517.8482s\n",
      "\titers: 3400, epoch: 3 | loss: 0.3222390\n",
      "\tspeed: 0.0156s/iter; left time: 516.2539s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1476547\n",
      "\tspeed: 0.0156s/iter; left time: 514.5614s\n",
      "\titers: 3600, epoch: 3 | loss: 0.3198989\n",
      "\tspeed: 0.0156s/iter; left time: 513.1508s\n",
      "\titers: 3700, epoch: 3 | loss: 0.5220202\n",
      "\tspeed: 0.0154s/iter; left time: 503.9596s\n",
      "\titers: 3800, epoch: 3 | loss: 0.4066803\n",
      "\tspeed: 0.0141s/iter; left time: 459.5398s\n",
      "\titers: 3900, epoch: 3 | loss: 0.2744858\n",
      "\tspeed: 0.0141s/iter; left time: 458.0981s\n",
      "\titers: 4000, epoch: 3 | loss: 0.3076485\n",
      "\tspeed: 0.0141s/iter; left time: 456.6258s\n",
      "\titers: 4100, epoch: 3 | loss: 0.4998716\n",
      "\tspeed: 0.0140s/iter; left time: 453.7121s\n",
      "\titers: 4200, epoch: 3 | loss: 0.2976494\n",
      "\tspeed: 0.0140s/iter; left time: 451.8182s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0392920\n",
      "\tspeed: 0.0140s/iter; left time: 450.8101s\n",
      "\titers: 4400, epoch: 3 | loss: 0.2962868\n",
      "\tspeed: 0.0140s/iter; left time: 449.6143s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0836275\n",
      "\tspeed: 0.0140s/iter; left time: 448.2610s\n",
      "Epoch: 3 cost time: 68.87158107757568\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.2922518 Vali Loss: 0.0583794 Test Loss: 0.1905015\n",
      "Validation loss decreased (0.060413 --> 0.058379).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2729012\n",
      "\tspeed: 0.1282s/iter; left time: 4075.4685s\n",
      "\titers: 200, epoch: 4 | loss: 0.4456950\n",
      "\tspeed: 0.0141s/iter; left time: 445.5639s\n",
      "\titers: 300, epoch: 4 | loss: 0.1154193\n",
      "\tspeed: 0.0140s/iter; left time: 443.0462s\n",
      "\titers: 400, epoch: 4 | loss: 0.2273879\n",
      "\tspeed: 0.0140s/iter; left time: 441.9415s\n",
      "\titers: 500, epoch: 4 | loss: 0.4214844\n",
      "\tspeed: 0.0140s/iter; left time: 440.4094s\n",
      "\titers: 600, epoch: 4 | loss: 0.1621453\n",
      "\tspeed: 0.0140s/iter; left time: 438.6468s\n",
      "\titers: 700, epoch: 4 | loss: 0.2436033\n",
      "\tspeed: 0.0140s/iter; left time: 437.3383s\n",
      "\titers: 800, epoch: 4 | loss: 0.3758518\n",
      "\tspeed: 0.0140s/iter; left time: 435.9481s\n",
      "\titers: 900, epoch: 4 | loss: 0.2742046\n",
      "\tspeed: 0.0140s/iter; left time: 434.4041s\n",
      "\titers: 1000, epoch: 4 | loss: 0.3283065\n",
      "\tspeed: 0.0140s/iter; left time: 433.0536s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1993921\n",
      "\tspeed: 0.0140s/iter; left time: 431.4598s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1423602\n",
      "\tspeed: 0.0140s/iter; left time: 430.0379s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1516986\n",
      "\tspeed: 0.0140s/iter; left time: 429.1823s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1469276\n",
      "\tspeed: 0.0140s/iter; left time: 427.7438s\n",
      "\titers: 1500, epoch: 4 | loss: 0.4927508\n",
      "\tspeed: 0.0140s/iter; left time: 426.2498s\n",
      "\titers: 1600, epoch: 4 | loss: 0.4981211\n",
      "\tspeed: 0.0140s/iter; left time: 424.7116s\n",
      "\titers: 1700, epoch: 4 | loss: 0.2866821\n",
      "\tspeed: 0.0140s/iter; left time: 423.1683s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1164652\n",
      "\tspeed: 0.0140s/iter; left time: 421.3988s\n",
      "\titers: 1900, epoch: 4 | loss: 0.5461766\n",
      "\tspeed: 0.0140s/iter; left time: 420.1632s\n",
      "\titers: 2000, epoch: 4 | loss: 0.3231722\n",
      "\tspeed: 0.0140s/iter; left time: 418.8870s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0252099\n",
      "\tspeed: 0.0140s/iter; left time: 417.6425s\n",
      "\titers: 2200, epoch: 4 | loss: 0.2896142\n",
      "\tspeed: 0.0140s/iter; left time: 416.0356s\n",
      "\titers: 2300, epoch: 4 | loss: 0.8444842\n",
      "\tspeed: 0.0140s/iter; left time: 414.7113s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1374569\n",
      "\tspeed: 0.0140s/iter; left time: 413.1853s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0992355\n",
      "\tspeed: 0.0140s/iter; left time: 411.9867s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0799621\n",
      "\tspeed: 0.0140s/iter; left time: 410.9584s\n",
      "\titers: 2700, epoch: 4 | loss: 1.6888560\n",
      "\tspeed: 0.0140s/iter; left time: 409.2217s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0500158\n",
      "\tspeed: 0.0140s/iter; left time: 407.5558s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0584435\n",
      "\tspeed: 0.0140s/iter; left time: 406.0545s\n",
      "\titers: 3000, epoch: 4 | loss: 0.3134277\n",
      "\tspeed: 0.0140s/iter; left time: 404.7216s\n",
      "\titers: 3100, epoch: 4 | loss: 0.4478836\n",
      "\tspeed: 0.0140s/iter; left time: 403.8894s\n",
      "\titers: 3200, epoch: 4 | loss: 0.8873140\n",
      "\tspeed: 0.0140s/iter; left time: 401.9093s\n",
      "\titers: 3300, epoch: 4 | loss: 0.4163284\n",
      "\tspeed: 0.0140s/iter; left time: 400.8362s\n",
      "\titers: 3400, epoch: 4 | loss: 0.2759792\n",
      "\tspeed: 0.0140s/iter; left time: 399.0670s\n",
      "\titers: 3500, epoch: 4 | loss: 0.1105232\n",
      "\tspeed: 0.0140s/iter; left time: 397.8954s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0593343\n",
      "\tspeed: 0.0140s/iter; left time: 396.2280s\n",
      "\titers: 3700, epoch: 4 | loss: 0.4375457\n",
      "\tspeed: 0.0140s/iter; left time: 394.9466s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0625646\n",
      "\tspeed: 0.0140s/iter; left time: 393.3984s\n",
      "\titers: 3900, epoch: 4 | loss: 0.4662163\n",
      "\tspeed: 0.0140s/iter; left time: 392.1456s\n",
      "\titers: 4000, epoch: 4 | loss: 0.4881860\n",
      "\tspeed: 0.0140s/iter; left time: 390.7132s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1531499\n",
      "\tspeed: 0.0140s/iter; left time: 389.2266s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1176822\n",
      "\tspeed: 0.0140s/iter; left time: 387.7125s\n",
      "\titers: 4300, epoch: 4 | loss: 0.5434667\n",
      "\tspeed: 0.0140s/iter; left time: 386.3993s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1342659\n",
      "\tspeed: 0.0140s/iter; left time: 385.0770s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1551849\n",
      "\tspeed: 0.0140s/iter; left time: 383.6285s\n",
      "Epoch: 4 cost time: 64.11759209632874\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.2809055 Vali Loss: 0.0585247 Test Loss: 0.1783701\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0880558\n",
      "\tspeed: 0.1289s/iter; left time: 3510.3128s\n",
      "\titers: 200, epoch: 5 | loss: 0.1564829\n",
      "\tspeed: 0.0140s/iter; left time: 380.2950s\n",
      "\titers: 300, epoch: 5 | loss: 0.1883882\n",
      "\tspeed: 0.0140s/iter; left time: 379.2472s\n",
      "\titers: 400, epoch: 5 | loss: 0.1214893\n",
      "\tspeed: 0.0140s/iter; left time: 377.7746s\n",
      "\titers: 500, epoch: 5 | loss: 0.4818010\n",
      "\tspeed: 0.0140s/iter; left time: 376.0898s\n",
      "\titers: 600, epoch: 5 | loss: 0.1428251\n",
      "\tspeed: 0.0140s/iter; left time: 374.6828s\n",
      "\titers: 700, epoch: 5 | loss: 0.0741752\n",
      "\tspeed: 0.0140s/iter; left time: 373.2845s\n",
      "\titers: 800, epoch: 5 | loss: 0.2535464\n",
      "\tspeed: 0.0140s/iter; left time: 371.9721s\n",
      "\titers: 900, epoch: 5 | loss: 0.2515979\n",
      "\tspeed: 0.0140s/iter; left time: 370.4083s\n",
      "\titers: 1000, epoch: 5 | loss: 0.2180068\n",
      "\tspeed: 0.0140s/iter; left time: 369.3183s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0931573\n",
      "\tspeed: 0.0140s/iter; left time: 367.9522s\n",
      "\titers: 1200, epoch: 5 | loss: 0.2679835\n",
      "\tspeed: 0.0140s/iter; left time: 366.3912s\n",
      "\titers: 1300, epoch: 5 | loss: 0.2437160\n",
      "\tspeed: 0.0140s/iter; left time: 364.9228s\n",
      "\titers: 1400, epoch: 5 | loss: 0.2215938\n",
      "\tspeed: 0.0140s/iter; left time: 363.2915s\n",
      "\titers: 1500, epoch: 5 | loss: 0.2196783\n",
      "\tspeed: 0.0140s/iter; left time: 361.9695s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0918548\n",
      "\tspeed: 0.0140s/iter; left time: 360.5793s\n",
      "\titers: 1700, epoch: 5 | loss: 0.5194184\n",
      "\tspeed: 0.0141s/iter; left time: 362.2511s\n",
      "\titers: 1800, epoch: 5 | loss: 0.3710390\n",
      "\tspeed: 0.0141s/iter; left time: 360.9850s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1075591\n",
      "\tspeed: 0.0141s/iter; left time: 359.7541s\n",
      "\titers: 2000, epoch: 5 | loss: 0.8165565\n",
      "\tspeed: 0.0141s/iter; left time: 358.1386s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1562864\n",
      "\tspeed: 0.0141s/iter; left time: 356.8186s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0831181\n",
      "\tspeed: 0.0141s/iter; left time: 355.3827s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0658718\n",
      "\tspeed: 0.0140s/iter; left time: 351.0426s\n",
      "\titers: 2400, epoch: 5 | loss: 0.2674426\n",
      "\tspeed: 0.0140s/iter; left time: 349.2997s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1842517\n",
      "\tspeed: 0.0140s/iter; left time: 347.8525s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1741490\n",
      "\tspeed: 0.0141s/iter; left time: 348.2811s\n",
      "\titers: 2700, epoch: 5 | loss: 0.4754460\n",
      "\tspeed: 0.0141s/iter; left time: 348.2473s\n",
      "\titers: 2800, epoch: 5 | loss: 0.4109983\n",
      "\tspeed: 0.0141s/iter; left time: 346.8067s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1381988\n",
      "\tspeed: 0.0141s/iter; left time: 345.2102s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0883127\n",
      "\tspeed: 0.0141s/iter; left time: 343.7549s\n",
      "\titers: 3100, epoch: 5 | loss: 0.2988701\n",
      "\tspeed: 0.0141s/iter; left time: 342.2347s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0323471\n",
      "\tspeed: 0.0141s/iter; left time: 340.8200s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1674488\n",
      "\tspeed: 0.0141s/iter; left time: 339.3739s\n",
      "\titers: 3400, epoch: 5 | loss: 0.3019461\n",
      "\tspeed: 0.0141s/iter; left time: 338.0451s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1387046\n",
      "\tspeed: 0.0141s/iter; left time: 336.5515s\n",
      "\titers: 3600, epoch: 5 | loss: 0.2240725\n",
      "\tspeed: 0.0141s/iter; left time: 335.3296s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0786002\n",
      "\tspeed: 0.0141s/iter; left time: 333.7727s\n",
      "\titers: 3800, epoch: 5 | loss: 0.3254410\n",
      "\tspeed: 0.0141s/iter; left time: 332.5565s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1082060\n",
      "\tspeed: 0.0141s/iter; left time: 331.0259s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1626577\n",
      "\tspeed: 0.0141s/iter; left time: 329.5309s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1228246\n",
      "\tspeed: 0.0141s/iter; left time: 328.0005s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1571139\n",
      "\tspeed: 0.0141s/iter; left time: 326.9805s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1287513\n",
      "\tspeed: 0.0141s/iter; left time: 325.4520s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0505132\n",
      "\tspeed: 0.0141s/iter; left time: 323.9285s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1846888\n",
      "\tspeed: 0.0141s/iter; left time: 322.5021s\n",
      "Epoch: 5 cost time: 64.39672327041626\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.2766972 Vali Loss: 0.0574169 Test Loss: 0.1737942\n",
      "Validation loss decreased (0.058379 --> 0.057417).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1264361\n",
      "\tspeed: 0.1271s/iter; left time: 2883.0237s\n",
      "\titers: 200, epoch: 6 | loss: 0.4390817\n",
      "\tspeed: 0.0141s/iter; left time: 317.3478s\n",
      "\titers: 300, epoch: 6 | loss: 0.1428680\n",
      "\tspeed: 0.0141s/iter; left time: 316.1729s\n",
      "\titers: 400, epoch: 6 | loss: 0.3339571\n",
      "\tspeed: 0.0141s/iter; left time: 314.8169s\n",
      "\titers: 500, epoch: 6 | loss: 0.2804878\n",
      "\tspeed: 0.0148s/iter; left time: 328.9195s\n",
      "\titers: 600, epoch: 6 | loss: 0.3322040\n",
      "\tspeed: 0.0155s/iter; left time: 343.6654s\n",
      "\titers: 700, epoch: 6 | loss: 0.0418491\n",
      "\tspeed: 0.0141s/iter; left time: 310.3598s\n",
      "\titers: 800, epoch: 6 | loss: 0.1265827\n",
      "\tspeed: 0.0141s/iter; left time: 308.9017s\n",
      "\titers: 900, epoch: 6 | loss: 0.2494720\n",
      "\tspeed: 0.0141s/iter; left time: 307.6111s\n",
      "\titers: 1000, epoch: 6 | loss: 0.4723036\n",
      "\tspeed: 0.0141s/iter; left time: 306.1604s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0392897\n",
      "\tspeed: 0.0141s/iter; left time: 304.6379s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1220175\n",
      "\tspeed: 0.0141s/iter; left time: 303.6872s\n",
      "\titers: 1300, epoch: 6 | loss: 0.2778322\n",
      "\tspeed: 0.0141s/iter; left time: 302.3460s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1438985\n",
      "\tspeed: 0.0141s/iter; left time: 300.7632s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1815650\n",
      "\tspeed: 0.0141s/iter; left time: 299.4548s\n",
      "\titers: 1600, epoch: 6 | loss: 0.4723527\n",
      "\tspeed: 0.0141s/iter; left time: 297.9638s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1308907\n",
      "\tspeed: 0.0141s/iter; left time: 296.6513s\n",
      "\titers: 1800, epoch: 6 | loss: 0.4399307\n",
      "\tspeed: 0.0141s/iter; left time: 295.1628s\n",
      "\titers: 1900, epoch: 6 | loss: 0.2598130\n",
      "\tspeed: 0.0141s/iter; left time: 293.8281s\n",
      "\titers: 2000, epoch: 6 | loss: 0.2356987\n",
      "\tspeed: 0.0141s/iter; left time: 292.3336s\n",
      "\titers: 2100, epoch: 6 | loss: 0.3787157\n",
      "\tspeed: 0.0141s/iter; left time: 290.8689s\n",
      "\titers: 2200, epoch: 6 | loss: 0.4537935\n",
      "\tspeed: 0.0141s/iter; left time: 289.5874s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0962219\n",
      "\tspeed: 0.0141s/iter; left time: 287.7341s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1166076\n",
      "\tspeed: 0.0140s/iter; left time: 286.1722s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1482508\n",
      "\tspeed: 0.0140s/iter; left time: 284.7405s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1283825\n",
      "\tspeed: 0.0140s/iter; left time: 283.3472s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1277082\n",
      "\tspeed: 0.0140s/iter; left time: 281.9884s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1688256\n",
      "\tspeed: 0.0140s/iter; left time: 280.5028s\n",
      "\titers: 2900, epoch: 6 | loss: 0.3336467\n",
      "\tspeed: 0.0140s/iter; left time: 279.1451s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0509669\n",
      "\tspeed: 0.0140s/iter; left time: 277.7857s\n",
      "\titers: 3100, epoch: 6 | loss: 0.3061397\n",
      "\tspeed: 0.0140s/iter; left time: 276.3801s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1552431\n",
      "\tspeed: 0.0140s/iter; left time: 274.9358s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1594056\n",
      "\tspeed: 0.0140s/iter; left time: 273.5957s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0332294\n",
      "\tspeed: 0.0141s/iter; left time: 272.2334s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0682224\n",
      "\tspeed: 0.0141s/iter; left time: 270.8555s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1567226\n",
      "\tspeed: 0.0140s/iter; left time: 269.3991s\n",
      "\titers: 3700, epoch: 6 | loss: 0.4239681\n",
      "\tspeed: 0.0140s/iter; left time: 267.9309s\n",
      "\titers: 3800, epoch: 6 | loss: 1.5150323\n",
      "\tspeed: 0.0140s/iter; left time: 266.4773s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0613331\n",
      "\tspeed: 0.0140s/iter; left time: 265.1604s\n",
      "\titers: 4000, epoch: 6 | loss: 0.2794779\n",
      "\tspeed: 0.0140s/iter; left time: 263.6959s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0407852\n",
      "\tspeed: 0.0140s/iter; left time: 262.3905s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1232646\n",
      "\tspeed: 0.0140s/iter; left time: 260.9388s\n",
      "\titers: 4300, epoch: 6 | loss: 0.4612697\n",
      "\tspeed: 0.0140s/iter; left time: 259.5117s\n",
      "\titers: 4400, epoch: 6 | loss: 0.2544086\n",
      "\tspeed: 0.0140s/iter; left time: 258.0764s\n",
      "\titers: 4500, epoch: 6 | loss: 0.3896407\n",
      "\tspeed: 0.0140s/iter; left time: 256.5925s\n",
      "Epoch: 6 cost time: 64.50711727142334\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.2725495 Vali Loss: 0.0562026 Test Loss: 0.1718835\n",
      "Validation loss decreased (0.057417 --> 0.056203).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2597155\n",
      "\tspeed: 0.1272s/iter; left time: 2305.8740s\n",
      "\titers: 200, epoch: 7 | loss: 0.4035389\n",
      "\tspeed: 0.0142s/iter; left time: 255.4840s\n",
      "\titers: 300, epoch: 7 | loss: 0.4153611\n",
      "\tspeed: 0.0155s/iter; left time: 278.2544s\n",
      "\titers: 400, epoch: 7 | loss: 0.1305397\n",
      "\tspeed: 0.0142s/iter; left time: 253.3078s\n",
      "\titers: 500, epoch: 7 | loss: 0.2310044\n",
      "\tspeed: 0.0141s/iter; left time: 250.1453s\n",
      "\titers: 600, epoch: 7 | loss: 0.1360722\n",
      "\tspeed: 0.0141s/iter; left time: 248.6312s\n",
      "\titers: 700, epoch: 7 | loss: 0.0521346\n",
      "\tspeed: 0.0141s/iter; left time: 247.3022s\n",
      "\titers: 800, epoch: 7 | loss: 0.1162401\n",
      "\tspeed: 0.0141s/iter; left time: 245.7516s\n",
      "\titers: 900, epoch: 7 | loss: 0.6136909\n",
      "\tspeed: 0.0141s/iter; left time: 244.4734s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0610773\n",
      "\tspeed: 0.0141s/iter; left time: 242.7592s\n",
      "\titers: 1100, epoch: 7 | loss: 0.5706466\n",
      "\tspeed: 0.0141s/iter; left time: 241.4059s\n",
      "\titers: 1200, epoch: 7 | loss: 0.2490954\n",
      "\tspeed: 0.0141s/iter; left time: 239.8671s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1780881\n",
      "\tspeed: 0.0141s/iter; left time: 238.4579s\n",
      "\titers: 1400, epoch: 7 | loss: 0.4136143\n",
      "\tspeed: 0.0141s/iter; left time: 237.1126s\n",
      "\titers: 1500, epoch: 7 | loss: 0.3179590\n",
      "\tspeed: 0.0146s/iter; left time: 243.9579s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1655527\n",
      "\tspeed: 0.0156s/iter; left time: 258.9224s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1035318\n",
      "\tspeed: 0.0156s/iter; left time: 257.4678s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1628979\n",
      "\tspeed: 0.0156s/iter; left time: 255.8137s\n",
      "\titers: 1900, epoch: 7 | loss: 0.1247913\n",
      "\tspeed: 0.0156s/iter; left time: 254.2524s\n",
      "\titers: 2000, epoch: 7 | loss: 0.1248756\n",
      "\tspeed: 0.0156s/iter; left time: 252.6564s\n",
      "\titers: 2100, epoch: 7 | loss: 0.3442615\n",
      "\tspeed: 0.0156s/iter; left time: 251.2344s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1477194\n",
      "\tspeed: 0.0156s/iter; left time: 249.5893s\n",
      "\titers: 2300, epoch: 7 | loss: 0.1314349\n",
      "\tspeed: 0.0156s/iter; left time: 247.9888s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1099561\n",
      "\tspeed: 0.0156s/iter; left time: 246.4768s\n",
      "\titers: 2500, epoch: 7 | loss: 0.3649368\n",
      "\tspeed: 0.0156s/iter; left time: 244.8621s\n",
      "\titers: 2600, epoch: 7 | loss: 0.2047831\n",
      "\tspeed: 0.0156s/iter; left time: 243.3504s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0917890\n",
      "\tspeed: 0.0156s/iter; left time: 241.8048s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1006172\n",
      "\tspeed: 0.0156s/iter; left time: 240.1919s\n",
      "\titers: 2900, epoch: 7 | loss: 0.2053970\n",
      "\tspeed: 0.0156s/iter; left time: 238.7385s\n",
      "\titers: 3000, epoch: 7 | loss: 0.2105331\n",
      "\tspeed: 0.0147s/iter; left time: 224.0274s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0319761\n",
      "\tspeed: 0.0141s/iter; left time: 213.1504s\n",
      "\titers: 3200, epoch: 7 | loss: 0.7622904\n",
      "\tspeed: 0.0141s/iter; left time: 211.7329s\n",
      "\titers: 3300, epoch: 7 | loss: 0.1443934\n",
      "\tspeed: 0.0141s/iter; left time: 210.3509s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1592827\n",
      "\tspeed: 0.0141s/iter; left time: 208.8957s\n",
      "\titers: 3500, epoch: 7 | loss: 0.1459255\n",
      "\tspeed: 0.0141s/iter; left time: 207.5442s\n",
      "\titers: 3600, epoch: 7 | loss: 0.2941830\n",
      "\tspeed: 0.0141s/iter; left time: 206.1169s\n",
      "\titers: 3700, epoch: 7 | loss: 0.7101361\n",
      "\tspeed: 0.0154s/iter; left time: 223.0429s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1610054\n",
      "\tspeed: 0.0162s/iter; left time: 233.6844s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0782712\n",
      "\tspeed: 0.0162s/iter; left time: 232.0258s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1910882\n",
      "\tspeed: 0.0162s/iter; left time: 230.2911s\n",
      "\titers: 4100, epoch: 7 | loss: 0.3296064\n",
      "\tspeed: 0.0162s/iter; left time: 228.6777s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0412606\n",
      "\tspeed: 0.0162s/iter; left time: 227.1095s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0348166\n",
      "\tspeed: 0.0162s/iter; left time: 225.3332s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0854207\n",
      "\tspeed: 0.0162s/iter; left time: 223.6426s\n",
      "\titers: 4500, epoch: 7 | loss: 0.3370064\n",
      "\tspeed: 0.0162s/iter; left time: 222.1822s\n",
      "Epoch: 7 cost time: 68.86754035949707\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.2705250 Vali Loss: 0.0569156 Test Loss: 0.1739820\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2226354\n",
      "\tspeed: 0.1272s/iter; left time: 1726.1355s\n",
      "\titers: 200, epoch: 8 | loss: 0.1433507\n",
      "\tspeed: 0.0161s/iter; left time: 216.6794s\n",
      "\titers: 300, epoch: 8 | loss: 0.3457256\n",
      "\tspeed: 0.0161s/iter; left time: 215.0592s\n",
      "\titers: 400, epoch: 8 | loss: 0.0883231\n",
      "\tspeed: 0.0161s/iter; left time: 213.2302s\n",
      "\titers: 500, epoch: 8 | loss: 0.4579684\n",
      "\tspeed: 0.0161s/iter; left time: 211.5654s\n",
      "\titers: 600, epoch: 8 | loss: 0.4579333\n",
      "\tspeed: 0.0161s/iter; left time: 210.0163s\n",
      "\titers: 700, epoch: 8 | loss: 0.5302798\n",
      "\tspeed: 0.0161s/iter; left time: 208.4743s\n",
      "\titers: 800, epoch: 8 | loss: 0.2285219\n",
      "\tspeed: 0.0161s/iter; left time: 206.9287s\n",
      "\titers: 900, epoch: 8 | loss: 0.1737982\n",
      "\tspeed: 0.0161s/iter; left time: 205.1775s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1349272\n",
      "\tspeed: 0.0161s/iter; left time: 203.7900s\n",
      "\titers: 1100, epoch: 8 | loss: 0.3057255\n",
      "\tspeed: 0.0161s/iter; left time: 201.9493s\n",
      "\titers: 1200, epoch: 8 | loss: 0.3201330\n",
      "\tspeed: 0.0161s/iter; left time: 200.2646s\n",
      "\titers: 1300, epoch: 8 | loss: 0.7482631\n",
      "\tspeed: 0.0161s/iter; left time: 198.9136s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0878849\n",
      "\tspeed: 0.0161s/iter; left time: 197.1367s\n",
      "\titers: 1500, epoch: 8 | loss: 0.1610191\n",
      "\tspeed: 0.0161s/iter; left time: 195.5102s\n",
      "\titers: 1600, epoch: 8 | loss: 0.2864799\n",
      "\tspeed: 0.0161s/iter; left time: 193.9217s\n",
      "\titers: 1700, epoch: 8 | loss: 0.2594196\n",
      "\tspeed: 0.0161s/iter; left time: 192.3125s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1954113\n",
      "\tspeed: 0.0161s/iter; left time: 190.6456s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1814294\n",
      "\tspeed: 0.0161s/iter; left time: 189.0905s\n",
      "\titers: 2000, epoch: 8 | loss: 0.2583822\n",
      "\tspeed: 0.0161s/iter; left time: 187.5783s\n",
      "\titers: 2100, epoch: 8 | loss: 0.1277341\n",
      "\tspeed: 0.0161s/iter; left time: 186.6546s\n",
      "\titers: 2200, epoch: 8 | loss: 0.2588015\n",
      "\tspeed: 0.0161s/iter; left time: 184.5788s\n",
      "\titers: 2300, epoch: 8 | loss: 0.3219819\n",
      "\tspeed: 0.0162s/iter; left time: 184.0036s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1653954\n",
      "\tspeed: 0.0162s/iter; left time: 182.3776s\n",
      "\titers: 2500, epoch: 8 | loss: 0.2250382\n",
      "\tspeed: 0.0162s/iter; left time: 180.7619s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0765266\n",
      "\tspeed: 0.0162s/iter; left time: 179.2378s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1875719\n",
      "\tspeed: 0.0148s/iter; left time: 162.5104s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0894967\n",
      "\tspeed: 0.0141s/iter; left time: 153.2279s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1565789\n",
      "\tspeed: 0.0141s/iter; left time: 151.7238s\n",
      "\titers: 3000, epoch: 8 | loss: 0.6181287\n",
      "\tspeed: 0.0141s/iter; left time: 150.3380s\n",
      "\titers: 3100, epoch: 8 | loss: 0.5194961\n",
      "\tspeed: 0.0141s/iter; left time: 148.9260s\n",
      "\titers: 3200, epoch: 8 | loss: 0.3870826\n",
      "\tspeed: 0.0141s/iter; left time: 147.5247s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0753879\n",
      "\tspeed: 0.0141s/iter; left time: 146.0054s\n",
      "\titers: 3400, epoch: 8 | loss: 0.1942541\n",
      "\tspeed: 0.0141s/iter; left time: 144.6530s\n",
      "\titers: 3500, epoch: 8 | loss: 0.2704040\n",
      "\tspeed: 0.0141s/iter; left time: 143.2790s\n",
      "\titers: 3600, epoch: 8 | loss: 0.1049828\n",
      "\tspeed: 0.0141s/iter; left time: 141.8055s\n",
      "\titers: 3700, epoch: 8 | loss: 0.6679686\n",
      "\tspeed: 0.0141s/iter; left time: 140.4523s\n",
      "\titers: 3800, epoch: 8 | loss: 0.5630763\n",
      "\tspeed: 0.0141s/iter; left time: 139.0584s\n",
      "\titers: 3900, epoch: 8 | loss: 0.2448678\n",
      "\tspeed: 0.0141s/iter; left time: 137.7125s\n",
      "\titers: 4000, epoch: 8 | loss: 0.1955845\n",
      "\tspeed: 0.0141s/iter; left time: 136.2444s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0790389\n",
      "\tspeed: 0.0141s/iter; left time: 134.7501s\n",
      "\titers: 4200, epoch: 8 | loss: 0.1819675\n",
      "\tspeed: 0.0141s/iter; left time: 133.3698s\n",
      "\titers: 4300, epoch: 8 | loss: 0.3872960\n",
      "\tspeed: 0.0141s/iter; left time: 132.0113s\n",
      "\titers: 4400, epoch: 8 | loss: 0.6940799\n",
      "\tspeed: 0.0141s/iter; left time: 130.7878s\n",
      "\titers: 4500, epoch: 8 | loss: 0.4792289\n",
      "\tspeed: 0.0141s/iter; left time: 129.4450s\n",
      "Epoch: 8 cost time: 69.74417543411255\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.2698705 Vali Loss: 0.0567117 Test Loss: 0.1720694\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2149780\n",
      "\tspeed: 0.1439s/iter; left time: 1296.8743s\n",
      "\titers: 200, epoch: 9 | loss: 0.3134566\n",
      "\tspeed: 0.0141s/iter; left time: 125.4074s\n",
      "\titers: 300, epoch: 9 | loss: 0.0733222\n",
      "\tspeed: 0.0141s/iter; left time: 123.9360s\n",
      "\titers: 400, epoch: 9 | loss: 0.4223291\n",
      "\tspeed: 0.0141s/iter; left time: 122.5739s\n",
      "\titers: 500, epoch: 9 | loss: 0.1817223\n",
      "\tspeed: 0.0141s/iter; left time: 121.1435s\n",
      "\titers: 600, epoch: 9 | loss: 0.0411910\n",
      "\tspeed: 0.0141s/iter; left time: 119.6934s\n",
      "\titers: 700, epoch: 9 | loss: 0.3276144\n",
      "\tspeed: 0.0141s/iter; left time: 118.3159s\n",
      "\titers: 800, epoch: 9 | loss: 0.1316721\n",
      "\tspeed: 0.0141s/iter; left time: 116.8953s\n",
      "\titers: 900, epoch: 9 | loss: 0.1507556\n",
      "\tspeed: 0.0141s/iter; left time: 115.6155s\n",
      "\titers: 1000, epoch: 9 | loss: 0.2900465\n",
      "\tspeed: 0.0141s/iter; left time: 114.1625s\n",
      "\titers: 1100, epoch: 9 | loss: 0.1964942\n",
      "\tspeed: 0.0141s/iter; left time: 112.7814s\n",
      "\titers: 1200, epoch: 9 | loss: 0.6812623\n",
      "\tspeed: 0.0141s/iter; left time: 111.2579s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0598678\n",
      "\tspeed: 0.0141s/iter; left time: 109.8474s\n",
      "\titers: 1400, epoch: 9 | loss: 0.4034106\n",
      "\tspeed: 0.0140s/iter; left time: 108.3088s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1524218\n",
      "\tspeed: 0.0140s/iter; left time: 106.9164s\n",
      "\titers: 1600, epoch: 9 | loss: 0.1794012\n",
      "\tspeed: 0.0141s/iter; left time: 105.6162s\n",
      "\titers: 1700, epoch: 9 | loss: 0.2042039\n",
      "\tspeed: 0.0140s/iter; left time: 104.0544s\n",
      "\titers: 1800, epoch: 9 | loss: 0.1265645\n",
      "\tspeed: 0.0141s/iter; left time: 102.8657s\n",
      "\titers: 1900, epoch: 9 | loss: 0.1149886\n",
      "\tspeed: 0.0141s/iter; left time: 101.3349s\n",
      "\titers: 2000, epoch: 9 | loss: 0.1141925\n",
      "\tspeed: 0.0140s/iter; left time: 99.9084s\n",
      "\titers: 2100, epoch: 9 | loss: 0.1024108\n",
      "\tspeed: 0.0140s/iter; left time: 98.3894s\n",
      "\titers: 2200, epoch: 9 | loss: 0.3285394\n",
      "\tspeed: 0.0140s/iter; left time: 97.0075s\n",
      "\titers: 2300, epoch: 9 | loss: 0.3725604\n",
      "\tspeed: 0.0140s/iter; left time: 95.6466s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0629377\n",
      "\tspeed: 0.0140s/iter; left time: 94.2811s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0599823\n",
      "\tspeed: 0.0141s/iter; left time: 92.9253s\n",
      "\titers: 2600, epoch: 9 | loss: 0.1969987\n",
      "\tspeed: 0.0141s/iter; left time: 91.4800s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0989079\n",
      "\tspeed: 0.0140s/iter; left time: 90.0523s\n",
      "\titers: 2800, epoch: 9 | loss: 0.5203624\n",
      "\tspeed: 0.0140s/iter; left time: 88.6292s\n",
      "\titers: 2900, epoch: 9 | loss: 0.1439077\n",
      "\tspeed: 0.0140s/iter; left time: 87.2265s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1519664\n",
      "\tspeed: 0.0140s/iter; left time: 85.8069s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0739309\n",
      "\tspeed: 0.0140s/iter; left time: 84.4044s\n",
      "\titers: 3200, epoch: 9 | loss: 0.1250259\n",
      "\tspeed: 0.0140s/iter; left time: 82.9989s\n",
      "\titers: 3300, epoch: 9 | loss: 0.5085780\n",
      "\tspeed: 0.0140s/iter; left time: 81.5889s\n",
      "\titers: 3400, epoch: 9 | loss: 0.4546180\n",
      "\tspeed: 0.0140s/iter; left time: 80.2047s\n",
      "\titers: 3500, epoch: 9 | loss: 0.1333515\n",
      "\tspeed: 0.0140s/iter; left time: 78.7903s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0908429\n",
      "\tspeed: 0.0140s/iter; left time: 77.3855s\n",
      "\titers: 3700, epoch: 9 | loss: 0.1454023\n",
      "\tspeed: 0.0140s/iter; left time: 75.9625s\n",
      "\titers: 3800, epoch: 9 | loss: 0.2695239\n",
      "\tspeed: 0.0140s/iter; left time: 74.5749s\n",
      "\titers: 3900, epoch: 9 | loss: 0.1161639\n",
      "\tspeed: 0.0140s/iter; left time: 73.1974s\n",
      "\titers: 4000, epoch: 9 | loss: 0.2113513\n",
      "\tspeed: 0.0140s/iter; left time: 71.7523s\n",
      "\titers: 4100, epoch: 9 | loss: 0.1571488\n",
      "\tspeed: 0.0140s/iter; left time: 70.3597s\n",
      "\titers: 4200, epoch: 9 | loss: 0.1373355\n",
      "\tspeed: 0.0140s/iter; left time: 68.9537s\n",
      "\titers: 4300, epoch: 9 | loss: 0.2623435\n",
      "\tspeed: 0.0140s/iter; left time: 67.5530s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0991141\n",
      "\tspeed: 0.0140s/iter; left time: 66.1263s\n",
      "\titers: 4500, epoch: 9 | loss: 0.3491871\n",
      "\tspeed: 0.0140s/iter; left time: 64.7123s\n",
      "Epoch: 9 cost time: 64.2527003288269\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.2691013 Vali Loss: 0.0566774 Test Loss: 0.1717267\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1718991994857788, mae:0.2624526917934418\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash scripts/TimeXer.sh \\\n",
    "     --features S \\\n",
    "     --enc_in 1  --dec_in 1  --c_out 1 \\\n",
    "     --dropout 0.2  --learning_rate 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f47f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       total_load          \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             2                   Dec In:             2                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3676846\n",
      "\tspeed: 0.0275s/iter; left time: 1250.1775s\n",
      "\titers: 200, epoch: 1 | loss: 0.1404903\n",
      "\tspeed: 0.0157s/iter; left time: 711.6371s\n",
      "\titers: 300, epoch: 1 | loss: 0.1099914\n",
      "\tspeed: 0.0157s/iter; left time: 709.0767s\n",
      "\titers: 400, epoch: 1 | loss: 0.3881031\n",
      "\tspeed: 0.0157s/iter; left time: 709.1499s\n",
      "\titers: 500, epoch: 1 | loss: 0.1226875\n",
      "\tspeed: 0.0157s/iter; left time: 708.7738s\n",
      "\titers: 600, epoch: 1 | loss: 0.2116724\n",
      "\tspeed: 0.0156s/iter; left time: 703.3775s\n",
      "\titers: 700, epoch: 1 | loss: 0.1869592\n",
      "\tspeed: 0.0157s/iter; left time: 703.0356s\n",
      "\titers: 800, epoch: 1 | loss: 1.3587711\n",
      "\tspeed: 0.0157s/iter; left time: 700.9815s\n",
      "\titers: 900, epoch: 1 | loss: 0.3849565\n",
      "\tspeed: 0.0157s/iter; left time: 699.6246s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0895703\n",
      "\tspeed: 0.0157s/iter; left time: 698.5463s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0578777\n",
      "\tspeed: 0.0156s/iter; left time: 694.5550s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1587559\n",
      "\tspeed: 0.0146s/iter; left time: 648.1649s\n",
      "\titers: 1300, epoch: 1 | loss: 0.3355158\n",
      "\tspeed: 0.0156s/iter; left time: 689.2730s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0505141\n",
      "\tspeed: 0.0156s/iter; left time: 688.1734s\n",
      "\titers: 1500, epoch: 1 | loss: 0.2300821\n",
      "\tspeed: 0.0156s/iter; left time: 686.8764s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2553958\n",
      "\tspeed: 0.0156s/iter; left time: 684.5337s\n",
      "\titers: 1700, epoch: 1 | loss: 0.5477462\n",
      "\tspeed: 0.0156s/iter; left time: 682.6086s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0991546\n",
      "\tspeed: 0.0156s/iter; left time: 681.0790s\n",
      "\titers: 1900, epoch: 1 | loss: 0.3114483\n",
      "\tspeed: 0.0142s/iter; left time: 618.9557s\n",
      "\titers: 2000, epoch: 1 | loss: 0.9888611\n",
      "\tspeed: 0.0141s/iter; left time: 613.0635s\n",
      "\titers: 2100, epoch: 1 | loss: 0.3054241\n",
      "\tspeed: 0.0141s/iter; left time: 611.5345s\n",
      "\titers: 2200, epoch: 1 | loss: 0.4572117\n",
      "\tspeed: 0.0141s/iter; left time: 610.2856s\n",
      "\titers: 2300, epoch: 1 | loss: 0.2485329\n",
      "\tspeed: 0.0142s/iter; left time: 614.4601s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1851395\n",
      "\tspeed: 0.0141s/iter; left time: 606.9438s\n",
      "\titers: 2500, epoch: 1 | loss: 0.3935739\n",
      "\tspeed: 0.0141s/iter; left time: 606.3373s\n",
      "\titers: 2600, epoch: 1 | loss: 0.0883165\n",
      "\tspeed: 0.0141s/iter; left time: 605.0005s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2877699\n",
      "\tspeed: 0.0141s/iter; left time: 602.9780s\n",
      "\titers: 2800, epoch: 1 | loss: 0.3498083\n",
      "\tspeed: 0.0141s/iter; left time: 601.3618s\n",
      "\titers: 2900, epoch: 1 | loss: 0.5320386\n",
      "\tspeed: 0.0141s/iter; left time: 600.2518s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1544494\n",
      "\tspeed: 0.0150s/iter; left time: 636.3016s\n",
      "\titers: 3100, epoch: 1 | loss: 0.3090791\n",
      "\tspeed: 0.0156s/iter; left time: 660.6368s\n",
      "\titers: 3200, epoch: 1 | loss: 0.3355292\n",
      "\tspeed: 0.0156s/iter; left time: 660.5719s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0774272\n",
      "\tspeed: 0.0142s/iter; left time: 599.3066s\n",
      "\titers: 3400, epoch: 1 | loss: 0.5450728\n",
      "\tspeed: 0.0141s/iter; left time: 594.5935s\n",
      "\titers: 3500, epoch: 1 | loss: 0.3889353\n",
      "\tspeed: 0.0141s/iter; left time: 591.5965s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1994203\n",
      "\tspeed: 0.0141s/iter; left time: 590.6905s\n",
      "\titers: 3700, epoch: 1 | loss: 0.2276378\n",
      "\tspeed: 0.0143s/iter; left time: 596.5711s\n",
      "\titers: 3800, epoch: 1 | loss: 0.3469604\n",
      "\tspeed: 0.0141s/iter; left time: 590.2741s\n",
      "\titers: 3900, epoch: 1 | loss: 2.0299549\n",
      "\tspeed: 0.0141s/iter; left time: 588.1615s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3227320\n",
      "\tspeed: 0.0141s/iter; left time: 586.6676s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1134193\n",
      "\tspeed: 0.0141s/iter; left time: 584.9405s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1181846\n",
      "\tspeed: 0.0141s/iter; left time: 583.1737s\n",
      "\titers: 4300, epoch: 1 | loss: 0.2587058\n",
      "\tspeed: 0.0141s/iter; left time: 581.9611s\n",
      "\titers: 4400, epoch: 1 | loss: 0.2112115\n",
      "\tspeed: 0.0141s/iter; left time: 580.6214s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1811521\n",
      "\tspeed: 0.0141s/iter; left time: 579.0862s\n",
      "Epoch: 1 cost time: 68.54184937477112\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.3363332 Vali Loss: 0.0605032 Test Loss: 0.1963029\n",
      "Validation loss decreased (inf --> 0.060503).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.0703401\n",
      "\tspeed: 0.1479s/iter; left time: 6048.2687s\n",
      "\titers: 200, epoch: 2 | loss: 0.0719624\n",
      "\tspeed: 0.0141s/iter; left time: 575.4429s\n",
      "\titers: 300, epoch: 2 | loss: 0.1432039\n",
      "\tspeed: 0.0141s/iter; left time: 572.7315s\n",
      "\titers: 400, epoch: 2 | loss: 0.2850066\n",
      "\tspeed: 0.0141s/iter; left time: 571.8003s\n",
      "\titers: 500, epoch: 2 | loss: 0.3138332\n",
      "\tspeed: 0.0141s/iter; left time: 569.9635s\n",
      "\titers: 600, epoch: 2 | loss: 0.2035883\n",
      "\tspeed: 0.0141s/iter; left time: 568.8045s\n",
      "\titers: 700, epoch: 2 | loss: 0.1200366\n",
      "\tspeed: 0.0141s/iter; left time: 567.4299s\n",
      "\titers: 800, epoch: 2 | loss: 0.1189715\n",
      "\tspeed: 0.0141s/iter; left time: 566.0129s\n",
      "\titers: 900, epoch: 2 | loss: 0.0614688\n",
      "\tspeed: 0.0141s/iter; left time: 564.6122s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1379998\n",
      "\tspeed: 0.0141s/iter; left time: 563.3284s\n",
      "\titers: 1100, epoch: 2 | loss: 0.7444700\n",
      "\tspeed: 0.0141s/iter; left time: 562.0331s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2368307\n",
      "\tspeed: 0.0141s/iter; left time: 560.0339s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1215949\n",
      "\tspeed: 0.0141s/iter; left time: 559.1018s\n",
      "\titers: 1400, epoch: 2 | loss: 0.3161659\n",
      "\tspeed: 0.0141s/iter; left time: 557.4399s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0500806\n",
      "\tspeed: 0.0141s/iter; left time: 556.3849s\n",
      "\titers: 1600, epoch: 2 | loss: 0.3242455\n",
      "\tspeed: 0.0141s/iter; left time: 554.6973s\n",
      "\titers: 1700, epoch: 2 | loss: 0.5232036\n",
      "\tspeed: 0.0141s/iter; left time: 554.1540s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0747865\n",
      "\tspeed: 0.0141s/iter; left time: 551.6784s\n",
      "\titers: 1900, epoch: 2 | loss: 0.2288692\n",
      "\tspeed: 0.0141s/iter; left time: 550.7713s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1825961\n",
      "\tspeed: 0.0141s/iter; left time: 549.7405s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1084340\n",
      "\tspeed: 0.0141s/iter; left time: 549.3332s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1624535\n",
      "\tspeed: 0.0141s/iter; left time: 547.1365s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1707280\n",
      "\tspeed: 0.0141s/iter; left time: 546.1727s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2653317\n",
      "\tspeed: 0.0141s/iter; left time: 544.0744s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1032632\n",
      "\tspeed: 0.0141s/iter; left time: 542.0216s\n",
      "\titers: 2600, epoch: 2 | loss: 0.5598326\n",
      "\tspeed: 0.0141s/iter; left time: 542.1376s\n",
      "\titers: 2700, epoch: 2 | loss: 0.2206382\n",
      "\tspeed: 0.0141s/iter; left time: 540.8462s\n",
      "\titers: 2800, epoch: 2 | loss: 0.2264004\n",
      "\tspeed: 0.0141s/iter; left time: 539.1164s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1942445\n",
      "\tspeed: 0.0141s/iter; left time: 537.7457s\n",
      "\titers: 3000, epoch: 2 | loss: 0.3117536\n",
      "\tspeed: 0.0141s/iter; left time: 536.1500s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1244244\n",
      "\tspeed: 0.0141s/iter; left time: 534.7379s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2794003\n",
      "\tspeed: 0.0141s/iter; left time: 533.3381s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0841874\n",
      "\tspeed: 0.0141s/iter; left time: 532.3345s\n",
      "\titers: 3400, epoch: 2 | loss: 0.2709217\n",
      "\tspeed: 0.0141s/iter; left time: 530.9507s\n",
      "\titers: 3500, epoch: 2 | loss: 0.4260456\n",
      "\tspeed: 0.0141s/iter; left time: 529.2804s\n",
      "\titers: 3600, epoch: 2 | loss: 0.3075401\n",
      "\tspeed: 0.0141s/iter; left time: 528.0347s\n",
      "\titers: 3700, epoch: 2 | loss: 0.7304548\n",
      "\tspeed: 0.0141s/iter; left time: 526.9669s\n",
      "\titers: 3800, epoch: 2 | loss: 0.2644292\n",
      "\tspeed: 0.0141s/iter; left time: 525.4961s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1523101\n",
      "\tspeed: 0.0141s/iter; left time: 524.6585s\n",
      "\titers: 4000, epoch: 2 | loss: 0.4212762\n",
      "\tspeed: 0.0141s/iter; left time: 522.6182s\n",
      "\titers: 4100, epoch: 2 | loss: 0.4454587\n",
      "\tspeed: 0.0141s/iter; left time: 521.1258s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1539889\n",
      "\tspeed: 0.0141s/iter; left time: 519.7229s\n",
      "\titers: 4300, epoch: 2 | loss: 0.2281446\n",
      "\tspeed: 0.0141s/iter; left time: 518.1589s\n",
      "\titers: 4400, epoch: 2 | loss: 0.2114647\n",
      "\tspeed: 0.0141s/iter; left time: 517.4066s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1168795\n",
      "\tspeed: 0.0142s/iter; left time: 516.4811s\n",
      "Epoch: 2 cost time: 64.49871516227722\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.3097140 Vali Loss: 0.0603845 Test Loss: 0.1869950\n",
      "Validation loss decreased (0.060503 --> 0.060384).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4115865\n",
      "\tspeed: 0.1278s/iter; left time: 4643.9036s\n",
      "\titers: 200, epoch: 3 | loss: 0.0814113\n",
      "\tspeed: 0.0141s/iter; left time: 512.1507s\n",
      "\titers: 300, epoch: 3 | loss: 0.3761446\n",
      "\tspeed: 0.0141s/iter; left time: 510.2561s\n",
      "\titers: 400, epoch: 3 | loss: 0.1954257\n",
      "\tspeed: 0.0141s/iter; left time: 509.0990s\n",
      "\titers: 500, epoch: 3 | loss: 0.2836287\n",
      "\tspeed: 0.0141s/iter; left time: 507.6248s\n",
      "\titers: 600, epoch: 3 | loss: 0.2097838\n",
      "\tspeed: 0.0141s/iter; left time: 506.3359s\n",
      "\titers: 700, epoch: 3 | loss: 0.9691353\n",
      "\tspeed: 0.0141s/iter; left time: 505.1484s\n",
      "\titers: 800, epoch: 3 | loss: 0.2635390\n",
      "\tspeed: 0.0141s/iter; left time: 504.1681s\n",
      "\titers: 900, epoch: 3 | loss: 0.1553553\n",
      "\tspeed: 0.0141s/iter; left time: 502.2648s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3570809\n",
      "\tspeed: 0.0141s/iter; left time: 501.0947s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1609818\n",
      "\tspeed: 0.0141s/iter; left time: 499.2362s\n",
      "\titers: 1200, epoch: 3 | loss: 0.3075131\n",
      "\tspeed: 0.0141s/iter; left time: 498.5286s\n",
      "\titers: 1300, epoch: 3 | loss: 0.5821737\n",
      "\tspeed: 0.0141s/iter; left time: 496.6947s\n",
      "\titers: 1400, epoch: 3 | loss: 0.2664058\n",
      "\tspeed: 0.0141s/iter; left time: 494.9873s\n",
      "\titers: 1500, epoch: 3 | loss: 0.3486721\n",
      "\tspeed: 0.0141s/iter; left time: 493.5154s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1894915\n",
      "\tspeed: 0.0141s/iter; left time: 492.3035s\n",
      "\titers: 1700, epoch: 3 | loss: 0.9328072\n",
      "\tspeed: 0.0141s/iter; left time: 490.7906s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1077970\n",
      "\tspeed: 0.0141s/iter; left time: 489.0775s\n",
      "\titers: 1900, epoch: 3 | loss: 0.6193236\n",
      "\tspeed: 0.0141s/iter; left time: 487.5664s\n",
      "\titers: 2000, epoch: 3 | loss: 1.4425159\n",
      "\tspeed: 0.0141s/iter; left time: 486.2094s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1318075\n",
      "\tspeed: 0.0141s/iter; left time: 483.5225s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1049105\n",
      "\tspeed: 0.0141s/iter; left time: 481.5991s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1753344\n",
      "\tspeed: 0.0141s/iter; left time: 480.3662s\n",
      "\titers: 2400, epoch: 3 | loss: 0.2540891\n",
      "\tspeed: 0.0141s/iter; left time: 478.7432s\n",
      "\titers: 2500, epoch: 3 | loss: 0.2715454\n",
      "\tspeed: 0.0141s/iter; left time: 477.2309s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1086555\n",
      "\tspeed: 0.0141s/iter; left time: 475.8633s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0918617\n",
      "\tspeed: 0.0141s/iter; left time: 474.5579s\n",
      "\titers: 2800, epoch: 3 | loss: 0.2762808\n",
      "\tspeed: 0.0141s/iter; left time: 473.3828s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1581229\n",
      "\tspeed: 0.0141s/iter; left time: 471.8132s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0475110\n",
      "\tspeed: 0.0141s/iter; left time: 470.4070s\n",
      "\titers: 3100, epoch: 3 | loss: 0.3732664\n",
      "\tspeed: 0.0141s/iter; left time: 469.0589s\n",
      "\titers: 3200, epoch: 3 | loss: 0.5359902\n",
      "\tspeed: 0.0141s/iter; left time: 468.4659s\n",
      "\titers: 3300, epoch: 3 | loss: 0.2162604\n",
      "\tspeed: 0.0141s/iter; left time: 466.7590s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2764487\n",
      "\tspeed: 0.0141s/iter; left time: 465.7214s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1871134\n",
      "\tspeed: 0.0141s/iter; left time: 464.0708s\n",
      "\titers: 3600, epoch: 3 | loss: 0.2902356\n",
      "\tspeed: 0.0141s/iter; left time: 462.7532s\n",
      "\titers: 3700, epoch: 3 | loss: 0.6125311\n",
      "\tspeed: 0.0141s/iter; left time: 461.2407s\n",
      "\titers: 3800, epoch: 3 | loss: 0.1796664\n",
      "\tspeed: 0.0141s/iter; left time: 459.4805s\n",
      "\titers: 3900, epoch: 3 | loss: 0.3417931\n",
      "\tspeed: 0.0141s/iter; left time: 458.0895s\n",
      "\titers: 4000, epoch: 3 | loss: 0.3497542\n",
      "\tspeed: 0.0141s/iter; left time: 457.1042s\n",
      "\titers: 4100, epoch: 3 | loss: 0.5751470\n",
      "\tspeed: 0.0141s/iter; left time: 455.4585s\n",
      "\titers: 4200, epoch: 3 | loss: 0.2062784\n",
      "\tspeed: 0.0141s/iter; left time: 454.0830s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0636833\n",
      "\tspeed: 0.0141s/iter; left time: 452.9571s\n",
      "\titers: 4400, epoch: 3 | loss: 0.2423922\n",
      "\tspeed: 0.0141s/iter; left time: 453.3390s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1228503\n",
      "\tspeed: 0.0141s/iter; left time: 451.7202s\n",
      "Epoch: 3 cost time: 64.48862862586975\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.2944145 Vali Loss: 0.0563647 Test Loss: 0.1877061\n",
      "Validation loss decreased (0.060384 --> 0.056365).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2427242\n",
      "\tspeed: 0.1485s/iter; left time: 4718.9261s\n",
      "\titers: 200, epoch: 4 | loss: 0.4625361\n",
      "\tspeed: 0.0141s/iter; left time: 447.0445s\n",
      "\titers: 300, epoch: 4 | loss: 0.1247056\n",
      "\tspeed: 0.0141s/iter; left time: 444.8204s\n",
      "\titers: 400, epoch: 4 | loss: 0.2299508\n",
      "\tspeed: 0.0141s/iter; left time: 444.0913s\n",
      "\titers: 500, epoch: 4 | loss: 0.4212248\n",
      "\tspeed: 0.0141s/iter; left time: 442.3903s\n",
      "\titers: 600, epoch: 4 | loss: 0.2159409\n",
      "\tspeed: 0.0154s/iter; left time: 480.6332s\n",
      "\titers: 700, epoch: 4 | loss: 0.2738367\n",
      "\tspeed: 0.0157s/iter; left time: 488.3061s\n",
      "\titers: 800, epoch: 4 | loss: 0.3965350\n",
      "\tspeed: 0.0157s/iter; left time: 486.9183s\n",
      "\titers: 900, epoch: 4 | loss: 0.2799758\n",
      "\tspeed: 0.0157s/iter; left time: 485.1436s\n",
      "\titers: 1000, epoch: 4 | loss: 0.4198048\n",
      "\tspeed: 0.0156s/iter; left time: 483.1825s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1838372\n",
      "\tspeed: 0.0154s/iter; left time: 473.5339s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1133237\n",
      "\tspeed: 0.0141s/iter; left time: 432.5366s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1825278\n",
      "\tspeed: 0.0141s/iter; left time: 431.1197s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1717496\n",
      "\tspeed: 0.0141s/iter; left time: 429.5178s\n",
      "\titers: 1500, epoch: 4 | loss: 0.4716010\n",
      "\tspeed: 0.0141s/iter; left time: 428.1056s\n",
      "\titers: 1600, epoch: 4 | loss: 0.2143224\n",
      "\tspeed: 0.0141s/iter; left time: 426.6818s\n",
      "\titers: 1700, epoch: 4 | loss: 0.2933012\n",
      "\tspeed: 0.0141s/iter; left time: 425.4709s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1210226\n",
      "\tspeed: 0.0141s/iter; left time: 423.9939s\n",
      "\titers: 1900, epoch: 4 | loss: 0.5580448\n",
      "\tspeed: 0.0141s/iter; left time: 422.5418s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1911645\n",
      "\tspeed: 0.0141s/iter; left time: 421.1717s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0334535\n",
      "\tspeed: 0.0141s/iter; left time: 419.7960s\n",
      "\titers: 2200, epoch: 4 | loss: 0.3016357\n",
      "\tspeed: 0.0141s/iter; left time: 418.7915s\n",
      "\titers: 2300, epoch: 4 | loss: 0.7272102\n",
      "\tspeed: 0.0141s/iter; left time: 416.9516s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1918381\n",
      "\tspeed: 0.0141s/iter; left time: 416.4947s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0648724\n",
      "\tspeed: 0.0141s/iter; left time: 414.4455s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0477637\n",
      "\tspeed: 0.0141s/iter; left time: 412.9039s\n",
      "\titers: 2700, epoch: 4 | loss: 1.6807092\n",
      "\tspeed: 0.0141s/iter; left time: 411.3537s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0432437\n",
      "\tspeed: 0.0141s/iter; left time: 409.8507s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0427540\n",
      "\tspeed: 0.0141s/iter; left time: 408.6029s\n",
      "\titers: 3000, epoch: 4 | loss: 0.3510906\n",
      "\tspeed: 0.0141s/iter; left time: 407.3140s\n",
      "\titers: 3100, epoch: 4 | loss: 0.3916327\n",
      "\tspeed: 0.0141s/iter; left time: 405.7495s\n",
      "\titers: 3200, epoch: 4 | loss: 0.8161072\n",
      "\tspeed: 0.0141s/iter; left time: 403.9771s\n",
      "\titers: 3300, epoch: 4 | loss: 0.3166616\n",
      "\tspeed: 0.0141s/iter; left time: 402.5087s\n",
      "\titers: 3400, epoch: 4 | loss: 0.2635221\n",
      "\tspeed: 0.0141s/iter; left time: 401.0369s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0856357\n",
      "\tspeed: 0.0141s/iter; left time: 399.7526s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0603554\n",
      "\tspeed: 0.0141s/iter; left time: 398.4258s\n",
      "\titers: 3700, epoch: 4 | loss: 0.4011421\n",
      "\tspeed: 0.0141s/iter; left time: 396.7976s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0730614\n",
      "\tspeed: 0.0141s/iter; left time: 395.3443s\n",
      "\titers: 3900, epoch: 4 | loss: 0.3385572\n",
      "\tspeed: 0.0141s/iter; left time: 393.9774s\n",
      "\titers: 4000, epoch: 4 | loss: 0.3546160\n",
      "\tspeed: 0.0141s/iter; left time: 392.5857s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1273789\n",
      "\tspeed: 0.0141s/iter; left time: 391.2332s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1382279\n",
      "\tspeed: 0.0141s/iter; left time: 390.0787s\n",
      "\titers: 4300, epoch: 4 | loss: 0.5704690\n",
      "\tspeed: 0.0141s/iter; left time: 388.7123s\n",
      "\titers: 4400, epoch: 4 | loss: 0.2700304\n",
      "\tspeed: 0.0141s/iter; left time: 387.1899s\n",
      "\titers: 4500, epoch: 4 | loss: 0.2597667\n",
      "\tspeed: 0.0150s/iter; left time: 411.3526s\n",
      "Epoch: 4 cost time: 65.57820391654968\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.2866033 Vali Loss: 0.0562889 Test Loss: 0.1789689\n",
      "Validation loss decreased (0.056365 --> 0.056289).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1005845\n",
      "\tspeed: 0.1373s/iter; left time: 3740.1169s\n",
      "\titers: 200, epoch: 5 | loss: 0.2286910\n",
      "\tspeed: 0.0141s/iter; left time: 382.4894s\n",
      "\titers: 300, epoch: 5 | loss: 0.1972680\n",
      "\tspeed: 0.0141s/iter; left time: 380.8308s\n",
      "\titers: 400, epoch: 5 | loss: 0.0976106\n",
      "\tspeed: 0.0141s/iter; left time: 378.7220s\n",
      "\titers: 500, epoch: 5 | loss: 0.5567783\n",
      "\tspeed: 0.0141s/iter; left time: 377.6868s\n",
      "\titers: 600, epoch: 5 | loss: 0.1326059\n",
      "\tspeed: 0.0141s/iter; left time: 376.1717s\n",
      "\titers: 700, epoch: 5 | loss: 0.0652153\n",
      "\tspeed: 0.0141s/iter; left time: 374.8718s\n",
      "\titers: 800, epoch: 5 | loss: 0.2721089\n",
      "\tspeed: 0.0141s/iter; left time: 373.3128s\n",
      "\titers: 900, epoch: 5 | loss: 0.2310668\n",
      "\tspeed: 0.0141s/iter; left time: 371.6278s\n",
      "\titers: 1000, epoch: 5 | loss: 0.2374832\n",
      "\tspeed: 0.0141s/iter; left time: 370.0348s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1172336\n",
      "\tspeed: 0.0141s/iter; left time: 368.6537s\n",
      "\titers: 1200, epoch: 5 | loss: 0.3154186\n",
      "\tspeed: 0.0141s/iter; left time: 367.4788s\n",
      "\titers: 1300, epoch: 5 | loss: 0.2209245\n",
      "\tspeed: 0.0141s/iter; left time: 366.3164s\n",
      "\titers: 1400, epoch: 5 | loss: 0.2211376\n",
      "\tspeed: 0.0141s/iter; left time: 364.5915s\n",
      "\titers: 1500, epoch: 5 | loss: 0.2693039\n",
      "\tspeed: 0.0141s/iter; left time: 363.0701s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1001311\n",
      "\tspeed: 0.0141s/iter; left time: 361.5719s\n",
      "\titers: 1700, epoch: 5 | loss: 0.6241095\n",
      "\tspeed: 0.0141s/iter; left time: 360.1558s\n",
      "\titers: 1800, epoch: 5 | loss: 0.3146918\n",
      "\tspeed: 0.0141s/iter; left time: 358.9337s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0991713\n",
      "\tspeed: 0.0141s/iter; left time: 357.8931s\n",
      "\titers: 2000, epoch: 5 | loss: 0.8668392\n",
      "\tspeed: 0.0140s/iter; left time: 355.8531s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1528670\n",
      "\tspeed: 0.0141s/iter; left time: 354.8013s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0913932\n",
      "\tspeed: 0.0140s/iter; left time: 353.0445s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0730040\n",
      "\tspeed: 0.0141s/iter; left time: 351.8894s\n",
      "\titers: 2400, epoch: 5 | loss: 0.2381098\n",
      "\tspeed: 0.0141s/iter; left time: 350.2947s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1593702\n",
      "\tspeed: 0.0141s/iter; left time: 349.0852s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1722798\n",
      "\tspeed: 0.0141s/iter; left time: 347.6129s\n",
      "\titers: 2700, epoch: 5 | loss: 0.4596456\n",
      "\tspeed: 0.0141s/iter; left time: 346.0965s\n",
      "\titers: 2800, epoch: 5 | loss: 0.3393207\n",
      "\tspeed: 0.0140s/iter; left time: 344.5336s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1561917\n",
      "\tspeed: 0.0140s/iter; left time: 343.0604s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1300622\n",
      "\tspeed: 0.0140s/iter; left time: 341.7856s\n",
      "\titers: 3100, epoch: 5 | loss: 0.2378967\n",
      "\tspeed: 0.0141s/iter; left time: 340.5503s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0412902\n",
      "\tspeed: 0.0141s/iter; left time: 339.1075s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1468263\n",
      "\tspeed: 0.0141s/iter; left time: 337.6379s\n",
      "\titers: 3400, epoch: 5 | loss: 0.2968962\n",
      "\tspeed: 0.0141s/iter; left time: 336.4100s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1159941\n",
      "\tspeed: 0.0141s/iter; left time: 335.6630s\n",
      "\titers: 3600, epoch: 5 | loss: 0.2402422\n",
      "\tspeed: 0.0141s/iter; left time: 334.2555s\n",
      "\titers: 3700, epoch: 5 | loss: 0.1017670\n",
      "\tspeed: 0.0141s/iter; left time: 332.8265s\n",
      "\titers: 3800, epoch: 5 | loss: 0.2696635\n",
      "\tspeed: 0.0141s/iter; left time: 331.5418s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1646564\n",
      "\tspeed: 0.0141s/iter; left time: 330.1299s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1597243\n",
      "\tspeed: 0.0141s/iter; left time: 328.5554s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1227129\n",
      "\tspeed: 0.0141s/iter; left time: 327.1725s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1905816\n",
      "\tspeed: 0.0141s/iter; left time: 325.5655s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1163650\n",
      "\tspeed: 0.0141s/iter; left time: 324.4008s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0398296\n",
      "\tspeed: 0.0141s/iter; left time: 322.8400s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1986293\n",
      "\tspeed: 0.0141s/iter; left time: 321.4524s\n",
      "Epoch: 5 cost time: 64.31092691421509\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.2802289 Vali Loss: 0.0552886 Test Loss: 0.1762019\n",
      "Validation loss decreased (0.056289 --> 0.055289).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1534819\n",
      "\tspeed: 0.1251s/iter; left time: 2837.5706s\n",
      "\titers: 200, epoch: 6 | loss: 0.3683384\n",
      "\tspeed: 0.0141s/iter; left time: 317.7265s\n",
      "\titers: 300, epoch: 6 | loss: 0.1452729\n",
      "\tspeed: 0.0141s/iter; left time: 316.6479s\n",
      "\titers: 400, epoch: 6 | loss: 0.3239608\n",
      "\tspeed: 0.0141s/iter; left time: 314.8175s\n",
      "\titers: 500, epoch: 6 | loss: 0.2891684\n",
      "\tspeed: 0.0141s/iter; left time: 313.0922s\n",
      "\titers: 600, epoch: 6 | loss: 0.3631411\n",
      "\tspeed: 0.0141s/iter; left time: 311.7065s\n",
      "\titers: 700, epoch: 6 | loss: 0.0430496\n",
      "\tspeed: 0.0141s/iter; left time: 310.2593s\n",
      "\titers: 800, epoch: 6 | loss: 0.1240842\n",
      "\tspeed: 0.0141s/iter; left time: 308.9334s\n",
      "\titers: 900, epoch: 6 | loss: 0.1196532\n",
      "\tspeed: 0.0141s/iter; left time: 307.4202s\n",
      "\titers: 1000, epoch: 6 | loss: 0.5040914\n",
      "\tspeed: 0.0141s/iter; left time: 306.3206s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0468999\n",
      "\tspeed: 0.0141s/iter; left time: 304.6802s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1911769\n",
      "\tspeed: 0.0141s/iter; left time: 303.1913s\n",
      "\titers: 1300, epoch: 6 | loss: 0.2718339\n",
      "\tspeed: 0.0141s/iter; left time: 301.8715s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1585297\n",
      "\tspeed: 0.0141s/iter; left time: 300.5840s\n",
      "\titers: 1500, epoch: 6 | loss: 0.2475505\n",
      "\tspeed: 0.0141s/iter; left time: 299.0579s\n",
      "\titers: 1600, epoch: 6 | loss: 0.4980441\n",
      "\tspeed: 0.0141s/iter; left time: 297.6925s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1223587\n",
      "\tspeed: 0.0141s/iter; left time: 296.3790s\n",
      "\titers: 1800, epoch: 6 | loss: 0.5194589\n",
      "\tspeed: 0.0141s/iter; left time: 294.7334s\n",
      "\titers: 1900, epoch: 6 | loss: 0.3203695\n",
      "\tspeed: 0.0141s/iter; left time: 293.3759s\n",
      "\titers: 2000, epoch: 6 | loss: 0.2567331\n",
      "\tspeed: 0.0141s/iter; left time: 292.2559s\n",
      "\titers: 2100, epoch: 6 | loss: 0.3491486\n",
      "\tspeed: 0.0141s/iter; left time: 290.5227s\n",
      "\titers: 2200, epoch: 6 | loss: 0.6961545\n",
      "\tspeed: 0.0141s/iter; left time: 289.4332s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1082973\n",
      "\tspeed: 0.0141s/iter; left time: 287.9016s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1650442\n",
      "\tspeed: 0.0141s/iter; left time: 287.3659s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1609367\n",
      "\tspeed: 0.0141s/iter; left time: 285.6610s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1076267\n",
      "\tspeed: 0.0141s/iter; left time: 284.2118s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1233291\n",
      "\tspeed: 0.0141s/iter; left time: 283.0234s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1958387\n",
      "\tspeed: 0.0141s/iter; left time: 281.5114s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2795580\n",
      "\tspeed: 0.0141s/iter; left time: 280.1119s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0609355\n",
      "\tspeed: 0.0141s/iter; left time: 278.8153s\n",
      "\titers: 3100, epoch: 6 | loss: 0.2955863\n",
      "\tspeed: 0.0141s/iter; left time: 277.2533s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1616287\n",
      "\tspeed: 0.0141s/iter; left time: 275.9914s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1461491\n",
      "\tspeed: 0.0141s/iter; left time: 274.4080s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0496004\n",
      "\tspeed: 0.0141s/iter; left time: 273.0142s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0494106\n",
      "\tspeed: 0.0141s/iter; left time: 271.8388s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1558506\n",
      "\tspeed: 0.0141s/iter; left time: 270.0163s\n",
      "\titers: 3700, epoch: 6 | loss: 0.3339466\n",
      "\tspeed: 0.0141s/iter; left time: 268.9777s\n",
      "\titers: 3800, epoch: 6 | loss: 0.9648066\n",
      "\tspeed: 0.0141s/iter; left time: 267.5146s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0797880\n",
      "\tspeed: 0.0141s/iter; left time: 266.2293s\n",
      "\titers: 4000, epoch: 6 | loss: 0.3359041\n",
      "\tspeed: 0.0141s/iter; left time: 264.5277s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0470338\n",
      "\tspeed: 0.0141s/iter; left time: 264.2447s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1649871\n",
      "\tspeed: 0.0141s/iter; left time: 262.6108s\n",
      "\titers: 4300, epoch: 6 | loss: 0.7638242\n",
      "\tspeed: 0.0141s/iter; left time: 261.2146s\n",
      "\titers: 4400, epoch: 6 | loss: 0.2437174\n",
      "\tspeed: 0.0141s/iter; left time: 259.7697s\n",
      "\titers: 4500, epoch: 6 | loss: 0.3743777\n",
      "\tspeed: 0.0141s/iter; left time: 258.3345s\n",
      "Epoch: 6 cost time: 64.39379262924194\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.2776745 Vali Loss: 0.0537211 Test Loss: 0.1756156\n",
      "Validation loss decreased (0.055289 --> 0.053721).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2719644\n",
      "\tspeed: 0.1281s/iter; left time: 2320.5950s\n",
      "\titers: 200, epoch: 7 | loss: 0.4120745\n",
      "\tspeed: 0.0157s/iter; left time: 282.4810s\n",
      "\titers: 300, epoch: 7 | loss: 0.4221486\n",
      "\tspeed: 0.0156s/iter; left time: 280.3841s\n",
      "\titers: 400, epoch: 7 | loss: 0.1495334\n",
      "\tspeed: 0.0156s/iter; left time: 278.7770s\n",
      "\titers: 500, epoch: 7 | loss: 0.1950033\n",
      "\tspeed: 0.0156s/iter; left time: 277.2502s\n",
      "\titers: 600, epoch: 7 | loss: 0.1052044\n",
      "\tspeed: 0.0157s/iter; left time: 275.9749s\n",
      "\titers: 700, epoch: 7 | loss: 0.0411526\n",
      "\tspeed: 0.0157s/iter; left time: 274.5869s\n",
      "\titers: 800, epoch: 7 | loss: 0.1163772\n",
      "\tspeed: 0.0156s/iter; left time: 272.5317s\n",
      "\titers: 900, epoch: 7 | loss: 0.6085428\n",
      "\tspeed: 0.0157s/iter; left time: 271.5180s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0614651\n",
      "\tspeed: 0.0157s/iter; left time: 269.7013s\n",
      "\titers: 1100, epoch: 7 | loss: 0.7244350\n",
      "\tspeed: 0.0156s/iter; left time: 267.8108s\n",
      "\titers: 1200, epoch: 7 | loss: 0.2495125\n",
      "\tspeed: 0.0156s/iter; left time: 266.3386s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1780154\n",
      "\tspeed: 0.0153s/iter; left time: 258.8735s\n",
      "\titers: 1400, epoch: 7 | loss: 0.3403092\n",
      "\tspeed: 0.0144s/iter; left time: 241.6808s\n",
      "\titers: 1500, epoch: 7 | loss: 0.3966939\n",
      "\tspeed: 0.0156s/iter; left time: 261.5678s\n",
      "\titers: 1600, epoch: 7 | loss: 0.2713275\n",
      "\tspeed: 0.0156s/iter; left time: 259.9418s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1116609\n",
      "\tspeed: 0.0156s/iter; left time: 258.3994s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1621218\n",
      "\tspeed: 0.0156s/iter; left time: 256.8418s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0825078\n",
      "\tspeed: 0.0156s/iter; left time: 255.2124s\n",
      "\titers: 2000, epoch: 7 | loss: 0.2024559\n",
      "\tspeed: 0.0156s/iter; left time: 253.7481s\n",
      "\titers: 2100, epoch: 7 | loss: 0.3231623\n",
      "\tspeed: 0.0150s/iter; left time: 242.0950s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1084157\n",
      "\tspeed: 0.0151s/iter; left time: 241.3616s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0871870\n",
      "\tspeed: 0.0149s/iter; left time: 237.0764s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1414096\n",
      "\tspeed: 0.0141s/iter; left time: 222.7515s\n",
      "\titers: 2500, epoch: 7 | loss: 0.3237130\n",
      "\tspeed: 0.0142s/iter; left time: 222.5182s\n",
      "\titers: 2600, epoch: 7 | loss: 0.2035306\n",
      "\tspeed: 0.0141s/iter; left time: 220.4758s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0984872\n",
      "\tspeed: 0.0141s/iter; left time: 219.2470s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0774698\n",
      "\tspeed: 0.0141s/iter; left time: 217.8496s\n",
      "\titers: 2900, epoch: 7 | loss: 0.2370466\n",
      "\tspeed: 0.0141s/iter; left time: 215.6652s\n",
      "\titers: 3000, epoch: 7 | loss: 0.2249215\n",
      "\tspeed: 0.0141s/iter; left time: 214.2132s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0316031\n",
      "\tspeed: 0.0141s/iter; left time: 212.7828s\n",
      "\titers: 3200, epoch: 7 | loss: 0.7371847\n",
      "\tspeed: 0.0141s/iter; left time: 211.4049s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0974698\n",
      "\tspeed: 0.0141s/iter; left time: 210.2620s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1820732\n",
      "\tspeed: 0.0141s/iter; left time: 208.6228s\n",
      "\titers: 3500, epoch: 7 | loss: 0.2102431\n",
      "\tspeed: 0.0141s/iter; left time: 207.2516s\n",
      "\titers: 3600, epoch: 7 | loss: 0.3492902\n",
      "\tspeed: 0.0141s/iter; left time: 205.8440s\n",
      "\titers: 3700, epoch: 7 | loss: 0.6010002\n",
      "\tspeed: 0.0141s/iter; left time: 204.4310s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1513310\n",
      "\tspeed: 0.0141s/iter; left time: 202.9218s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0786360\n",
      "\tspeed: 0.0141s/iter; left time: 201.5892s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1460377\n",
      "\tspeed: 0.0141s/iter; left time: 200.1193s\n",
      "\titers: 4100, epoch: 7 | loss: 0.2862543\n",
      "\tspeed: 0.0141s/iter; left time: 198.7429s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0462324\n",
      "\tspeed: 0.0141s/iter; left time: 197.4109s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0385479\n",
      "\tspeed: 0.0141s/iter; left time: 196.0180s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0558786\n",
      "\tspeed: 0.0141s/iter; left time: 194.4739s\n",
      "\titers: 4500, epoch: 7 | loss: 0.3853295\n",
      "\tspeed: 0.0141s/iter; left time: 193.1815s\n",
      "Epoch: 7 cost time: 67.6364336013794\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.2758962 Vali Loss: 0.0539867 Test Loss: 0.1755168\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1951349\n",
      "\tspeed: 0.1255s/iter; left time: 1703.0607s\n",
      "\titers: 200, epoch: 8 | loss: 0.1249537\n",
      "\tspeed: 0.0141s/iter; left time: 189.3646s\n",
      "\titers: 300, epoch: 8 | loss: 0.3367389\n",
      "\tspeed: 0.0141s/iter; left time: 188.7750s\n",
      "\titers: 400, epoch: 8 | loss: 0.0971159\n",
      "\tspeed: 0.0142s/iter; left time: 187.9804s\n",
      "\titers: 500, epoch: 8 | loss: 0.4685519\n",
      "\tspeed: 0.0141s/iter; left time: 185.6967s\n",
      "\titers: 600, epoch: 8 | loss: 0.5829732\n",
      "\tspeed: 0.0141s/iter; left time: 184.2563s\n",
      "\titers: 700, epoch: 8 | loss: 0.6046711\n",
      "\tspeed: 0.0141s/iter; left time: 182.4182s\n",
      "\titers: 800, epoch: 8 | loss: 0.1982522\n",
      "\tspeed: 0.0141s/iter; left time: 180.8221s\n",
      "\titers: 900, epoch: 8 | loss: 0.1636255\n",
      "\tspeed: 0.0141s/iter; left time: 179.9917s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1670766\n",
      "\tspeed: 0.0142s/iter; left time: 179.9853s\n",
      "\titers: 1100, epoch: 8 | loss: 0.3962857\n",
      "\tspeed: 0.0142s/iter; left time: 178.4319s\n",
      "\titers: 1200, epoch: 8 | loss: 0.2914205\n",
      "\tspeed: 0.0142s/iter; left time: 176.5812s\n",
      "\titers: 1300, epoch: 8 | loss: 0.9791428\n",
      "\tspeed: 0.0142s/iter; left time: 175.6487s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0700439\n",
      "\tspeed: 0.0142s/iter; left time: 174.1216s\n",
      "\titers: 1500, epoch: 8 | loss: 0.2032858\n",
      "\tspeed: 0.0142s/iter; left time: 172.5715s\n",
      "\titers: 1600, epoch: 8 | loss: 0.2267547\n",
      "\tspeed: 0.0142s/iter; left time: 171.1850s\n",
      "\titers: 1700, epoch: 8 | loss: 0.1460934\n",
      "\tspeed: 0.0142s/iter; left time: 169.7654s\n",
      "\titers: 1800, epoch: 8 | loss: 0.2210935\n",
      "\tspeed: 0.0142s/iter; left time: 168.3592s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1868719\n",
      "\tspeed: 0.0142s/iter; left time: 166.8946s\n",
      "\titers: 2000, epoch: 8 | loss: 0.2420631\n",
      "\tspeed: 0.0142s/iter; left time: 165.4324s\n",
      "\titers: 2100, epoch: 8 | loss: 0.1091900\n",
      "\tspeed: 0.0142s/iter; left time: 164.0989s\n",
      "\titers: 2200, epoch: 8 | loss: 0.2010849\n",
      "\tspeed: 0.0142s/iter; left time: 162.6908s\n",
      "\titers: 2300, epoch: 8 | loss: 0.2656489\n",
      "\tspeed: 0.0142s/iter; left time: 161.0028s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1323050\n",
      "\tspeed: 0.0141s/iter; left time: 159.2239s\n",
      "\titers: 2500, epoch: 8 | loss: 0.2059726\n",
      "\tspeed: 0.0142s/iter; left time: 158.0692s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0834353\n",
      "\tspeed: 0.0142s/iter; left time: 156.6168s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1525191\n",
      "\tspeed: 0.0142s/iter; left time: 155.2201s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0992469\n",
      "\tspeed: 0.0142s/iter; left time: 153.8485s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1758872\n",
      "\tspeed: 0.0142s/iter; left time: 152.4013s\n",
      "\titers: 3000, epoch: 8 | loss: 0.9450158\n",
      "\tspeed: 0.0142s/iter; left time: 151.7133s\n",
      "\titers: 3100, epoch: 8 | loss: 0.5421641\n",
      "\tspeed: 0.0142s/iter; left time: 150.3513s\n",
      "\titers: 3200, epoch: 8 | loss: 0.3930854\n",
      "\tspeed: 0.0142s/iter; left time: 148.9184s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0525866\n",
      "\tspeed: 0.0142s/iter; left time: 147.1000s\n",
      "\titers: 3400, epoch: 8 | loss: 0.1445827\n",
      "\tspeed: 0.0142s/iter; left time: 145.4676s\n",
      "\titers: 3500, epoch: 8 | loss: 0.2329766\n",
      "\tspeed: 0.0142s/iter; left time: 143.9363s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0961709\n",
      "\tspeed: 0.0142s/iter; left time: 142.5933s\n",
      "\titers: 3700, epoch: 8 | loss: 0.5406616\n",
      "\tspeed: 0.0142s/iter; left time: 141.0784s\n",
      "\titers: 3800, epoch: 8 | loss: 0.4929996\n",
      "\tspeed: 0.0142s/iter; left time: 139.6483s\n",
      "\titers: 3900, epoch: 8 | loss: 0.2293096\n",
      "\tspeed: 0.0142s/iter; left time: 138.5017s\n",
      "\titers: 4000, epoch: 8 | loss: 0.1958664\n",
      "\tspeed: 0.0142s/iter; left time: 137.6795s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0639715\n",
      "\tspeed: 0.0142s/iter; left time: 136.1325s\n",
      "\titers: 4200, epoch: 8 | loss: 0.1833756\n",
      "\tspeed: 0.0142s/iter; left time: 134.6934s\n",
      "\titers: 4300, epoch: 8 | loss: 0.3585663\n",
      "\tspeed: 0.0142s/iter; left time: 133.2686s\n",
      "\titers: 4400, epoch: 8 | loss: 0.7304029\n",
      "\tspeed: 0.0142s/iter; left time: 131.8173s\n",
      "\titers: 4500, epoch: 8 | loss: 0.6523141\n",
      "\tspeed: 0.0142s/iter; left time: 130.3693s\n",
      "Epoch: 8 cost time: 64.7883038520813\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.2761211 Vali Loss: 0.0541804 Test Loss: 0.1753599\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1689291\n",
      "\tspeed: 0.1249s/iter; left time: 1125.9102s\n",
      "\titers: 200, epoch: 9 | loss: 0.3279323\n",
      "\tspeed: 0.0141s/iter; left time: 125.5476s\n",
      "\titers: 300, epoch: 9 | loss: 0.0670463\n",
      "\tspeed: 0.0141s/iter; left time: 124.1051s\n",
      "\titers: 400, epoch: 9 | loss: 0.3156407\n",
      "\tspeed: 0.0141s/iter; left time: 122.7305s\n",
      "\titers: 500, epoch: 9 | loss: 0.2629442\n",
      "\tspeed: 0.0141s/iter; left time: 121.3558s\n",
      "\titers: 600, epoch: 9 | loss: 0.0348598\n",
      "\tspeed: 0.0141s/iter; left time: 119.8617s\n",
      "\titers: 700, epoch: 9 | loss: 0.3113619\n",
      "\tspeed: 0.0141s/iter; left time: 118.5917s\n",
      "\titers: 800, epoch: 9 | loss: 0.0922184\n",
      "\tspeed: 0.0141s/iter; left time: 116.7955s\n",
      "\titers: 900, epoch: 9 | loss: 0.1808001\n",
      "\tspeed: 0.0140s/iter; left time: 115.3324s\n",
      "\titers: 1000, epoch: 9 | loss: 0.3825843\n",
      "\tspeed: 0.0140s/iter; left time: 113.9540s\n",
      "\titers: 1100, epoch: 9 | loss: 0.1935930\n",
      "\tspeed: 0.0141s/iter; left time: 112.6144s\n",
      "\titers: 1200, epoch: 9 | loss: 0.6018860\n",
      "\tspeed: 0.0141s/iter; left time: 111.2065s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0906038\n",
      "\tspeed: 0.0141s/iter; left time: 109.7954s\n",
      "\titers: 1400, epoch: 9 | loss: 0.4040660\n",
      "\tspeed: 0.0141s/iter; left time: 108.3529s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1912995\n",
      "\tspeed: 0.0141s/iter; left time: 106.9641s\n",
      "\titers: 1600, epoch: 9 | loss: 0.2275476\n",
      "\tspeed: 0.0140s/iter; left time: 105.5282s\n",
      "\titers: 1700, epoch: 9 | loss: 0.1784000\n",
      "\tspeed: 0.0141s/iter; left time: 104.2230s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0648353\n",
      "\tspeed: 0.0141s/iter; left time: 102.7362s\n",
      "\titers: 1900, epoch: 9 | loss: 0.1004524\n",
      "\tspeed: 0.0141s/iter; left time: 101.3891s\n",
      "\titers: 2000, epoch: 9 | loss: 0.1285883\n",
      "\tspeed: 0.0141s/iter; left time: 100.1381s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0817868\n",
      "\tspeed: 0.0141s/iter; left time: 98.7401s\n",
      "\titers: 2200, epoch: 9 | loss: 0.3068593\n",
      "\tspeed: 0.0141s/iter; left time: 97.2999s\n",
      "\titers: 2300, epoch: 9 | loss: 0.3806482\n",
      "\tspeed: 0.0141s/iter; left time: 95.9417s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0538073\n",
      "\tspeed: 0.0141s/iter; left time: 94.5511s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0624239\n",
      "\tspeed: 0.0141s/iter; left time: 93.1139s\n",
      "\titers: 2600, epoch: 9 | loss: 0.3604922\n",
      "\tspeed: 0.0141s/iter; left time: 91.7196s\n",
      "\titers: 2700, epoch: 9 | loss: 0.1070566\n",
      "\tspeed: 0.0141s/iter; left time: 90.2346s\n",
      "\titers: 2800, epoch: 9 | loss: 0.4635253\n",
      "\tspeed: 0.0141s/iter; left time: 88.8337s\n",
      "\titers: 2900, epoch: 9 | loss: 0.1419870\n",
      "\tspeed: 0.0141s/iter; left time: 87.4944s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1357905\n",
      "\tspeed: 0.0141s/iter; left time: 86.0791s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0676575\n",
      "\tspeed: 0.0141s/iter; left time: 84.6511s\n",
      "\titers: 3200, epoch: 9 | loss: 0.2497378\n",
      "\tspeed: 0.0141s/iter; left time: 83.2042s\n",
      "\titers: 3300, epoch: 9 | loss: 0.4862020\n",
      "\tspeed: 0.0140s/iter; left time: 81.6265s\n",
      "\titers: 3400, epoch: 9 | loss: 0.4971119\n",
      "\tspeed: 0.0140s/iter; left time: 80.2135s\n",
      "\titers: 3500, epoch: 9 | loss: 0.1289376\n",
      "\tspeed: 0.0141s/iter; left time: 78.8912s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0917218\n",
      "\tspeed: 0.0141s/iter; left time: 77.4948s\n",
      "\titers: 3700, epoch: 9 | loss: 0.1825398\n",
      "\tspeed: 0.0141s/iter; left time: 76.0949s\n",
      "\titers: 3800, epoch: 9 | loss: 0.3004114\n",
      "\tspeed: 0.0141s/iter; left time: 74.7217s\n",
      "\titers: 3900, epoch: 9 | loss: 0.1119415\n",
      "\tspeed: 0.0141s/iter; left time: 73.2453s\n",
      "\titers: 4000, epoch: 9 | loss: 0.1580825\n",
      "\tspeed: 0.0141s/iter; left time: 71.8143s\n",
      "\titers: 4100, epoch: 9 | loss: 0.2723106\n",
      "\tspeed: 0.0142s/iter; left time: 71.2555s\n",
      "\titers: 4200, epoch: 9 | loss: 0.1582368\n",
      "\tspeed: 0.0142s/iter; left time: 69.8722s\n",
      "\titers: 4300, epoch: 9 | loss: 0.1715502\n",
      "\tspeed: 0.0142s/iter; left time: 68.3862s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0994491\n",
      "\tspeed: 0.0142s/iter; left time: 66.9950s\n",
      "\titers: 4500, epoch: 9 | loss: 0.3356845\n",
      "\tspeed: 0.0142s/iter; left time: 65.5400s\n",
      "Epoch: 9 cost time: 64.40542197227478\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.2745027 Vali Loss: 0.0540760 Test Loss: 0.1752905\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['total_load']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17561522126197815, mae:0.26442843675613403\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash scripts/TimeXer.sh --features MS --predictor total_load --enc_in 2  --dec_in 2  --c_out 1 --dropout 0.2  --learning_rate 5e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
