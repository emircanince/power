{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "111f562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juju time\n"
     ]
    }
   ],
   "source": [
    "print('juju time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b5690",
   "metadata": {},
   "source": [
    "# initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ECL_168_24          Model:              iTransformer        \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          forecast_data.csv   Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0005              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ECL_168_24_iTransformer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 200, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 80, in train\n",
      "    train_data, train_loader = self._get_data(flag='train')\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 28, in _get_data\n",
      "    data_set, data_loader = data_provider(self.args, flag)\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_factory.py\", line 69, in data_provider\n",
      "    data_set = Data(\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_loader.py\", line 235, in __init__\n",
      "    self.__read_data__()\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_loader.py\", line 257, in __read_data__\n",
      "    df_raw = df_raw[['date'] + cols + [self.target]]\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/core/frame.py\", line 3813, in __getitem__\n",
      "    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict\n",
      "    self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing\n",
      "    raise KeyError(f\"{not_found} not in index\")\n",
      "KeyError: \"['electricity_price'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!bash /home/RDC/inceemir/power/scripts/iTransformer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38e7b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1835497\n",
      "\tspeed: 0.0264s/iter; left time: 1201.7183s\n",
      "\titers: 200, epoch: 1 | loss: 0.1153142\n",
      "\tspeed: 0.0146s/iter; left time: 663.8387s\n",
      "\titers: 300, epoch: 1 | loss: 0.1916097\n",
      "\tspeed: 0.0146s/iter; left time: 661.5674s\n",
      "terminate called after throwing an instance of 'std::system_error'\n",
      "  what():  Broken pipe\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bash /home/RDC/inceemir/power/scripts/TimeXer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65c9ce",
   "metadata": {},
   "source": [
    "### predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1761947\n",
      "\tspeed: 0.0256s/iter; left time: 1164.8455s\n",
      "\titers: 200, epoch: 1 | loss: 0.0938281\n",
      "\tspeed: 0.0142s/iter; left time: 643.1781s\n",
      "\titers: 300, epoch: 1 | loss: 0.0687024\n",
      "\tspeed: 0.0142s/iter; left time: 641.4423s\n",
      "\titers: 400, epoch: 1 | loss: 0.1241657\n",
      "\tspeed: 0.0142s/iter; left time: 642.6551s\n",
      "\titers: 500, epoch: 1 | loss: 0.0327702\n",
      "\tspeed: 0.0142s/iter; left time: 637.7068s\n",
      "\titers: 600, epoch: 1 | loss: 0.0715009\n",
      "\tspeed: 0.0141s/iter; left time: 635.8824s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451532\n",
      "\tspeed: 0.0142s/iter; left time: 635.0910s\n",
      "\titers: 800, epoch: 1 | loss: 0.5053760\n",
      "\tspeed: 0.0142s/iter; left time: 633.5301s\n",
      "\titers: 900, epoch: 1 | loss: 0.2560097\n",
      "\tspeed: 0.0142s/iter; left time: 631.8121s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0734115\n",
      "\tspeed: 0.0141s/iter; left time: 630.1653s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0425444\n",
      "\tspeed: 0.0142s/iter; left time: 629.0557s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0585851\n",
      "\tspeed: 0.0141s/iter; left time: 626.5199s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2605752\n",
      "\tspeed: 0.0141s/iter; left time: 625.2519s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0309507\n",
      "\tspeed: 0.0141s/iter; left time: 624.0798s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1115317\n",
      "\tspeed: 0.0141s/iter; left time: 622.1581s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1843507\n",
      "\tspeed: 0.0141s/iter; left time: 621.1129s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3890318\n",
      "\tspeed: 0.0141s/iter; left time: 619.2079s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0720876\n",
      "\tspeed: 0.0141s/iter; left time: 618.4178s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1775747\n",
      "\tspeed: 0.0141s/iter; left time: 616.7576s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3624939\n",
      "\tspeed: 0.0141s/iter; left time: 615.3805s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1508106\n",
      "\tspeed: 0.0141s/iter; left time: 613.9906s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3487170\n",
      "\tspeed: 0.0141s/iter; left time: 612.1984s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1623183\n",
      "\tspeed: 0.0141s/iter; left time: 610.6057s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1627411\n",
      "\tspeed: 0.0141s/iter; left time: 609.6374s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2751411\n",
      "\tspeed: 0.0142s/iter; left time: 609.7013s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1169626\n",
      "\tspeed: 0.0141s/iter; left time: 607.5077s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2052497\n",
      "\tspeed: 0.0141s/iter; left time: 605.0361s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2055999\n",
      "\tspeed: 0.0141s/iter; left time: 604.2881s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2338544\n",
      "\tspeed: 0.0141s/iter; left time: 602.6514s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0882175\n",
      "\tspeed: 0.0141s/iter; left time: 601.4127s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0823141\n",
      "\tspeed: 0.0141s/iter; left time: 599.7167s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1962520\n",
      "\tspeed: 0.0141s/iter; left time: 598.4791s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0351911\n",
      "\tspeed: 0.0141s/iter; left time: 596.9468s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4122079\n",
      "\tspeed: 0.0141s/iter; left time: 595.2397s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0856364\n",
      "\tspeed: 0.0141s/iter; left time: 593.4993s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1057309\n",
      "\tspeed: 0.0141s/iter; left time: 592.6613s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0931639\n",
      "\tspeed: 0.0141s/iter; left time: 591.3906s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1785975\n",
      "\tspeed: 0.0141s/iter; left time: 590.3463s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3997110\n",
      "\tspeed: 0.0142s/iter; left time: 589.4153s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2769776\n",
      "\tspeed: 0.0142s/iter; left time: 590.0380s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0955917\n",
      "\tspeed: 0.0142s/iter; left time: 588.5485s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0872184\n",
      "\tspeed: 0.0142s/iter; left time: 587.9703s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1506448\n",
      "\tspeed: 0.0142s/iter; left time: 585.5942s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0404093\n",
      "\tspeed: 0.0142s/iter; left time: 584.2741s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0716922\n",
      "\tspeed: 0.0142s/iter; left time: 582.8253s\n",
      "Epoch: 1 cost time: 65.63423943519592\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1763682 Vali Loss: 0.0384911 Test Loss: 0.1245387\n",
      "Validation loss decreased (inf --> 0.038491).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0965484\n",
      "\tspeed: 0.1281s/iter; left time: 5237.9183s\n",
      "\titers: 200, epoch: 2 | loss: 0.0394090\n",
      "\tspeed: 0.0158s/iter; left time: 644.7432s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977589\n",
      "\tspeed: 0.0157s/iter; left time: 640.8606s\n",
      "\titers: 400, epoch: 2 | loss: 0.1882427\n",
      "\tspeed: 0.0157s/iter; left time: 637.8098s\n",
      "\titers: 500, epoch: 2 | loss: 0.3373528\n",
      "\tspeed: 0.0157s/iter; left time: 636.7980s\n",
      "\titers: 600, epoch: 2 | loss: 0.1316223\n",
      "\tspeed: 0.0157s/iter; left time: 635.1944s\n",
      "\titers: 700, epoch: 2 | loss: 0.0733484\n",
      "\tspeed: 0.0143s/iter; left time: 574.5923s\n",
      "\titers: 800, epoch: 2 | loss: 0.0557397\n",
      "\tspeed: 0.0141s/iter; left time: 566.9357s\n",
      "\titers: 900, epoch: 2 | loss: 0.0551844\n",
      "\tspeed: 0.0141s/iter; left time: 565.5030s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1973986\n",
      "\tspeed: 0.0141s/iter; left time: 564.9616s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2021171\n",
      "\tspeed: 0.0141s/iter; left time: 562.8582s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1249596\n",
      "\tspeed: 0.0141s/iter; left time: 561.3448s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1480998\n",
      "\tspeed: 0.0141s/iter; left time: 559.8638s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1150100\n",
      "\tspeed: 0.0141s/iter; left time: 558.2361s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0475104\n",
      "\tspeed: 0.0146s/iter; left time: 576.7796s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1711187\n",
      "\tspeed: 0.0142s/iter; left time: 558.8135s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2710866\n",
      "\tspeed: 0.0142s/iter; left time: 557.4796s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1182006\n",
      "\tspeed: 0.0142s/iter; left time: 556.1922s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1678239\n",
      "\tspeed: 0.0142s/iter; left time: 554.0613s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0585374\n",
      "\tspeed: 0.0142s/iter; left time: 552.7996s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0376854\n",
      "\tspeed: 0.0142s/iter; left time: 551.7186s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177671\n",
      "\tspeed: 0.0142s/iter; left time: 550.0896s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1420685\n",
      "\tspeed: 0.0142s/iter; left time: 548.4687s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1342927\n",
      "\tspeed: 0.0142s/iter; left time: 547.0321s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0575067\n",
      "\tspeed: 0.0142s/iter; left time: 545.3330s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2137174\n",
      "\tspeed: 0.0142s/iter; left time: 543.8850s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1351328\n",
      "\tspeed: 0.0142s/iter; left time: 543.3677s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0589181\n",
      "\tspeed: 0.0142s/iter; left time: 541.7461s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0820708\n",
      "\tspeed: 0.0142s/iter; left time: 540.0748s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1777464\n",
      "\tspeed: 0.0142s/iter; left time: 538.4378s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0429570\n",
      "\tspeed: 0.0142s/iter; left time: 537.0400s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1527276\n",
      "\tspeed: 0.0142s/iter; left time: 535.5759s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0607781\n",
      "\tspeed: 0.0142s/iter; left time: 534.2153s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0930832\n",
      "\tspeed: 0.0142s/iter; left time: 532.7197s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1257465\n",
      "\tspeed: 0.0142s/iter; left time: 531.6051s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1400915\n",
      "\tspeed: 0.0142s/iter; left time: 529.8619s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1529161\n",
      "\tspeed: 0.0142s/iter; left time: 528.5605s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1372309\n",
      "\tspeed: 0.0142s/iter; left time: 527.0908s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1924893\n",
      "\tspeed: 0.0142s/iter; left time: 526.0210s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2247270\n",
      "\tspeed: 0.0142s/iter; left time: 524.5193s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171593\n",
      "\tspeed: 0.0142s/iter; left time: 523.0703s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0934797\n",
      "\tspeed: 0.0142s/iter; left time: 521.6859s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1451997\n",
      "\tspeed: 0.0142s/iter; left time: 520.5373s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1581813\n",
      "\tspeed: 0.0142s/iter; left time: 519.0269s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0497790\n",
      "\tspeed: 0.0142s/iter; left time: 517.6743s\n",
      "Epoch: 2 cost time: 65.72112560272217\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1446429 Vali Loss: 0.0337670 Test Loss: 0.1147559\n",
      "Validation loss decreased (0.038491 --> 0.033767).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0888895\n",
      "\tspeed: 0.1244s/iter; left time: 4519.1067s\n",
      "\titers: 200, epoch: 3 | loss: 0.0433673\n",
      "\tspeed: 0.0143s/iter; left time: 516.7872s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213155\n",
      "\tspeed: 0.0142s/iter; left time: 514.6841s\n",
      "\titers: 400, epoch: 3 | loss: 0.0755301\n",
      "\tspeed: 0.0143s/iter; left time: 513.8223s\n",
      "\titers: 500, epoch: 3 | loss: 0.0728352\n",
      "\tspeed: 0.0143s/iter; left time: 512.1804s\n",
      "\titers: 600, epoch: 3 | loss: 0.0727856\n",
      "\tspeed: 0.0143s/iter; left time: 510.9433s\n",
      "\titers: 700, epoch: 3 | loss: 0.3633784\n",
      "\tspeed: 0.0142s/iter; left time: 508.1414s\n",
      "\titers: 800, epoch: 3 | loss: 0.0919219\n",
      "\tspeed: 0.0142s/iter; left time: 506.7590s\n",
      "\titers: 900, epoch: 3 | loss: 0.0787504\n",
      "\tspeed: 0.0142s/iter; left time: 505.3284s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1017697\n",
      "\tspeed: 0.0142s/iter; left time: 503.9259s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1278257\n",
      "\tspeed: 0.0142s/iter; left time: 502.6370s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1174518\n",
      "\tspeed: 0.0142s/iter; left time: 502.0895s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2299570\n",
      "\tspeed: 0.0143s/iter; left time: 500.9126s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1301254\n",
      "\tspeed: 0.0142s/iter; left time: 499.1722s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1960834\n",
      "\tspeed: 0.0142s/iter; left time: 497.7263s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1536359\n",
      "\tspeed: 0.0143s/iter; left time: 497.0936s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5119310\n",
      "\tspeed: 0.0143s/iter; left time: 495.1460s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0803905\n",
      "\tspeed: 0.0142s/iter; left time: 493.3753s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1686091\n",
      "\tspeed: 0.0143s/iter; left time: 492.2820s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2706951\n",
      "\tspeed: 0.0142s/iter; left time: 490.7832s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0878483\n",
      "\tspeed: 0.0142s/iter; left time: 488.9907s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0488082\n",
      "\tspeed: 0.0142s/iter; left time: 487.5268s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1471822\n",
      "\tspeed: 0.0143s/iter; left time: 486.7056s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1000195\n",
      "\tspeed: 0.0142s/iter; left time: 484.9381s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1408294\n",
      "\tspeed: 0.0143s/iter; left time: 483.7233s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0522313\n",
      "\tspeed: 0.0142s/iter; left time: 482.0732s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0433277\n",
      "\tspeed: 0.0142s/iter; left time: 480.4433s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1223619\n",
      "\tspeed: 0.0142s/iter; left time: 479.0811s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0673580\n",
      "\tspeed: 0.0143s/iter; left time: 478.0369s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0574459\n",
      "\tspeed: 0.0142s/iter; left time: 476.4311s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1538857\n",
      "\tspeed: 0.0142s/iter; left time: 474.8062s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2254446\n",
      "\tspeed: 0.0142s/iter; left time: 473.5165s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0779002\n",
      "\tspeed: 0.0142s/iter; left time: 472.1171s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1575561\n",
      "\tspeed: 0.0142s/iter; left time: 470.4007s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0331902\n",
      "\tspeed: 0.0142s/iter; left time: 469.0136s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1007734\n",
      "\tspeed: 0.0142s/iter; left time: 467.7516s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1122994\n",
      "\tspeed: 0.0142s/iter; left time: 466.4144s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0607264\n",
      "\tspeed: 0.0142s/iter; left time: 464.6758s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1815582\n",
      "\tspeed: 0.0143s/iter; left time: 463.7564s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1472391\n",
      "\tspeed: 0.0143s/iter; left time: 462.6541s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1834221\n",
      "\tspeed: 0.0142s/iter; left time: 460.6333s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0803711\n",
      "\tspeed: 0.0142s/iter; left time: 459.3079s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0216850\n",
      "\tspeed: 0.0142s/iter; left time: 457.5813s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0932727\n",
      "\tspeed: 0.0142s/iter; left time: 456.3650s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0153473\n",
      "\tspeed: 0.0142s/iter; left time: 454.8076s\n",
      "Epoch: 3 cost time: 65.07271933555603\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1149970 Vali Loss: 0.0354637 Test Loss: 0.1141755\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0631109\n",
      "\tspeed: 0.1266s/iter; left time: 4022.5866s\n",
      "\titers: 200, epoch: 4 | loss: 0.2875030\n",
      "\tspeed: 0.0147s/iter; left time: 464.7026s\n",
      "\titers: 300, epoch: 4 | loss: 0.0210192\n",
      "\tspeed: 0.0142s/iter; left time: 449.8199s\n",
      "\titers: 400, epoch: 4 | loss: 0.0991706\n",
      "\tspeed: 0.0142s/iter; left time: 448.1617s\n",
      "\titers: 500, epoch: 4 | loss: 0.0960686\n",
      "\tspeed: 0.0148s/iter; left time: 465.9793s\n",
      "\titers: 600, epoch: 4 | loss: 0.1770177\n",
      "\tspeed: 0.0157s/iter; left time: 491.3295s\n",
      "\titers: 700, epoch: 4 | loss: 0.1712226\n",
      "\tspeed: 0.0157s/iter; left time: 489.6184s\n",
      "\titers: 800, epoch: 4 | loss: 0.1566384\n",
      "\tspeed: 0.0148s/iter; left time: 458.8354s\n",
      "\titers: 900, epoch: 4 | loss: 0.1039117\n",
      "\tspeed: 0.0146s/iter; left time: 453.4049s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0762816\n",
      "\tspeed: 0.0157s/iter; left time: 485.1991s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1103869\n",
      "\tspeed: 0.0157s/iter; left time: 483.5553s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0567773\n",
      "\tspeed: 0.0157s/iter; left time: 481.5137s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0957227\n",
      "\tspeed: 0.0157s/iter; left time: 480.1614s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0460843\n",
      "\tspeed: 0.0157s/iter; left time: 479.1415s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1783445\n",
      "\tspeed: 0.0157s/iter; left time: 476.8830s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0993816\n",
      "\tspeed: 0.0157s/iter; left time: 475.4677s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1247748\n",
      "\tspeed: 0.0157s/iter; left time: 473.6540s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0204686\n",
      "\tspeed: 0.0149s/iter; left time: 447.1990s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0617034\n",
      "\tspeed: 0.0142s/iter; left time: 425.1502s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1042527\n",
      "\tspeed: 0.0142s/iter; left time: 423.7498s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0174318\n",
      "\tspeed: 0.0142s/iter; left time: 422.4315s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1664865\n",
      "\tspeed: 0.0142s/iter; left time: 420.6944s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1293821\n",
      "\tspeed: 0.0142s/iter; left time: 419.3099s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0772202\n",
      "\tspeed: 0.0142s/iter; left time: 418.0027s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0164892\n",
      "\tspeed: 0.0142s/iter; left time: 416.8478s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0531578\n",
      "\tspeed: 0.0142s/iter; left time: 415.3650s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5676972\n",
      "\tspeed: 0.0142s/iter; left time: 414.0172s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0195277\n",
      "\tspeed: 0.0142s/iter; left time: 412.5013s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0262614\n",
      "\tspeed: 0.0142s/iter; left time: 411.0733s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1373415\n",
      "\tspeed: 0.0142s/iter; left time: 409.6808s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0994673\n",
      "\tspeed: 0.0142s/iter; left time: 408.3945s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1655515\n",
      "\tspeed: 0.0142s/iter; left time: 406.9246s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0791031\n",
      "\tspeed: 0.0142s/iter; left time: 405.4681s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0591687\n",
      "\tspeed: 0.0142s/iter; left time: 404.1167s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0567421\n",
      "\tspeed: 0.0142s/iter; left time: 402.4513s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0513307\n",
      "\tspeed: 0.0142s/iter; left time: 401.2040s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1378802\n",
      "\tspeed: 0.0142s/iter; left time: 399.6859s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0390188\n",
      "\tspeed: 0.0142s/iter; left time: 398.2535s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0773032\n",
      "\tspeed: 0.0142s/iter; left time: 396.8678s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0881940\n",
      "\tspeed: 0.0142s/iter; left time: 395.3179s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0252138\n",
      "\tspeed: 0.0142s/iter; left time: 394.1114s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0641742\n",
      "\tspeed: 0.0142s/iter; left time: 392.6401s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2728181\n",
      "\tspeed: 0.0142s/iter; left time: 391.2596s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0952311\n",
      "\tspeed: 0.0142s/iter; left time: 389.8738s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0663833\n",
      "\tspeed: 0.0156s/iter; left time: 425.9113s\n",
      "Epoch: 4 cost time: 66.90452027320862\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0959688 Vali Loss: 0.0351006 Test Loss: 0.1077463\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0373502\n",
      "\tspeed: 0.1325s/iter; left time: 3607.7164s\n",
      "\titers: 200, epoch: 5 | loss: 0.0524443\n",
      "\tspeed: 0.0142s/iter; left time: 384.9354s\n",
      "\titers: 300, epoch: 5 | loss: 0.0598457\n",
      "\tspeed: 0.0142s/iter; left time: 383.9007s\n",
      "\titers: 400, epoch: 5 | loss: 0.0432743\n",
      "\tspeed: 0.0142s/iter; left time: 382.2519s\n",
      "\titers: 500, epoch: 5 | loss: 0.1162245\n",
      "\tspeed: 0.0142s/iter; left time: 380.9326s\n",
      "\titers: 600, epoch: 5 | loss: 0.0336827\n",
      "\tspeed: 0.0142s/iter; left time: 379.3639s\n",
      "\titers: 700, epoch: 5 | loss: 0.0636778\n",
      "\tspeed: 0.0142s/iter; left time: 377.7301s\n",
      "\titers: 800, epoch: 5 | loss: 0.0793169\n",
      "\tspeed: 0.0142s/iter; left time: 376.2956s\n",
      "\titers: 900, epoch: 5 | loss: 0.0391933\n",
      "\tspeed: 0.0142s/iter; left time: 375.1709s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1438223\n",
      "\tspeed: 0.0141s/iter; left time: 372.4373s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0300231\n",
      "\tspeed: 0.0141s/iter; left time: 370.9310s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1026424\n",
      "\tspeed: 0.0142s/iter; left time: 370.4638s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0712850\n",
      "\tspeed: 0.0142s/iter; left time: 368.9338s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1095237\n",
      "\tspeed: 0.0142s/iter; left time: 367.6705s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1186156\n",
      "\tspeed: 0.0142s/iter; left time: 366.2991s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0570681\n",
      "\tspeed: 0.0142s/iter; left time: 365.0771s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1760297\n",
      "\tspeed: 0.0142s/iter; left time: 363.4837s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0911143\n",
      "\tspeed: 0.0142s/iter; left time: 362.1158s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0277795\n",
      "\tspeed: 0.0142s/iter; left time: 360.3776s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5396386\n",
      "\tspeed: 0.0142s/iter; left time: 358.9431s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0865949\n",
      "\tspeed: 0.0142s/iter; left time: 357.8641s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0422525\n",
      "\tspeed: 0.0142s/iter; left time: 356.4402s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0285333\n",
      "\tspeed: 0.0142s/iter; left time: 354.9210s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0772292\n",
      "\tspeed: 0.0142s/iter; left time: 353.5433s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0591693\n",
      "\tspeed: 0.0142s/iter; left time: 352.2319s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0688789\n",
      "\tspeed: 0.0142s/iter; left time: 351.3013s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0948981\n",
      "\tspeed: 0.0142s/iter; left time: 349.8915s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0655452\n",
      "\tspeed: 0.0142s/iter; left time: 348.4187s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0620498\n",
      "\tspeed: 0.0142s/iter; left time: 347.0249s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0421777\n",
      "\tspeed: 0.0142s/iter; left time: 345.5936s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0343712\n",
      "\tspeed: 0.0142s/iter; left time: 344.0935s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0124530\n",
      "\tspeed: 0.0142s/iter; left time: 342.5363s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0510148\n",
      "\tspeed: 0.0142s/iter; left time: 341.3392s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0607961\n",
      "\tspeed: 0.0142s/iter; left time: 339.9712s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0542052\n",
      "\tspeed: 0.0142s/iter; left time: 338.5181s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0656159\n",
      "\tspeed: 0.0142s/iter; left time: 336.9125s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0490740\n",
      "\tspeed: 0.0142s/iter; left time: 335.4960s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1356292\n",
      "\tspeed: 0.0142s/iter; left time: 334.1332s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0560427\n",
      "\tspeed: 0.0142s/iter; left time: 333.3644s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0542096\n",
      "\tspeed: 0.0142s/iter; left time: 331.4337s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0280492\n",
      "\tspeed: 0.0142s/iter; left time: 330.1641s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0362821\n",
      "\tspeed: 0.0142s/iter; left time: 328.6361s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1139971\n",
      "\tspeed: 0.0142s/iter; left time: 327.0342s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0214874\n",
      "\tspeed: 0.0142s/iter; left time: 325.5649s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0509721\n",
      "\tspeed: 0.0142s/iter; left time: 324.2552s\n",
      "Epoch: 5 cost time: 64.82965755462646\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0878741 Vali Loss: 0.0343938 Test Loss: 0.1081526\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11478278785943985, mae:0.20999355614185333\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42356dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1753043\n",
      "\tspeed: 0.0258s/iter; left time: 1173.3269s\n",
      "\titers: 200, epoch: 1 | loss: 0.1077347\n",
      "\tspeed: 0.0143s/iter; left time: 648.5238s\n",
      "\titers: 300, epoch: 1 | loss: 0.0663826\n",
      "\tspeed: 0.0143s/iter; left time: 647.4835s\n",
      "\titers: 400, epoch: 1 | loss: 0.1280175\n",
      "\tspeed: 0.0143s/iter; left time: 645.7706s\n",
      "\titers: 500, epoch: 1 | loss: 0.0312991\n",
      "\tspeed: 0.0143s/iter; left time: 644.6994s\n",
      "\titers: 600, epoch: 1 | loss: 0.0673031\n",
      "\tspeed: 0.0143s/iter; left time: 642.7849s\n",
      "\titers: 700, epoch: 1 | loss: 0.1459749\n",
      "\tspeed: 0.0143s/iter; left time: 639.8827s\n",
      "\titers: 800, epoch: 1 | loss: 0.4970564\n",
      "\tspeed: 0.0143s/iter; left time: 638.9296s\n",
      "\titers: 900, epoch: 1 | loss: 0.2613052\n",
      "\tspeed: 0.0143s/iter; left time: 637.3148s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0759784\n",
      "\tspeed: 0.0143s/iter; left time: 636.8739s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0833589\n",
      "\tspeed: 0.0143s/iter; left time: 634.5848s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0704285\n",
      "\tspeed: 0.0143s/iter; left time: 632.5448s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2727672\n",
      "\tspeed: 0.0143s/iter; left time: 631.3209s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0231198\n",
      "\tspeed: 0.0142s/iter; left time: 628.9848s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1133188\n",
      "\tspeed: 0.0143s/iter; left time: 627.7501s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1999876\n",
      "\tspeed: 0.0143s/iter; left time: 626.3392s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4395665\n",
      "\tspeed: 0.0143s/iter; left time: 625.0662s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0567457\n",
      "\tspeed: 0.0143s/iter; left time: 623.7234s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1737622\n",
      "\tspeed: 0.0143s/iter; left time: 623.2228s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3616043\n",
      "\tspeed: 0.0143s/iter; left time: 621.6172s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2101596\n",
      "\tspeed: 0.0143s/iter; left time: 620.1093s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3242244\n",
      "\tspeed: 0.0143s/iter; left time: 618.2630s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1569029\n",
      "\tspeed: 0.0143s/iter; left time: 616.3304s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1768864\n",
      "\tspeed: 0.0143s/iter; left time: 615.5566s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2277374\n",
      "\tspeed: 0.0143s/iter; left time: 614.0405s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1055813\n",
      "\tspeed: 0.0143s/iter; left time: 612.6475s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1563688\n",
      "\tspeed: 0.0143s/iter; left time: 610.8906s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2034613\n",
      "\tspeed: 0.0143s/iter; left time: 609.5223s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1688126\n",
      "\tspeed: 0.0143s/iter; left time: 608.0049s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0908880\n",
      "\tspeed: 0.0143s/iter; left time: 607.2736s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0868602\n",
      "\tspeed: 0.0143s/iter; left time: 605.3318s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1677474\n",
      "\tspeed: 0.0143s/iter; left time: 603.6675s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0260000\n",
      "\tspeed: 0.0143s/iter; left time: 602.0883s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2454062\n",
      "\tspeed: 0.0143s/iter; left time: 600.6781s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0880173\n",
      "\tspeed: 0.0143s/iter; left time: 599.2367s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1130646\n",
      "\tspeed: 0.0143s/iter; left time: 597.9081s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0845899\n",
      "\tspeed: 0.0143s/iter; left time: 597.4733s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1550690\n",
      "\tspeed: 0.0143s/iter; left time: 595.7796s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3135510\n",
      "\tspeed: 0.0143s/iter; left time: 593.8042s\n",
      "\titers: 4000, epoch: 1 | loss: 0.4224209\n",
      "\tspeed: 0.0143s/iter; left time: 592.8163s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0906006\n",
      "\tspeed: 0.0143s/iter; left time: 590.9078s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0793019\n",
      "\tspeed: 0.0143s/iter; left time: 589.3897s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1526469\n",
      "\tspeed: 0.0143s/iter; left time: 588.2358s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0259536\n",
      "\tspeed: 0.0143s/iter; left time: 586.6235s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0581182\n",
      "\tspeed: 0.0143s/iter; left time: 585.5157s\n",
      "Epoch: 1 cost time: 66.18902659416199\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1786124 Vali Loss: 0.0385811 Test Loss: 0.1275217\n",
      "Validation loss decreased (inf --> 0.038581).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0790856\n",
      "\tspeed: 0.1332s/iter; left time: 5446.0730s\n",
      "\titers: 200, epoch: 2 | loss: 0.0379522\n",
      "\tspeed: 0.0153s/iter; left time: 625.5194s\n",
      "\titers: 300, epoch: 2 | loss: 0.0849036\n",
      "\tspeed: 0.0144s/iter; left time: 584.0060s\n",
      "\titers: 400, epoch: 2 | loss: 0.0969812\n",
      "\tspeed: 0.0143s/iter; left time: 581.8680s\n",
      "\titers: 500, epoch: 2 | loss: 0.3665311\n",
      "\tspeed: 0.0143s/iter; left time: 580.0896s\n",
      "\titers: 600, epoch: 2 | loss: 0.1241603\n",
      "\tspeed: 0.0143s/iter; left time: 578.2134s\n",
      "\titers: 700, epoch: 2 | loss: 0.0816992\n",
      "\tspeed: 0.0143s/iter; left time: 576.6102s\n",
      "\titers: 800, epoch: 2 | loss: 0.0563953\n",
      "\tspeed: 0.0143s/iter; left time: 576.3240s\n",
      "\titers: 900, epoch: 2 | loss: 0.0453718\n",
      "\tspeed: 0.0143s/iter; left time: 574.5756s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1969409\n",
      "\tspeed: 0.0143s/iter; left time: 572.4654s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1826231\n",
      "\tspeed: 0.0143s/iter; left time: 571.0188s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1227199\n",
      "\tspeed: 0.0143s/iter; left time: 569.5721s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1078722\n",
      "\tspeed: 0.0147s/iter; left time: 584.5907s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1170693\n",
      "\tspeed: 0.0143s/iter; left time: 567.1038s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0553412\n",
      "\tspeed: 0.0143s/iter; left time: 565.6764s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1090526\n",
      "\tspeed: 0.0143s/iter; left time: 564.1277s\n",
      "\titers: 1700, epoch: 2 | loss: 0.4210885\n",
      "\tspeed: 0.0143s/iter; left time: 562.4476s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1200532\n",
      "\tspeed: 0.0143s/iter; left time: 560.8553s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1354636\n",
      "\tspeed: 0.0143s/iter; left time: 559.2543s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0677640\n",
      "\tspeed: 0.0143s/iter; left time: 557.6635s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0688940\n",
      "\tspeed: 0.0143s/iter; left time: 556.6514s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1914614\n",
      "\tspeed: 0.0143s/iter; left time: 554.7551s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1282514\n",
      "\tspeed: 0.0143s/iter; left time: 553.4559s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1180599\n",
      "\tspeed: 0.0143s/iter; left time: 551.5005s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0794819\n",
      "\tspeed: 0.0143s/iter; left time: 550.1091s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2328810\n",
      "\tspeed: 0.0143s/iter; left time: 548.9451s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1043982\n",
      "\tspeed: 0.0143s/iter; left time: 547.3161s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0455933\n",
      "\tspeed: 0.0143s/iter; left time: 545.8316s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0651997\n",
      "\tspeed: 0.0143s/iter; left time: 544.3706s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0983221\n",
      "\tspeed: 0.0143s/iter; left time: 542.9704s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1045333\n",
      "\tspeed: 0.0143s/iter; left time: 541.2793s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1798421\n",
      "\tspeed: 0.0143s/iter; left time: 540.0264s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0568886\n",
      "\tspeed: 0.0143s/iter; left time: 538.6375s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0764247\n",
      "\tspeed: 0.0143s/iter; left time: 537.2343s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1899243\n",
      "\tspeed: 0.0143s/iter; left time: 535.8130s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1324636\n",
      "\tspeed: 0.0143s/iter; left time: 534.3476s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2148753\n",
      "\tspeed: 0.0143s/iter; left time: 532.7610s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1371614\n",
      "\tspeed: 0.0143s/iter; left time: 531.3737s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1989816\n",
      "\tspeed: 0.0143s/iter; left time: 530.0118s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2433214\n",
      "\tspeed: 0.0143s/iter; left time: 528.6828s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0918373\n",
      "\tspeed: 0.0143s/iter; left time: 527.1499s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0785700\n",
      "\tspeed: 0.0143s/iter; left time: 525.7181s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1248799\n",
      "\tspeed: 0.0143s/iter; left time: 524.4058s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1390696\n",
      "\tspeed: 0.0143s/iter; left time: 523.1050s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0406284\n",
      "\tspeed: 0.0143s/iter; left time: 521.3190s\n",
      "Epoch: 2 cost time: 65.74969339370728\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1521022 Vali Loss: 0.0345455 Test Loss: 0.1193821\n",
      "Validation loss decreased (0.038581 --> 0.034546).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0997194\n",
      "\tspeed: 0.1452s/iter; left time: 5277.0503s\n",
      "\titers: 200, epoch: 3 | loss: 0.0525237\n",
      "\tspeed: 0.0143s/iter; left time: 519.5690s\n",
      "\titers: 300, epoch: 3 | loss: 0.0964992\n",
      "\tspeed: 0.0144s/iter; left time: 518.6388s\n",
      "\titers: 400, epoch: 3 | loss: 0.1255521\n",
      "\tspeed: 0.0143s/iter; left time: 516.3454s\n",
      "\titers: 500, epoch: 3 | loss: 0.0975445\n",
      "\tspeed: 0.0143s/iter; left time: 514.8425s\n",
      "\titers: 600, epoch: 3 | loss: 0.1105032\n",
      "\tspeed: 0.0143s/iter; left time: 513.5152s\n",
      "\titers: 700, epoch: 3 | loss: 0.2843919\n",
      "\tspeed: 0.0143s/iter; left time: 512.1169s\n",
      "\titers: 800, epoch: 3 | loss: 0.1513453\n",
      "\tspeed: 0.0143s/iter; left time: 510.5021s\n",
      "\titers: 900, epoch: 3 | loss: 0.0568057\n",
      "\tspeed: 0.0143s/iter; left time: 508.8210s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0950229\n",
      "\tspeed: 0.0143s/iter; left time: 508.2896s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1467626\n",
      "\tspeed: 0.0143s/iter; left time: 506.5539s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1541756\n",
      "\tspeed: 0.0143s/iter; left time: 505.0213s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2895573\n",
      "\tspeed: 0.0143s/iter; left time: 503.4204s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1408771\n",
      "\tspeed: 0.0143s/iter; left time: 501.8663s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2322203\n",
      "\tspeed: 0.0143s/iter; left time: 500.2876s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1393859\n",
      "\tspeed: 0.0143s/iter; left time: 498.9596s\n",
      "\titers: 1700, epoch: 3 | loss: 0.6099322\n",
      "\tspeed: 0.0143s/iter; left time: 497.4339s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1088450\n",
      "\tspeed: 0.0143s/iter; left time: 495.9630s\n",
      "\titers: 1900, epoch: 3 | loss: 0.2049181\n",
      "\tspeed: 0.0143s/iter; left time: 494.4086s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2307991\n",
      "\tspeed: 0.0143s/iter; left time: 493.1587s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1079544\n",
      "\tspeed: 0.0143s/iter; left time: 491.8059s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0673332\n",
      "\tspeed: 0.0143s/iter; left time: 490.4366s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1312494\n",
      "\tspeed: 0.0143s/iter; left time: 487.9583s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1002727\n",
      "\tspeed: 0.0143s/iter; left time: 486.5263s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1334111\n",
      "\tspeed: 0.0143s/iter; left time: 484.9179s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1158223\n",
      "\tspeed: 0.0143s/iter; left time: 483.3841s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0509393\n",
      "\tspeed: 0.0143s/iter; left time: 482.1344s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1000815\n",
      "\tspeed: 0.0143s/iter; left time: 480.5944s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0876475\n",
      "\tspeed: 0.0143s/iter; left time: 479.1759s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0417903\n",
      "\tspeed: 0.0143s/iter; left time: 477.7813s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1384540\n",
      "\tspeed: 0.0143s/iter; left time: 476.3746s\n",
      "\titers: 3200, epoch: 3 | loss: 0.5688301\n",
      "\tspeed: 0.0143s/iter; left time: 474.6690s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0893088\n",
      "\tspeed: 0.0143s/iter; left time: 473.3560s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2971028\n",
      "\tspeed: 0.0143s/iter; left time: 471.9682s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0404648\n",
      "\tspeed: 0.0143s/iter; left time: 470.5366s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1201640\n",
      "\tspeed: 0.0143s/iter; left time: 469.1082s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1489238\n",
      "\tspeed: 0.0143s/iter; left time: 467.6312s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0727765\n",
      "\tspeed: 0.0143s/iter; left time: 466.3023s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1715078\n",
      "\tspeed: 0.0143s/iter; left time: 464.9800s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2004449\n",
      "\tspeed: 0.0143s/iter; left time: 463.8717s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2037411\n",
      "\tspeed: 0.0143s/iter; left time: 462.1247s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0610477\n",
      "\tspeed: 0.0143s/iter; left time: 460.5351s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0275362\n",
      "\tspeed: 0.0143s/iter; left time: 458.9889s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0782880\n",
      "\tspeed: 0.0143s/iter; left time: 457.6048s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0308673\n",
      "\tspeed: 0.0143s/iter; left time: 456.3043s\n",
      "Epoch: 3 cost time: 65.42899060249329\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1274727 Vali Loss: 0.0343468 Test Loss: 0.1166713\n",
      "Validation loss decreased (0.034546 --> 0.034347).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0576389\n",
      "\tspeed: 0.1495s/iter; left time: 4751.7935s\n",
      "\titers: 200, epoch: 4 | loss: 0.2752374\n",
      "\tspeed: 0.0143s/iter; left time: 453.4512s\n",
      "\titers: 300, epoch: 4 | loss: 0.0287914\n",
      "\tspeed: 0.0143s/iter; left time: 452.6230s\n",
      "\titers: 400, epoch: 4 | loss: 0.1751598\n",
      "\tspeed: 0.0143s/iter; left time: 451.0526s\n",
      "\titers: 500, epoch: 4 | loss: 0.1396465\n",
      "\tspeed: 0.0143s/iter; left time: 449.0816s\n",
      "\titers: 600, epoch: 4 | loss: 0.1032840\n",
      "\tspeed: 0.0143s/iter; left time: 447.8762s\n",
      "\titers: 700, epoch: 4 | loss: 0.0747593\n",
      "\tspeed: 0.0143s/iter; left time: 445.5066s\n",
      "\titers: 800, epoch: 4 | loss: 0.1653925\n",
      "\tspeed: 0.0143s/iter; left time: 444.7517s\n",
      "\titers: 900, epoch: 4 | loss: 0.1157264\n",
      "\tspeed: 0.0143s/iter; left time: 442.5873s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1395583\n",
      "\tspeed: 0.0143s/iter; left time: 441.8260s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1273917\n",
      "\tspeed: 0.0143s/iter; left time: 439.6216s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0411865\n",
      "\tspeed: 0.0143s/iter; left time: 438.7033s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0974761\n",
      "\tspeed: 0.0143s/iter; left time: 437.4123s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0506707\n",
      "\tspeed: 0.0143s/iter; left time: 435.7898s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1848618\n",
      "\tspeed: 0.0143s/iter; left time: 433.9786s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0760100\n",
      "\tspeed: 0.0143s/iter; left time: 432.4020s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1537352\n",
      "\tspeed: 0.0143s/iter; left time: 431.0256s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0287306\n",
      "\tspeed: 0.0143s/iter; left time: 429.7192s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0946563\n",
      "\tspeed: 0.0143s/iter; left time: 428.4916s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1493269\n",
      "\tspeed: 0.0143s/iter; left time: 426.9828s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0160322\n",
      "\tspeed: 0.0143s/iter; left time: 425.5181s\n",
      "\titers: 2200, epoch: 4 | loss: 0.2072097\n",
      "\tspeed: 0.0143s/iter; left time: 424.0185s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1288340\n",
      "\tspeed: 0.0143s/iter; left time: 422.4558s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1190259\n",
      "\tspeed: 0.0143s/iter; left time: 421.3674s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0509818\n",
      "\tspeed: 0.0143s/iter; left time: 419.7670s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0380869\n",
      "\tspeed: 0.0143s/iter; left time: 418.3455s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5603182\n",
      "\tspeed: 0.0143s/iter; left time: 416.8394s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0248226\n",
      "\tspeed: 0.0143s/iter; left time: 415.4482s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0351221\n",
      "\tspeed: 0.0143s/iter; left time: 414.0409s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1171454\n",
      "\tspeed: 0.0143s/iter; left time: 412.5089s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1115870\n",
      "\tspeed: 0.0143s/iter; left time: 411.2381s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1424984\n",
      "\tspeed: 0.0143s/iter; left time: 409.6695s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0709415\n",
      "\tspeed: 0.0143s/iter; left time: 408.2239s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0610360\n",
      "\tspeed: 0.0143s/iter; left time: 406.8197s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0664296\n",
      "\tspeed: 0.0143s/iter; left time: 405.3607s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0657415\n",
      "\tspeed: 0.0143s/iter; left time: 403.9640s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2103222\n",
      "\tspeed: 0.0143s/iter; left time: 402.5224s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0405048\n",
      "\tspeed: 0.0143s/iter; left time: 401.1661s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1097481\n",
      "\tspeed: 0.0143s/iter; left time: 399.7466s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0850392\n",
      "\tspeed: 0.0143s/iter; left time: 398.2493s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0461364\n",
      "\tspeed: 0.0143s/iter; left time: 396.7290s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1007793\n",
      "\tspeed: 0.0143s/iter; left time: 395.2295s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3017981\n",
      "\tspeed: 0.0143s/iter; left time: 393.8593s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0936708\n",
      "\tspeed: 0.0143s/iter; left time: 392.5531s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1077963\n",
      "\tspeed: 0.0143s/iter; left time: 391.1875s\n",
      "Epoch: 4 cost time: 65.37706017494202\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1103817 Vali Loss: 0.0338532 Test Loss: 0.1128421\n",
      "Validation loss decreased (0.034347 --> 0.033853).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0785104\n",
      "\tspeed: 0.1344s/iter; left time: 3660.3146s\n",
      "\titers: 200, epoch: 5 | loss: 0.0603779\n",
      "\tspeed: 0.0143s/iter; left time: 387.7020s\n",
      "\titers: 300, epoch: 5 | loss: 0.0673461\n",
      "\tspeed: 0.0143s/iter; left time: 385.7172s\n",
      "\titers: 400, epoch: 5 | loss: 0.0562438\n",
      "\tspeed: 0.0143s/iter; left time: 384.3370s\n",
      "\titers: 500, epoch: 5 | loss: 0.1110560\n",
      "\tspeed: 0.0143s/iter; left time: 382.9871s\n",
      "\titers: 600, epoch: 5 | loss: 0.0206359\n",
      "\tspeed: 0.0142s/iter; left time: 380.1928s\n",
      "\titers: 700, epoch: 5 | loss: 0.0403416\n",
      "\tspeed: 0.0142s/iter; left time: 378.4831s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935556\n",
      "\tspeed: 0.0142s/iter; left time: 377.7551s\n",
      "\titers: 900, epoch: 5 | loss: 0.0640305\n",
      "\tspeed: 0.0143s/iter; left time: 377.0929s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1415762\n",
      "\tspeed: 0.0143s/iter; left time: 376.4362s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0272753\n",
      "\tspeed: 0.0143s/iter; left time: 374.8492s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0947147\n",
      "\tspeed: 0.0143s/iter; left time: 373.3749s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0735672\n",
      "\tspeed: 0.0143s/iter; left time: 371.6714s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0991888\n",
      "\tspeed: 0.0143s/iter; left time: 370.2813s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1163013\n",
      "\tspeed: 0.0143s/iter; left time: 368.8423s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0541945\n",
      "\tspeed: 0.0143s/iter; left time: 367.2822s\n",
      "\titers: 1700, epoch: 5 | loss: 0.2281307\n",
      "\tspeed: 0.0143s/iter; left time: 366.1739s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1138795\n",
      "\tspeed: 0.0143s/iter; left time: 364.4841s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0326305\n",
      "\tspeed: 0.0143s/iter; left time: 363.2519s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5340424\n",
      "\tspeed: 0.0143s/iter; left time: 361.6265s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0946761\n",
      "\tspeed: 0.0143s/iter; left time: 360.2783s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0630995\n",
      "\tspeed: 0.0143s/iter; left time: 359.5449s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0433988\n",
      "\tspeed: 0.0143s/iter; left time: 357.3959s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0820433\n",
      "\tspeed: 0.0143s/iter; left time: 356.2648s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0811024\n",
      "\tspeed: 0.0143s/iter; left time: 354.5751s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0739033\n",
      "\tspeed: 0.0143s/iter; left time: 353.4176s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1003244\n",
      "\tspeed: 0.0143s/iter; left time: 351.7462s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0556785\n",
      "\tspeed: 0.0143s/iter; left time: 350.4890s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0445681\n",
      "\tspeed: 0.0143s/iter; left time: 348.8074s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0500327\n",
      "\tspeed: 0.0143s/iter; left time: 347.4248s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0351428\n",
      "\tspeed: 0.0143s/iter; left time: 345.7975s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0171011\n",
      "\tspeed: 0.0143s/iter; left time: 344.4504s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0872114\n",
      "\tspeed: 0.0143s/iter; left time: 343.0984s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0506429\n",
      "\tspeed: 0.0143s/iter; left time: 341.6187s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0529571\n",
      "\tspeed: 0.0143s/iter; left time: 340.2506s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0670077\n",
      "\tspeed: 0.0143s/iter; left time: 338.8575s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0593882\n",
      "\tspeed: 0.0143s/iter; left time: 337.8283s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0870268\n",
      "\tspeed: 0.0143s/iter; left time: 336.3720s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0545563\n",
      "\tspeed: 0.0143s/iter; left time: 334.2546s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0549904\n",
      "\tspeed: 0.0143s/iter; left time: 332.9030s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0207501\n",
      "\tspeed: 0.0143s/iter; left time: 331.4902s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0344409\n",
      "\tspeed: 0.0143s/iter; left time: 330.0174s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1144929\n",
      "\tspeed: 0.0143s/iter; left time: 328.5998s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0145858\n",
      "\tspeed: 0.0143s/iter; left time: 327.4190s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0490095\n",
      "\tspeed: 0.0143s/iter; left time: 326.8196s\n",
      "Epoch: 5 cost time: 65.31713509559631\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1001827 Vali Loss: 0.0331114 Test Loss: 0.1135699\n",
      "Validation loss decreased (0.033853 --> 0.033111).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0289830\n",
      "\tspeed: 0.1314s/iter; left time: 2980.0058s\n",
      "\titers: 200, epoch: 6 | loss: 0.1611261\n",
      "\tspeed: 0.0144s/iter; left time: 324.5843s\n",
      "\titers: 300, epoch: 6 | loss: 0.1635068\n",
      "\tspeed: 0.0144s/iter; left time: 323.6290s\n",
      "\titers: 400, epoch: 6 | loss: 0.0940145\n",
      "\tspeed: 0.0144s/iter; left time: 321.8925s\n",
      "\titers: 500, epoch: 6 | loss: 0.1440269\n",
      "\tspeed: 0.0143s/iter; left time: 319.1066s\n",
      "\titers: 600, epoch: 6 | loss: 0.1533241\n",
      "\tspeed: 0.0143s/iter; left time: 316.9516s\n",
      "\titers: 700, epoch: 6 | loss: 0.0402026\n",
      "\tspeed: 0.0143s/iter; left time: 315.2954s\n",
      "\titers: 800, epoch: 6 | loss: 0.0591984\n",
      "\tspeed: 0.0143s/iter; left time: 313.7216s\n",
      "\titers: 900, epoch: 6 | loss: 0.0640327\n",
      "\tspeed: 0.0143s/iter; left time: 312.3594s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1813027\n",
      "\tspeed: 0.0143s/iter; left time: 310.8926s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0330169\n",
      "\tspeed: 0.0143s/iter; left time: 309.9816s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1211472\n",
      "\tspeed: 0.0143s/iter; left time: 308.4745s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1289187\n",
      "\tspeed: 0.0143s/iter; left time: 306.8812s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1124306\n",
      "\tspeed: 0.0143s/iter; left time: 305.1833s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0523304\n",
      "\tspeed: 0.0143s/iter; left time: 304.0215s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1770798\n",
      "\tspeed: 0.0143s/iter; left time: 302.3167s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0525290\n",
      "\tspeed: 0.0143s/iter; left time: 301.0085s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2427282\n",
      "\tspeed: 0.0143s/iter; left time: 299.4717s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1466224\n",
      "\tspeed: 0.0143s/iter; left time: 297.9847s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1722656\n",
      "\tspeed: 0.0143s/iter; left time: 296.6157s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0973565\n",
      "\tspeed: 0.0143s/iter; left time: 295.0768s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0982651\n",
      "\tspeed: 0.0143s/iter; left time: 293.8191s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1670761\n",
      "\tspeed: 0.0143s/iter; left time: 292.3226s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0614178\n",
      "\tspeed: 0.0143s/iter; left time: 290.8919s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0990296\n",
      "\tspeed: 0.0143s/iter; left time: 289.4294s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0630544\n",
      "\tspeed: 0.0143s/iter; left time: 288.3516s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0764563\n",
      "\tspeed: 0.0143s/iter; left time: 286.5012s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0984251\n",
      "\tspeed: 0.0143s/iter; left time: 285.3941s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2259469\n",
      "\tspeed: 0.0143s/iter; left time: 283.7993s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0337761\n",
      "\tspeed: 0.0143s/iter; left time: 282.6019s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1085882\n",
      "\tspeed: 0.0143s/iter; left time: 280.7977s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1164428\n",
      "\tspeed: 0.0143s/iter; left time: 279.5493s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0689914\n",
      "\tspeed: 0.0143s/iter; left time: 278.0140s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0299434\n",
      "\tspeed: 0.0143s/iter; left time: 276.6437s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0147749\n",
      "\tspeed: 0.0143s/iter; left time: 275.1261s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0851746\n",
      "\tspeed: 0.0143s/iter; left time: 273.7501s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1055528\n",
      "\tspeed: 0.0143s/iter; left time: 272.2451s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1835547\n",
      "\tspeed: 0.0143s/iter; left time: 270.8564s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0599276\n",
      "\tspeed: 0.0143s/iter; left time: 269.3777s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1783794\n",
      "\tspeed: 0.0143s/iter; left time: 268.1476s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0205125\n",
      "\tspeed: 0.0143s/iter; left time: 266.5804s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0307689\n",
      "\tspeed: 0.0143s/iter; left time: 265.3740s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1395018\n",
      "\tspeed: 0.0143s/iter; left time: 263.6947s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1043912\n",
      "\tspeed: 0.0143s/iter; left time: 262.4060s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0789568\n",
      "\tspeed: 0.0143s/iter; left time: 260.8832s\n",
      "Epoch: 6 cost time: 65.37473559379578\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0953701 Vali Loss: 0.0329509 Test Loss: 0.1174688\n",
      "Validation loss decreased (0.033111 --> 0.032951).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1099629\n",
      "\tspeed: 0.1308s/iter; left time: 2370.7217s\n",
      "\titers: 200, epoch: 7 | loss: 0.1983022\n",
      "\tspeed: 0.0152s/iter; left time: 273.7080s\n",
      "\titers: 300, epoch: 7 | loss: 0.2044026\n",
      "\tspeed: 0.0143s/iter; left time: 256.2727s\n",
      "\titers: 400, epoch: 7 | loss: 0.0523858\n",
      "\tspeed: 0.0144s/iter; left time: 256.0533s\n",
      "\titers: 500, epoch: 7 | loss: 0.0523795\n",
      "\tspeed: 0.0144s/iter; left time: 254.6979s\n",
      "\titers: 600, epoch: 7 | loss: 0.0645101\n",
      "\tspeed: 0.0144s/iter; left time: 252.9191s\n",
      "\titers: 700, epoch: 7 | loss: 0.0158990\n",
      "\tspeed: 0.0144s/iter; left time: 251.8109s\n",
      "\titers: 800, epoch: 7 | loss: 0.0501764\n",
      "\tspeed: 0.0144s/iter; left time: 250.2673s\n",
      "\titers: 900, epoch: 7 | loss: 0.2272983\n",
      "\tspeed: 0.0144s/iter; left time: 249.3777s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0393277\n",
      "\tspeed: 0.0144s/iter; left time: 247.6218s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2189463\n",
      "\tspeed: 0.0144s/iter; left time: 245.8552s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0656718\n",
      "\tspeed: 0.0144s/iter; left time: 244.4109s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1074567\n",
      "\tspeed: 0.0144s/iter; left time: 242.9561s\n",
      "\titers: 1400, epoch: 7 | loss: 0.1027504\n",
      "\tspeed: 0.0144s/iter; left time: 241.6214s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0823208\n",
      "\tspeed: 0.0144s/iter; left time: 239.9467s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0757674\n",
      "\tspeed: 0.0144s/iter; left time: 238.6061s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0475362\n",
      "\tspeed: 0.0143s/iter; left time: 236.9655s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0532699\n",
      "\tspeed: 0.0144s/iter; left time: 235.7734s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0399863\n",
      "\tspeed: 0.0143s/iter; left time: 234.1857s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0485556\n",
      "\tspeed: 0.0143s/iter; left time: 232.6045s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0464956\n",
      "\tspeed: 0.0143s/iter; left time: 231.1354s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0618479\n",
      "\tspeed: 0.0150s/iter; left time: 239.5234s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0385197\n",
      "\tspeed: 0.0158s/iter; left time: 251.8087s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0497813\n",
      "\tspeed: 0.0158s/iter; left time: 250.0648s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1650084\n",
      "\tspeed: 0.0158s/iter; left time: 248.4805s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1035766\n",
      "\tspeed: 0.0158s/iter; left time: 246.8846s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0367910\n",
      "\tspeed: 0.0158s/iter; left time: 245.3347s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0380885\n",
      "\tspeed: 0.0158s/iter; left time: 243.7446s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0708064\n",
      "\tspeed: 0.0158s/iter; left time: 242.2300s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0722591\n",
      "\tspeed: 0.0158s/iter; left time: 240.7152s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0153503\n",
      "\tspeed: 0.0158s/iter; left time: 239.0031s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1690415\n",
      "\tspeed: 0.0158s/iter; left time: 237.4094s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0604177\n",
      "\tspeed: 0.0158s/iter; left time: 235.8014s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0354910\n",
      "\tspeed: 0.0158s/iter; left time: 234.2009s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0404593\n",
      "\tspeed: 0.0158s/iter; left time: 232.7060s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1022016\n",
      "\tspeed: 0.0158s/iter; left time: 231.0932s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1732755\n",
      "\tspeed: 0.0158s/iter; left time: 229.5772s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0473625\n",
      "\tspeed: 0.0158s/iter; left time: 227.8547s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0481395\n",
      "\tspeed: 0.0158s/iter; left time: 226.3479s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0492032\n",
      "\tspeed: 0.0158s/iter; left time: 224.7951s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0823483\n",
      "\tspeed: 0.0158s/iter; left time: 223.1469s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0353368\n",
      "\tspeed: 0.0158s/iter; left time: 221.6048s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0261534\n",
      "\tspeed: 0.0158s/iter; left time: 219.9722s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0425033\n",
      "\tspeed: 0.0158s/iter; left time: 218.3875s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0789986\n",
      "\tspeed: 0.0158s/iter; left time: 216.8263s\n",
      "Epoch: 7 cost time: 69.38067126274109\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0916367 Vali Loss: 0.0328634 Test Loss: 0.1175968\n",
      "Validation loss decreased (0.032951 --> 0.032863).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0646106\n",
      "\tspeed: 0.1352s/iter; left time: 1834.6909s\n",
      "\titers: 200, epoch: 8 | loss: 0.0365684\n",
      "\tspeed: 0.0143s/iter; left time: 193.0600s\n",
      "\titers: 300, epoch: 8 | loss: 0.1116534\n",
      "\tspeed: 0.0143s/iter; left time: 191.2947s\n",
      "\titers: 400, epoch: 8 | loss: 0.0461679\n",
      "\tspeed: 0.0143s/iter; left time: 190.0281s\n",
      "\titers: 500, epoch: 8 | loss: 0.2314401\n",
      "\tspeed: 0.0143s/iter; left time: 188.5825s\n",
      "\titers: 600, epoch: 8 | loss: 0.0839031\n",
      "\tspeed: 0.0143s/iter; left time: 187.0397s\n",
      "\titers: 700, epoch: 8 | loss: 0.1920541\n",
      "\tspeed: 0.0143s/iter; left time: 185.6111s\n",
      "\titers: 800, epoch: 8 | loss: 0.1616083\n",
      "\tspeed: 0.0143s/iter; left time: 184.0757s\n",
      "\titers: 900, epoch: 8 | loss: 0.0827668\n",
      "\tspeed: 0.0143s/iter; left time: 182.6983s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0735827\n",
      "\tspeed: 0.0143s/iter; left time: 181.2673s\n",
      "\titers: 1100, epoch: 8 | loss: 0.1058869\n",
      "\tspeed: 0.0143s/iter; left time: 179.7129s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1269293\n",
      "\tspeed: 0.0143s/iter; left time: 177.9508s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3649760\n",
      "\tspeed: 0.0143s/iter; left time: 176.4971s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0339007\n",
      "\tspeed: 0.0143s/iter; left time: 175.0933s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0779374\n",
      "\tspeed: 0.0143s/iter; left time: 173.5839s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1486876\n",
      "\tspeed: 0.0143s/iter; left time: 172.1622s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0225649\n",
      "\tspeed: 0.0143s/iter; left time: 170.7183s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1194144\n",
      "\tspeed: 0.0143s/iter; left time: 169.2803s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1004532\n",
      "\tspeed: 0.0143s/iter; left time: 167.9693s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1020600\n",
      "\tspeed: 0.0143s/iter; left time: 166.4173s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0428479\n",
      "\tspeed: 0.0143s/iter; left time: 165.0169s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0516911\n",
      "\tspeed: 0.0143s/iter; left time: 163.5652s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1383002\n",
      "\tspeed: 0.0143s/iter; left time: 162.1772s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0410548\n",
      "\tspeed: 0.0143s/iter; left time: 160.7337s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0609355\n",
      "\tspeed: 0.0143s/iter; left time: 159.3438s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0593496\n",
      "\tspeed: 0.0143s/iter; left time: 157.9571s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0326984\n",
      "\tspeed: 0.0143s/iter; left time: 156.4132s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0495661\n",
      "\tspeed: 0.0143s/iter; left time: 155.0118s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0984135\n",
      "\tspeed: 0.0143s/iter; left time: 153.5695s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2793420\n",
      "\tspeed: 0.0143s/iter; left time: 152.2271s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1072215\n",
      "\tspeed: 0.0143s/iter; left time: 150.8236s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1355446\n",
      "\tspeed: 0.0143s/iter; left time: 149.3909s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0680997\n",
      "\tspeed: 0.0143s/iter; left time: 147.9624s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0551747\n",
      "\tspeed: 0.0143s/iter; left time: 146.4959s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1548904\n",
      "\tspeed: 0.0143s/iter; left time: 145.1340s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0308465\n",
      "\tspeed: 0.0143s/iter; left time: 143.7071s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2271353\n",
      "\tspeed: 0.0143s/iter; left time: 142.2841s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0655918\n",
      "\tspeed: 0.0143s/iter; left time: 140.8678s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0590320\n",
      "\tspeed: 0.0143s/iter; left time: 139.3500s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0414677\n",
      "\tspeed: 0.0143s/iter; left time: 137.9710s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0348895\n",
      "\tspeed: 0.0143s/iter; left time: 136.4880s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0957698\n",
      "\tspeed: 0.0143s/iter; left time: 135.1330s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0716058\n",
      "\tspeed: 0.0143s/iter; left time: 133.5901s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1349273\n",
      "\tspeed: 0.0143s/iter; left time: 132.1605s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1439957\n",
      "\tspeed: 0.0143s/iter; left time: 130.7171s\n",
      "Epoch: 8 cost time: 65.31617093086243\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0920218 Vali Loss: 0.0331228 Test Loss: 0.1174829\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0535825\n",
      "\tspeed: 0.1360s/iter; left time: 1225.0575s\n",
      "\titers: 200, epoch: 9 | loss: 0.1106407\n",
      "\tspeed: 0.0143s/iter; left time: 127.4471s\n",
      "\titers: 300, epoch: 9 | loss: 0.0347949\n",
      "\tspeed: 0.0143s/iter; left time: 125.7545s\n",
      "\titers: 400, epoch: 9 | loss: 0.1138489\n",
      "\tspeed: 0.0143s/iter; left time: 124.3579s\n",
      "\titers: 500, epoch: 9 | loss: 0.0886443\n",
      "\tspeed: 0.0143s/iter; left time: 123.0225s\n",
      "\titers: 600, epoch: 9 | loss: 0.0302212\n",
      "\tspeed: 0.0143s/iter; left time: 121.4353s\n",
      "\titers: 700, epoch: 9 | loss: 0.0968511\n",
      "\tspeed: 0.0143s/iter; left time: 120.0051s\n",
      "\titers: 800, epoch: 9 | loss: 0.0317942\n",
      "\tspeed: 0.0143s/iter; left time: 119.0095s\n",
      "\titers: 900, epoch: 9 | loss: 0.0531709\n",
      "\tspeed: 0.0143s/iter; left time: 117.4276s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0905531\n",
      "\tspeed: 0.0143s/iter; left time: 116.0209s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0539447\n",
      "\tspeed: 0.0143s/iter; left time: 114.6267s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1935368\n",
      "\tspeed: 0.0143s/iter; left time: 113.2896s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0276913\n",
      "\tspeed: 0.0143s/iter; left time: 111.6949s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2232114\n",
      "\tspeed: 0.0143s/iter; left time: 110.3871s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0570007\n",
      "\tspeed: 0.0143s/iter; left time: 108.8264s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0444966\n",
      "\tspeed: 0.0143s/iter; left time: 107.3622s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0703715\n",
      "\tspeed: 0.0143s/iter; left time: 105.8747s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0350657\n",
      "\tspeed: 0.0143s/iter; left time: 104.3981s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0234423\n",
      "\tspeed: 0.0143s/iter; left time: 102.9729s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0483236\n",
      "\tspeed: 0.0143s/iter; left time: 101.6095s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0578080\n",
      "\tspeed: 0.0143s/iter; left time: 100.1264s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1185361\n",
      "\tspeed: 0.0143s/iter; left time: 98.7325s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1000079\n",
      "\tspeed: 0.0143s/iter; left time: 97.3038s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0301648\n",
      "\tspeed: 0.0143s/iter; left time: 95.9818s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0218247\n",
      "\tspeed: 0.0143s/iter; left time: 94.3904s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0637089\n",
      "\tspeed: 0.0143s/iter; left time: 92.8169s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0423516\n",
      "\tspeed: 0.0143s/iter; left time: 91.3935s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1393107\n",
      "\tspeed: 0.0143s/iter; left time: 90.0141s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0422159\n",
      "\tspeed: 0.0143s/iter; left time: 88.7536s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1203136\n",
      "\tspeed: 0.0143s/iter; left time: 87.3506s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0312540\n",
      "\tspeed: 0.0143s/iter; left time: 85.8569s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0746162\n",
      "\tspeed: 0.0143s/iter; left time: 84.4240s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2612969\n",
      "\tspeed: 0.0143s/iter; left time: 83.0256s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0971725\n",
      "\tspeed: 0.0143s/iter; left time: 81.6020s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0960424\n",
      "\tspeed: 0.0143s/iter; left time: 80.1719s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0499137\n",
      "\tspeed: 0.0143s/iter; left time: 78.7386s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0694873\n",
      "\tspeed: 0.0143s/iter; left time: 77.2831s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1424918\n",
      "\tspeed: 0.0143s/iter; left time: 75.9754s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0801451\n",
      "\tspeed: 0.0143s/iter; left time: 74.5074s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0869360\n",
      "\tspeed: 0.0143s/iter; left time: 73.0061s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0581198\n",
      "\tspeed: 0.0143s/iter; left time: 71.5716s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0757257\n",
      "\tspeed: 0.0143s/iter; left time: 70.2046s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0744628\n",
      "\tspeed: 0.0143s/iter; left time: 68.7490s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0329054\n",
      "\tspeed: 0.0143s/iter; left time: 67.3408s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0298722\n",
      "\tspeed: 0.0143s/iter; left time: 65.9141s\n",
      "Epoch: 9 cost time: 65.36292600631714\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0899772 Vali Loss: 0.0330352 Test Loss: 0.1174494\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0631996\n",
      "\tspeed: 0.1295s/iter; left time: 577.2407s\n",
      "\titers: 200, epoch: 10 | loss: 0.0888136\n",
      "\tspeed: 0.0159s/iter; left time: 69.3257s\n",
      "\titers: 300, epoch: 10 | loss: 0.0344665\n",
      "\tspeed: 0.0159s/iter; left time: 67.5430s\n",
      "\titers: 400, epoch: 10 | loss: 0.0750812\n",
      "\tspeed: 0.0159s/iter; left time: 66.1110s\n",
      "\titers: 500, epoch: 10 | loss: 0.0558594\n",
      "\tspeed: 0.0159s/iter; left time: 64.4422s\n",
      "\titers: 600, epoch: 10 | loss: 0.0876804\n",
      "\tspeed: 0.0159s/iter; left time: 62.7538s\n",
      "\titers: 700, epoch: 10 | loss: 0.0662050\n",
      "\tspeed: 0.0159s/iter; left time: 61.1817s\n",
      "\titers: 800, epoch: 10 | loss: 0.2021831\n",
      "\tspeed: 0.0159s/iter; left time: 59.6699s\n",
      "\titers: 900, epoch: 10 | loss: 0.0844504\n",
      "\tspeed: 0.0145s/iter; left time: 52.9133s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0459830\n",
      "\tspeed: 0.0143s/iter; left time: 50.9395s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0626340\n",
      "\tspeed: 0.0143s/iter; left time: 49.5095s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0457475\n",
      "\tspeed: 0.0143s/iter; left time: 48.0788s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0496145\n",
      "\tspeed: 0.0143s/iter; left time: 46.7031s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0630570\n",
      "\tspeed: 0.0143s/iter; left time: 45.1899s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2365202\n",
      "\tspeed: 0.0143s/iter; left time: 43.8011s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0952215\n",
      "\tspeed: 0.0143s/iter; left time: 42.3254s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0266585\n",
      "\tspeed: 0.0143s/iter; left time: 40.8968s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0776278\n",
      "\tspeed: 0.0143s/iter; left time: 39.4710s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1086182\n",
      "\tspeed: 0.0143s/iter; left time: 38.0548s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0596949\n",
      "\tspeed: 0.0143s/iter; left time: 36.5988s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0564876\n",
      "\tspeed: 0.0143s/iter; left time: 35.1929s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0702155\n",
      "\tspeed: 0.0143s/iter; left time: 33.7411s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1480383\n",
      "\tspeed: 0.0143s/iter; left time: 32.3225s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0790886\n",
      "\tspeed: 0.0143s/iter; left time: 30.8737s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2116390\n",
      "\tspeed: 0.0143s/iter; left time: 29.4410s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1041928\n",
      "\tspeed: 0.0143s/iter; left time: 28.0087s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0744508\n",
      "\tspeed: 0.0143s/iter; left time: 26.5689s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0650729\n",
      "\tspeed: 0.0143s/iter; left time: 25.1464s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0580915\n",
      "\tspeed: 0.0147s/iter; left time: 24.3770s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0443436\n",
      "\tspeed: 0.0159s/iter; left time: 24.6753s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0541247\n",
      "\tspeed: 0.0159s/iter; left time: 23.0914s\n",
      "\titers: 3200, epoch: 10 | loss: 0.1113902\n",
      "\tspeed: 0.0159s/iter; left time: 21.5048s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0853350\n",
      "\tspeed: 0.0159s/iter; left time: 19.9094s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1607841\n",
      "\tspeed: 0.0159s/iter; left time: 18.3290s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0213557\n",
      "\tspeed: 0.0159s/iter; left time: 16.7460s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1540724\n",
      "\tspeed: 0.0159s/iter; left time: 15.1643s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0910978\n",
      "\tspeed: 0.0159s/iter; left time: 13.5769s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0684156\n",
      "\tspeed: 0.0159s/iter; left time: 11.9873s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0224562\n",
      "\tspeed: 0.0159s/iter; left time: 10.4025s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0542614\n",
      "\tspeed: 0.0159s/iter; left time: 8.8168s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1472813\n",
      "\tspeed: 0.0159s/iter; left time: 7.2323s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0513240\n",
      "\tspeed: 0.0159s/iter; left time: 5.6456s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0382041\n",
      "\tspeed: 0.0159s/iter; left time: 4.0579s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0476256\n",
      "\tspeed: 0.0159s/iter; left time: 2.4738s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1135630\n",
      "\tspeed: 0.0159s/iter; left time: 0.8881s\n",
      "Epoch: 10 cost time: 69.3731861114502\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0902528 Vali Loss: 0.0330214 Test Loss: 0.1175839\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11762779206037521, mae:0.20591463148593903\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1750453\n",
      "\tspeed: 0.0273s/iter; left time: 1238.7531s\n",
      "\titers: 200, epoch: 1 | loss: 0.0986526\n",
      "\tspeed: 0.0156s/iter; left time: 706.0369s\n",
      "\titers: 300, epoch: 1 | loss: 0.0693140\n",
      "\tspeed: 0.0155s/iter; left time: 703.4079s\n",
      "\titers: 400, epoch: 1 | loss: 0.1302993\n",
      "\tspeed: 0.0156s/iter; left time: 702.8105s\n",
      "\titers: 500, epoch: 1 | loss: 0.0357760\n",
      "\tspeed: 0.0156s/iter; left time: 702.8763s\n",
      "\titers: 600, epoch: 1 | loss: 0.0710588\n",
      "\tspeed: 0.0155s/iter; left time: 697.9676s\n",
      "\titers: 700, epoch: 1 | loss: 0.1424736\n",
      "\tspeed: 0.0156s/iter; left time: 698.1219s\n",
      "\titers: 800, epoch: 1 | loss: 0.4935403\n",
      "\tspeed: 0.0155s/iter; left time: 695.8727s\n",
      "\titers: 900, epoch: 1 | loss: 0.2640402\n",
      "\tspeed: 0.0155s/iter; left time: 694.2717s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0780762\n",
      "\tspeed: 0.0155s/iter; left time: 691.9762s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0592716\n",
      "\tspeed: 0.0155s/iter; left time: 690.8274s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0613589\n",
      "\tspeed: 0.0155s/iter; left time: 687.8953s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2604851\n",
      "\tspeed: 0.0156s/iter; left time: 688.2734s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0300129\n",
      "\tspeed: 0.0155s/iter; left time: 685.6376s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1288587\n",
      "\tspeed: 0.0155s/iter; left time: 683.6889s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1958280\n",
      "\tspeed: 0.0155s/iter; left time: 682.4619s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4338458\n",
      "\tspeed: 0.0155s/iter; left time: 680.6714s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0614051\n",
      "\tspeed: 0.0155s/iter; left time: 679.1872s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1824192\n",
      "\tspeed: 0.0155s/iter; left time: 677.5985s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3970298\n",
      "\tspeed: 0.0155s/iter; left time: 675.7840s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1622045\n",
      "\tspeed: 0.0155s/iter; left time: 674.0204s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3438289\n",
      "\tspeed: 0.0155s/iter; left time: 672.9882s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1691433\n",
      "\tspeed: 0.0154s/iter; left time: 666.0893s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1882255\n",
      "\tspeed: 0.0141s/iter; left time: 607.7791s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2796445\n",
      "\tspeed: 0.0141s/iter; left time: 606.5369s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1148230\n",
      "\tspeed: 0.0141s/iter; left time: 605.3044s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1674909\n",
      "\tspeed: 0.0141s/iter; left time: 603.7495s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1961166\n",
      "\tspeed: 0.0141s/iter; left time: 602.2684s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2504083\n",
      "\tspeed: 0.0141s/iter; left time: 600.8555s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0981979\n",
      "\tspeed: 0.0141s/iter; left time: 599.3926s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0790760\n",
      "\tspeed: 0.0141s/iter; left time: 598.2619s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2204227\n",
      "\tspeed: 0.0141s/iter; left time: 596.6071s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0367366\n",
      "\tspeed: 0.0141s/iter; left time: 595.2122s\n",
      "\titers: 3400, epoch: 1 | loss: 0.3772850\n",
      "\tspeed: 0.0141s/iter; left time: 593.9599s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0860394\n",
      "\tspeed: 0.0141s/iter; left time: 592.2408s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1037896\n",
      "\tspeed: 0.0141s/iter; left time: 590.9085s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0836873\n",
      "\tspeed: 0.0141s/iter; left time: 589.3422s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1524573\n",
      "\tspeed: 0.0141s/iter; left time: 587.9608s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3999090\n",
      "\tspeed: 0.0141s/iter; left time: 586.8800s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2680612\n",
      "\tspeed: 0.0141s/iter; left time: 585.3624s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0791855\n",
      "\tspeed: 0.0141s/iter; left time: 584.0378s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1020011\n",
      "\tspeed: 0.0141s/iter; left time: 582.1432s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1590880\n",
      "\tspeed: 0.0141s/iter; left time: 580.9169s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0435403\n",
      "\tspeed: 0.0141s/iter; left time: 579.6114s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0729997\n",
      "\tspeed: 0.0150s/iter; left time: 614.5700s\n",
      "Epoch: 1 cost time: 68.87987685203552\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1798450 Vali Loss: 0.0385564 Test Loss: 0.1259200\n",
      "Validation loss decreased (inf --> 0.038556).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1084619\n",
      "\tspeed: 0.1261s/iter; left time: 5157.0979s\n",
      "\titers: 200, epoch: 2 | loss: 0.0454620\n",
      "\tspeed: 0.0140s/iter; left time: 572.2402s\n",
      "\titers: 300, epoch: 2 | loss: 0.0867445\n",
      "\tspeed: 0.0140s/iter; left time: 570.7214s\n",
      "\titers: 400, epoch: 2 | loss: 0.1204490\n",
      "\tspeed: 0.0140s/iter; left time: 569.7304s\n",
      "\titers: 500, epoch: 2 | loss: 0.2254681\n",
      "\tspeed: 0.0140s/iter; left time: 567.4216s\n",
      "\titers: 600, epoch: 2 | loss: 0.1112931\n",
      "\tspeed: 0.0140s/iter; left time: 566.9524s\n",
      "\titers: 700, epoch: 2 | loss: 0.0973362\n",
      "\tspeed: 0.0140s/iter; left time: 565.1715s\n",
      "\titers: 800, epoch: 2 | loss: 0.0474080\n",
      "\tspeed: 0.0140s/iter; left time: 563.6216s\n",
      "\titers: 900, epoch: 2 | loss: 0.0510080\n",
      "\tspeed: 0.0140s/iter; left time: 562.1647s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1940967\n",
      "\tspeed: 0.0140s/iter; left time: 560.5084s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2158491\n",
      "\tspeed: 0.0140s/iter; left time: 559.4570s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1620346\n",
      "\tspeed: 0.0140s/iter; left time: 557.7214s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1147979\n",
      "\tspeed: 0.0140s/iter; left time: 556.1113s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1281408\n",
      "\tspeed: 0.0140s/iter; left time: 554.5911s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0488797\n",
      "\tspeed: 0.0140s/iter; left time: 553.5387s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0824104\n",
      "\tspeed: 0.0140s/iter; left time: 552.3202s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2018268\n",
      "\tspeed: 0.0140s/iter; left time: 550.6679s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0919800\n",
      "\tspeed: 0.0140s/iter; left time: 548.9606s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1202603\n",
      "\tspeed: 0.0140s/iter; left time: 548.0784s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0692922\n",
      "\tspeed: 0.0140s/iter; left time: 546.2978s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0546693\n",
      "\tspeed: 0.0140s/iter; left time: 544.8657s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1325690\n",
      "\tspeed: 0.0160s/iter; left time: 619.5807s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1530341\n",
      "\tspeed: 0.0161s/iter; left time: 622.6709s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1340312\n",
      "\tspeed: 0.0161s/iter; left time: 621.0650s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0621151\n",
      "\tspeed: 0.0161s/iter; left time: 619.8177s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1817942\n",
      "\tspeed: 0.0161s/iter; left time: 616.5780s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1099020\n",
      "\tspeed: 0.0161s/iter; left time: 615.2762s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0707040\n",
      "\tspeed: 0.0159s/iter; left time: 606.0636s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0805482\n",
      "\tspeed: 0.0156s/iter; left time: 592.6202s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1291864\n",
      "\tspeed: 0.0156s/iter; left time: 591.0648s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0454051\n",
      "\tspeed: 0.0156s/iter; left time: 589.8660s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1567767\n",
      "\tspeed: 0.0156s/iter; left time: 588.1854s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0504156\n",
      "\tspeed: 0.0153s/iter; left time: 577.8977s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0766628\n",
      "\tspeed: 0.0140s/iter; left time: 526.3940s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1834736\n",
      "\tspeed: 0.0140s/iter; left time: 525.3975s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1622138\n",
      "\tspeed: 0.0140s/iter; left time: 523.6589s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1311818\n",
      "\tspeed: 0.0140s/iter; left time: 522.7544s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1210568\n",
      "\tspeed: 0.0140s/iter; left time: 521.0851s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2345525\n",
      "\tspeed: 0.0140s/iter; left time: 519.4750s\n",
      "\titers: 4000, epoch: 2 | loss: 0.3807611\n",
      "\tspeed: 0.0140s/iter; left time: 518.1774s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1053217\n",
      "\tspeed: 0.0140s/iter; left time: 516.8941s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1064705\n",
      "\tspeed: 0.0140s/iter; left time: 515.2552s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1125944\n",
      "\tspeed: 0.0140s/iter; left time: 513.7825s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1500449\n",
      "\tspeed: 0.0140s/iter; left time: 512.3070s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0639796\n",
      "\tspeed: 0.0140s/iter; left time: 511.0893s\n",
      "Epoch: 2 cost time: 66.24019026756287\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1435090 Vali Loss: 0.0360805 Test Loss: 0.1242945\n",
      "Validation loss decreased (0.038556 --> 0.036080).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1127197\n",
      "\tspeed: 0.1409s/iter; left time: 5121.1030s\n",
      "\titers: 200, epoch: 3 | loss: 0.0349977\n",
      "\tspeed: 0.0156s/iter; left time: 565.5911s\n",
      "\titers: 300, epoch: 3 | loss: 0.0958721\n",
      "\tspeed: 0.0146s/iter; left time: 527.3061s\n",
      "\titers: 400, epoch: 3 | loss: 0.0667603\n",
      "\tspeed: 0.0142s/iter; left time: 511.0726s\n",
      "\titers: 500, epoch: 3 | loss: 0.0830596\n",
      "\tspeed: 0.0142s/iter; left time: 508.9147s\n",
      "\titers: 600, epoch: 3 | loss: 0.1104254\n",
      "\tspeed: 0.0141s/iter; left time: 506.9095s\n",
      "\titers: 700, epoch: 3 | loss: 0.2730089\n",
      "\tspeed: 0.0141s/iter; left time: 505.6100s\n",
      "\titers: 800, epoch: 3 | loss: 0.1221922\n",
      "\tspeed: 0.0142s/iter; left time: 504.9003s\n",
      "\titers: 900, epoch: 3 | loss: 0.0698011\n",
      "\tspeed: 0.0142s/iter; left time: 503.0352s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0932118\n",
      "\tspeed: 0.0141s/iter; left time: 501.4173s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0853688\n",
      "\tspeed: 0.0141s/iter; left time: 499.8975s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1320890\n",
      "\tspeed: 0.0141s/iter; left time: 498.6022s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2777387\n",
      "\tspeed: 0.0142s/iter; left time: 497.5679s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1123153\n",
      "\tspeed: 0.0142s/iter; left time: 496.1841s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2199660\n",
      "\tspeed: 0.0142s/iter; left time: 494.4672s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1516347\n",
      "\tspeed: 0.0141s/iter; left time: 492.9609s\n",
      "\titers: 1700, epoch: 3 | loss: 0.3804110\n",
      "\tspeed: 0.0142s/iter; left time: 491.8288s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0781511\n",
      "\tspeed: 0.0141s/iter; left time: 489.9783s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1622395\n",
      "\tspeed: 0.0141s/iter; left time: 488.5386s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2034501\n",
      "\tspeed: 0.0142s/iter; left time: 487.4241s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0788254\n",
      "\tspeed: 0.0142s/iter; left time: 485.9311s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0639623\n",
      "\tspeed: 0.0142s/iter; left time: 484.6280s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1209723\n",
      "\tspeed: 0.0141s/iter; left time: 483.0303s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1469334\n",
      "\tspeed: 0.0141s/iter; left time: 481.4789s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1264428\n",
      "\tspeed: 0.0142s/iter; left time: 480.3243s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0313640\n",
      "\tspeed: 0.0141s/iter; left time: 478.5217s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0427104\n",
      "\tspeed: 0.0146s/iter; left time: 491.6308s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0939123\n",
      "\tspeed: 0.0156s/iter; left time: 523.9317s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0605273\n",
      "\tspeed: 0.0156s/iter; left time: 522.7632s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0471309\n",
      "\tspeed: 0.0155s/iter; left time: 516.9645s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1472992\n",
      "\tspeed: 0.0141s/iter; left time: 471.3820s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2923927\n",
      "\tspeed: 0.0141s/iter; left time: 470.0775s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0755630\n",
      "\tspeed: 0.0141s/iter; left time: 468.7299s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1332128\n",
      "\tspeed: 0.0141s/iter; left time: 467.2005s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0291694\n",
      "\tspeed: 0.0141s/iter; left time: 465.8480s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1216315\n",
      "\tspeed: 0.0141s/iter; left time: 464.2650s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0922807\n",
      "\tspeed: 0.0141s/iter; left time: 462.8793s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0714464\n",
      "\tspeed: 0.0141s/iter; left time: 461.3644s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1765684\n",
      "\tspeed: 0.0141s/iter; left time: 460.3016s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1284116\n",
      "\tspeed: 0.0141s/iter; left time: 458.7977s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1956069\n",
      "\tspeed: 0.0141s/iter; left time: 457.3176s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0650602\n",
      "\tspeed: 0.0141s/iter; left time: 456.0260s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0283783\n",
      "\tspeed: 0.0141s/iter; left time: 454.5372s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0794239\n",
      "\tspeed: 0.0141s/iter; left time: 453.0895s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0271310\n",
      "\tspeed: 0.0141s/iter; left time: 451.5710s\n",
      "Epoch: 3 cost time: 65.48156809806824\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1117065 Vali Loss: 0.0327506 Test Loss: 0.1157892\n",
      "Validation loss decreased (0.036080 --> 0.032751).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0694353\n",
      "\tspeed: 0.1244s/iter; left time: 3955.0969s\n",
      "\titers: 200, epoch: 4 | loss: 0.2536150\n",
      "\tspeed: 0.0142s/iter; left time: 448.4222s\n",
      "\titers: 300, epoch: 4 | loss: 0.0392176\n",
      "\tspeed: 0.0142s/iter; left time: 447.5641s\n",
      "\titers: 400, epoch: 4 | loss: 0.1150045\n",
      "\tspeed: 0.0142s/iter; left time: 445.7799s\n",
      "\titers: 500, epoch: 4 | loss: 0.0615782\n",
      "\tspeed: 0.0142s/iter; left time: 444.4393s\n",
      "\titers: 600, epoch: 4 | loss: 0.1069522\n",
      "\tspeed: 0.0142s/iter; left time: 442.9383s\n",
      "\titers: 700, epoch: 4 | loss: 0.1127013\n",
      "\tspeed: 0.0142s/iter; left time: 441.2832s\n",
      "\titers: 800, epoch: 4 | loss: 0.1477412\n",
      "\tspeed: 0.0142s/iter; left time: 440.6289s\n",
      "\titers: 900, epoch: 4 | loss: 0.0798545\n",
      "\tspeed: 0.0142s/iter; left time: 438.9140s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1506715\n",
      "\tspeed: 0.0142s/iter; left time: 437.0660s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0802182\n",
      "\tspeed: 0.0142s/iter; left time: 435.7186s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0664146\n",
      "\tspeed: 0.0142s/iter; left time: 434.5215s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0803368\n",
      "\tspeed: 0.0142s/iter; left time: 433.0880s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0496724\n",
      "\tspeed: 0.0142s/iter; left time: 431.6276s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2626966\n",
      "\tspeed: 0.0142s/iter; left time: 430.2352s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0849125\n",
      "\tspeed: 0.0141s/iter; left time: 428.3492s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1040561\n",
      "\tspeed: 0.0141s/iter; left time: 427.0108s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0233057\n",
      "\tspeed: 0.0142s/iter; left time: 425.7982s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0372018\n",
      "\tspeed: 0.0142s/iter; left time: 424.3931s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1155394\n",
      "\tspeed: 0.0142s/iter; left time: 423.0622s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0230513\n",
      "\tspeed: 0.0142s/iter; left time: 421.5464s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1827137\n",
      "\tspeed: 0.0142s/iter; left time: 420.0851s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1269017\n",
      "\tspeed: 0.0142s/iter; left time: 418.6721s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0962540\n",
      "\tspeed: 0.0142s/iter; left time: 417.2588s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0248383\n",
      "\tspeed: 0.0142s/iter; left time: 415.9376s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0576897\n",
      "\tspeed: 0.0142s/iter; left time: 414.4039s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5715809\n",
      "\tspeed: 0.0142s/iter; left time: 413.0000s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0196525\n",
      "\tspeed: 0.0141s/iter; left time: 411.4419s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0338509\n",
      "\tspeed: 0.0142s/iter; left time: 410.1775s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1261639\n",
      "\tspeed: 0.0141s/iter; left time: 408.5357s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0930139\n",
      "\tspeed: 0.0142s/iter; left time: 407.4086s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1146367\n",
      "\tspeed: 0.0141s/iter; left time: 405.8668s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0792971\n",
      "\tspeed: 0.0142s/iter; left time: 404.5151s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0708127\n",
      "\tspeed: 0.0142s/iter; left time: 403.1703s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0343159\n",
      "\tspeed: 0.0142s/iter; left time: 401.8563s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0482372\n",
      "\tspeed: 0.0141s/iter; left time: 400.2085s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1357945\n",
      "\tspeed: 0.0142s/iter; left time: 398.9635s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0493133\n",
      "\tspeed: 0.0142s/iter; left time: 397.5411s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0908506\n",
      "\tspeed: 0.0142s/iter; left time: 396.0789s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0717595\n",
      "\tspeed: 0.0141s/iter; left time: 394.5647s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0392473\n",
      "\tspeed: 0.0141s/iter; left time: 392.9788s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0571737\n",
      "\tspeed: 0.0141s/iter; left time: 391.6658s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2541614\n",
      "\tspeed: 0.0141s/iter; left time: 390.2534s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1019018\n",
      "\tspeed: 0.0142s/iter; left time: 389.1013s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0682563\n",
      "\tspeed: 0.0141s/iter; left time: 387.4591s\n",
      "Epoch: 4 cost time: 64.70969343185425\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0936633 Vali Loss: 0.0315414 Test Loss: 0.1140375\n",
      "Validation loss decreased (0.032751 --> 0.031541).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0413422\n",
      "\tspeed: 0.1248s/iter; left time: 3397.1252s\n",
      "\titers: 200, epoch: 5 | loss: 0.0818925\n",
      "\tspeed: 0.0141s/iter; left time: 381.4522s\n",
      "\titers: 300, epoch: 5 | loss: 0.0684421\n",
      "\tspeed: 0.0141s/iter; left time: 380.1174s\n",
      "\titers: 400, epoch: 5 | loss: 0.0359451\n",
      "\tspeed: 0.0141s/iter; left time: 378.5992s\n",
      "\titers: 500, epoch: 5 | loss: 0.1169416\n",
      "\tspeed: 0.0141s/iter; left time: 377.0412s\n",
      "\titers: 600, epoch: 5 | loss: 0.0232839\n",
      "\tspeed: 0.0141s/iter; left time: 375.6439s\n",
      "\titers: 700, epoch: 5 | loss: 0.0647462\n",
      "\tspeed: 0.0141s/iter; left time: 374.3267s\n",
      "\titers: 800, epoch: 5 | loss: 0.0709734\n",
      "\tspeed: 0.0141s/iter; left time: 373.0174s\n",
      "\titers: 900, epoch: 5 | loss: 0.0345730\n",
      "\tspeed: 0.0141s/iter; left time: 371.4675s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1239711\n",
      "\tspeed: 0.0140s/iter; left time: 369.8382s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0356770\n",
      "\tspeed: 0.0141s/iter; left time: 368.6946s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1087995\n",
      "\tspeed: 0.0140s/iter; left time: 366.9886s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0727367\n",
      "\tspeed: 0.0140s/iter; left time: 365.6247s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0736063\n",
      "\tspeed: 0.0141s/iter; left time: 364.5640s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1336058\n",
      "\tspeed: 0.0141s/iter; left time: 363.0822s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0620684\n",
      "\tspeed: 0.0141s/iter; left time: 361.7979s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1242147\n",
      "\tspeed: 0.0141s/iter; left time: 360.2744s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0738285\n",
      "\tspeed: 0.0141s/iter; left time: 358.8706s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0302699\n",
      "\tspeed: 0.0141s/iter; left time: 357.5637s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5253404\n",
      "\tspeed: 0.0141s/iter; left time: 356.0539s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0542336\n",
      "\tspeed: 0.0141s/iter; left time: 354.6930s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0362175\n",
      "\tspeed: 0.0141s/iter; left time: 353.8094s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0311085\n",
      "\tspeed: 0.0141s/iter; left time: 352.0133s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0869372\n",
      "\tspeed: 0.0141s/iter; left time: 350.6186s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0726962\n",
      "\tspeed: 0.0141s/iter; left time: 348.9712s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0443350\n",
      "\tspeed: 0.0141s/iter; left time: 347.6245s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0865461\n",
      "\tspeed: 0.0141s/iter; left time: 346.4843s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0853011\n",
      "\tspeed: 0.0141s/iter; left time: 345.1979s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0755313\n",
      "\tspeed: 0.0141s/iter; left time: 343.5807s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0338696\n",
      "\tspeed: 0.0141s/iter; left time: 342.0238s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0368937\n",
      "\tspeed: 0.0141s/iter; left time: 340.6365s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0192847\n",
      "\tspeed: 0.0141s/iter; left time: 339.1057s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0663951\n",
      "\tspeed: 0.0141s/iter; left time: 337.8330s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0780979\n",
      "\tspeed: 0.0141s/iter; left time: 336.3099s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0400156\n",
      "\tspeed: 0.0141s/iter; left time: 335.0029s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0894562\n",
      "\tspeed: 0.0141s/iter; left time: 333.5515s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0557314\n",
      "\tspeed: 0.0141s/iter; left time: 332.2471s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1218853\n",
      "\tspeed: 0.0141s/iter; left time: 330.7871s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0491604\n",
      "\tspeed: 0.0141s/iter; left time: 329.7679s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0466786\n",
      "\tspeed: 0.0141s/iter; left time: 328.2061s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0269382\n",
      "\tspeed: 0.0141s/iter; left time: 326.5647s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0488806\n",
      "\tspeed: 0.0141s/iter; left time: 325.1494s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1023692\n",
      "\tspeed: 0.0141s/iter; left time: 323.5947s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0168385\n",
      "\tspeed: 0.0141s/iter; left time: 322.3443s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0587420\n",
      "\tspeed: 0.0141s/iter; left time: 321.0253s\n",
      "Epoch: 5 cost time: 64.26018190383911\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0851896 Vali Loss: 0.0325436 Test Loss: 0.1158307\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0325069\n",
      "\tspeed: 0.1245s/iter; left time: 2823.5776s\n",
      "\titers: 200, epoch: 6 | loss: 0.1262978\n",
      "\tspeed: 0.0141s/iter; left time: 318.2494s\n",
      "\titers: 300, epoch: 6 | loss: 0.0947527\n",
      "\tspeed: 0.0141s/iter; left time: 316.7381s\n",
      "\titers: 400, epoch: 6 | loss: 0.0830778\n",
      "\tspeed: 0.0152s/iter; left time: 339.0241s\n",
      "\titers: 500, epoch: 6 | loss: 0.0918976\n",
      "\tspeed: 0.0162s/iter; left time: 360.5529s\n",
      "\titers: 600, epoch: 6 | loss: 0.1526328\n",
      "\tspeed: 0.0162s/iter; left time: 358.5313s\n",
      "\titers: 700, epoch: 6 | loss: 0.0318347\n",
      "\tspeed: 0.0162s/iter; left time: 357.1218s\n",
      "\titers: 800, epoch: 6 | loss: 0.0659793\n",
      "\tspeed: 0.0162s/iter; left time: 355.2881s\n",
      "\titers: 900, epoch: 6 | loss: 0.0426042\n",
      "\tspeed: 0.0162s/iter; left time: 353.6084s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1309268\n",
      "\tspeed: 0.0162s/iter; left time: 352.0383s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0334980\n",
      "\tspeed: 0.0162s/iter; left time: 350.4129s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0603199\n",
      "\tspeed: 0.0162s/iter; left time: 348.7221s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1014504\n",
      "\tspeed: 0.0162s/iter; left time: 347.1913s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0505050\n",
      "\tspeed: 0.0162s/iter; left time: 345.5928s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0776042\n",
      "\tspeed: 0.0162s/iter; left time: 343.8900s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1313531\n",
      "\tspeed: 0.0159s/iter; left time: 336.4272s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0501765\n",
      "\tspeed: 0.0153s/iter; left time: 322.4724s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1875056\n",
      "\tspeed: 0.0162s/iter; left time: 339.4181s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1033079\n",
      "\tspeed: 0.0162s/iter; left time: 337.7979s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1389752\n",
      "\tspeed: 0.0162s/iter; left time: 336.2057s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1033946\n",
      "\tspeed: 0.0162s/iter; left time: 334.6845s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1036058\n",
      "\tspeed: 0.0162s/iter; left time: 332.9595s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0978907\n",
      "\tspeed: 0.0162s/iter; left time: 331.4444s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0535899\n",
      "\tspeed: 0.0162s/iter; left time: 329.8384s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0456146\n",
      "\tspeed: 0.0162s/iter; left time: 328.3209s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0584717\n",
      "\tspeed: 0.0162s/iter; left time: 326.6932s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0662157\n",
      "\tspeed: 0.0162s/iter; left time: 325.2005s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0763167\n",
      "\tspeed: 0.0162s/iter; left time: 323.5874s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0953692\n",
      "\tspeed: 0.0162s/iter; left time: 322.0887s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0272528\n",
      "\tspeed: 0.0162s/iter; left time: 320.2790s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0548477\n",
      "\tspeed: 0.0162s/iter; left time: 318.6871s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0522385\n",
      "\tspeed: 0.0156s/iter; left time: 304.7555s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0645773\n",
      "\tspeed: 0.0141s/iter; left time: 274.7208s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0235564\n",
      "\tspeed: 0.0141s/iter; left time: 273.3026s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0109253\n",
      "\tspeed: 0.0141s/iter; left time: 271.9480s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0894742\n",
      "\tspeed: 0.0141s/iter; left time: 270.5162s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0941710\n",
      "\tspeed: 0.0141s/iter; left time: 268.9632s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1207255\n",
      "\tspeed: 0.0141s/iter; left time: 267.6093s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0381158\n",
      "\tspeed: 0.0141s/iter; left time: 266.2711s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0858604\n",
      "\tspeed: 0.0141s/iter; left time: 264.8101s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0139142\n",
      "\tspeed: 0.0141s/iter; left time: 263.3040s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0356113\n",
      "\tspeed: 0.0141s/iter; left time: 261.9740s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1043971\n",
      "\tspeed: 0.0141s/iter; left time: 260.6832s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0624197\n",
      "\tspeed: 0.0141s/iter; left time: 259.0878s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0674458\n",
      "\tspeed: 0.0141s/iter; left time: 257.6038s\n",
      "Epoch: 6 cost time: 70.2037844657898\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0817312 Vali Loss: 0.0320862 Test Loss: 0.1143577\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0805206\n",
      "\tspeed: 0.1247s/iter; left time: 2260.0819s\n",
      "\titers: 200, epoch: 7 | loss: 0.1106957\n",
      "\tspeed: 0.0141s/iter; left time: 253.7714s\n",
      "\titers: 300, epoch: 7 | loss: 0.1473908\n",
      "\tspeed: 0.0141s/iter; left time: 252.4086s\n",
      "\titers: 400, epoch: 7 | loss: 0.0644306\n",
      "\tspeed: 0.0141s/iter; left time: 251.0207s\n",
      "\titers: 500, epoch: 7 | loss: 0.0351286\n",
      "\tspeed: 0.0141s/iter; left time: 249.4939s\n",
      "\titers: 600, epoch: 7 | loss: 0.0507353\n",
      "\tspeed: 0.0141s/iter; left time: 248.0820s\n",
      "\titers: 700, epoch: 7 | loss: 0.0232976\n",
      "\tspeed: 0.0141s/iter; left time: 246.6630s\n",
      "\titers: 800, epoch: 7 | loss: 0.0555681\n",
      "\tspeed: 0.0141s/iter; left time: 245.2745s\n",
      "\titers: 900, epoch: 7 | loss: 0.1560813\n",
      "\tspeed: 0.0141s/iter; left time: 243.8272s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0276720\n",
      "\tspeed: 0.0141s/iter; left time: 242.4000s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1840050\n",
      "\tspeed: 0.0141s/iter; left time: 240.9758s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0463805\n",
      "\tspeed: 0.0141s/iter; left time: 239.5916s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0815297\n",
      "\tspeed: 0.0141s/iter; left time: 238.2307s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0965929\n",
      "\tspeed: 0.0141s/iter; left time: 236.7998s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1013292\n",
      "\tspeed: 0.0141s/iter; left time: 235.4452s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0438934\n",
      "\tspeed: 0.0141s/iter; left time: 234.0159s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0436664\n",
      "\tspeed: 0.0141s/iter; left time: 232.5264s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0541283\n",
      "\tspeed: 0.0141s/iter; left time: 231.3521s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0263246\n",
      "\tspeed: 0.0141s/iter; left time: 230.0110s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0462369\n",
      "\tspeed: 0.0141s/iter; left time: 228.9002s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0580633\n",
      "\tspeed: 0.0141s/iter; left time: 227.3570s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0383869\n",
      "\tspeed: 0.0141s/iter; left time: 225.5199s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0495071\n",
      "\tspeed: 0.0141s/iter; left time: 224.1541s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0527458\n",
      "\tspeed: 0.0141s/iter; left time: 223.1383s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0646458\n",
      "\tspeed: 0.0141s/iter; left time: 221.6653s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0607015\n",
      "\tspeed: 0.0141s/iter; left time: 220.3339s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0285556\n",
      "\tspeed: 0.0141s/iter; left time: 218.8532s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0314296\n",
      "\tspeed: 0.0141s/iter; left time: 217.1355s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0751430\n",
      "\tspeed: 0.0141s/iter; left time: 215.6280s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0600111\n",
      "\tspeed: 0.0141s/iter; left time: 214.1923s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0205084\n",
      "\tspeed: 0.0141s/iter; left time: 213.0089s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1222879\n",
      "\tspeed: 0.0141s/iter; left time: 211.7167s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0561661\n",
      "\tspeed: 0.0141s/iter; left time: 210.0842s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0345161\n",
      "\tspeed: 0.0141s/iter; left time: 208.9880s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0358787\n",
      "\tspeed: 0.0141s/iter; left time: 207.3525s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0444971\n",
      "\tspeed: 0.0141s/iter; left time: 205.8822s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1544201\n",
      "\tspeed: 0.0141s/iter; left time: 204.9055s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0277495\n",
      "\tspeed: 0.0141s/iter; left time: 203.2493s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0450809\n",
      "\tspeed: 0.0141s/iter; left time: 201.6636s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0490745\n",
      "\tspeed: 0.0141s/iter; left time: 200.3345s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0659482\n",
      "\tspeed: 0.0141s/iter; left time: 198.9141s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0262400\n",
      "\tspeed: 0.0141s/iter; left time: 197.5826s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0208854\n",
      "\tspeed: 0.0141s/iter; left time: 196.3064s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0193085\n",
      "\tspeed: 0.0141s/iter; left time: 194.7165s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0992975\n",
      "\tspeed: 0.0141s/iter; left time: 193.1785s\n",
      "Epoch: 7 cost time: 64.39640736579895\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0787467 Vali Loss: 0.0318654 Test Loss: 0.1146283\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11406522244215012, mae:0.20758919417858124\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --predictor \"solar_forecast,wind_forecast,total_load\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1835497\n",
      "\tspeed: 0.0264s/iter; left time: 1198.4606s\n",
      "\titers: 200, epoch: 1 | loss: 0.1153142\n",
      "\tspeed: 0.0146s/iter; left time: 663.2480s\n",
      "\titers: 300, epoch: 1 | loss: 0.1916097\n",
      "\tspeed: 0.0146s/iter; left time: 660.8086s\n",
      "\titers: 400, epoch: 1 | loss: 0.1894591\n",
      "\tspeed: 0.0146s/iter; left time: 660.4658s\n",
      "\titers: 500, epoch: 1 | loss: 0.1693326\n",
      "\tspeed: 0.0146s/iter; left time: 658.7704s\n",
      "\titers: 600, epoch: 1 | loss: 0.1171933\n",
      "\tspeed: 0.0146s/iter; left time: 656.8764s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451544\n",
      "\tspeed: 0.0146s/iter; left time: 654.3513s\n",
      "\titers: 800, epoch: 1 | loss: 0.1126527\n",
      "\tspeed: 0.0146s/iter; left time: 652.3474s\n",
      "\titers: 900, epoch: 1 | loss: 0.1244560\n",
      "\tspeed: 0.0146s/iter; left time: 651.1774s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1651168\n",
      "\tspeed: 0.0146s/iter; left time: 649.8275s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1482705\n",
      "\tspeed: 0.0146s/iter; left time: 647.8670s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1323465\n",
      "\tspeed: 0.0146s/iter; left time: 645.7034s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2229661\n",
      "\tspeed: 0.0146s/iter; left time: 644.5905s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1464737\n",
      "\tspeed: 0.0146s/iter; left time: 642.8415s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0776521\n",
      "\tspeed: 0.0146s/iter; left time: 642.5232s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1507899\n",
      "\tspeed: 0.0146s/iter; left time: 640.8956s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1011101\n",
      "\tspeed: 0.0146s/iter; left time: 639.5151s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1450867\n",
      "\tspeed: 0.0146s/iter; left time: 637.4890s\n",
      "\titers: 1900, epoch: 1 | loss: 0.3305567\n",
      "\tspeed: 0.0146s/iter; left time: 635.9728s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1160365\n",
      "\tspeed: 0.0146s/iter; left time: 634.8197s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1068159\n",
      "\tspeed: 0.0146s/iter; left time: 632.7674s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1328086\n",
      "\tspeed: 0.0146s/iter; left time: 631.2765s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1062998\n",
      "\tspeed: 0.0146s/iter; left time: 629.5844s\n",
      "\titers: 2400, epoch: 1 | loss: 0.3595074\n",
      "\tspeed: 0.0146s/iter; left time: 628.8541s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1744194\n",
      "\tspeed: 0.0146s/iter; left time: 626.9015s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1474980\n",
      "\tspeed: 0.0146s/iter; left time: 625.9682s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0937831\n",
      "\tspeed: 0.0146s/iter; left time: 624.1589s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2184623\n",
      "\tspeed: 0.0146s/iter; left time: 622.6268s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1529506\n",
      "\tspeed: 0.0146s/iter; left time: 620.9815s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0590975\n",
      "\tspeed: 0.0146s/iter; left time: 619.8301s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0855486\n",
      "\tspeed: 0.0146s/iter; left time: 617.9233s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1507652\n",
      "\tspeed: 0.0146s/iter; left time: 616.6483s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1902263\n",
      "\tspeed: 0.0146s/iter; left time: 614.8790s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1578428\n",
      "\tspeed: 0.0146s/iter; left time: 613.4785s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1384976\n",
      "\tspeed: 0.0145s/iter; left time: 611.6783s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1221287\n",
      "\tspeed: 0.0146s/iter; left time: 610.5782s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1167372\n",
      "\tspeed: 0.0146s/iter; left time: 609.0919s\n",
      "\titers: 3800, epoch: 1 | loss: 0.2298422\n",
      "\tspeed: 0.0146s/iter; left time: 607.6163s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1026639\n",
      "\tspeed: 0.0145s/iter; left time: 605.7216s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1765046\n",
      "\tspeed: 0.0145s/iter; left time: 604.2763s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1406819\n",
      "\tspeed: 0.0146s/iter; left time: 603.1854s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1274727\n",
      "\tspeed: 0.0146s/iter; left time: 601.9448s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1364025\n",
      "\tspeed: 0.0146s/iter; left time: 600.3125s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1974474\n",
      "\tspeed: 0.0146s/iter; left time: 598.7863s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0846185\n",
      "\tspeed: 0.0146s/iter; left time: 597.3376s\n",
      "Epoch: 1 cost time: 67.59556317329407\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1706725 Vali Loss: 0.0736029 Test Loss: 0.0914102\n",
      "Validation loss decreased (inf --> 0.073603).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1685341\n",
      "\tspeed: 0.1324s/iter; left time: 5413.1299s\n",
      "\titers: 200, epoch: 2 | loss: 0.1408903\n",
      "\tspeed: 0.0146s/iter; left time: 593.9326s\n",
      "\titers: 300, epoch: 2 | loss: 0.1952439\n",
      "\tspeed: 0.0146s/iter; left time: 592.5243s\n",
      "\titers: 400, epoch: 2 | loss: 0.1452914\n",
      "\tspeed: 0.0146s/iter; left time: 590.8355s\n",
      "\titers: 500, epoch: 2 | loss: 0.1664198\n",
      "\tspeed: 0.0146s/iter; left time: 589.8673s\n",
      "\titers: 600, epoch: 2 | loss: 0.1740848\n",
      "\tspeed: 0.0146s/iter; left time: 589.8943s\n",
      "\titers: 700, epoch: 2 | loss: 0.2172345\n",
      "\tspeed: 0.0146s/iter; left time: 588.4361s\n",
      "\titers: 800, epoch: 2 | loss: 0.0882750\n",
      "\tspeed: 0.0146s/iter; left time: 586.8439s\n",
      "\titers: 900, epoch: 2 | loss: 0.1143197\n",
      "\tspeed: 0.0146s/iter; left time: 585.2427s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1582641\n",
      "\tspeed: 0.0146s/iter; left time: 584.8263s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1356126\n",
      "\tspeed: 0.0146s/iter; left time: 583.0111s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1227969\n",
      "\tspeed: 0.0146s/iter; left time: 582.0174s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1380577\n",
      "\tspeed: 0.0146s/iter; left time: 579.9620s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0903574\n",
      "\tspeed: 0.0146s/iter; left time: 578.1408s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2111163\n",
      "\tspeed: 0.0146s/iter; left time: 576.3701s\n",
      "\titers: 1600, epoch: 2 | loss: 0.2305768\n",
      "\tspeed: 0.0146s/iter; left time: 575.1953s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2302151\n",
      "\tspeed: 0.0146s/iter; left time: 574.4916s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1184028\n",
      "\tspeed: 0.0146s/iter; left time: 572.8590s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0814160\n",
      "\tspeed: 0.0146s/iter; left time: 571.6548s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1361259\n",
      "\tspeed: 0.0146s/iter; left time: 569.8594s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1527939\n",
      "\tspeed: 0.0146s/iter; left time: 567.7813s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0811446\n",
      "\tspeed: 0.0146s/iter; left time: 566.4914s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1963743\n",
      "\tspeed: 0.0146s/iter; left time: 564.6233s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1024969\n",
      "\tspeed: 0.0146s/iter; left time: 563.4531s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0811993\n",
      "\tspeed: 0.0146s/iter; left time: 562.2528s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1305210\n",
      "\tspeed: 0.0146s/iter; left time: 560.9304s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0777922\n",
      "\tspeed: 0.0146s/iter; left time: 559.0705s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1406435\n",
      "\tspeed: 0.0146s/iter; left time: 557.8961s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1390175\n",
      "\tspeed: 0.0146s/iter; left time: 556.2891s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1143527\n",
      "\tspeed: 0.0146s/iter; left time: 554.5319s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1569093\n",
      "\tspeed: 0.0146s/iter; left time: 553.0674s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2626752\n",
      "\tspeed: 0.0146s/iter; left time: 551.6456s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1914656\n",
      "\tspeed: 0.0146s/iter; left time: 550.1949s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1442455\n",
      "\tspeed: 0.0146s/iter; left time: 548.8203s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0962215\n",
      "\tspeed: 0.0146s/iter; left time: 547.4519s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0784452\n",
      "\tspeed: 0.0146s/iter; left time: 545.9458s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1026973\n",
      "\tspeed: 0.0146s/iter; left time: 544.4996s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0938692\n",
      "\tspeed: 0.0146s/iter; left time: 542.9421s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1268843\n",
      "\tspeed: 0.0146s/iter; left time: 541.3725s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1141753\n",
      "\tspeed: 0.0146s/iter; left time: 539.8855s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1039462\n",
      "\tspeed: 0.0146s/iter; left time: 538.4136s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1155951\n",
      "\tspeed: 0.0146s/iter; left time: 537.0909s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1503512\n",
      "\tspeed: 0.0146s/iter; left time: 535.7993s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1256710\n",
      "\tspeed: 0.0146s/iter; left time: 534.2114s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0616412\n",
      "\tspeed: 0.0146s/iter; left time: 532.6742s\n",
      "Epoch: 2 cost time: 66.71718764305115\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1416156 Vali Loss: 0.0705284 Test Loss: 0.0882831\n",
      "Validation loss decreased (0.073603 --> 0.070528).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2076444\n",
      "\tspeed: 0.1288s/iter; left time: 4679.9905s\n",
      "\titers: 200, epoch: 3 | loss: 0.1206173\n",
      "\tspeed: 0.0146s/iter; left time: 530.5893s\n",
      "\titers: 300, epoch: 3 | loss: 0.1300245\n",
      "\tspeed: 0.0146s/iter; left time: 529.2263s\n",
      "\titers: 400, epoch: 3 | loss: 0.1121544\n",
      "\tspeed: 0.0146s/iter; left time: 527.0922s\n",
      "\titers: 500, epoch: 3 | loss: 0.1187190\n",
      "\tspeed: 0.0147s/iter; left time: 527.3673s\n",
      "\titers: 600, epoch: 3 | loss: 0.1955557\n",
      "\tspeed: 0.0147s/iter; left time: 525.5522s\n",
      "\titers: 700, epoch: 3 | loss: 0.0896333\n",
      "\tspeed: 0.0147s/iter; left time: 524.3398s\n",
      "\titers: 800, epoch: 3 | loss: 0.1293100\n",
      "\tspeed: 0.0147s/iter; left time: 522.2519s\n",
      "\titers: 900, epoch: 3 | loss: 0.1069431\n",
      "\tspeed: 0.0146s/iter; left time: 520.6575s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1430923\n",
      "\tspeed: 0.0147s/iter; left time: 519.4898s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1434563\n",
      "\tspeed: 0.0147s/iter; left time: 517.7603s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0989282\n",
      "\tspeed: 0.0146s/iter; left time: 515.9130s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1284610\n",
      "\tspeed: 0.0147s/iter; left time: 515.2703s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1228089\n",
      "\tspeed: 0.0146s/iter; left time: 513.2560s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1053697\n",
      "\tspeed: 0.0146s/iter; left time: 511.5090s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1538387\n",
      "\tspeed: 0.0146s/iter; left time: 510.1792s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0847474\n",
      "\tspeed: 0.0146s/iter; left time: 508.7775s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1803394\n",
      "\tspeed: 0.0146s/iter; left time: 506.8666s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0738180\n",
      "\tspeed: 0.0146s/iter; left time: 505.3336s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1080962\n",
      "\tspeed: 0.0146s/iter; left time: 503.9992s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0741946\n",
      "\tspeed: 0.0146s/iter; left time: 502.9877s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0929136\n",
      "\tspeed: 0.0146s/iter; left time: 501.1450s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1887815\n",
      "\tspeed: 0.0146s/iter; left time: 500.1353s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1330007\n",
      "\tspeed: 0.0146s/iter; left time: 498.4399s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0777819\n",
      "\tspeed: 0.0146s/iter; left time: 496.7583s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1148708\n",
      "\tspeed: 0.0146s/iter; left time: 495.0413s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0894905\n",
      "\tspeed: 0.0146s/iter; left time: 493.7835s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1278787\n",
      "\tspeed: 0.0146s/iter; left time: 492.1154s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1174078\n",
      "\tspeed: 0.0146s/iter; left time: 490.7364s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1663690\n",
      "\tspeed: 0.0146s/iter; left time: 489.5100s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1134720\n",
      "\tspeed: 0.0146s/iter; left time: 488.0121s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0919659\n",
      "\tspeed: 0.0147s/iter; left time: 487.1766s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1346410\n",
      "\tspeed: 0.0146s/iter; left time: 485.2274s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1435977\n",
      "\tspeed: 0.0146s/iter; left time: 483.8655s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1649803\n",
      "\tspeed: 0.0146s/iter; left time: 481.9596s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1071629\n",
      "\tspeed: 0.0146s/iter; left time: 480.5473s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1634591\n",
      "\tspeed: 0.0146s/iter; left time: 479.0389s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0894507\n",
      "\tspeed: 0.0146s/iter; left time: 477.7691s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0702681\n",
      "\tspeed: 0.0146s/iter; left time: 476.4016s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1284213\n",
      "\tspeed: 0.0146s/iter; left time: 474.9486s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0869000\n",
      "\tspeed: 0.0146s/iter; left time: 473.5670s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0891982\n",
      "\tspeed: 0.0146s/iter; left time: 471.8729s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1132093\n",
      "\tspeed: 0.0146s/iter; left time: 470.1210s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1695403\n",
      "\tspeed: 0.0146s/iter; left time: 468.6657s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1036632\n",
      "\tspeed: 0.0146s/iter; left time: 467.2384s\n",
      "Epoch: 3 cost time: 66.93058323860168\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1243915 Vali Loss: 0.0660020 Test Loss: 0.0876061\n",
      "Validation loss decreased (0.070528 --> 0.066002).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2082417\n",
      "\tspeed: 0.1378s/iter; left time: 4380.1156s\n",
      "\titers: 200, epoch: 4 | loss: 0.0879470\n",
      "\tspeed: 0.0146s/iter; left time: 463.4772s\n",
      "\titers: 300, epoch: 4 | loss: 0.0844748\n",
      "\tspeed: 0.0146s/iter; left time: 461.7110s\n",
      "\titers: 400, epoch: 4 | loss: 0.0571169\n",
      "\tspeed: 0.0146s/iter; left time: 459.8605s\n",
      "\titers: 500, epoch: 4 | loss: 0.1313982\n",
      "\tspeed: 0.0146s/iter; left time: 459.0274s\n",
      "\titers: 600, epoch: 4 | loss: 0.0538558\n",
      "\tspeed: 0.0146s/iter; left time: 457.0620s\n",
      "\titers: 700, epoch: 4 | loss: 0.0985864\n",
      "\tspeed: 0.0146s/iter; left time: 456.3896s\n",
      "\titers: 800, epoch: 4 | loss: 0.0826372\n",
      "\tspeed: 0.0146s/iter; left time: 454.6549s\n",
      "\titers: 900, epoch: 4 | loss: 0.1546814\n",
      "\tspeed: 0.0146s/iter; left time: 453.1626s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1964880\n",
      "\tspeed: 0.0146s/iter; left time: 451.4425s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0809737\n",
      "\tspeed: 0.0146s/iter; left time: 450.1504s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0824177\n",
      "\tspeed: 0.0146s/iter; left time: 448.7387s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1521258\n",
      "\tspeed: 0.0146s/iter; left time: 446.9120s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1076525\n",
      "\tspeed: 0.0146s/iter; left time: 445.2163s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1242884\n",
      "\tspeed: 0.0146s/iter; left time: 443.7902s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1456586\n",
      "\tspeed: 0.0146s/iter; left time: 442.0796s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1359025\n",
      "\tspeed: 0.0146s/iter; left time: 440.8826s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1271009\n",
      "\tspeed: 0.0146s/iter; left time: 439.3997s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1163095\n",
      "\tspeed: 0.0146s/iter; left time: 437.8775s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0998775\n",
      "\tspeed: 0.0146s/iter; left time: 436.6855s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1244938\n",
      "\tspeed: 0.0146s/iter; left time: 434.1573s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1313602\n",
      "\tspeed: 0.0146s/iter; left time: 432.6230s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1287825\n",
      "\tspeed: 0.0146s/iter; left time: 431.2892s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0953010\n",
      "\tspeed: 0.0146s/iter; left time: 429.7387s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0837687\n",
      "\tspeed: 0.0146s/iter; left time: 428.1990s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0952145\n",
      "\tspeed: 0.0146s/iter; left time: 426.6588s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0418774\n",
      "\tspeed: 0.0146s/iter; left time: 425.1878s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1078189\n",
      "\tspeed: 0.0146s/iter; left time: 423.6735s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1114410\n",
      "\tspeed: 0.0146s/iter; left time: 422.3450s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1259274\n",
      "\tspeed: 0.0146s/iter; left time: 420.7508s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1017504\n",
      "\tspeed: 0.0146s/iter; left time: 419.3068s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0830042\n",
      "\tspeed: 0.0146s/iter; left time: 417.7449s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1550459\n",
      "\tspeed: 0.0146s/iter; left time: 416.4347s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1215862\n",
      "\tspeed: 0.0146s/iter; left time: 414.9070s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0617131\n",
      "\tspeed: 0.0146s/iter; left time: 413.4944s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0773357\n",
      "\tspeed: 0.0146s/iter; left time: 412.0447s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2605830\n",
      "\tspeed: 0.0146s/iter; left time: 410.5139s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1460643\n",
      "\tspeed: 0.0146s/iter; left time: 409.1815s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0593700\n",
      "\tspeed: 0.0146s/iter; left time: 407.6804s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0639021\n",
      "\tspeed: 0.0146s/iter; left time: 406.1830s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0791090\n",
      "\tspeed: 0.0146s/iter; left time: 404.8926s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1184223\n",
      "\tspeed: 0.0146s/iter; left time: 403.2624s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1046427\n",
      "\tspeed: 0.0146s/iter; left time: 401.8235s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1162956\n",
      "\tspeed: 0.0146s/iter; left time: 400.3052s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0984423\n",
      "\tspeed: 0.0146s/iter; left time: 398.9708s\n",
      "Epoch: 4 cost time: 66.69370436668396\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1151214 Vali Loss: 0.0669468 Test Loss: 0.0887885\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1009906\n",
      "\tspeed: 0.1432s/iter; left time: 3899.5092s\n",
      "\titers: 200, epoch: 5 | loss: 0.0777524\n",
      "\tspeed: 0.0146s/iter; left time: 395.1943s\n",
      "\titers: 300, epoch: 5 | loss: 0.1353675\n",
      "\tspeed: 0.0146s/iter; left time: 393.3751s\n",
      "\titers: 400, epoch: 5 | loss: 0.0968147\n",
      "\tspeed: 0.0146s/iter; left time: 392.1103s\n",
      "\titers: 500, epoch: 5 | loss: 0.0950768\n",
      "\tspeed: 0.0146s/iter; left time: 390.4915s\n",
      "\titers: 600, epoch: 5 | loss: 0.1142523\n",
      "\tspeed: 0.0145s/iter; left time: 388.6379s\n",
      "\titers: 700, epoch: 5 | loss: 0.0550596\n",
      "\tspeed: 0.0145s/iter; left time: 387.3279s\n",
      "\titers: 800, epoch: 5 | loss: 0.1476153\n",
      "\tspeed: 0.0145s/iter; left time: 385.8031s\n",
      "\titers: 900, epoch: 5 | loss: 0.0811148\n",
      "\tspeed: 0.0145s/iter; left time: 384.3281s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0831191\n",
      "\tspeed: 0.0145s/iter; left time: 382.8378s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1377074\n",
      "\tspeed: 0.0145s/iter; left time: 381.3256s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1203765\n",
      "\tspeed: 0.0145s/iter; left time: 379.7614s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1446262\n",
      "\tspeed: 0.0145s/iter; left time: 378.5670s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1223935\n",
      "\tspeed: 0.0145s/iter; left time: 376.9528s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0934269\n",
      "\tspeed: 0.0145s/iter; left time: 375.4244s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1294899\n",
      "\tspeed: 0.0145s/iter; left time: 374.0602s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0639271\n",
      "\tspeed: 0.0145s/iter; left time: 372.7117s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0859756\n",
      "\tspeed: 0.0145s/iter; left time: 371.1996s\n",
      "\titers: 1900, epoch: 5 | loss: 0.2059999\n",
      "\tspeed: 0.0145s/iter; left time: 369.7190s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0671764\n",
      "\tspeed: 0.0145s/iter; left time: 368.1049s\n",
      "\titers: 2100, epoch: 5 | loss: 0.2054187\n",
      "\tspeed: 0.0145s/iter; left time: 366.7534s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0821730\n",
      "\tspeed: 0.0145s/iter; left time: 365.3101s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0920942\n",
      "\tspeed: 0.0145s/iter; left time: 363.9301s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1508584\n",
      "\tspeed: 0.0145s/iter; left time: 362.5463s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0996743\n",
      "\tspeed: 0.0145s/iter; left time: 360.9803s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1005093\n",
      "\tspeed: 0.0145s/iter; left time: 359.5808s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0800282\n",
      "\tspeed: 0.0145s/iter; left time: 358.3204s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1182793\n",
      "\tspeed: 0.0145s/iter; left time: 356.8446s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0986310\n",
      "\tspeed: 0.0145s/iter; left time: 355.2558s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0844812\n",
      "\tspeed: 0.0145s/iter; left time: 353.8896s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1785090\n",
      "\tspeed: 0.0145s/iter; left time: 352.4337s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1082306\n",
      "\tspeed: 0.0145s/iter; left time: 350.9963s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0951147\n",
      "\tspeed: 0.0145s/iter; left time: 349.5833s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1191685\n",
      "\tspeed: 0.0145s/iter; left time: 348.1213s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0991600\n",
      "\tspeed: 0.0145s/iter; left time: 346.6401s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0785601\n",
      "\tspeed: 0.0145s/iter; left time: 345.2201s\n",
      "\titers: 3700, epoch: 5 | loss: 0.1352019\n",
      "\tspeed: 0.0145s/iter; left time: 343.5914s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0827713\n",
      "\tspeed: 0.0146s/iter; left time: 342.3844s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1440006\n",
      "\tspeed: 0.0145s/iter; left time: 340.6463s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0901359\n",
      "\tspeed: 0.0145s/iter; left time: 339.2940s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0983312\n",
      "\tspeed: 0.0145s/iter; left time: 337.7468s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0835969\n",
      "\tspeed: 0.0145s/iter; left time: 336.3140s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1077833\n",
      "\tspeed: 0.0145s/iter; left time: 334.9898s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0909086\n",
      "\tspeed: 0.0145s/iter; left time: 333.4883s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1491358\n",
      "\tspeed: 0.0145s/iter; left time: 332.0044s\n",
      "Epoch: 5 cost time: 66.47137379646301\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1099477 Vali Loss: 0.0653416 Test Loss: 0.0872289\n",
      "Validation loss decreased (0.066002 --> 0.065342).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0840790\n",
      "\tspeed: 0.1298s/iter; left time: 2943.9917s\n",
      "\titers: 200, epoch: 6 | loss: 0.0682715\n",
      "\tspeed: 0.0146s/iter; left time: 329.6407s\n",
      "\titers: 300, epoch: 6 | loss: 0.0777930\n",
      "\tspeed: 0.0146s/iter; left time: 328.6880s\n",
      "\titers: 400, epoch: 6 | loss: 0.1050574\n",
      "\tspeed: 0.0146s/iter; left time: 326.9632s\n",
      "\titers: 500, epoch: 6 | loss: 0.1035563\n",
      "\tspeed: 0.0146s/iter; left time: 325.0494s\n",
      "\titers: 600, epoch: 6 | loss: 0.1057217\n",
      "\tspeed: 0.0146s/iter; left time: 323.7168s\n",
      "\titers: 700, epoch: 6 | loss: 0.0925243\n",
      "\tspeed: 0.0146s/iter; left time: 322.3698s\n",
      "\titers: 800, epoch: 6 | loss: 0.0897140\n",
      "\tspeed: 0.0146s/iter; left time: 320.7172s\n",
      "\titers: 900, epoch: 6 | loss: 0.0808573\n",
      "\tspeed: 0.0146s/iter; left time: 319.3661s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0709313\n",
      "\tspeed: 0.0146s/iter; left time: 317.8064s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1130559\n",
      "\tspeed: 0.0146s/iter; left time: 316.1923s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0651264\n",
      "\tspeed: 0.0146s/iter; left time: 314.8628s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1058737\n",
      "\tspeed: 0.0146s/iter; left time: 313.3778s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0639276\n",
      "\tspeed: 0.0146s/iter; left time: 311.9391s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0786646\n",
      "\tspeed: 0.0146s/iter; left time: 310.3800s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1710585\n",
      "\tspeed: 0.0146s/iter; left time: 308.9671s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1003297\n",
      "\tspeed: 0.0146s/iter; left time: 307.4128s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0898466\n",
      "\tspeed: 0.0146s/iter; left time: 306.2633s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0957313\n",
      "\tspeed: 0.0146s/iter; left time: 304.6844s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1510548\n",
      "\tspeed: 0.0146s/iter; left time: 303.1275s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1156729\n",
      "\tspeed: 0.0146s/iter; left time: 301.5979s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0716318\n",
      "\tspeed: 0.0146s/iter; left time: 300.2423s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0978160\n",
      "\tspeed: 0.0146s/iter; left time: 298.7342s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0787583\n",
      "\tspeed: 0.0146s/iter; left time: 297.2689s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0632978\n",
      "\tspeed: 0.0146s/iter; left time: 295.7819s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0600352\n",
      "\tspeed: 0.0146s/iter; left time: 294.4907s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1230741\n",
      "\tspeed: 0.0146s/iter; left time: 292.8999s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0512644\n",
      "\tspeed: 0.0146s/iter; left time: 291.4080s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0788324\n",
      "\tspeed: 0.0146s/iter; left time: 289.9185s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0699454\n",
      "\tspeed: 0.0146s/iter; left time: 288.3640s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1063982\n",
      "\tspeed: 0.0146s/iter; left time: 286.9267s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1146473\n",
      "\tspeed: 0.0146s/iter; left time: 285.3935s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1368030\n",
      "\tspeed: 0.0146s/iter; left time: 284.0259s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0741549\n",
      "\tspeed: 0.0146s/iter; left time: 282.6152s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0589319\n",
      "\tspeed: 0.0146s/iter; left time: 280.8675s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1086436\n",
      "\tspeed: 0.0146s/iter; left time: 279.4327s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1096720\n",
      "\tspeed: 0.0146s/iter; left time: 277.9825s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1756727\n",
      "\tspeed: 0.0146s/iter; left time: 276.6435s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0765743\n",
      "\tspeed: 0.0146s/iter; left time: 275.0862s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1247278\n",
      "\tspeed: 0.0146s/iter; left time: 273.7236s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1016554\n",
      "\tspeed: 0.0146s/iter; left time: 272.2722s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1100100\n",
      "\tspeed: 0.0146s/iter; left time: 270.8321s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1156157\n",
      "\tspeed: 0.0146s/iter; left time: 269.3517s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0884752\n",
      "\tspeed: 0.0146s/iter; left time: 267.8291s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0928946\n",
      "\tspeed: 0.0146s/iter; left time: 267.1462s\n",
      "Epoch: 6 cost time: 66.69454002380371\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.1072982 Vali Loss: 0.0663171 Test Loss: 0.0908449\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0657965\n",
      "\tspeed: 0.1398s/iter; left time: 2533.0463s\n",
      "\titers: 200, epoch: 7 | loss: 0.0988712\n",
      "\tspeed: 0.0147s/iter; left time: 264.5566s\n",
      "\titers: 300, epoch: 7 | loss: 0.1269787\n",
      "\tspeed: 0.0147s/iter; left time: 263.5689s\n",
      "\titers: 400, epoch: 7 | loss: 0.0901025\n",
      "\tspeed: 0.0146s/iter; left time: 260.9862s\n",
      "\titers: 500, epoch: 7 | loss: 0.0984829\n",
      "\tspeed: 0.0146s/iter; left time: 259.4584s\n",
      "\titers: 600, epoch: 7 | loss: 0.2235246\n",
      "\tspeed: 0.0146s/iter; left time: 257.9777s\n",
      "\titers: 700, epoch: 7 | loss: 0.1385790\n",
      "\tspeed: 0.0146s/iter; left time: 256.4878s\n",
      "\titers: 800, epoch: 7 | loss: 0.1290635\n",
      "\tspeed: 0.0146s/iter; left time: 255.0738s\n",
      "\titers: 900, epoch: 7 | loss: 0.0908079\n",
      "\tspeed: 0.0146s/iter; left time: 253.6375s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0594939\n",
      "\tspeed: 0.0146s/iter; left time: 252.1245s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1778095\n",
      "\tspeed: 0.0146s/iter; left time: 250.6873s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1196685\n",
      "\tspeed: 0.0147s/iter; left time: 249.8545s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0851565\n",
      "\tspeed: 0.0147s/iter; left time: 248.0810s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0633190\n",
      "\tspeed: 0.0147s/iter; left time: 247.0388s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1687127\n",
      "\tspeed: 0.0147s/iter; left time: 245.4864s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1145796\n",
      "\tspeed: 0.0147s/iter; left time: 244.3855s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0912381\n",
      "\tspeed: 0.0147s/iter; left time: 242.5751s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0964783\n",
      "\tspeed: 0.0154s/iter; left time: 253.2853s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0938445\n",
      "\tspeed: 0.0163s/iter; left time: 266.0217s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0532764\n",
      "\tspeed: 0.0163s/iter; left time: 264.0401s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0595038\n",
      "\tspeed: 0.0163s/iter; left time: 262.4204s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1614323\n",
      "\tspeed: 0.0163s/iter; left time: 260.5792s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0794722\n",
      "\tspeed: 0.0163s/iter; left time: 259.0510s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1633375\n",
      "\tspeed: 0.0163s/iter; left time: 257.6975s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1133566\n",
      "\tspeed: 0.0163s/iter; left time: 256.0616s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0887792\n",
      "\tspeed: 0.0163s/iter; left time: 254.3051s\n",
      "\titers: 2700, epoch: 7 | loss: 0.1432370\n",
      "\tspeed: 0.0163s/iter; left time: 252.4307s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0714819\n",
      "\tspeed: 0.0163s/iter; left time: 250.8959s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1141355\n",
      "\tspeed: 0.0163s/iter; left time: 249.2339s\n",
      "\titers: 3000, epoch: 7 | loss: 0.1135379\n",
      "\tspeed: 0.0163s/iter; left time: 247.7777s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0924033\n",
      "\tspeed: 0.0157s/iter; left time: 236.8156s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0805888\n",
      "\tspeed: 0.0147s/iter; left time: 220.2908s\n",
      "\titers: 3300, epoch: 7 | loss: 0.1004262\n",
      "\tspeed: 0.0147s/iter; left time: 219.0015s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1299303\n",
      "\tspeed: 0.0147s/iter; left time: 217.4985s\n",
      "\titers: 3500, epoch: 7 | loss: 0.1917235\n",
      "\tspeed: 0.0147s/iter; left time: 215.8834s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0454723\n",
      "\tspeed: 0.0147s/iter; left time: 214.3926s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1126813\n",
      "\tspeed: 0.0147s/iter; left time: 212.8699s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1342876\n",
      "\tspeed: 0.0146s/iter; left time: 211.2239s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0649522\n",
      "\tspeed: 0.0146s/iter; left time: 209.6325s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1152801\n",
      "\tspeed: 0.0146s/iter; left time: 208.2860s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0809043\n",
      "\tspeed: 0.0146s/iter; left time: 206.7893s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1248428\n",
      "\tspeed: 0.0146s/iter; left time: 205.1816s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0984467\n",
      "\tspeed: 0.0146s/iter; left time: 203.7443s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0777029\n",
      "\tspeed: 0.0146s/iter; left time: 202.3764s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0851263\n",
      "\tspeed: 0.0147s/iter; left time: 201.0144s\n",
      "Epoch: 7 cost time: 69.14008450508118\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.1069643 Vali Loss: 0.0661995 Test Loss: 0.0902857\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1070758\n",
      "\tspeed: 0.1353s/iter; left time: 1835.6847s\n",
      "\titers: 200, epoch: 8 | loss: 0.1145796\n",
      "\tspeed: 0.0146s/iter; left time: 196.8793s\n",
      "\titers: 300, epoch: 8 | loss: 0.0953521\n",
      "\tspeed: 0.0146s/iter; left time: 195.1206s\n",
      "\titers: 400, epoch: 8 | loss: 0.0840584\n",
      "\tspeed: 0.0146s/iter; left time: 193.5952s\n",
      "\titers: 500, epoch: 8 | loss: 0.1449923\n",
      "\tspeed: 0.0146s/iter; left time: 191.9794s\n",
      "\titers: 600, epoch: 8 | loss: 0.1074190\n",
      "\tspeed: 0.0146s/iter; left time: 190.6951s\n",
      "\titers: 700, epoch: 8 | loss: 0.1119742\n",
      "\tspeed: 0.0146s/iter; left time: 189.0158s\n",
      "\titers: 800, epoch: 8 | loss: 0.1322702\n",
      "\tspeed: 0.0146s/iter; left time: 187.6910s\n",
      "\titers: 900, epoch: 8 | loss: 0.1035301\n",
      "\tspeed: 0.0146s/iter; left time: 186.2598s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1473969\n",
      "\tspeed: 0.0146s/iter; left time: 184.7536s\n",
      "\titers: 1100, epoch: 8 | loss: 0.1768054\n",
      "\tspeed: 0.0146s/iter; left time: 183.2312s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1141749\n",
      "\tspeed: 0.0146s/iter; left time: 181.7765s\n",
      "\titers: 1300, epoch: 8 | loss: 0.1154452\n",
      "\tspeed: 0.0146s/iter; left time: 180.2026s\n",
      "\titers: 1400, epoch: 8 | loss: 0.1266104\n",
      "\tspeed: 0.0146s/iter; left time: 178.8061s\n",
      "\titers: 1500, epoch: 8 | loss: 0.1545639\n",
      "\tspeed: 0.0146s/iter; left time: 177.2006s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0909835\n",
      "\tspeed: 0.0146s/iter; left time: 175.7748s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0786316\n",
      "\tspeed: 0.0146s/iter; left time: 174.3304s\n",
      "\titers: 1800, epoch: 8 | loss: 0.2185486\n",
      "\tspeed: 0.0146s/iter; left time: 173.0393s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1291891\n",
      "\tspeed: 0.0146s/iter; left time: 171.5518s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1761144\n",
      "\tspeed: 0.0146s/iter; left time: 170.0883s\n",
      "\titers: 2100, epoch: 8 | loss: 0.1588298\n",
      "\tspeed: 0.0146s/iter; left time: 168.6213s\n",
      "\titers: 2200, epoch: 8 | loss: 0.1429343\n",
      "\tspeed: 0.0146s/iter; left time: 167.1352s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0873736\n",
      "\tspeed: 0.0146s/iter; left time: 165.6547s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1052906\n",
      "\tspeed: 0.0146s/iter; left time: 164.2054s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0790414\n",
      "\tspeed: 0.0146s/iter; left time: 162.6879s\n",
      "\titers: 2600, epoch: 8 | loss: 0.1140674\n",
      "\tspeed: 0.0146s/iter; left time: 161.2487s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1633579\n",
      "\tspeed: 0.0146s/iter; left time: 159.7823s\n",
      "\titers: 2800, epoch: 8 | loss: 0.1175348\n",
      "\tspeed: 0.0146s/iter; left time: 158.4573s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0923500\n",
      "\tspeed: 0.0146s/iter; left time: 156.9472s\n",
      "\titers: 3000, epoch: 8 | loss: 0.1072185\n",
      "\tspeed: 0.0146s/iter; left time: 155.4887s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0845513\n",
      "\tspeed: 0.0146s/iter; left time: 153.9587s\n",
      "\titers: 3200, epoch: 8 | loss: 0.2268294\n",
      "\tspeed: 0.0146s/iter; left time: 152.5122s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0661287\n",
      "\tspeed: 0.0146s/iter; left time: 151.0372s\n",
      "\titers: 3400, epoch: 8 | loss: 0.1394139\n",
      "\tspeed: 0.0146s/iter; left time: 149.6248s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0928707\n",
      "\tspeed: 0.0146s/iter; left time: 148.1470s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0885622\n",
      "\tspeed: 0.0146s/iter; left time: 146.6766s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0795707\n",
      "\tspeed: 0.0146s/iter; left time: 145.1833s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0776304\n",
      "\tspeed: 0.0146s/iter; left time: 143.8278s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0835515\n",
      "\tspeed: 0.0146s/iter; left time: 142.3635s\n",
      "\titers: 4000, epoch: 8 | loss: 0.1084633\n",
      "\tspeed: 0.0146s/iter; left time: 140.8572s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0988553\n",
      "\tspeed: 0.0146s/iter; left time: 139.3804s\n",
      "\titers: 4200, epoch: 8 | loss: 0.2052981\n",
      "\tspeed: 0.0146s/iter; left time: 137.9767s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0742925\n",
      "\tspeed: 0.0146s/iter; left time: 136.5330s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1400330\n",
      "\tspeed: 0.0146s/iter; left time: 135.0889s\n",
      "\titers: 4500, epoch: 8 | loss: 0.0734369\n",
      "\tspeed: 0.0146s/iter; left time: 133.5560s\n",
      "Epoch: 8 cost time: 66.63521409034729\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.1059544 Vali Loss: 0.0657462 Test Loss: 0.0895326\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "mse:0.08726100623607635, mae:0.16931110620498657\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --predictor \"solar_forecast,total_load\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a45142",
   "metadata": {},
   "source": [
    "## double_check if --predictor works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7e03c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1761947\n",
      "\tspeed: 0.0259s/iter; left time: 1178.9031s\n",
      "\titers: 200, epoch: 1 | loss: 0.0938281\n",
      "\tspeed: 0.0141s/iter; left time: 639.7948s\n",
      "\titers: 300, epoch: 1 | loss: 0.0687024\n",
      "\tspeed: 0.0141s/iter; left time: 638.0547s\n",
      "\titers: 400, epoch: 1 | loss: 0.1241657\n",
      "\tspeed: 0.0141s/iter; left time: 637.3304s\n",
      "\titers: 500, epoch: 1 | loss: 0.0327702\n",
      "\tspeed: 0.0141s/iter; left time: 636.2257s\n",
      "\titers: 600, epoch: 1 | loss: 0.0715009\n",
      "\tspeed: 0.0141s/iter; left time: 633.8842s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451532\n",
      "\tspeed: 0.0141s/iter; left time: 631.1828s\n",
      "\titers: 800, epoch: 1 | loss: 0.5053760\n",
      "\tspeed: 0.0141s/iter; left time: 631.6281s\n",
      "\titers: 900, epoch: 1 | loss: 0.2560097\n",
      "\tspeed: 0.0141s/iter; left time: 630.5993s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0734115\n",
      "\tspeed: 0.0141s/iter; left time: 629.0664s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0425444\n",
      "\tspeed: 0.0141s/iter; left time: 626.8392s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0585851\n",
      "\tspeed: 0.0141s/iter; left time: 625.8456s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2605752\n",
      "\tspeed: 0.0141s/iter; left time: 625.0328s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0309507\n",
      "\tspeed: 0.0141s/iter; left time: 622.9305s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1115317\n",
      "\tspeed: 0.0141s/iter; left time: 621.5004s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1843507\n",
      "\tspeed: 0.0141s/iter; left time: 619.4640s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3890318\n",
      "\tspeed: 0.0141s/iter; left time: 618.5352s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0720876\n",
      "\tspeed: 0.0141s/iter; left time: 616.8484s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1775747\n",
      "\tspeed: 0.0141s/iter; left time: 616.5075s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3624939\n",
      "\tspeed: 0.0141s/iter; left time: 615.2336s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1508106\n",
      "\tspeed: 0.0141s/iter; left time: 612.5721s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3487170\n",
      "\tspeed: 0.0141s/iter; left time: 611.0994s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1623183\n",
      "\tspeed: 0.0141s/iter; left time: 609.4673s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1627411\n",
      "\tspeed: 0.0141s/iter; left time: 608.0986s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2751411\n",
      "\tspeed: 0.0141s/iter; left time: 606.6731s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1169626\n",
      "\tspeed: 0.0141s/iter; left time: 605.3767s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2052497\n",
      "\tspeed: 0.0141s/iter; left time: 603.8049s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2055999\n",
      "\tspeed: 0.0141s/iter; left time: 602.3710s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2338544\n",
      "\tspeed: 0.0141s/iter; left time: 601.2013s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0882175\n",
      "\tspeed: 0.0141s/iter; left time: 599.8117s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0823141\n",
      "\tspeed: 0.0141s/iter; left time: 598.1707s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1962520\n",
      "\tspeed: 0.0141s/iter; left time: 596.6500s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0351911\n",
      "\tspeed: 0.0141s/iter; left time: 595.4474s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4122079\n",
      "\tspeed: 0.0141s/iter; left time: 594.2118s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0856364\n",
      "\tspeed: 0.0141s/iter; left time: 592.8527s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1057309\n",
      "\tspeed: 0.0141s/iter; left time: 591.0827s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0931639\n",
      "\tspeed: 0.0141s/iter; left time: 589.7665s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1785975\n",
      "\tspeed: 0.0141s/iter; left time: 588.2029s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3997110\n",
      "\tspeed: 0.0141s/iter; left time: 587.3413s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2769776\n",
      "\tspeed: 0.0141s/iter; left time: 586.0302s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0955917\n",
      "\tspeed: 0.0141s/iter; left time: 584.7557s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0872184\n",
      "\tspeed: 0.0141s/iter; left time: 582.9713s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1506448\n",
      "\tspeed: 0.0141s/iter; left time: 581.5125s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0404093\n",
      "\tspeed: 0.0141s/iter; left time: 580.3512s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0716922\n",
      "\tspeed: 0.0141s/iter; left time: 579.0975s\n",
      "Epoch: 1 cost time: 65.47285079956055\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1763682 Vali Loss: 0.0384911 Test Loss: 0.1245387\n",
      "Validation loss decreased (inf --> 0.038491).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0965484\n",
      "\tspeed: 0.1287s/iter; left time: 5262.0853s\n",
      "\titers: 200, epoch: 2 | loss: 0.0394090\n",
      "\tspeed: 0.0156s/iter; left time: 636.3697s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977589\n",
      "\tspeed: 0.0156s/iter; left time: 636.1801s\n",
      "\titers: 400, epoch: 2 | loss: 0.1882427\n",
      "\tspeed: 0.0156s/iter; left time: 633.2201s\n",
      "\titers: 500, epoch: 2 | loss: 0.3373528\n",
      "\tspeed: 0.0156s/iter; left time: 632.5444s\n",
      "\titers: 600, epoch: 2 | loss: 0.1316223\n",
      "\tspeed: 0.0156s/iter; left time: 630.3578s\n",
      "\titers: 700, epoch: 2 | loss: 0.0733484\n",
      "\tspeed: 0.0156s/iter; left time: 628.6443s\n",
      "\titers: 800, epoch: 2 | loss: 0.0557397\n",
      "\tspeed: 0.0156s/iter; left time: 626.8579s\n",
      "\titers: 900, epoch: 2 | loss: 0.0551844\n",
      "\tspeed: 0.0156s/iter; left time: 625.5580s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1973986\n",
      "\tspeed: 0.0156s/iter; left time: 623.6723s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2021171\n",
      "\tspeed: 0.0156s/iter; left time: 622.3020s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1249596\n",
      "\tspeed: 0.0156s/iter; left time: 620.6253s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1480998\n",
      "\tspeed: 0.0156s/iter; left time: 618.6854s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1150100\n",
      "\tspeed: 0.0156s/iter; left time: 617.5579s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0475104\n",
      "\tspeed: 0.0156s/iter; left time: 616.4415s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1711187\n",
      "\tspeed: 0.0156s/iter; left time: 614.6388s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2710866\n",
      "\tspeed: 0.0156s/iter; left time: 612.8842s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1182006\n",
      "\tspeed: 0.0156s/iter; left time: 611.4321s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1678239\n",
      "\tspeed: 0.0156s/iter; left time: 609.8736s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0585374\n",
      "\tspeed: 0.0156s/iter; left time: 608.1424s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0376854\n",
      "\tspeed: 0.0156s/iter; left time: 606.9865s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177671\n",
      "\tspeed: 0.0156s/iter; left time: 606.0807s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1420685\n",
      "\tspeed: 0.0156s/iter; left time: 604.6212s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1342927\n",
      "\tspeed: 0.0156s/iter; left time: 603.0487s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0575067\n",
      "\tspeed: 0.0156s/iter; left time: 601.4871s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2137174\n",
      "\tspeed: 0.0156s/iter; left time: 599.9044s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1351328\n",
      "\tspeed: 0.0156s/iter; left time: 598.5740s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0589181\n",
      "\tspeed: 0.0156s/iter; left time: 597.0536s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0820708\n",
      "\tspeed: 0.0156s/iter; left time: 595.2887s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1777464\n",
      "\tspeed: 0.0156s/iter; left time: 593.6892s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0429570\n",
      "\tspeed: 0.0156s/iter; left time: 592.1534s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1527276\n",
      "\tspeed: 0.0156s/iter; left time: 590.6772s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0607781\n",
      "\tspeed: 0.0156s/iter; left time: 589.2170s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0930832\n",
      "\tspeed: 0.0156s/iter; left time: 587.4754s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1257465\n",
      "\tspeed: 0.0156s/iter; left time: 585.9121s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1400915\n",
      "\tspeed: 0.0156s/iter; left time: 585.0001s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1529161\n",
      "\tspeed: 0.0157s/iter; left time: 584.0744s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1372309\n",
      "\tspeed: 0.0157s/iter; left time: 582.5541s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1924893\n",
      "\tspeed: 0.0156s/iter; left time: 579.6560s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2247270\n",
      "\tspeed: 0.0156s/iter; left time: 578.5305s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171593\n",
      "\tspeed: 0.0156s/iter; left time: 576.3770s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0934797\n",
      "\tspeed: 0.0154s/iter; left time: 565.8379s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1451997\n",
      "\tspeed: 0.0141s/iter; left time: 516.4310s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1581813\n",
      "\tspeed: 0.0141s/iter; left time: 515.2207s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0497790\n",
      "\tspeed: 0.0141s/iter; left time: 513.4393s\n",
      "Epoch: 2 cost time: 70.81944274902344\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1446429 Vali Loss: 0.0337670 Test Loss: 0.1147559\n",
      "Validation loss decreased (0.038491 --> 0.033767).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0888895\n",
      "\tspeed: 0.1269s/iter; left time: 4611.4013s\n",
      "\titers: 200, epoch: 3 | loss: 0.0433673\n",
      "\tspeed: 0.0142s/iter; left time: 512.8495s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213155\n",
      "\tspeed: 0.0141s/iter; left time: 511.3187s\n",
      "\titers: 400, epoch: 3 | loss: 0.0755301\n",
      "\tspeed: 0.0141s/iter; left time: 506.8012s\n",
      "\titers: 500, epoch: 3 | loss: 0.0728352\n",
      "\tspeed: 0.0141s/iter; left time: 505.4411s\n",
      "\titers: 600, epoch: 3 | loss: 0.0727856\n",
      "\tspeed: 0.0141s/iter; left time: 504.5524s\n",
      "\titers: 700, epoch: 3 | loss: 0.3633784\n",
      "\tspeed: 0.0141s/iter; left time: 503.4709s\n",
      "\titers: 800, epoch: 3 | loss: 0.0919219\n",
      "\tspeed: 0.0141s/iter; left time: 501.4582s\n",
      "\titers: 900, epoch: 3 | loss: 0.0787504\n",
      "\tspeed: 0.0141s/iter; left time: 499.5170s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1017697\n",
      "\tspeed: 0.0141s/iter; left time: 498.1321s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1278257\n",
      "\tspeed: 0.0141s/iter; left time: 496.9002s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1174518\n",
      "\tspeed: 0.0141s/iter; left time: 495.3111s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2299570\n",
      "\tspeed: 0.0141s/iter; left time: 493.9083s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1301254\n",
      "\tspeed: 0.0141s/iter; left time: 495.1015s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1960834\n",
      "\tspeed: 0.0141s/iter; left time: 493.7379s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1536359\n",
      "\tspeed: 0.0141s/iter; left time: 492.2389s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5119310\n",
      "\tspeed: 0.0141s/iter; left time: 490.8239s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0803905\n",
      "\tspeed: 0.0141s/iter; left time: 489.5990s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1686091\n",
      "\tspeed: 0.0141s/iter; left time: 487.9959s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2706951\n",
      "\tspeed: 0.0141s/iter; left time: 486.6687s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0878483\n",
      "\tspeed: 0.0141s/iter; left time: 485.1940s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0488082\n",
      "\tspeed: 0.0141s/iter; left time: 483.8406s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1471822\n",
      "\tspeed: 0.0141s/iter; left time: 482.4344s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1000195\n",
      "\tspeed: 0.0141s/iter; left time: 480.9460s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1408294\n",
      "\tspeed: 0.0141s/iter; left time: 479.3430s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0522313\n",
      "\tspeed: 0.0141s/iter; left time: 477.9672s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0433277\n",
      "\tspeed: 0.0141s/iter; left time: 476.5958s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1223619\n",
      "\tspeed: 0.0141s/iter; left time: 475.0307s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0673580\n",
      "\tspeed: 0.0141s/iter; left time: 473.6923s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0574459\n",
      "\tspeed: 0.0141s/iter; left time: 472.2799s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1538857\n",
      "\tspeed: 0.0141s/iter; left time: 470.9464s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2254446\n",
      "\tspeed: 0.0141s/iter; left time: 469.5556s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0779002\n",
      "\tspeed: 0.0141s/iter; left time: 468.0915s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1575561\n",
      "\tspeed: 0.0141s/iter; left time: 466.6158s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0331902\n",
      "\tspeed: 0.0141s/iter; left time: 465.2930s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1007734\n",
      "\tspeed: 0.0141s/iter; left time: 463.8472s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1122994\n",
      "\tspeed: 0.0141s/iter; left time: 462.5974s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0607264\n",
      "\tspeed: 0.0142s/iter; left time: 464.7051s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1815582\n",
      "\tspeed: 0.0142s/iter; left time: 462.8875s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1472391\n",
      "\tspeed: 0.0142s/iter; left time: 460.7037s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1834221\n",
      "\tspeed: 0.0142s/iter; left time: 460.1559s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0803711\n",
      "\tspeed: 0.0142s/iter; left time: 458.7393s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0216850\n",
      "\tspeed: 0.0142s/iter; left time: 457.2265s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0932727\n",
      "\tspeed: 0.0142s/iter; left time: 455.9872s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0153473\n",
      "\tspeed: 0.0142s/iter; left time: 454.6169s\n",
      "Epoch: 3 cost time: 64.63933849334717\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1149970 Vali Loss: 0.0354637 Test Loss: 0.1141755\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0631109\n",
      "\tspeed: 0.1292s/iter; left time: 4108.1356s\n",
      "\titers: 200, epoch: 4 | loss: 0.2875030\n",
      "\tspeed: 0.0157s/iter; left time: 497.3675s\n",
      "\titers: 300, epoch: 4 | loss: 0.0210192\n",
      "\tspeed: 0.0157s/iter; left time: 494.3715s\n",
      "\titers: 400, epoch: 4 | loss: 0.0991706\n",
      "\tspeed: 0.0157s/iter; left time: 493.6718s\n",
      "\titers: 500, epoch: 4 | loss: 0.0960686\n",
      "\tspeed: 0.0156s/iter; left time: 490.9520s\n",
      "\titers: 600, epoch: 4 | loss: 0.1770177\n",
      "\tspeed: 0.0156s/iter; left time: 489.3830s\n",
      "\titers: 700, epoch: 4 | loss: 0.1712226\n",
      "\tspeed: 0.0142s/iter; left time: 442.7227s\n",
      "\titers: 800, epoch: 4 | loss: 0.1566384\n",
      "\tspeed: 0.0142s/iter; left time: 440.3933s\n",
      "\titers: 900, epoch: 4 | loss: 0.1039117\n",
      "\tspeed: 0.0142s/iter; left time: 439.1945s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0762816\n",
      "\tspeed: 0.0142s/iter; left time: 438.3556s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1103869\n",
      "\tspeed: 0.0142s/iter; left time: 435.9370s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0567773\n",
      "\tspeed: 0.0142s/iter; left time: 434.4681s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0957227\n",
      "\tspeed: 0.0142s/iter; left time: 433.3250s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0460843\n",
      "\tspeed: 0.0142s/iter; left time: 432.3381s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1783445\n",
      "\tspeed: 0.0142s/iter; left time: 430.3858s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0993816\n",
      "\tspeed: 0.0142s/iter; left time: 428.8625s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1247748\n",
      "\tspeed: 0.0141s/iter; left time: 427.0152s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0204686\n",
      "\tspeed: 0.0141s/iter; left time: 425.6727s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0617034\n",
      "\tspeed: 0.0153s/iter; left time: 459.6496s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1042527\n",
      "\tspeed: 0.0157s/iter; left time: 468.3542s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0174318\n",
      "\tspeed: 0.0157s/iter; left time: 466.6580s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1664865\n",
      "\tspeed: 0.0156s/iter; left time: 464.5764s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1293821\n",
      "\tspeed: 0.0157s/iter; left time: 463.0591s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0772202\n",
      "\tspeed: 0.0156s/iter; left time: 461.2248s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0164892\n",
      "\tspeed: 0.0157s/iter; left time: 459.9046s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0531578\n",
      "\tspeed: 0.0157s/iter; left time: 458.3432s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5676972\n",
      "\tspeed: 0.0152s/iter; left time: 444.4955s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0195277\n",
      "\tspeed: 0.0142s/iter; left time: 411.8357s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0262614\n",
      "\tspeed: 0.0141s/iter; left time: 410.1378s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1373415\n",
      "\tspeed: 0.0142s/iter; left time: 408.8561s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0994673\n",
      "\tspeed: 0.0141s/iter; left time: 407.1487s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1655515\n",
      "\tspeed: 0.0141s/iter; left time: 405.5480s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0791031\n",
      "\tspeed: 0.0141s/iter; left time: 404.2052s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0591687\n",
      "\tspeed: 0.0141s/iter; left time: 402.6877s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0567421\n",
      "\tspeed: 0.0141s/iter; left time: 401.3279s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0513307\n",
      "\tspeed: 0.0141s/iter; left time: 399.8095s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1378802\n",
      "\tspeed: 0.0141s/iter; left time: 398.6475s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0390188\n",
      "\tspeed: 0.0141s/iter; left time: 397.1102s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0773032\n",
      "\tspeed: 0.0141s/iter; left time: 395.9401s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0881940\n",
      "\tspeed: 0.0141s/iter; left time: 394.1576s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0252138\n",
      "\tspeed: 0.0141s/iter; left time: 392.7564s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0641742\n",
      "\tspeed: 0.0141s/iter; left time: 391.5107s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2728181\n",
      "\tspeed: 0.0141s/iter; left time: 390.1427s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0952311\n",
      "\tspeed: 0.0143s/iter; left time: 393.9370s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0663833\n",
      "\tspeed: 0.0157s/iter; left time: 428.7049s\n",
      "Epoch: 4 cost time: 67.1778461933136\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0959688 Vali Loss: 0.0351006 Test Loss: 0.1077463\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0373502\n",
      "\tspeed: 0.1271s/iter; left time: 3460.9395s\n",
      "\titers: 200, epoch: 5 | loss: 0.0524443\n",
      "\tspeed: 0.0142s/iter; left time: 384.0230s\n",
      "\titers: 300, epoch: 5 | loss: 0.0598457\n",
      "\tspeed: 0.0141s/iter; left time: 382.1891s\n",
      "\titers: 400, epoch: 5 | loss: 0.0432743\n",
      "\tspeed: 0.0142s/iter; left time: 381.3229s\n",
      "\titers: 500, epoch: 5 | loss: 0.1162245\n",
      "\tspeed: 0.0141s/iter; left time: 379.6350s\n",
      "\titers: 600, epoch: 5 | loss: 0.0336827\n",
      "\tspeed: 0.0142s/iter; left time: 378.3802s\n",
      "\titers: 700, epoch: 5 | loss: 0.0636778\n",
      "\tspeed: 0.0141s/iter; left time: 376.4089s\n",
      "\titers: 800, epoch: 5 | loss: 0.0793169\n",
      "\tspeed: 0.0142s/iter; left time: 375.7522s\n",
      "\titers: 900, epoch: 5 | loss: 0.0391933\n",
      "\tspeed: 0.0142s/iter; left time: 374.5062s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1438223\n",
      "\tspeed: 0.0141s/iter; left time: 372.4558s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0300231\n",
      "\tspeed: 0.0141s/iter; left time: 370.9920s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1026424\n",
      "\tspeed: 0.0141s/iter; left time: 369.3808s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0712850\n",
      "\tspeed: 0.0141s/iter; left time: 368.0422s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1095237\n",
      "\tspeed: 0.0141s/iter; left time: 366.7142s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1186156\n",
      "\tspeed: 0.0141s/iter; left time: 365.2964s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0570681\n",
      "\tspeed: 0.0142s/iter; left time: 364.1228s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1760297\n",
      "\tspeed: 0.0141s/iter; left time: 362.4344s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0911143\n",
      "\tspeed: 0.0141s/iter; left time: 361.0864s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0277795\n",
      "\tspeed: 0.0141s/iter; left time: 359.5838s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5396386\n",
      "\tspeed: 0.0141s/iter; left time: 358.0571s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0865949\n",
      "\tspeed: 0.0141s/iter; left time: 356.8162s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0422525\n",
      "\tspeed: 0.0141s/iter; left time: 355.2935s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0285333\n",
      "\tspeed: 0.0141s/iter; left time: 353.8749s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0772292\n",
      "\tspeed: 0.0141s/iter; left time: 352.5781s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0591693\n",
      "\tspeed: 0.0141s/iter; left time: 351.3224s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0688789\n",
      "\tspeed: 0.0141s/iter; left time: 349.7761s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0948981\n",
      "\tspeed: 0.0141s/iter; left time: 348.3962s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0655452\n",
      "\tspeed: 0.0141s/iter; left time: 346.9344s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0620498\n",
      "\tspeed: 0.0141s/iter; left time: 345.5697s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0421777\n",
      "\tspeed: 0.0141s/iter; left time: 344.2101s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0343712\n",
      "\tspeed: 0.0141s/iter; left time: 342.7227s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0124530\n",
      "\tspeed: 0.0141s/iter; left time: 341.3950s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0510148\n",
      "\tspeed: 0.0142s/iter; left time: 340.0400s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0607961\n",
      "\tspeed: 0.0142s/iter; left time: 338.6901s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0542052\n",
      "\tspeed: 0.0141s/iter; left time: 337.1507s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0656159\n",
      "\tspeed: 0.0141s/iter; left time: 335.7453s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0490740\n",
      "\tspeed: 0.0142s/iter; left time: 334.4708s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1356292\n",
      "\tspeed: 0.0142s/iter; left time: 333.0542s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0560427\n",
      "\tspeed: 0.0142s/iter; left time: 331.6496s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0542096\n",
      "\tspeed: 0.0142s/iter; left time: 330.2074s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0280492\n",
      "\tspeed: 0.0142s/iter; left time: 328.7699s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0362821\n",
      "\tspeed: 0.0142s/iter; left time: 327.4121s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1139971\n",
      "\tspeed: 0.0142s/iter; left time: 325.9673s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0214874\n",
      "\tspeed: 0.0142s/iter; left time: 324.5394s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0509721\n",
      "\tspeed: 0.0142s/iter; left time: 323.1765s\n",
      "Epoch: 5 cost time: 64.68251919746399\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0878741 Vali Loss: 0.0343938 Test Loss: 0.1081526\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0342860\n",
      "\tspeed: 0.1406s/iter; left time: 3188.2294s\n",
      "\titers: 200, epoch: 6 | loss: 0.1143943\n",
      "\tspeed: 0.0141s/iter; left time: 319.1339s\n",
      "\titers: 300, epoch: 6 | loss: 0.0845980\n",
      "\tspeed: 0.0141s/iter; left time: 317.7903s\n",
      "\titers: 400, epoch: 6 | loss: 0.0725596\n",
      "\tspeed: 0.0141s/iter; left time: 316.5239s\n",
      "\titers: 500, epoch: 6 | loss: 0.0767262\n",
      "\tspeed: 0.0141s/iter; left time: 315.0488s\n",
      "\titers: 600, epoch: 6 | loss: 0.1322113\n",
      "\tspeed: 0.0141s/iter; left time: 313.4492s\n",
      "\titers: 700, epoch: 6 | loss: 0.0343189\n",
      "\tspeed: 0.0141s/iter; left time: 311.9653s\n",
      "\titers: 800, epoch: 6 | loss: 0.0838100\n",
      "\tspeed: 0.0141s/iter; left time: 310.5980s\n",
      "\titers: 900, epoch: 6 | loss: 0.0520985\n",
      "\tspeed: 0.0141s/iter; left time: 309.0584s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1338356\n",
      "\tspeed: 0.0141s/iter; left time: 307.5761s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0501162\n",
      "\tspeed: 0.0141s/iter; left time: 306.2763s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0938265\n",
      "\tspeed: 0.0141s/iter; left time: 304.9357s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1072335\n",
      "\tspeed: 0.0141s/iter; left time: 303.7095s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0549773\n",
      "\tspeed: 0.0141s/iter; left time: 302.2787s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0886215\n",
      "\tspeed: 0.0141s/iter; left time: 301.0008s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1584283\n",
      "\tspeed: 0.0141s/iter; left time: 299.5589s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0475080\n",
      "\tspeed: 0.0141s/iter; left time: 298.1647s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2330126\n",
      "\tspeed: 0.0141s/iter; left time: 296.7207s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1334384\n",
      "\tspeed: 0.0141s/iter; left time: 295.3288s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1563935\n",
      "\tspeed: 0.0141s/iter; left time: 293.9052s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0990143\n",
      "\tspeed: 0.0141s/iter; left time: 292.4686s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0800819\n",
      "\tspeed: 0.0142s/iter; left time: 291.1519s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1321079\n",
      "\tspeed: 0.0141s/iter; left time: 289.6438s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0605488\n",
      "\tspeed: 0.0141s/iter; left time: 288.1865s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0390510\n",
      "\tspeed: 0.0141s/iter; left time: 286.7250s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0626740\n",
      "\tspeed: 0.0141s/iter; left time: 285.3510s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0709718\n",
      "\tspeed: 0.0141s/iter; left time: 283.9660s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0537722\n",
      "\tspeed: 0.0141s/iter; left time: 282.4678s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0838269\n",
      "\tspeed: 0.0141s/iter; left time: 281.1497s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0225125\n",
      "\tspeed: 0.0141s/iter; left time: 279.6660s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0691343\n",
      "\tspeed: 0.0141s/iter; left time: 278.2148s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0691569\n",
      "\tspeed: 0.0142s/iter; left time: 277.0844s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0687092\n",
      "\tspeed: 0.0142s/iter; left time: 275.6515s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0259810\n",
      "\tspeed: 0.0141s/iter; left time: 274.1476s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0091006\n",
      "\tspeed: 0.0141s/iter; left time: 272.7358s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0910254\n",
      "\tspeed: 0.0142s/iter; left time: 271.4001s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0742034\n",
      "\tspeed: 0.0141s/iter; left time: 269.8847s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1335053\n",
      "\tspeed: 0.0141s/iter; left time: 268.4473s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0383737\n",
      "\tspeed: 0.0141s/iter; left time: 267.0746s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0639717\n",
      "\tspeed: 0.0142s/iter; left time: 265.7824s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0199149\n",
      "\tspeed: 0.0142s/iter; left time: 264.2777s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0504591\n",
      "\tspeed: 0.0142s/iter; left time: 262.8816s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0916515\n",
      "\tspeed: 0.0141s/iter; left time: 261.4299s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0636952\n",
      "\tspeed: 0.0141s/iter; left time: 260.0106s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0720719\n",
      "\tspeed: 0.0141s/iter; left time: 258.5739s\n",
      "Epoch: 6 cost time: 64.6695647239685\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0838598 Vali Loss: 0.0338680 Test Loss: 0.1082330\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0740386\n",
      "\tspeed: 0.1260s/iter; left time: 2282.5054s\n",
      "\titers: 200, epoch: 7 | loss: 0.1177322\n",
      "\tspeed: 0.0142s/iter; left time: 255.8902s\n",
      "\titers: 300, epoch: 7 | loss: 0.1634380\n",
      "\tspeed: 0.0142s/iter; left time: 254.2281s\n",
      "\titers: 400, epoch: 7 | loss: 0.0430431\n",
      "\tspeed: 0.0142s/iter; left time: 252.4329s\n",
      "\titers: 500, epoch: 7 | loss: 0.0305881\n",
      "\tspeed: 0.0142s/iter; left time: 251.2721s\n",
      "\titers: 600, epoch: 7 | loss: 0.0586626\n",
      "\tspeed: 0.0142s/iter; left time: 250.0447s\n",
      "\titers: 700, epoch: 7 | loss: 0.0157601\n",
      "\tspeed: 0.0142s/iter; left time: 248.1073s\n",
      "\titers: 800, epoch: 7 | loss: 0.0633146\n",
      "\tspeed: 0.0142s/iter; left time: 246.6878s\n",
      "\titers: 900, epoch: 7 | loss: 0.1631053\n",
      "\tspeed: 0.0142s/iter; left time: 245.2017s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0280740\n",
      "\tspeed: 0.0141s/iter; left time: 243.6227s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2191906\n",
      "\tspeed: 0.0142s/iter; left time: 242.5069s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0345329\n",
      "\tspeed: 0.0142s/iter; left time: 241.0038s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0882538\n",
      "\tspeed: 0.0142s/iter; left time: 239.6500s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0843781\n",
      "\tspeed: 0.0142s/iter; left time: 238.0970s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0997902\n",
      "\tspeed: 0.0142s/iter; left time: 236.7697s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0567520\n",
      "\tspeed: 0.0141s/iter; left time: 235.1457s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0368787\n",
      "\tspeed: 0.0141s/iter; left time: 233.7050s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0727305\n",
      "\tspeed: 0.0142s/iter; left time: 232.3622s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0438579\n",
      "\tspeed: 0.0142s/iter; left time: 231.1101s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0338232\n",
      "\tspeed: 0.0142s/iter; left time: 229.5280s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0344776\n",
      "\tspeed: 0.0141s/iter; left time: 228.0596s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0407905\n",
      "\tspeed: 0.0141s/iter; left time: 226.5211s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0512159\n",
      "\tspeed: 0.0141s/iter; left time: 225.2225s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0460093\n",
      "\tspeed: 0.0141s/iter; left time: 223.7982s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0768550\n",
      "\tspeed: 0.0141s/iter; left time: 222.4162s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0562639\n",
      "\tspeed: 0.0141s/iter; left time: 221.0290s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0378491\n",
      "\tspeed: 0.0141s/iter; left time: 219.4777s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0329542\n",
      "\tspeed: 0.0142s/iter; left time: 218.2077s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0686258\n",
      "\tspeed: 0.0141s/iter; left time: 216.7683s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0579222\n",
      "\tspeed: 0.0141s/iter; left time: 215.2809s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0167130\n",
      "\tspeed: 0.0141s/iter; left time: 213.8620s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1276155\n",
      "\tspeed: 0.0141s/iter; left time: 212.5225s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0701685\n",
      "\tspeed: 0.0141s/iter; left time: 211.0698s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0401235\n",
      "\tspeed: 0.0141s/iter; left time: 209.6595s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0514985\n",
      "\tspeed: 0.0141s/iter; left time: 208.2584s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0500706\n",
      "\tspeed: 0.0141s/iter; left time: 206.8035s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1626368\n",
      "\tspeed: 0.0141s/iter; left time: 205.4231s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0417629\n",
      "\tspeed: 0.0141s/iter; left time: 203.3667s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0764063\n",
      "\tspeed: 0.0141s/iter; left time: 201.9449s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0714385\n",
      "\tspeed: 0.0141s/iter; left time: 200.4400s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0747561\n",
      "\tspeed: 0.0141s/iter; left time: 199.1196s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0166666\n",
      "\tspeed: 0.0141s/iter; left time: 197.5307s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0198066\n",
      "\tspeed: 0.0141s/iter; left time: 196.1817s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0251551\n",
      "\tspeed: 0.0141s/iter; left time: 194.6988s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0811137\n",
      "\tspeed: 0.0141s/iter; left time: 193.2348s\n",
      "Epoch: 7 cost time: 64.65675067901611\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0810585 Vali Loss: 0.0343636 Test Loss: 0.1088597\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11478278785943985, mae:0.20999355614185333\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77cf15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          forecast_data.csv   Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1763421\n",
      "\tspeed: 0.0258s/iter; left time: 1173.3273s\n",
      "\titers: 200, epoch: 1 | loss: 0.0911674\n",
      "\tspeed: 0.0142s/iter; left time: 644.8077s\n",
      "\titers: 300, epoch: 1 | loss: 0.0685877\n",
      "\tspeed: 0.0142s/iter; left time: 642.5082s\n",
      "\titers: 400, epoch: 1 | loss: 0.1262829\n",
      "\tspeed: 0.0141s/iter; left time: 638.0869s\n",
      "\titers: 500, epoch: 1 | loss: 0.0332341\n",
      "\tspeed: 0.0145s/iter; left time: 654.1832s\n",
      "\titers: 600, epoch: 1 | loss: 0.0700410\n",
      "\tspeed: 0.0157s/iter; left time: 704.2183s\n",
      "\titers: 700, epoch: 1 | loss: 0.1557147\n",
      "\tspeed: 0.0157s/iter; left time: 702.8665s\n",
      "\titers: 800, epoch: 1 | loss: 0.5180982\n",
      "\tspeed: 0.0157s/iter; left time: 701.0336s\n",
      "\titers: 900, epoch: 1 | loss: 0.2673880\n",
      "\tspeed: 0.0157s/iter; left time: 698.9204s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0766063\n",
      "\tspeed: 0.0157s/iter; left time: 697.8888s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0634679\n",
      "\tspeed: 0.0156s/iter; left time: 692.1944s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0618385\n",
      "\tspeed: 0.0156s/iter; left time: 691.4171s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2637696\n",
      "\tspeed: 0.0156s/iter; left time: 689.9706s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0316852\n",
      "\tspeed: 0.0156s/iter; left time: 689.8124s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1121096\n",
      "\tspeed: 0.0156s/iter; left time: 688.5038s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2117470\n",
      "\tspeed: 0.0156s/iter; left time: 686.4465s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3498498\n",
      "\tspeed: 0.0156s/iter; left time: 685.4120s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0784601\n",
      "\tspeed: 0.0156s/iter; left time: 683.9339s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1818510\n",
      "\tspeed: 0.0156s/iter; left time: 681.9298s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3695084\n",
      "\tspeed: 0.0156s/iter; left time: 680.8922s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1544316\n",
      "\tspeed: 0.0156s/iter; left time: 679.0832s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2958900\n",
      "\tspeed: 0.0156s/iter; left time: 678.2265s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1671727\n",
      "\tspeed: 0.0156s/iter; left time: 675.9495s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1797693\n",
      "\tspeed: 0.0156s/iter; left time: 674.8025s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2635589\n",
      "\tspeed: 0.0156s/iter; left time: 672.9695s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1146011\n",
      "\tspeed: 0.0156s/iter; left time: 671.6396s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1742201\n",
      "\tspeed: 0.0156s/iter; left time: 670.5269s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2107886\n",
      "\tspeed: 0.0156s/iter; left time: 668.3996s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2342728\n",
      "\tspeed: 0.0156s/iter; left time: 666.6338s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0898760\n",
      "\tspeed: 0.0156s/iter; left time: 665.0495s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0890393\n",
      "\tspeed: 0.0156s/iter; left time: 663.3908s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1720884\n",
      "\tspeed: 0.0156s/iter; left time: 661.9798s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0325332\n",
      "\tspeed: 0.0156s/iter; left time: 660.5474s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4197335\n",
      "\tspeed: 0.0156s/iter; left time: 658.5720s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0850450\n",
      "\tspeed: 0.0156s/iter; left time: 657.4305s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1089428\n",
      "\tspeed: 0.0156s/iter; left time: 655.7080s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1310410\n",
      "\tspeed: 0.0156s/iter; left time: 654.1666s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1826571\n",
      "\tspeed: 0.0156s/iter; left time: 652.6021s\n",
      "\titers: 3900, epoch: 1 | loss: 0.5308020\n",
      "\tspeed: 0.0156s/iter; left time: 650.9539s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2345040\n",
      "\tspeed: 0.0156s/iter; left time: 649.1797s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0851049\n",
      "\tspeed: 0.0156s/iter; left time: 647.8010s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0862194\n",
      "\tspeed: 0.0156s/iter; left time: 645.9648s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1847207\n",
      "\tspeed: 0.0156s/iter; left time: 644.5231s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0432579\n",
      "\tspeed: 0.0156s/iter; left time: 642.9368s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0613752\n",
      "\tspeed: 0.0156s/iter; left time: 641.6316s\n",
      "Epoch: 1 cost time: 71.72045516967773\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1769887 Vali Loss: 0.0382646 Test Loss: 0.1222035\n",
      "Validation loss decreased (inf --> 0.038265).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0708423\n",
      "\tspeed: 0.1286s/iter; left time: 5260.6449s\n",
      "\titers: 200, epoch: 2 | loss: 0.0370787\n",
      "\tspeed: 0.0141s/iter; left time: 576.7704s\n",
      "\titers: 300, epoch: 2 | loss: 0.0813771\n",
      "\tspeed: 0.0142s/iter; left time: 577.9926s\n",
      "\titers: 400, epoch: 2 | loss: 0.1123772\n",
      "\tspeed: 0.0143s/iter; left time: 578.5790s\n",
      "\titers: 500, epoch: 2 | loss: 0.2678525\n",
      "\tspeed: 0.0143s/iter; left time: 577.3985s\n",
      "\titers: 600, epoch: 2 | loss: 0.1221668\n",
      "\tspeed: 0.0143s/iter; left time: 575.9523s\n",
      "\titers: 700, epoch: 2 | loss: 0.0826460\n",
      "\tspeed: 0.0143s/iter; left time: 574.2947s\n",
      "\titers: 800, epoch: 2 | loss: 0.0691161\n",
      "\tspeed: 0.0143s/iter; left time: 572.8056s\n",
      "\titers: 900, epoch: 2 | loss: 0.0595039\n",
      "\tspeed: 0.0142s/iter; left time: 571.3652s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1339937\n",
      "\tspeed: 0.0143s/iter; left time: 570.0434s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2605190\n",
      "\tspeed: 0.0143s/iter; left time: 568.7765s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1351536\n",
      "\tspeed: 0.0143s/iter; left time: 567.2326s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1300198\n",
      "\tspeed: 0.0142s/iter; left time: 565.5687s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1067906\n",
      "\tspeed: 0.0143s/iter; left time: 564.5393s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0396041\n",
      "\tspeed: 0.0142s/iter; left time: 562.7365s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1783482\n",
      "\tspeed: 0.0143s/iter; left time: 562.2829s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2730483\n",
      "\tspeed: 0.0143s/iter; left time: 560.0490s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0993784\n",
      "\tspeed: 0.0143s/iter; left time: 558.8151s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1332634\n",
      "\tspeed: 0.0143s/iter; left time: 557.2041s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0613768\n",
      "\tspeed: 0.0143s/iter; left time: 555.9376s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0445131\n",
      "\tspeed: 0.0142s/iter; left time: 554.2151s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177808\n",
      "\tspeed: 0.0143s/iter; left time: 553.4023s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1914830\n",
      "\tspeed: 0.0142s/iter; left time: 551.3567s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1263148\n",
      "\tspeed: 0.0143s/iter; left time: 550.1429s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0652605\n",
      "\tspeed: 0.0143s/iter; left time: 548.6049s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1919345\n",
      "\tspeed: 0.0143s/iter; left time: 547.2382s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1276122\n",
      "\tspeed: 0.0142s/iter; left time: 545.6935s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0812606\n",
      "\tspeed: 0.0142s/iter; left time: 541.1286s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0512082\n",
      "\tspeed: 0.0141s/iter; left time: 538.6044s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1353437\n",
      "\tspeed: 0.0141s/iter; left time: 537.5873s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0347083\n",
      "\tspeed: 0.0141s/iter; left time: 535.8445s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1915552\n",
      "\tspeed: 0.0141s/iter; left time: 534.2354s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0515440\n",
      "\tspeed: 0.0141s/iter; left time: 532.9506s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0765117\n",
      "\tspeed: 0.0141s/iter; left time: 531.6369s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1664551\n",
      "\tspeed: 0.0141s/iter; left time: 530.4840s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1474820\n",
      "\tspeed: 0.0141s/iter; left time: 528.6746s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2194349\n",
      "\tspeed: 0.0141s/iter; left time: 527.3932s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1482259\n",
      "\tspeed: 0.0141s/iter; left time: 525.6824s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1790596\n",
      "\tspeed: 0.0141s/iter; left time: 524.1471s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1764774\n",
      "\tspeed: 0.0141s/iter; left time: 522.7673s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1136319\n",
      "\tspeed: 0.0141s/iter; left time: 521.5404s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0763719\n",
      "\tspeed: 0.0141s/iter; left time: 520.4807s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1289414\n",
      "\tspeed: 0.0141s/iter; left time: 518.6691s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1127094\n",
      "\tspeed: 0.0141s/iter; left time: 517.1696s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0328601\n",
      "\tspeed: 0.0141s/iter; left time: 515.5902s\n",
      "Epoch: 2 cost time: 64.95170736312866\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1433447 Vali Loss: 0.0342802 Test Loss: 0.1165591\n",
      "Validation loss decreased (0.038265 --> 0.034280).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0887557\n",
      "\tspeed: 0.1268s/iter; left time: 4608.1868s\n",
      "\titers: 200, epoch: 3 | loss: 0.0631191\n",
      "\tspeed: 0.0142s/iter; left time: 515.1709s\n",
      "\titers: 300, epoch: 3 | loss: 0.0991641\n",
      "\tspeed: 0.0143s/iter; left time: 515.3044s\n",
      "\titers: 400, epoch: 3 | loss: 0.0675852\n",
      "\tspeed: 0.0142s/iter; left time: 513.4379s\n",
      "\titers: 500, epoch: 3 | loss: 0.0757974\n",
      "\tspeed: 0.0142s/iter; left time: 511.8602s\n",
      "\titers: 600, epoch: 3 | loss: 0.0908751\n",
      "\tspeed: 0.0142s/iter; left time: 510.5139s\n",
      "\titers: 700, epoch: 3 | loss: 0.3168572\n",
      "\tspeed: 0.0142s/iter; left time: 508.8479s\n",
      "\titers: 800, epoch: 3 | loss: 0.1619128\n",
      "\tspeed: 0.0142s/iter; left time: 507.2366s\n",
      "\titers: 900, epoch: 3 | loss: 0.0694767\n",
      "\tspeed: 0.0142s/iter; left time: 505.8696s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1536930\n",
      "\tspeed: 0.0142s/iter; left time: 504.2793s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1661171\n",
      "\tspeed: 0.0142s/iter; left time: 503.0589s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1236042\n",
      "\tspeed: 0.0143s/iter; left time: 502.2613s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1861919\n",
      "\tspeed: 0.0142s/iter; left time: 500.5297s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1547329\n",
      "\tspeed: 0.0142s/iter; left time: 498.9197s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2044825\n",
      "\tspeed: 0.0142s/iter; left time: 497.6545s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1272103\n",
      "\tspeed: 0.0142s/iter; left time: 496.3783s\n",
      "\titers: 1700, epoch: 3 | loss: 0.4056962\n",
      "\tspeed: 0.0142s/iter; left time: 494.4574s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0842609\n",
      "\tspeed: 0.0142s/iter; left time: 493.3769s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1753680\n",
      "\tspeed: 0.0142s/iter; left time: 491.9148s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2123944\n",
      "\tspeed: 0.0142s/iter; left time: 490.3919s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0610114\n",
      "\tspeed: 0.0142s/iter; left time: 488.4439s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0484819\n",
      "\tspeed: 0.0142s/iter; left time: 486.7364s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0948231\n",
      "\tspeed: 0.0142s/iter; left time: 485.2453s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1175820\n",
      "\tspeed: 0.0142s/iter; left time: 483.9607s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1470246\n",
      "\tspeed: 0.0142s/iter; left time: 482.7438s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0496472\n",
      "\tspeed: 0.0142s/iter; left time: 481.0281s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0424449\n",
      "\tspeed: 0.0142s/iter; left time: 479.7850s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1079681\n",
      "\tspeed: 0.0142s/iter; left time: 478.2752s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0759586\n",
      "\tspeed: 0.0142s/iter; left time: 476.9532s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0450513\n",
      "\tspeed: 0.0142s/iter; left time: 475.6594s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1647832\n",
      "\tspeed: 0.0142s/iter; left time: 474.3326s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2094353\n",
      "\tspeed: 0.0142s/iter; left time: 472.0222s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0807754\n",
      "\tspeed: 0.0142s/iter; left time: 469.8333s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1764615\n",
      "\tspeed: 0.0142s/iter; left time: 468.4175s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0432965\n",
      "\tspeed: 0.0142s/iter; left time: 467.1035s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0880627\n",
      "\tspeed: 0.0142s/iter; left time: 465.5978s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1241715\n",
      "\tspeed: 0.0142s/iter; left time: 464.2001s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0632678\n",
      "\tspeed: 0.0142s/iter; left time: 462.8677s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1618874\n",
      "\tspeed: 0.0142s/iter; left time: 461.4887s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1856516\n",
      "\tspeed: 0.0142s/iter; left time: 459.8602s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1578915\n",
      "\tspeed: 0.0142s/iter; left time: 458.4873s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0730709\n",
      "\tspeed: 0.0142s/iter; left time: 457.0249s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0223834\n",
      "\tspeed: 0.0142s/iter; left time: 455.7292s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1014316\n",
      "\tspeed: 0.0142s/iter; left time: 454.0410s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0368690\n",
      "\tspeed: 0.0142s/iter; left time: 452.6776s\n",
      "Epoch: 3 cost time: 65.0121443271637\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1130808 Vali Loss: 0.0338713 Test Loss: 0.1123349\n",
      "Validation loss decreased (0.034280 --> 0.033871).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0677192\n",
      "\tspeed: 0.1261s/iter; left time: 4008.9335s\n",
      "\titers: 200, epoch: 4 | loss: 0.2672075\n",
      "\tspeed: 0.0142s/iter; left time: 448.6302s\n",
      "\titers: 300, epoch: 4 | loss: 0.0282373\n",
      "\tspeed: 0.0142s/iter; left time: 447.5160s\n",
      "\titers: 400, epoch: 4 | loss: 0.1770267\n",
      "\tspeed: 0.0142s/iter; left time: 446.6296s\n",
      "\titers: 500, epoch: 4 | loss: 0.0735326\n",
      "\tspeed: 0.0142s/iter; left time: 445.2600s\n",
      "\titers: 600, epoch: 4 | loss: 0.1257664\n",
      "\tspeed: 0.0142s/iter; left time: 443.6483s\n",
      "\titers: 700, epoch: 4 | loss: 0.0916248\n",
      "\tspeed: 0.0142s/iter; left time: 442.3228s\n",
      "\titers: 800, epoch: 4 | loss: 0.1966325\n",
      "\tspeed: 0.0142s/iter; left time: 440.8190s\n",
      "\titers: 900, epoch: 4 | loss: 0.0958001\n",
      "\tspeed: 0.0142s/iter; left time: 439.3946s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0797571\n",
      "\tspeed: 0.0142s/iter; left time: 438.1535s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1135025\n",
      "\tspeed: 0.0142s/iter; left time: 437.2515s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0453799\n",
      "\tspeed: 0.0142s/iter; left time: 435.9374s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0836156\n",
      "\tspeed: 0.0142s/iter; left time: 434.6987s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0572280\n",
      "\tspeed: 0.0142s/iter; left time: 432.3444s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2131865\n",
      "\tspeed: 0.0142s/iter; left time: 431.0140s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0784413\n",
      "\tspeed: 0.0142s/iter; left time: 429.9952s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1073378\n",
      "\tspeed: 0.0142s/iter; left time: 429.3298s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0196976\n",
      "\tspeed: 0.0142s/iter; left time: 428.0250s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0491908\n",
      "\tspeed: 0.0142s/iter; left time: 426.4025s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1022145\n",
      "\tspeed: 0.0142s/iter; left time: 424.9867s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0206197\n",
      "\tspeed: 0.0142s/iter; left time: 423.3758s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1852808\n",
      "\tspeed: 0.0142s/iter; left time: 422.4189s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0907707\n",
      "\tspeed: 0.0142s/iter; left time: 420.9105s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0849595\n",
      "\tspeed: 0.0142s/iter; left time: 419.6875s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0310438\n",
      "\tspeed: 0.0142s/iter; left time: 418.0614s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0472595\n",
      "\tspeed: 0.0142s/iter; left time: 415.8130s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5556297\n",
      "\tspeed: 0.0142s/iter; left time: 413.9071s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0225049\n",
      "\tspeed: 0.0142s/iter; left time: 412.4287s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0267110\n",
      "\tspeed: 0.0142s/iter; left time: 411.4001s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0906036\n",
      "\tspeed: 0.0142s/iter; left time: 409.6533s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0865840\n",
      "\tspeed: 0.0142s/iter; left time: 408.3391s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1268385\n",
      "\tspeed: 0.0142s/iter; left time: 406.7265s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0671829\n",
      "\tspeed: 0.0142s/iter; left time: 405.4020s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0654759\n",
      "\tspeed: 0.0142s/iter; left time: 403.8095s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0569802\n",
      "\tspeed: 0.0142s/iter; left time: 402.3678s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0494764\n",
      "\tspeed: 0.0142s/iter; left time: 401.2768s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1321240\n",
      "\tspeed: 0.0142s/iter; left time: 399.7351s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0398145\n",
      "\tspeed: 0.0142s/iter; left time: 398.5971s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0794026\n",
      "\tspeed: 0.0142s/iter; left time: 397.0324s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0670096\n",
      "\tspeed: 0.0142s/iter; left time: 395.6256s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0210163\n",
      "\tspeed: 0.0142s/iter; left time: 394.1595s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0838922\n",
      "\tspeed: 0.0142s/iter; left time: 392.7541s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2237159\n",
      "\tspeed: 0.0142s/iter; left time: 391.3931s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0835053\n",
      "\tspeed: 0.0142s/iter; left time: 389.9627s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0761995\n",
      "\tspeed: 0.0142s/iter; left time: 388.5769s\n",
      "Epoch: 4 cost time: 64.93904209136963\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0947698 Vali Loss: 0.0322872 Test Loss: 0.1120607\n",
      "Validation loss decreased (0.033871 --> 0.032287).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0346444\n",
      "\tspeed: 0.1261s/iter; left time: 3433.3186s\n",
      "\titers: 200, epoch: 5 | loss: 0.0783560\n",
      "\tspeed: 0.0141s/iter; left time: 383.6034s\n",
      "\titers: 300, epoch: 5 | loss: 0.0484885\n",
      "\tspeed: 0.0141s/iter; left time: 382.4652s\n",
      "\titers: 400, epoch: 5 | loss: 0.0274945\n",
      "\tspeed: 0.0141s/iter; left time: 380.6468s\n",
      "\titers: 500, epoch: 5 | loss: 0.1355992\n",
      "\tspeed: 0.0141s/iter; left time: 379.2586s\n",
      "\titers: 600, epoch: 5 | loss: 0.0453184\n",
      "\tspeed: 0.0141s/iter; left time: 378.1924s\n",
      "\titers: 700, epoch: 5 | loss: 0.0420807\n",
      "\tspeed: 0.0142s/iter; left time: 377.6098s\n",
      "\titers: 800, epoch: 5 | loss: 0.0824672\n",
      "\tspeed: 0.0142s/iter; left time: 376.2725s\n",
      "\titers: 900, epoch: 5 | loss: 0.0466673\n",
      "\tspeed: 0.0142s/iter; left time: 374.6818s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1421037\n",
      "\tspeed: 0.0142s/iter; left time: 373.7283s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0354289\n",
      "\tspeed: 0.0142s/iter; left time: 371.8006s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0932426\n",
      "\tspeed: 0.0142s/iter; left time: 370.4415s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0745996\n",
      "\tspeed: 0.0142s/iter; left time: 369.0530s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1019057\n",
      "\tspeed: 0.0142s/iter; left time: 367.6516s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1201189\n",
      "\tspeed: 0.0142s/iter; left time: 366.1751s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0516186\n",
      "\tspeed: 0.0142s/iter; left time: 364.8492s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1378125\n",
      "\tspeed: 0.0142s/iter; left time: 363.5618s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0763469\n",
      "\tspeed: 0.0142s/iter; left time: 362.1209s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0245237\n",
      "\tspeed: 0.0142s/iter; left time: 360.6122s\n",
      "\titers: 2000, epoch: 5 | loss: 0.4386002\n",
      "\tspeed: 0.0142s/iter; left time: 359.3339s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0794068\n",
      "\tspeed: 0.0142s/iter; left time: 357.7037s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0555352\n",
      "\tspeed: 0.0142s/iter; left time: 356.3482s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0335050\n",
      "\tspeed: 0.0142s/iter; left time: 354.7829s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0682296\n",
      "\tspeed: 0.0142s/iter; left time: 353.5013s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0643012\n",
      "\tspeed: 0.0142s/iter; left time: 352.0242s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0632551\n",
      "\tspeed: 0.0142s/iter; left time: 350.6197s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1010250\n",
      "\tspeed: 0.0142s/iter; left time: 349.2415s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0811755\n",
      "\tspeed: 0.0142s/iter; left time: 347.6721s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0463221\n",
      "\tspeed: 0.0142s/iter; left time: 346.2889s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0464165\n",
      "\tspeed: 0.0142s/iter; left time: 344.8797s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0230623\n",
      "\tspeed: 0.0142s/iter; left time: 343.3450s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0151336\n",
      "\tspeed: 0.0142s/iter; left time: 341.9125s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0530929\n",
      "\tspeed: 0.0142s/iter; left time: 340.5399s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0590181\n",
      "\tspeed: 0.0142s/iter; left time: 339.8648s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0540718\n",
      "\tspeed: 0.0142s/iter; left time: 338.1798s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0775444\n",
      "\tspeed: 0.0142s/iter; left time: 336.4605s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0922274\n",
      "\tspeed: 0.0142s/iter; left time: 335.1193s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0900529\n",
      "\tspeed: 0.0142s/iter; left time: 333.8161s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0533731\n",
      "\tspeed: 0.0142s/iter; left time: 332.6950s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0691097\n",
      "\tspeed: 0.0142s/iter; left time: 330.8592s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0310082\n",
      "\tspeed: 0.0142s/iter; left time: 329.4365s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0478831\n",
      "\tspeed: 0.0142s/iter; left time: 327.8496s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1158995\n",
      "\tspeed: 0.0142s/iter; left time: 326.3985s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0272351\n",
      "\tspeed: 0.0142s/iter; left time: 325.0011s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0553124\n",
      "\tspeed: 0.0142s/iter; left time: 323.7286s\n",
      "Epoch: 5 cost time: 64.84556674957275\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0862461 Vali Loss: 0.0314426 Test Loss: 0.1096691\n",
      "Validation loss decreased (0.032287 --> 0.031443).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0319804\n",
      "\tspeed: 0.1276s/iter; left time: 2892.7666s\n",
      "\titers: 200, epoch: 6 | loss: 0.1281487\n",
      "\tspeed: 0.0146s/iter; left time: 329.6042s\n",
      "\titers: 300, epoch: 6 | loss: 0.0806120\n",
      "\tspeed: 0.0142s/iter; left time: 319.8207s\n",
      "\titers: 400, epoch: 6 | loss: 0.0730463\n",
      "\tspeed: 0.0142s/iter; left time: 318.2561s\n",
      "\titers: 500, epoch: 6 | loss: 0.0813001\n",
      "\tspeed: 0.0142s/iter; left time: 316.4588s\n",
      "\titers: 600, epoch: 6 | loss: 0.1437902\n",
      "\tspeed: 0.0142s/iter; left time: 315.1405s\n",
      "\titers: 700, epoch: 6 | loss: 0.0333109\n",
      "\tspeed: 0.0142s/iter; left time: 313.6946s\n",
      "\titers: 800, epoch: 6 | loss: 0.0767790\n",
      "\tspeed: 0.0142s/iter; left time: 312.2031s\n",
      "\titers: 900, epoch: 6 | loss: 0.0508947\n",
      "\tspeed: 0.0142s/iter; left time: 311.2650s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1351563\n",
      "\tspeed: 0.0142s/iter; left time: 309.6477s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0504169\n",
      "\tspeed: 0.0142s/iter; left time: 308.1871s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1000161\n",
      "\tspeed: 0.0142s/iter; left time: 306.7356s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0954765\n",
      "\tspeed: 0.0142s/iter; left time: 305.2567s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0415520\n",
      "\tspeed: 0.0142s/iter; left time: 303.9673s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0714341\n",
      "\tspeed: 0.0142s/iter; left time: 302.2397s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1349517\n",
      "\tspeed: 0.0142s/iter; left time: 300.9711s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0497032\n",
      "\tspeed: 0.0142s/iter; left time: 299.4909s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2016151\n",
      "\tspeed: 0.0142s/iter; left time: 298.1229s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1066509\n",
      "\tspeed: 0.0142s/iter; left time: 296.6844s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1433756\n",
      "\tspeed: 0.0142s/iter; left time: 295.3084s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0885042\n",
      "\tspeed: 0.0142s/iter; left time: 293.9313s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1302622\n",
      "\tspeed: 0.0142s/iter; left time: 292.5097s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0681458\n",
      "\tspeed: 0.0142s/iter; left time: 291.0355s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0666857\n",
      "\tspeed: 0.0142s/iter; left time: 289.7363s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0437274\n",
      "\tspeed: 0.0142s/iter; left time: 288.3590s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0632247\n",
      "\tspeed: 0.0142s/iter; left time: 286.9518s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0748692\n",
      "\tspeed: 0.0142s/iter; left time: 285.4593s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0550520\n",
      "\tspeed: 0.0142s/iter; left time: 283.9298s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1028431\n",
      "\tspeed: 0.0142s/iter; left time: 282.5655s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0264712\n",
      "\tspeed: 0.0142s/iter; left time: 281.2507s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0554059\n",
      "\tspeed: 0.0142s/iter; left time: 279.7971s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0530770\n",
      "\tspeed: 0.0142s/iter; left time: 278.3099s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0740471\n",
      "\tspeed: 0.0142s/iter; left time: 276.9257s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0470444\n",
      "\tspeed: 0.0142s/iter; left time: 275.4102s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0130280\n",
      "\tspeed: 0.0142s/iter; left time: 274.0002s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0681229\n",
      "\tspeed: 0.0142s/iter; left time: 272.7029s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0936383\n",
      "\tspeed: 0.0142s/iter; left time: 271.1476s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1275344\n",
      "\tspeed: 0.0142s/iter; left time: 269.8409s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0325485\n",
      "\tspeed: 0.0142s/iter; left time: 268.4253s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0699273\n",
      "\tspeed: 0.0142s/iter; left time: 266.9289s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0224311\n",
      "\tspeed: 0.0142s/iter; left time: 265.5231s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0382471\n",
      "\tspeed: 0.0142s/iter; left time: 264.1119s\n",
      "\titers: 4300, epoch: 6 | loss: 0.0927392\n",
      "\tspeed: 0.0142s/iter; left time: 262.6799s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0526579\n",
      "\tspeed: 0.0142s/iter; left time: 261.1910s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0730551\n",
      "\tspeed: 0.0142s/iter; left time: 259.8003s\n",
      "Epoch: 6 cost time: 65.21599268913269\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0828302 Vali Loss: 0.0314499 Test Loss: 0.1097774\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0866349\n",
      "\tspeed: 0.1248s/iter; left time: 2262.0122s\n",
      "\titers: 200, epoch: 7 | loss: 0.1155735\n",
      "\tspeed: 0.0142s/iter; left time: 255.8427s\n",
      "\titers: 300, epoch: 7 | loss: 0.1784418\n",
      "\tspeed: 0.0142s/iter; left time: 254.4313s\n",
      "\titers: 400, epoch: 7 | loss: 0.0557181\n",
      "\tspeed: 0.0142s/iter; left time: 252.9452s\n",
      "\titers: 500, epoch: 7 | loss: 0.0448637\n",
      "\tspeed: 0.0142s/iter; left time: 251.2563s\n",
      "\titers: 600, epoch: 7 | loss: 0.0567390\n",
      "\tspeed: 0.0141s/iter; left time: 248.5546s\n",
      "\titers: 700, epoch: 7 | loss: 0.0241459\n",
      "\tspeed: 0.0141s/iter; left time: 247.1678s\n",
      "\titers: 800, epoch: 7 | loss: 0.0622779\n",
      "\tspeed: 0.0141s/iter; left time: 245.6631s\n",
      "\titers: 900, epoch: 7 | loss: 0.1339148\n",
      "\tspeed: 0.0141s/iter; left time: 244.4957s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0321124\n",
      "\tspeed: 0.0141s/iter; left time: 242.7641s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1631328\n",
      "\tspeed: 0.0141s/iter; left time: 241.4773s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0411178\n",
      "\tspeed: 0.0141s/iter; left time: 239.7151s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0968408\n",
      "\tspeed: 0.0141s/iter; left time: 238.5184s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0870821\n",
      "\tspeed: 0.0141s/iter; left time: 237.0053s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0801791\n",
      "\tspeed: 0.0141s/iter; left time: 235.7022s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0594973\n",
      "\tspeed: 0.0141s/iter; left time: 234.1462s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0335212\n",
      "\tspeed: 0.0141s/iter; left time: 232.7603s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0368926\n",
      "\tspeed: 0.0141s/iter; left time: 231.3196s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0450576\n",
      "\tspeed: 0.0141s/iter; left time: 229.9025s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0347313\n",
      "\tspeed: 0.0141s/iter; left time: 228.6168s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0424165\n",
      "\tspeed: 0.0141s/iter; left time: 227.0664s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0402584\n",
      "\tspeed: 0.0141s/iter; left time: 225.7433s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0483931\n",
      "\tspeed: 0.0141s/iter; left time: 224.2754s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0466084\n",
      "\tspeed: 0.0141s/iter; left time: 222.8336s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0696267\n",
      "\tspeed: 0.0141s/iter; left time: 221.3997s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0498867\n",
      "\tspeed: 0.0141s/iter; left time: 220.0631s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0472143\n",
      "\tspeed: 0.0141s/iter; left time: 218.6932s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0286179\n",
      "\tspeed: 0.0141s/iter; left time: 217.3488s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0652074\n",
      "\tspeed: 0.0141s/iter; left time: 216.0445s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0806154\n",
      "\tspeed: 0.0141s/iter; left time: 215.0348s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0241131\n",
      "\tspeed: 0.0141s/iter; left time: 213.5940s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1368213\n",
      "\tspeed: 0.0141s/iter; left time: 212.2951s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0599695\n",
      "\tspeed: 0.0141s/iter; left time: 210.8201s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0379410\n",
      "\tspeed: 0.0141s/iter; left time: 209.4702s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0354008\n",
      "\tspeed: 0.0141s/iter; left time: 207.9548s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0736510\n",
      "\tspeed: 0.0141s/iter; left time: 206.6245s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1739008\n",
      "\tspeed: 0.0141s/iter; left time: 205.0868s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0485180\n",
      "\tspeed: 0.0141s/iter; left time: 203.7905s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0449934\n",
      "\tspeed: 0.0141s/iter; left time: 202.3480s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0739370\n",
      "\tspeed: 0.0141s/iter; left time: 200.8684s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0571640\n",
      "\tspeed: 0.0141s/iter; left time: 199.3963s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0302120\n",
      "\tspeed: 0.0141s/iter; left time: 197.9681s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0215335\n",
      "\tspeed: 0.0141s/iter; left time: 196.3796s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0245902\n",
      "\tspeed: 0.0141s/iter; left time: 195.4773s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0657642\n",
      "\tspeed: 0.0142s/iter; left time: 194.3232s\n",
      "Epoch: 7 cost time: 64.58608269691467\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0799027 Vali Loss: 0.0318890 Test Loss: 0.1099075\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0650190\n",
      "\tspeed: 0.1254s/iter; left time: 1701.4975s\n",
      "\titers: 200, epoch: 8 | loss: 0.0379673\n",
      "\tspeed: 0.0142s/iter; left time: 191.0015s\n",
      "\titers: 300, epoch: 8 | loss: 0.1171369\n",
      "\tspeed: 0.0142s/iter; left time: 189.5124s\n",
      "\titers: 400, epoch: 8 | loss: 0.0257347\n",
      "\tspeed: 0.0142s/iter; left time: 188.3233s\n",
      "\titers: 500, epoch: 8 | loss: 0.1941260\n",
      "\tspeed: 0.0142s/iter; left time: 186.8621s\n",
      "\titers: 600, epoch: 8 | loss: 0.1063102\n",
      "\tspeed: 0.0142s/iter; left time: 185.3719s\n",
      "\titers: 700, epoch: 8 | loss: 0.1239669\n",
      "\tspeed: 0.0142s/iter; left time: 183.9617s\n",
      "\titers: 800, epoch: 8 | loss: 0.1660410\n",
      "\tspeed: 0.0142s/iter; left time: 182.4297s\n",
      "\titers: 900, epoch: 8 | loss: 0.0780610\n",
      "\tspeed: 0.0142s/iter; left time: 181.1175s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0515431\n",
      "\tspeed: 0.0142s/iter; left time: 179.7386s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0882956\n",
      "\tspeed: 0.0142s/iter; left time: 178.2541s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1075122\n",
      "\tspeed: 0.0142s/iter; left time: 176.8778s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3586197\n",
      "\tspeed: 0.0142s/iter; left time: 175.4046s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0358899\n",
      "\tspeed: 0.0142s/iter; left time: 173.9648s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0764195\n",
      "\tspeed: 0.0142s/iter; left time: 172.5791s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1335103\n",
      "\tspeed: 0.0142s/iter; left time: 171.0633s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0673640\n",
      "\tspeed: 0.0142s/iter; left time: 170.0199s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1046597\n",
      "\tspeed: 0.0142s/iter; left time: 168.3504s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0721243\n",
      "\tspeed: 0.0146s/iter; left time: 171.9923s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0748461\n",
      "\tspeed: 0.0157s/iter; left time: 182.8195s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0329295\n",
      "\tspeed: 0.0142s/iter; left time: 164.2112s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0647665\n",
      "\tspeed: 0.0151s/iter; left time: 172.8613s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1034085\n",
      "\tspeed: 0.0152s/iter; left time: 172.8161s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0441008\n",
      "\tspeed: 0.0142s/iter; left time: 159.8509s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0601918\n",
      "\tspeed: 0.0155s/iter; left time: 173.5589s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0500528\n",
      "\tspeed: 0.0147s/iter; left time: 163.1430s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0394797\n",
      "\tspeed: 0.0142s/iter; left time: 155.5884s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0417367\n",
      "\tspeed: 0.0142s/iter; left time: 154.1964s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0667834\n",
      "\tspeed: 0.0142s/iter; left time: 152.4837s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2417478\n",
      "\tspeed: 0.0141s/iter; left time: 150.9202s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1085891\n",
      "\tspeed: 0.0141s/iter; left time: 149.3873s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1301135\n",
      "\tspeed: 0.0141s/iter; left time: 147.9791s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0353742\n",
      "\tspeed: 0.0141s/iter; left time: 146.6202s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0559303\n",
      "\tspeed: 0.0141s/iter; left time: 145.2149s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1247775\n",
      "\tspeed: 0.0141s/iter; left time: 143.7787s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0278944\n",
      "\tspeed: 0.0141s/iter; left time: 142.3277s\n",
      "\titers: 3700, epoch: 8 | loss: 0.1531375\n",
      "\tspeed: 0.0141s/iter; left time: 140.8675s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0711976\n",
      "\tspeed: 0.0141s/iter; left time: 139.4186s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0642341\n",
      "\tspeed: 0.0141s/iter; left time: 138.0387s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0397669\n",
      "\tspeed: 0.0141s/iter; left time: 136.6899s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0230157\n",
      "\tspeed: 0.0141s/iter; left time: 135.2397s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0790014\n",
      "\tspeed: 0.0141s/iter; left time: 133.8203s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0814686\n",
      "\tspeed: 0.0141s/iter; left time: 132.3635s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1217921\n",
      "\tspeed: 0.0141s/iter; left time: 130.9462s\n",
      "\titers: 4500, epoch: 8 | loss: 0.0966344\n",
      "\tspeed: 0.0141s/iter; left time: 129.5901s\n",
      "Epoch: 8 cost time: 65.3484890460968\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0800047 Vali Loss: 0.0319610 Test Loss: 0.1103049\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0600523\n",
      "\tspeed: 0.1338s/iter; left time: 1206.0626s\n",
      "\titers: 200, epoch: 9 | loss: 0.0487418\n",
      "\tspeed: 0.0157s/iter; left time: 140.1284s\n",
      "\titers: 300, epoch: 9 | loss: 0.0433959\n",
      "\tspeed: 0.0157s/iter; left time: 138.7354s\n",
      "\titers: 400, epoch: 9 | loss: 0.0904096\n",
      "\tspeed: 0.0157s/iter; left time: 137.0352s\n",
      "\titers: 500, epoch: 9 | loss: 0.0681607\n",
      "\tspeed: 0.0150s/iter; left time: 129.3881s\n",
      "\titers: 600, epoch: 9 | loss: 0.0289242\n",
      "\tspeed: 0.0141s/iter; left time: 120.3887s\n",
      "\titers: 700, epoch: 9 | loss: 0.1388253\n",
      "\tspeed: 0.0141s/iter; left time: 118.9120s\n",
      "\titers: 800, epoch: 9 | loss: 0.0502712\n",
      "\tspeed: 0.0141s/iter; left time: 117.3865s\n",
      "\titers: 900, epoch: 9 | loss: 0.0354139\n",
      "\tspeed: 0.0141s/iter; left time: 115.9396s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0521437\n",
      "\tspeed: 0.0141s/iter; left time: 114.4503s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0451012\n",
      "\tspeed: 0.0141s/iter; left time: 112.7710s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1797070\n",
      "\tspeed: 0.0141s/iter; left time: 111.4027s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0348372\n",
      "\tspeed: 0.0141s/iter; left time: 109.9176s\n",
      "\titers: 1400, epoch: 9 | loss: 0.1588385\n",
      "\tspeed: 0.0141s/iter; left time: 108.4992s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0631618\n",
      "\tspeed: 0.0141s/iter; left time: 107.2359s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0376816\n",
      "\tspeed: 0.0141s/iter; left time: 105.7870s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0712399\n",
      "\tspeed: 0.0141s/iter; left time: 104.3513s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0288822\n",
      "\tspeed: 0.0141s/iter; left time: 102.9932s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0223682\n",
      "\tspeed: 0.0141s/iter; left time: 101.6310s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0525243\n",
      "\tspeed: 0.0141s/iter; left time: 100.3376s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0298261\n",
      "\tspeed: 0.0141s/iter; left time: 98.9141s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1160899\n",
      "\tspeed: 0.0141s/iter; left time: 97.4955s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1097171\n",
      "\tspeed: 0.0141s/iter; left time: 96.0998s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0244775\n",
      "\tspeed: 0.0141s/iter; left time: 94.6887s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0383641\n",
      "\tspeed: 0.0141s/iter; left time: 93.2709s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0429010\n",
      "\tspeed: 0.0141s/iter; left time: 91.8869s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0384840\n",
      "\tspeed: 0.0141s/iter; left time: 90.4598s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1277316\n",
      "\tspeed: 0.0141s/iter; left time: 89.0629s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0405916\n",
      "\tspeed: 0.0141s/iter; left time: 87.6983s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0842399\n",
      "\tspeed: 0.0141s/iter; left time: 86.3340s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0302812\n",
      "\tspeed: 0.0141s/iter; left time: 85.0191s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0627885\n",
      "\tspeed: 0.0141s/iter; left time: 83.5943s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2010870\n",
      "\tspeed: 0.0141s/iter; left time: 82.1519s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1061479\n",
      "\tspeed: 0.0141s/iter; left time: 80.7455s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0494809\n",
      "\tspeed: 0.0141s/iter; left time: 79.3107s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0418020\n",
      "\tspeed: 0.0141s/iter; left time: 77.9231s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0760333\n",
      "\tspeed: 0.0141s/iter; left time: 76.4912s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1559199\n",
      "\tspeed: 0.0141s/iter; left time: 75.0748s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0551469\n",
      "\tspeed: 0.0141s/iter; left time: 73.7026s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0685438\n",
      "\tspeed: 0.0141s/iter; left time: 72.2682s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0416409\n",
      "\tspeed: 0.0141s/iter; left time: 70.8361s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0614735\n",
      "\tspeed: 0.0142s/iter; left time: 69.4944s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0527416\n",
      "\tspeed: 0.0141s/iter; left time: 68.0315s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0368895\n",
      "\tspeed: 0.0141s/iter; left time: 66.6069s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0276168\n",
      "\tspeed: 0.0141s/iter; left time: 65.1727s\n",
      "Epoch: 9 cost time: 65.28556156158447\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0781508 Vali Loss: 0.0318946 Test Loss: 0.1100924\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0509198\n",
      "\tspeed: 0.1399s/iter; left time: 623.6098s\n",
      "\titers: 200, epoch: 10 | loss: 0.0699041\n",
      "\tspeed: 0.0141s/iter; left time: 61.5740s\n",
      "\titers: 300, epoch: 10 | loss: 0.0562228\n",
      "\tspeed: 0.0141s/iter; left time: 60.1695s\n",
      "\titers: 400, epoch: 10 | loss: 0.0455549\n",
      "\tspeed: 0.0141s/iter; left time: 58.7507s\n",
      "\titers: 500, epoch: 10 | loss: 0.0590558\n",
      "\tspeed: 0.0141s/iter; left time: 57.3159s\n",
      "\titers: 600, epoch: 10 | loss: 0.0825835\n",
      "\tspeed: 0.0141s/iter; left time: 55.9327s\n",
      "\titers: 700, epoch: 10 | loss: 0.0511886\n",
      "\tspeed: 0.0141s/iter; left time: 54.5177s\n",
      "\titers: 800, epoch: 10 | loss: 0.1481780\n",
      "\tspeed: 0.0141s/iter; left time: 53.0844s\n",
      "\titers: 900, epoch: 10 | loss: 0.0864040\n",
      "\tspeed: 0.0141s/iter; left time: 51.6863s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0387923\n",
      "\tspeed: 0.0141s/iter; left time: 50.2596s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0558373\n",
      "\tspeed: 0.0141s/iter; left time: 48.8610s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0254163\n",
      "\tspeed: 0.0141s/iter; left time: 47.4433s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0425928\n",
      "\tspeed: 0.0141s/iter; left time: 46.0315s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0838863\n",
      "\tspeed: 0.0141s/iter; left time: 44.6069s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2801536\n",
      "\tspeed: 0.0141s/iter; left time: 43.2416s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0680354\n",
      "\tspeed: 0.0141s/iter; left time: 41.8140s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0217804\n",
      "\tspeed: 0.0141s/iter; left time: 40.3662s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0513207\n",
      "\tspeed: 0.0141s/iter; left time: 38.9507s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1245733\n",
      "\tspeed: 0.0141s/iter; left time: 37.5495s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0599212\n",
      "\tspeed: 0.0141s/iter; left time: 36.1230s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0409935\n",
      "\tspeed: 0.0141s/iter; left time: 34.7276s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0674154\n",
      "\tspeed: 0.0141s/iter; left time: 33.3064s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0985968\n",
      "\tspeed: 0.0141s/iter; left time: 31.8943s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0614144\n",
      "\tspeed: 0.0141s/iter; left time: 30.4678s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2128481\n",
      "\tspeed: 0.0141s/iter; left time: 29.0648s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1032259\n",
      "\tspeed: 0.0141s/iter; left time: 27.6509s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0575424\n",
      "\tspeed: 0.0141s/iter; left time: 26.2257s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0699627\n",
      "\tspeed: 0.0141s/iter; left time: 24.8171s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0553976\n",
      "\tspeed: 0.0141s/iter; left time: 23.4048s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0520110\n",
      "\tspeed: 0.0141s/iter; left time: 21.9898s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0276632\n",
      "\tspeed: 0.0141s/iter; left time: 20.5746s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0918682\n",
      "\tspeed: 0.0141s/iter; left time: 19.1588s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0690628\n",
      "\tspeed: 0.0141s/iter; left time: 17.7536s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1260092\n",
      "\tspeed: 0.0141s/iter; left time: 16.3338s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0271291\n",
      "\tspeed: 0.0141s/iter; left time: 14.9177s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1033404\n",
      "\tspeed: 0.0141s/iter; left time: 13.5094s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0948225\n",
      "\tspeed: 0.0141s/iter; left time: 12.1110s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0546271\n",
      "\tspeed: 0.0142s/iter; left time: 10.7225s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0271495\n",
      "\tspeed: 0.0142s/iter; left time: 9.3075s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0380873\n",
      "\tspeed: 0.0142s/iter; left time: 7.8886s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1221855\n",
      "\tspeed: 0.0142s/iter; left time: 6.4673s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0514477\n",
      "\tspeed: 0.0142s/iter; left time: 5.0415s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0330176\n",
      "\tspeed: 0.0142s/iter; left time: 3.6277s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0560528\n",
      "\tspeed: 0.0142s/iter; left time: 2.2112s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1235402\n",
      "\tspeed: 0.0142s/iter; left time: 0.7926s\n",
      "Epoch: 10 cost time: 64.66689372062683\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0787837 Vali Loss: 0.0318148 Test Loss: 0.1101695\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10968974977731705, mae:0.20709599554538727\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --data_path forecast_data.csv --features MS --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "faf284b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          forecast_data.csv   Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1759927\n",
      "\tspeed: 0.0277s/iter; left time: 1257.6033s\n",
      "\titers: 200, epoch: 1 | loss: 0.1005098\n",
      "\tspeed: 0.0157s/iter; left time: 713.6029s\n",
      "\titers: 300, epoch: 1 | loss: 0.0638654\n",
      "\tspeed: 0.0157s/iter; left time: 710.3213s\n",
      "\titers: 400, epoch: 1 | loss: 0.1360727\n",
      "\tspeed: 0.0157s/iter; left time: 709.2613s\n",
      "\titers: 500, epoch: 1 | loss: 0.0321013\n",
      "\tspeed: 0.0157s/iter; left time: 708.6350s\n",
      "\titers: 600, epoch: 1 | loss: 0.0772481\n",
      "\tspeed: 0.0153s/iter; left time: 687.5537s\n",
      "\titers: 700, epoch: 1 | loss: 0.1345826\n",
      "\tspeed: 0.0140s/iter; left time: 630.1097s\n",
      "\titers: 800, epoch: 1 | loss: 0.5376329\n",
      "\tspeed: 0.0140s/iter; left time: 628.2900s\n",
      "\titers: 900, epoch: 1 | loss: 0.2582480\n",
      "\tspeed: 0.0141s/iter; left time: 631.0382s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0790523\n",
      "\tspeed: 0.0142s/iter; left time: 634.6328s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0739570\n",
      "\tspeed: 0.0141s/iter; left time: 627.7564s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0714946\n",
      "\tspeed: 0.0141s/iter; left time: 627.3582s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2813226\n",
      "\tspeed: 0.0142s/iter; left time: 626.9722s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0259122\n",
      "\tspeed: 0.0142s/iter; left time: 625.4052s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1274829\n",
      "\tspeed: 0.0142s/iter; left time: 623.5839s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1681881\n",
      "\tspeed: 0.0152s/iter; left time: 668.4443s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4031706\n",
      "\tspeed: 0.0147s/iter; left time: 643.1592s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0563735\n",
      "\tspeed: 0.0162s/iter; left time: 707.4778s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1549343\n",
      "\tspeed: 0.0151s/iter; left time: 660.5284s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4181454\n",
      "\tspeed: 0.0141s/iter; left time: 613.7381s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1926515\n",
      "\tspeed: 0.0141s/iter; left time: 612.7259s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2900308\n",
      "\tspeed: 0.0141s/iter; left time: 612.6921s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1530581\n",
      "\tspeed: 0.0141s/iter; left time: 611.8991s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1707360\n",
      "\tspeed: 0.0147s/iter; left time: 634.7366s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2219595\n",
      "\tspeed: 0.0162s/iter; left time: 695.5587s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1108732\n",
      "\tspeed: 0.0161s/iter; left time: 693.6251s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1635533\n",
      "\tspeed: 0.0142s/iter; left time: 608.8870s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2325931\n",
      "\tspeed: 0.0142s/iter; left time: 605.0631s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1855148\n",
      "\tspeed: 0.0141s/iter; left time: 600.6799s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0875452\n",
      "\tspeed: 0.0142s/iter; left time: 603.2134s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0723489\n",
      "\tspeed: 0.0142s/iter; left time: 602.9926s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1847000\n",
      "\tspeed: 0.0142s/iter; left time: 601.4864s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0331125\n",
      "\tspeed: 0.0142s/iter; left time: 601.2754s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2570869\n",
      "\tspeed: 0.0141s/iter; left time: 595.5500s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0895515\n",
      "\tspeed: 0.0141s/iter; left time: 594.0826s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1073619\n",
      "\tspeed: 0.0141s/iter; left time: 591.3264s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0734465\n",
      "\tspeed: 0.0142s/iter; left time: 593.2640s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1570207\n",
      "\tspeed: 0.0142s/iter; left time: 592.8469s\n",
      "\titers: 3900, epoch: 1 | loss: 0.2763611\n",
      "\tspeed: 0.0142s/iter; left time: 591.5288s\n",
      "\titers: 4000, epoch: 1 | loss: 0.5206591\n",
      "\tspeed: 0.0141s/iter; left time: 587.3150s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0891215\n",
      "\tspeed: 0.0141s/iter; left time: 585.8130s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0833094\n",
      "\tspeed: 0.0141s/iter; left time: 584.4693s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1689244\n",
      "\tspeed: 0.0141s/iter; left time: 582.9222s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0301080\n",
      "\tspeed: 0.0141s/iter; left time: 579.6773s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0529098\n",
      "\tspeed: 0.0141s/iter; left time: 580.3432s\n",
      "Epoch: 1 cost time: 67.48741960525513\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1769363 Vali Loss: 0.0391474 Test Loss: 0.1294462\n",
      "Validation loss decreased (inf --> 0.039147).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0709434\n",
      "\tspeed: 0.1427s/iter; left time: 5837.0996s\n",
      "\titers: 200, epoch: 2 | loss: 0.0488707\n",
      "\tspeed: 0.0157s/iter; left time: 641.1757s\n",
      "\titers: 300, epoch: 2 | loss: 0.0839833\n",
      "\tspeed: 0.0157s/iter; left time: 638.9673s\n",
      "\titers: 400, epoch: 2 | loss: 0.0990587\n",
      "\tspeed: 0.0157s/iter; left time: 638.7586s\n",
      "\titers: 500, epoch: 2 | loss: 0.3297422\n",
      "\tspeed: 0.0147s/iter; left time: 595.6905s\n",
      "\titers: 600, epoch: 2 | loss: 0.0925497\n",
      "\tspeed: 0.0142s/iter; left time: 575.3879s\n",
      "\titers: 700, epoch: 2 | loss: 0.0814778\n",
      "\tspeed: 0.0142s/iter; left time: 572.8306s\n",
      "\titers: 800, epoch: 2 | loss: 0.0509366\n",
      "\tspeed: 0.0142s/iter; left time: 570.6741s\n",
      "\titers: 900, epoch: 2 | loss: 0.0423700\n",
      "\tspeed: 0.0142s/iter; left time: 568.1874s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1782139\n",
      "\tspeed: 0.0142s/iter; left time: 568.4940s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1907831\n",
      "\tspeed: 0.0142s/iter; left time: 566.8821s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1204457\n",
      "\tspeed: 0.0142s/iter; left time: 565.5181s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1226730\n",
      "\tspeed: 0.0142s/iter; left time: 564.0953s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1283185\n",
      "\tspeed: 0.0142s/iter; left time: 562.3497s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0706527\n",
      "\tspeed: 0.0142s/iter; left time: 561.3186s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1035237\n",
      "\tspeed: 0.0142s/iter; left time: 559.8214s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2888501\n",
      "\tspeed: 0.0142s/iter; left time: 558.3769s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1360618\n",
      "\tspeed: 0.0142s/iter; left time: 557.0221s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1179228\n",
      "\tspeed: 0.0142s/iter; left time: 555.7614s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0605080\n",
      "\tspeed: 0.0142s/iter; left time: 554.4117s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0684468\n",
      "\tspeed: 0.0142s/iter; left time: 553.2693s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1475488\n",
      "\tspeed: 0.0142s/iter; left time: 551.7349s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1355673\n",
      "\tspeed: 0.0142s/iter; left time: 550.3778s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1321094\n",
      "\tspeed: 0.0142s/iter; left time: 548.7788s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0747641\n",
      "\tspeed: 0.0142s/iter; left time: 545.3399s\n",
      "\titers: 2600, epoch: 2 | loss: 0.3410626\n",
      "\tspeed: 0.0142s/iter; left time: 543.8651s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1080215\n",
      "\tspeed: 0.0142s/iter; left time: 542.2836s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0430763\n",
      "\tspeed: 0.0142s/iter; left time: 542.3069s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0482149\n",
      "\tspeed: 0.0142s/iter; left time: 540.8932s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1212059\n",
      "\tspeed: 0.0142s/iter; left time: 539.3029s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0924855\n",
      "\tspeed: 0.0142s/iter; left time: 537.8009s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1888116\n",
      "\tspeed: 0.0142s/iter; left time: 536.5056s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0510624\n",
      "\tspeed: 0.0142s/iter; left time: 534.2630s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0708582\n",
      "\tspeed: 0.0142s/iter; left time: 532.8901s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1610978\n",
      "\tspeed: 0.0142s/iter; left time: 531.6570s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1541513\n",
      "\tspeed: 0.0142s/iter; left time: 530.0781s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1887961\n",
      "\tspeed: 0.0142s/iter; left time: 528.5937s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1578278\n",
      "\tspeed: 0.0142s/iter; left time: 526.9098s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2275678\n",
      "\tspeed: 0.0142s/iter; left time: 525.3504s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2216853\n",
      "\tspeed: 0.0142s/iter; left time: 524.2206s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0922666\n",
      "\tspeed: 0.0141s/iter; left time: 521.9223s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0640367\n",
      "\tspeed: 0.0141s/iter; left time: 518.1146s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1141457\n",
      "\tspeed: 0.0141s/iter; left time: 516.9972s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1244170\n",
      "\tspeed: 0.0141s/iter; left time: 515.6818s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0403202\n",
      "\tspeed: 0.0141s/iter; left time: 515.2830s\n",
      "Epoch: 2 cost time: 65.53769874572754\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1516635 Vali Loss: 0.0330107 Test Loss: 0.1132908\n",
      "Validation loss decreased (0.039147 --> 0.033011).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1018267\n",
      "\tspeed: 0.1264s/iter; left time: 4592.2089s\n",
      "\titers: 200, epoch: 3 | loss: 0.0484010\n",
      "\tspeed: 0.0141s/iter; left time: 512.0496s\n",
      "\titers: 300, epoch: 3 | loss: 0.1089981\n",
      "\tspeed: 0.0142s/iter; left time: 511.9695s\n",
      "\titers: 400, epoch: 3 | loss: 0.0869171\n",
      "\tspeed: 0.0141s/iter; left time: 509.2348s\n",
      "\titers: 500, epoch: 3 | loss: 0.0957361\n",
      "\tspeed: 0.0141s/iter; left time: 507.6931s\n",
      "\titers: 600, epoch: 3 | loss: 0.1074140\n",
      "\tspeed: 0.0141s/iter; left time: 505.8155s\n",
      "\titers: 700, epoch: 3 | loss: 0.3365041\n",
      "\tspeed: 0.0141s/iter; left time: 505.4359s\n",
      "\titers: 800, epoch: 3 | loss: 0.1526949\n",
      "\tspeed: 0.0151s/iter; left time: 537.3792s\n",
      "\titers: 900, epoch: 3 | loss: 0.0599587\n",
      "\tspeed: 0.0156s/iter; left time: 555.6622s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1092401\n",
      "\tspeed: 0.0156s/iter; left time: 553.7055s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1904721\n",
      "\tspeed: 0.0156s/iter; left time: 552.5528s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1729239\n",
      "\tspeed: 0.0156s/iter; left time: 550.6621s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2153694\n",
      "\tspeed: 0.0157s/iter; left time: 551.3259s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1360826\n",
      "\tspeed: 0.0157s/iter; left time: 548.7322s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2275563\n",
      "\tspeed: 0.0157s/iter; left time: 546.9400s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1521941\n",
      "\tspeed: 0.0156s/iter; left time: 544.6608s\n",
      "\titers: 1700, epoch: 3 | loss: 0.4756070\n",
      "\tspeed: 0.0157s/iter; left time: 543.9012s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0934537\n",
      "\tspeed: 0.0156s/iter; left time: 541.9869s\n",
      "\titers: 1900, epoch: 3 | loss: 0.2139462\n",
      "\tspeed: 0.0156s/iter; left time: 540.2676s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2198817\n",
      "\tspeed: 0.0156s/iter; left time: 538.8507s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0862591\n",
      "\tspeed: 0.0156s/iter; left time: 537.2220s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0581241\n",
      "\tspeed: 0.0157s/iter; left time: 536.7160s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1154659\n",
      "\tspeed: 0.0152s/iter; left time: 517.7934s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0916204\n",
      "\tspeed: 0.0157s/iter; left time: 535.0335s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1502590\n",
      "\tspeed: 0.0151s/iter; left time: 511.6720s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0821638\n",
      "\tspeed: 0.0141s/iter; left time: 478.4689s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0532584\n",
      "\tspeed: 0.0141s/iter; left time: 476.6548s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1231633\n",
      "\tspeed: 0.0141s/iter; left time: 474.8204s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0771413\n",
      "\tspeed: 0.0142s/iter; left time: 475.8107s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0379044\n",
      "\tspeed: 0.0142s/iter; left time: 475.2661s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1473286\n",
      "\tspeed: 0.0143s/iter; left time: 476.1460s\n",
      "\titers: 3200, epoch: 3 | loss: 0.6245164\n",
      "\tspeed: 0.0143s/iter; left time: 476.0751s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0808302\n",
      "\tspeed: 0.0143s/iter; left time: 473.2248s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2345002\n",
      "\tspeed: 0.0142s/iter; left time: 470.3834s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0410836\n",
      "\tspeed: 0.0143s/iter; left time: 470.5565s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1252303\n",
      "\tspeed: 0.0141s/iter; left time: 462.8410s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1074857\n",
      "\tspeed: 0.0141s/iter; left time: 461.6271s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0587929\n",
      "\tspeed: 0.0142s/iter; left time: 464.7484s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1475587\n",
      "\tspeed: 0.0142s/iter; left time: 462.8582s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2225686\n",
      "\tspeed: 0.0143s/iter; left time: 464.2702s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2237215\n",
      "\tspeed: 0.0144s/iter; left time: 465.0202s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0605643\n",
      "\tspeed: 0.0141s/iter; left time: 455.8279s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0303679\n",
      "\tspeed: 0.0143s/iter; left time: 458.0593s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0757557\n",
      "\tspeed: 0.0141s/iter; left time: 453.3052s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0266420\n",
      "\tspeed: 0.0141s/iter; left time: 451.3149s\n",
      "Epoch: 3 cost time: 67.36763453483582\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1268762 Vali Loss: 0.0333214 Test Loss: 0.1176284\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0677748\n",
      "\tspeed: 0.1258s/iter; left time: 3999.5041s\n",
      "\titers: 200, epoch: 4 | loss: 0.3070554\n",
      "\tspeed: 0.0143s/iter; left time: 451.5942s\n",
      "\titers: 300, epoch: 4 | loss: 0.0280400\n",
      "\tspeed: 0.0142s/iter; left time: 449.9820s\n",
      "\titers: 400, epoch: 4 | loss: 0.1560239\n",
      "\tspeed: 0.0142s/iter; left time: 448.5262s\n",
      "\titers: 500, epoch: 4 | loss: 0.1099010\n",
      "\tspeed: 0.0142s/iter; left time: 446.1259s\n",
      "\titers: 600, epoch: 4 | loss: 0.0953508\n",
      "\tspeed: 0.0142s/iter; left time: 445.3682s\n",
      "\titers: 700, epoch: 4 | loss: 0.0841895\n",
      "\tspeed: 0.0142s/iter; left time: 443.6112s\n",
      "\titers: 800, epoch: 4 | loss: 0.1544153\n",
      "\tspeed: 0.0142s/iter; left time: 442.1569s\n",
      "\titers: 900, epoch: 4 | loss: 0.1274071\n",
      "\tspeed: 0.0142s/iter; left time: 440.8456s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1229334\n",
      "\tspeed: 0.0142s/iter; left time: 439.3354s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1262303\n",
      "\tspeed: 0.0142s/iter; left time: 438.5884s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0552251\n",
      "\tspeed: 0.0143s/iter; left time: 437.3519s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1007653\n",
      "\tspeed: 0.0143s/iter; left time: 436.0007s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0506458\n",
      "\tspeed: 0.0142s/iter; left time: 433.8105s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2370257\n",
      "\tspeed: 0.0141s/iter; left time: 429.9567s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0753491\n",
      "\tspeed: 0.0142s/iter; left time: 428.9280s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1660052\n",
      "\tspeed: 0.0142s/iter; left time: 427.2374s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0490977\n",
      "\tspeed: 0.0141s/iter; left time: 425.2696s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0947329\n",
      "\tspeed: 0.0142s/iter; left time: 424.6405s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1272814\n",
      "\tspeed: 0.0142s/iter; left time: 423.0606s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0179249\n",
      "\tspeed: 0.0141s/iter; left time: 420.8437s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1878305\n",
      "\tspeed: 0.0141s/iter; left time: 419.1581s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1629618\n",
      "\tspeed: 0.0141s/iter; left time: 417.5783s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1148672\n",
      "\tspeed: 0.0142s/iter; left time: 417.3161s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0289091\n",
      "\tspeed: 0.0141s/iter; left time: 415.0432s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0827030\n",
      "\tspeed: 0.0141s/iter; left time: 413.2136s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5467109\n",
      "\tspeed: 0.0141s/iter; left time: 411.3194s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0275374\n",
      "\tspeed: 0.0141s/iter; left time: 409.6466s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0314616\n",
      "\tspeed: 0.0141s/iter; left time: 408.8677s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1049759\n",
      "\tspeed: 0.0141s/iter; left time: 407.3746s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1131950\n",
      "\tspeed: 0.0141s/iter; left time: 406.2923s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1471792\n",
      "\tspeed: 0.0141s/iter; left time: 404.8924s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0893551\n",
      "\tspeed: 0.0141s/iter; left time: 403.6356s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0585081\n",
      "\tspeed: 0.0141s/iter; left time: 401.8691s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0485755\n",
      "\tspeed: 0.0141s/iter; left time: 400.2752s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0397043\n",
      "\tspeed: 0.0141s/iter; left time: 398.7495s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1933319\n",
      "\tspeed: 0.0141s/iter; left time: 398.3421s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0393248\n",
      "\tspeed: 0.0141s/iter; left time: 397.2060s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1037640\n",
      "\tspeed: 0.0141s/iter; left time: 395.0622s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0824659\n",
      "\tspeed: 0.0141s/iter; left time: 393.2398s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0382093\n",
      "\tspeed: 0.0141s/iter; left time: 391.9108s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0830566\n",
      "\tspeed: 0.0141s/iter; left time: 390.2295s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3238772\n",
      "\tspeed: 0.0141s/iter; left time: 388.5410s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1110998\n",
      "\tspeed: 0.0141s/iter; left time: 386.4482s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1073991\n",
      "\tspeed: 0.0141s/iter; left time: 385.7494s\n",
      "Epoch: 4 cost time: 64.71519756317139\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1100078 Vali Loss: 0.0326034 Test Loss: 0.1216023\n",
      "Validation loss decreased (0.033011 --> 0.032603).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0626363\n",
      "\tspeed: 0.1255s/iter; left time: 3417.3267s\n",
      "\titers: 200, epoch: 5 | loss: 0.0651858\n",
      "\tspeed: 0.0142s/iter; left time: 384.3132s\n",
      "\titers: 300, epoch: 5 | loss: 0.0690542\n",
      "\tspeed: 0.0142s/iter; left time: 382.5256s\n",
      "\titers: 400, epoch: 5 | loss: 0.0530531\n",
      "\tspeed: 0.0142s/iter; left time: 381.3639s\n",
      "\titers: 500, epoch: 5 | loss: 0.1194133\n",
      "\tspeed: 0.0142s/iter; left time: 379.7262s\n",
      "\titers: 600, epoch: 5 | loss: 0.0193160\n",
      "\tspeed: 0.0142s/iter; left time: 378.4618s\n",
      "\titers: 700, epoch: 5 | loss: 0.0367207\n",
      "\tspeed: 0.0142s/iter; left time: 378.3272s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935420\n",
      "\tspeed: 0.0143s/iter; left time: 380.6305s\n",
      "\titers: 900, epoch: 5 | loss: 0.0621724\n",
      "\tspeed: 0.0147s/iter; left time: 388.8648s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1430875\n",
      "\tspeed: 0.0148s/iter; left time: 389.0192s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0329404\n",
      "\tspeed: 0.0144s/iter; left time: 376.9066s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1003864\n",
      "\tspeed: 0.0143s/iter; left time: 373.2510s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0802891\n",
      "\tspeed: 0.0144s/iter; left time: 375.9379s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1367226\n",
      "\tspeed: 0.0148s/iter; left time: 384.7229s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1316963\n",
      "\tspeed: 0.0147s/iter; left time: 379.0418s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0467243\n",
      "\tspeed: 0.0146s/iter; left time: 376.1045s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1635501\n",
      "\tspeed: 0.0149s/iter; left time: 383.0767s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1020788\n",
      "\tspeed: 0.0148s/iter; left time: 377.5999s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0319691\n",
      "\tspeed: 0.0144s/iter; left time: 366.6164s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5800079\n",
      "\tspeed: 0.0145s/iter; left time: 366.2584s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0890467\n",
      "\tspeed: 0.0144s/iter; left time: 362.9963s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0647310\n",
      "\tspeed: 0.0144s/iter; left time: 362.6014s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0395606\n",
      "\tspeed: 0.0144s/iter; left time: 361.1297s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0787048\n",
      "\tspeed: 0.0150s/iter; left time: 374.5179s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761334\n",
      "\tspeed: 0.0144s/iter; left time: 358.5467s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0758988\n",
      "\tspeed: 0.0152s/iter; left time: 375.7857s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1048270\n",
      "\tspeed: 0.0144s/iter; left time: 354.9787s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1204636\n",
      "\tspeed: 0.0145s/iter; left time: 354.5950s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0572192\n",
      "\tspeed: 0.0155s/iter; left time: 377.8371s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0559349\n",
      "\tspeed: 0.0150s/iter; left time: 365.1288s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0404658\n",
      "\tspeed: 0.0150s/iter; left time: 363.1180s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0190077\n",
      "\tspeed: 0.0145s/iter; left time: 349.0866s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0523258\n",
      "\tspeed: 0.0143s/iter; left time: 343.9169s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0526128\n",
      "\tspeed: 0.0149s/iter; left time: 355.4926s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0472551\n",
      "\tspeed: 0.0144s/iter; left time: 343.5573s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0672086\n",
      "\tspeed: 0.0144s/iter; left time: 342.9063s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0566152\n",
      "\tspeed: 0.0144s/iter; left time: 339.9545s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1113559\n",
      "\tspeed: 0.0161s/iter; left time: 379.9679s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0576941\n",
      "\tspeed: 0.0150s/iter; left time: 350.8607s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0448340\n",
      "\tspeed: 0.0149s/iter; left time: 347.7257s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0229164\n",
      "\tspeed: 0.0154s/iter; left time: 358.6646s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0602215\n",
      "\tspeed: 0.0145s/iter; left time: 335.8163s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0985747\n",
      "\tspeed: 0.0144s/iter; left time: 332.0601s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0227245\n",
      "\tspeed: 0.0145s/iter; left time: 332.8667s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0608477\n",
      "\tspeed: 0.0154s/iter; left time: 351.5616s\n",
      "Epoch: 5 cost time: 66.99641156196594\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0998084 Vali Loss: 0.0317413 Test Loss: 0.1186500\n",
      "Validation loss decreased (0.032603 --> 0.031741).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0277388\n",
      "\tspeed: 0.1373s/iter; left time: 3114.0233s\n",
      "\titers: 200, epoch: 6 | loss: 0.1703286\n",
      "\tspeed: 0.0145s/iter; left time: 328.2132s\n",
      "\titers: 300, epoch: 6 | loss: 0.1249021\n",
      "\tspeed: 0.0145s/iter; left time: 326.3546s\n",
      "\titers: 400, epoch: 6 | loss: 0.0752089\n",
      "\tspeed: 0.0144s/iter; left time: 322.3617s\n",
      "\titers: 500, epoch: 6 | loss: 0.1561552\n",
      "\tspeed: 0.0146s/iter; left time: 325.2999s\n",
      "\titers: 600, epoch: 6 | loss: 0.1479534\n",
      "\tspeed: 0.0144s/iter; left time: 318.8855s\n",
      "\titers: 700, epoch: 6 | loss: 0.0331219\n",
      "\tspeed: 0.0145s/iter; left time: 319.9126s\n",
      "\titers: 800, epoch: 6 | loss: 0.0567712\n",
      "\tspeed: 0.0144s/iter; left time: 317.0425s\n",
      "\titers: 900, epoch: 6 | loss: 0.0486447\n",
      "\tspeed: 0.0146s/iter; left time: 320.0136s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1846031\n",
      "\tspeed: 0.0144s/iter; left time: 313.2609s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0510034\n",
      "\tspeed: 0.0153s/iter; left time: 332.2444s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0916864\n",
      "\tspeed: 0.0146s/iter; left time: 314.6264s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1022680\n",
      "\tspeed: 0.0150s/iter; left time: 321.5902s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0707264\n",
      "\tspeed: 0.0153s/iter; left time: 327.3987s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0911812\n",
      "\tspeed: 0.0145s/iter; left time: 307.7024s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1554714\n",
      "\tspeed: 0.0145s/iter; left time: 308.0790s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0485573\n",
      "\tspeed: 0.0144s/iter; left time: 302.8960s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2463143\n",
      "\tspeed: 0.0144s/iter; left time: 302.4132s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1036867\n",
      "\tspeed: 0.0144s/iter; left time: 300.3925s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1787771\n",
      "\tspeed: 0.0146s/iter; left time: 303.0782s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1134794\n",
      "\tspeed: 0.0157s/iter; left time: 324.7279s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1263179\n",
      "\tspeed: 0.0148s/iter; left time: 304.5854s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1050347\n",
      "\tspeed: 0.0146s/iter; left time: 299.1413s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0861309\n",
      "\tspeed: 0.0159s/iter; left time: 323.6553s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0649742\n",
      "\tspeed: 0.0145s/iter; left time: 294.2193s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0782598\n",
      "\tspeed: 0.0144s/iter; left time: 289.7204s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0622398\n",
      "\tspeed: 0.0147s/iter; left time: 295.9014s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1098996\n",
      "\tspeed: 0.0145s/iter; left time: 289.8284s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2320417\n",
      "\tspeed: 0.0160s/iter; left time: 317.3908s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0310082\n",
      "\tspeed: 0.0146s/iter; left time: 289.0642s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0774086\n",
      "\tspeed: 0.0150s/iter; left time: 295.9798s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0803729\n",
      "\tspeed: 0.0150s/iter; left time: 292.9965s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1023466\n",
      "\tspeed: 0.0158s/iter; left time: 308.6861s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0215578\n",
      "\tspeed: 0.0165s/iter; left time: 318.7704s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0298279\n",
      "\tspeed: 0.0162s/iter; left time: 312.4534s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0747626\n",
      "\tspeed: 0.0145s/iter; left time: 278.8752s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1326391\n",
      "\tspeed: 0.0155s/iter; left time: 295.1146s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1249703\n",
      "\tspeed: 0.0150s/iter; left time: 284.7318s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0434858\n",
      "\tspeed: 0.0145s/iter; left time: 272.7936s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1853186\n",
      "\tspeed: 0.0147s/iter; left time: 275.7782s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0165570\n",
      "\tspeed: 0.0144s/iter; left time: 269.6517s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0281204\n",
      "\tspeed: 0.0153s/iter; left time: 283.7738s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1069887\n",
      "\tspeed: 0.0151s/iter; left time: 279.2996s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0844429\n",
      "\tspeed: 0.0147s/iter; left time: 269.2242s\n",
      "\titers: 4500, epoch: 6 | loss: 0.1098494\n",
      "\tspeed: 0.0144s/iter; left time: 262.3649s\n",
      "Epoch: 6 cost time: 68.01139569282532\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0954476 Vali Loss: 0.0316978 Test Loss: 0.1208863\n",
      "Validation loss decreased (0.031741 --> 0.031698).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1025192\n",
      "\tspeed: 0.1348s/iter; left time: 2442.4655s\n",
      "\titers: 200, epoch: 7 | loss: 0.2299474\n",
      "\tspeed: 0.0145s/iter; left time: 261.5187s\n",
      "\titers: 300, epoch: 7 | loss: 0.1932043\n",
      "\tspeed: 0.0152s/iter; left time: 273.1014s\n",
      "\titers: 400, epoch: 7 | loss: 0.0472199\n",
      "\tspeed: 0.0153s/iter; left time: 273.4219s\n",
      "\titers: 500, epoch: 7 | loss: 0.0327715\n",
      "\tspeed: 0.0150s/iter; left time: 265.8947s\n",
      "\titers: 600, epoch: 7 | loss: 0.0661617\n",
      "\tspeed: 0.0145s/iter; left time: 255.3465s\n",
      "\titers: 700, epoch: 7 | loss: 0.0148620\n",
      "\tspeed: 0.0149s/iter; left time: 261.7376s\n",
      "\titers: 800, epoch: 7 | loss: 0.0486824\n",
      "\tspeed: 0.0145s/iter; left time: 251.8018s\n",
      "\titers: 900, epoch: 7 | loss: 0.2101677\n",
      "\tspeed: 0.0145s/iter; left time: 251.3542s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0482192\n",
      "\tspeed: 0.0149s/iter; left time: 256.1207s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1973685\n",
      "\tspeed: 0.0145s/iter; left time: 248.4485s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0723647\n",
      "\tspeed: 0.0146s/iter; left time: 247.7255s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1097718\n",
      "\tspeed: 0.0145s/iter; left time: 245.2751s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0843925\n",
      "\tspeed: 0.0146s/iter; left time: 245.7593s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0898031\n",
      "\tspeed: 0.0147s/iter; left time: 245.2230s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0564848\n",
      "\tspeed: 0.0143s/iter; left time: 238.2659s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0320187\n",
      "\tspeed: 0.0144s/iter; left time: 237.3144s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0875859\n",
      "\tspeed: 0.0145s/iter; left time: 238.3398s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0746750\n",
      "\tspeed: 0.0147s/iter; left time: 239.1313s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0440867\n",
      "\tspeed: 0.0144s/iter; left time: 233.2856s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0390446\n",
      "\tspeed: 0.0143s/iter; left time: 231.3111s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0599097\n",
      "\tspeed: 0.0151s/iter; left time: 242.6928s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0415539\n",
      "\tspeed: 0.0146s/iter; left time: 232.6266s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0631474\n",
      "\tspeed: 0.0145s/iter; left time: 229.2150s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1793022\n",
      "\tspeed: 0.0143s/iter; left time: 224.7544s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0988206\n",
      "\tspeed: 0.0144s/iter; left time: 225.0680s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0328353\n",
      "\tspeed: 0.0150s/iter; left time: 232.8829s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0411637\n",
      "\tspeed: 0.0146s/iter; left time: 224.5524s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0697710\n",
      "\tspeed: 0.0148s/iter; left time: 226.1250s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0701139\n",
      "\tspeed: 0.0146s/iter; left time: 222.6103s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0191403\n",
      "\tspeed: 0.0146s/iter; left time: 220.8612s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1972307\n",
      "\tspeed: 0.0149s/iter; left time: 223.9078s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0636904\n",
      "\tspeed: 0.0146s/iter; left time: 217.2789s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0350361\n",
      "\tspeed: 0.0150s/iter; left time: 222.6560s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0439327\n",
      "\tspeed: 0.0148s/iter; left time: 218.0540s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1054622\n",
      "\tspeed: 0.0143s/iter; left time: 209.0849s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1744172\n",
      "\tspeed: 0.0144s/iter; left time: 209.3798s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0419758\n",
      "\tspeed: 0.0145s/iter; left time: 208.8814s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0580527\n",
      "\tspeed: 0.0146s/iter; left time: 208.4569s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0944986\n",
      "\tspeed: 0.0151s/iter; left time: 214.1367s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0736681\n",
      "\tspeed: 0.0146s/iter; left time: 205.5569s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0239612\n",
      "\tspeed: 0.0156s/iter; left time: 218.2604s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0172359\n",
      "\tspeed: 0.0145s/iter; left time: 201.6544s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0285303\n",
      "\tspeed: 0.0143s/iter; left time: 198.0359s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0823893\n",
      "\tspeed: 0.0145s/iter; left time: 199.3591s\n",
      "Epoch: 7 cost time: 67.08299350738525\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0922200 Vali Loss: 0.0318865 Test Loss: 0.1225609\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0701767\n",
      "\tspeed: 0.1342s/iter; left time: 1820.7950s\n",
      "\titers: 200, epoch: 8 | loss: 0.0378679\n",
      "\tspeed: 0.0146s/iter; left time: 196.9487s\n",
      "\titers: 300, epoch: 8 | loss: 0.1124579\n",
      "\tspeed: 0.0155s/iter; left time: 207.8112s\n",
      "\titers: 400, epoch: 8 | loss: 0.0779987\n",
      "\tspeed: 0.0143s/iter; left time: 190.2471s\n",
      "\titers: 500, epoch: 8 | loss: 0.1887622\n",
      "\tspeed: 0.0147s/iter; left time: 193.3048s\n",
      "\titers: 600, epoch: 8 | loss: 0.0902516\n",
      "\tspeed: 0.0152s/iter; left time: 199.2279s\n",
      "\titers: 700, epoch: 8 | loss: 0.2040251\n",
      "\tspeed: 0.0154s/iter; left time: 199.6537s\n",
      "\titers: 800, epoch: 8 | loss: 0.1398674\n",
      "\tspeed: 0.0150s/iter; left time: 193.5833s\n",
      "\titers: 900, epoch: 8 | loss: 0.0605964\n",
      "\tspeed: 0.0146s/iter; left time: 186.9834s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0825206\n",
      "\tspeed: 0.0146s/iter; left time: 185.2478s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0832172\n",
      "\tspeed: 0.0146s/iter; left time: 184.0676s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1660474\n",
      "\tspeed: 0.0150s/iter; left time: 187.6107s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3562001\n",
      "\tspeed: 0.0146s/iter; left time: 180.7427s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0369190\n",
      "\tspeed: 0.0145s/iter; left time: 178.3299s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0901217\n",
      "\tspeed: 0.0148s/iter; left time: 180.4446s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1177536\n",
      "\tspeed: 0.0146s/iter; left time: 176.0843s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0307235\n",
      "\tspeed: 0.0149s/iter; left time: 177.8798s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1507467\n",
      "\tspeed: 0.0146s/iter; left time: 173.6779s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0849749\n",
      "\tspeed: 0.0145s/iter; left time: 170.5726s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1214302\n",
      "\tspeed: 0.0151s/iter; left time: 176.0364s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0446351\n",
      "\tspeed: 0.0152s/iter; left time: 175.8921s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0803655\n",
      "\tspeed: 0.0144s/iter; left time: 165.3384s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1589385\n",
      "\tspeed: 0.0156s/iter; left time: 177.7320s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0460822\n",
      "\tspeed: 0.0146s/iter; left time: 164.6836s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0784630\n",
      "\tspeed: 0.0155s/iter; left time: 172.6999s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0655714\n",
      "\tspeed: 0.0143s/iter; left time: 158.6318s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0340518\n",
      "\tspeed: 0.0146s/iter; left time: 159.8564s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0548059\n",
      "\tspeed: 0.0148s/iter; left time: 160.6987s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0825656\n",
      "\tspeed: 0.0149s/iter; left time: 160.2786s\n",
      "\titers: 3000, epoch: 8 | loss: 0.3075091\n",
      "\tspeed: 0.0144s/iter; left time: 153.4755s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0925470\n",
      "\tspeed: 0.0144s/iter; left time: 152.3735s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1359998\n",
      "\tspeed: 0.0144s/iter; left time: 150.3580s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0587429\n",
      "\tspeed: 0.0144s/iter; left time: 149.6276s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0585539\n",
      "\tspeed: 0.0144s/iter; left time: 147.9234s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1413106\n",
      "\tspeed: 0.0144s/iter; left time: 146.0053s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0347230\n",
      "\tspeed: 0.0144s/iter; left time: 144.4579s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2237663\n",
      "\tspeed: 0.0151s/iter; left time: 150.4047s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0759090\n",
      "\tspeed: 0.0144s/iter; left time: 142.3921s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0628641\n",
      "\tspeed: 0.0143s/iter; left time: 140.1193s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0421785\n",
      "\tspeed: 0.0150s/iter; left time: 144.6569s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0302483\n",
      "\tspeed: 0.0143s/iter; left time: 136.3656s\n",
      "\titers: 4200, epoch: 8 | loss: 0.1038510\n",
      "\tspeed: 0.0147s/iter; left time: 139.2048s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0814080\n",
      "\tspeed: 0.0144s/iter; left time: 135.0354s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1293320\n",
      "\tspeed: 0.0143s/iter; left time: 132.7464s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1119517\n",
      "\tspeed: 0.0143s/iter; left time: 130.7937s\n",
      "Epoch: 8 cost time: 67.35407423973083\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0923995 Vali Loss: 0.0320617 Test Loss: 0.1230773\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0487307\n",
      "\tspeed: 0.1299s/iter; left time: 1170.8540s\n",
      "\titers: 200, epoch: 9 | loss: 0.0926895\n",
      "\tspeed: 0.0143s/iter; left time: 127.8343s\n",
      "\titers: 300, epoch: 9 | loss: 0.0423588\n",
      "\tspeed: 0.0145s/iter; left time: 127.5587s\n",
      "\titers: 400, epoch: 9 | loss: 0.1462673\n",
      "\tspeed: 0.0143s/iter; left time: 124.8612s\n",
      "\titers: 500, epoch: 9 | loss: 0.0739181\n",
      "\tspeed: 0.0144s/iter; left time: 124.1963s\n",
      "\titers: 600, epoch: 9 | loss: 0.0341225\n",
      "\tspeed: 0.0151s/iter; left time: 128.2408s\n",
      "\titers: 700, epoch: 9 | loss: 0.1056968\n",
      "\tspeed: 0.0143s/iter; left time: 119.8720s\n",
      "\titers: 800, epoch: 9 | loss: 0.0368858\n",
      "\tspeed: 0.0145s/iter; left time: 120.2715s\n",
      "\titers: 900, epoch: 9 | loss: 0.0399258\n",
      "\tspeed: 0.0147s/iter; left time: 120.4283s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0637278\n",
      "\tspeed: 0.0142s/iter; left time: 115.4124s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0641843\n",
      "\tspeed: 0.0142s/iter; left time: 114.0997s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1971078\n",
      "\tspeed: 0.0142s/iter; left time: 112.3984s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0329389\n",
      "\tspeed: 0.0142s/iter; left time: 110.9058s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2216890\n",
      "\tspeed: 0.0142s/iter; left time: 109.6336s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0481686\n",
      "\tspeed: 0.0142s/iter; left time: 108.1049s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0473219\n",
      "\tspeed: 0.0142s/iter; left time: 106.3597s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0945074\n",
      "\tspeed: 0.0141s/iter; left time: 104.8539s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0288875\n",
      "\tspeed: 0.0141s/iter; left time: 103.3945s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0194001\n",
      "\tspeed: 0.0142s/iter; left time: 102.4468s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0590996\n",
      "\tspeed: 0.0142s/iter; left time: 101.0417s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0471136\n",
      "\tspeed: 0.0142s/iter; left time: 99.5885s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1239936\n",
      "\tspeed: 0.0142s/iter; left time: 98.0884s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1041419\n",
      "\tspeed: 0.0142s/iter; left time: 96.6663s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0321019\n",
      "\tspeed: 0.0142s/iter; left time: 95.2500s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0243156\n",
      "\tspeed: 0.0142s/iter; left time: 93.8109s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0629507\n",
      "\tspeed: 0.0142s/iter; left time: 92.3758s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0497457\n",
      "\tspeed: 0.0142s/iter; left time: 90.9062s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1519527\n",
      "\tspeed: 0.0142s/iter; left time: 89.5503s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0573020\n",
      "\tspeed: 0.0142s/iter; left time: 87.9178s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1340358\n",
      "\tspeed: 0.0141s/iter; left time: 86.4472s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0285150\n",
      "\tspeed: 0.0141s/iter; left time: 85.0525s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0630654\n",
      "\tspeed: 0.0142s/iter; left time: 83.7661s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2300242\n",
      "\tspeed: 0.0142s/iter; left time: 82.3397s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1096455\n",
      "\tspeed: 0.0142s/iter; left time: 80.8857s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0853339\n",
      "\tspeed: 0.0142s/iter; left time: 79.5295s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0426191\n",
      "\tspeed: 0.0142s/iter; left time: 78.0863s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0697190\n",
      "\tspeed: 0.0142s/iter; left time: 76.6150s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1190345\n",
      "\tspeed: 0.0142s/iter; left time: 75.2844s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0921732\n",
      "\tspeed: 0.0142s/iter; left time: 73.8167s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0762893\n",
      "\tspeed: 0.0142s/iter; left time: 72.4364s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0668493\n",
      "\tspeed: 0.0142s/iter; left time: 71.0882s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0891116\n",
      "\tspeed: 0.0142s/iter; left time: 69.6386s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0638719\n",
      "\tspeed: 0.0142s/iter; left time: 68.2028s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0320236\n",
      "\tspeed: 0.0142s/iter; left time: 66.7632s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0251591\n",
      "\tspeed: 0.0142s/iter; left time: 65.3651s\n",
      "Epoch: 9 cost time: 65.21100616455078\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0904113 Vali Loss: 0.0319028 Test Loss: 0.1226591\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0626794\n",
      "\tspeed: 0.1313s/iter; left time: 585.2042s\n",
      "\titers: 200, epoch: 10 | loss: 0.0836334\n",
      "\tspeed: 0.0142s/iter; left time: 62.0171s\n",
      "\titers: 300, epoch: 10 | loss: 0.0381573\n",
      "\tspeed: 0.0142s/iter; left time: 60.4706s\n",
      "\titers: 400, epoch: 10 | loss: 0.0420326\n",
      "\tspeed: 0.0142s/iter; left time: 59.1059s\n",
      "\titers: 500, epoch: 10 | loss: 0.0429586\n",
      "\tspeed: 0.0142s/iter; left time: 57.6291s\n",
      "\titers: 600, epoch: 10 | loss: 0.0924656\n",
      "\tspeed: 0.0142s/iter; left time: 56.2078s\n",
      "\titers: 700, epoch: 10 | loss: 0.0645054\n",
      "\tspeed: 0.0142s/iter; left time: 54.7466s\n",
      "\titers: 800, epoch: 10 | loss: 0.1832157\n",
      "\tspeed: 0.0142s/iter; left time: 53.4711s\n",
      "\titers: 900, epoch: 10 | loss: 0.0973747\n",
      "\tspeed: 0.0142s/iter; left time: 51.8836s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0485435\n",
      "\tspeed: 0.0142s/iter; left time: 50.4629s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0583680\n",
      "\tspeed: 0.0142s/iter; left time: 49.0389s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0900433\n",
      "\tspeed: 0.0142s/iter; left time: 47.6474s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0391196\n",
      "\tspeed: 0.0142s/iter; left time: 46.2967s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0779811\n",
      "\tspeed: 0.0142s/iter; left time: 44.7974s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2405260\n",
      "\tspeed: 0.0142s/iter; left time: 43.3815s\n",
      "\titers: 1600, epoch: 10 | loss: 0.1432933\n",
      "\tspeed: 0.0142s/iter; left time: 41.9658s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0257242\n",
      "\tspeed: 0.0142s/iter; left time: 40.5193s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0938244\n",
      "\tspeed: 0.0142s/iter; left time: 39.1016s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1106036\n",
      "\tspeed: 0.0142s/iter; left time: 37.6693s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0758235\n",
      "\tspeed: 0.0141s/iter; left time: 36.0935s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0438861\n",
      "\tspeed: 0.0141s/iter; left time: 34.6509s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0639797\n",
      "\tspeed: 0.0141s/iter; left time: 33.2380s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1234691\n",
      "\tspeed: 0.0141s/iter; left time: 31.8765s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0876351\n",
      "\tspeed: 0.0141s/iter; left time: 30.4531s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2222401\n",
      "\tspeed: 0.0141s/iter; left time: 29.0562s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1523860\n",
      "\tspeed: 0.0141s/iter; left time: 27.6306s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0838694\n",
      "\tspeed: 0.0141s/iter; left time: 26.2321s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0548758\n",
      "\tspeed: 0.0141s/iter; left time: 24.8167s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0732583\n",
      "\tspeed: 0.0141s/iter; left time: 23.4056s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0450948\n",
      "\tspeed: 0.0141s/iter; left time: 21.9930s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0604523\n",
      "\tspeed: 0.0141s/iter; left time: 20.5672s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0880652\n",
      "\tspeed: 0.0141s/iter; left time: 19.1703s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0839868\n",
      "\tspeed: 0.0141s/iter; left time: 17.7547s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1347636\n",
      "\tspeed: 0.0141s/iter; left time: 16.3477s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0267478\n",
      "\tspeed: 0.0142s/iter; left time: 15.0132s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1878310\n",
      "\tspeed: 0.0142s/iter; left time: 13.5515s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0832595\n",
      "\tspeed: 0.0141s/iter; left time: 12.1088s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0642754\n",
      "\tspeed: 0.0141s/iter; left time: 10.6793s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0230879\n",
      "\tspeed: 0.0141s/iter; left time: 9.2669s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0511064\n",
      "\tspeed: 0.0141s/iter; left time: 7.8604s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1427160\n",
      "\tspeed: 0.0141s/iter; left time: 6.4451s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0587856\n",
      "\tspeed: 0.0141s/iter; left time: 5.0323s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0342961\n",
      "\tspeed: 0.0141s/iter; left time: 3.6176s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0534264\n",
      "\tspeed: 0.0141s/iter; left time: 2.2052s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1059394\n",
      "\tspeed: 0.0141s/iter; left time: 0.7918s\n",
      "Epoch: 10 cost time: 64.7721655368805\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0903920 Vali Loss: 0.0318728 Test Loss: 0.1227035\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1209254041314125, mae:0.2057265192270279\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --data_path forecast_data.csv --features MS --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c14e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       ALL                 \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'wind_off_forecast', 'wind_on_forecast', 'total_load', 'solar_capacity', 'wind_off_capacity', 'wind_on_capacity', 'daylight_hours', 'co2_price', 'gas_price', 'wind_forecast', 'wind_capacity', 'solar_penetration', 'wind_penetration', 'solar_binary', 'Hour', 'Day', 'Month', 'Year']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'wind_off_forecast', 'wind_on_forecast', 'total_load', 'solar_capacity', 'wind_off_capacity', 'wind_on_capacity', 'daylight_hours', 'co2_price', 'gas_price', 'wind_forecast', 'wind_capacity', 'solar_penetration', 'wind_penetration', 'solar_binary', 'Hour', 'Day', 'Month', 'Year']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'wind_off_forecast', 'wind_on_forecast', 'total_load', 'solar_capacity', 'wind_off_capacity', 'wind_on_capacity', 'daylight_hours', 'co2_price', 'gas_price', 'wind_forecast', 'wind_capacity', 'solar_penetration', 'wind_penetration', 'solar_binary', 'Hour', 'Day', 'Month', 'Year']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1649269\n",
      "\tspeed: 0.0277s/iter; left time: 1257.7410s\n",
      "\titers: 200, epoch: 1 | loss: 0.1089298\n",
      "\tspeed: 0.0157s/iter; left time: 711.9218s\n",
      "\titers: 300, epoch: 1 | loss: 0.0663502\n",
      "\tspeed: 0.0157s/iter; left time: 710.7251s\n",
      "\titers: 400, epoch: 1 | loss: 0.1366243\n",
      "\tspeed: 0.0157s/iter; left time: 707.7611s\n",
      "\titers: 500, epoch: 1 | loss: 0.0348217\n",
      "\tspeed: 0.0157s/iter; left time: 707.9029s\n",
      "\titers: 600, epoch: 1 | loss: 0.0921146\n",
      "\tspeed: 0.0157s/iter; left time: 707.6898s\n",
      "terminate called after throwing an instance of 'std::system_error'\n",
      "  what():  Broken pipe\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c0b3f",
   "metadata": {},
   "source": [
    "## S MS M trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9ecea",
   "metadata": {},
   "source": [
    "#### S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d850d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1880594\n",
      "\tspeed: 0.0256s/iter; left time: 347.1352s\n",
      "\titers: 200, epoch: 1 | loss: 0.1026268\n",
      "\tspeed: 0.0140s/iter; left time: 188.8761s\n",
      "\titers: 300, epoch: 1 | loss: 0.0646233\n",
      "\tspeed: 0.0140s/iter; left time: 187.1056s\n",
      "\titers: 400, epoch: 1 | loss: 0.1436839\n",
      "\tspeed: 0.0140s/iter; left time: 185.6777s\n",
      "\titers: 500, epoch: 1 | loss: 0.0337003\n",
      "\tspeed: 0.0140s/iter; left time: 184.2560s\n",
      "\titers: 600, epoch: 1 | loss: 0.0588938\n",
      "\tspeed: 0.0140s/iter; left time: 182.8511s\n",
      "\titers: 700, epoch: 1 | loss: 0.1358326\n",
      "\tspeed: 0.0140s/iter; left time: 181.5758s\n",
      "\titers: 800, epoch: 1 | loss: 0.5110136\n",
      "\tspeed: 0.0140s/iter; left time: 180.1718s\n",
      "\titers: 900, epoch: 1 | loss: 0.2601639\n",
      "\tspeed: 0.0140s/iter; left time: 178.6820s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0690488\n",
      "\tspeed: 0.0140s/iter; left time: 177.3620s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0552730\n",
      "\tspeed: 0.0141s/iter; left time: 176.7722s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0722296\n",
      "\tspeed: 0.0141s/iter; left time: 175.3165s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2892447\n",
      "\tspeed: 0.0141s/iter; left time: 174.0585s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0230870\n",
      "\tspeed: 0.0141s/iter; left time: 172.5673s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1299016\n",
      "\tspeed: 0.0141s/iter; left time: 171.0396s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1390953\n",
      "\tspeed: 0.0141s/iter; left time: 169.7933s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3027755\n",
      "\tspeed: 0.0141s/iter; left time: 168.3062s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0582245\n",
      "\tspeed: 0.0141s/iter; left time: 166.8372s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1958276\n",
      "\tspeed: 0.0141s/iter; left time: 165.5251s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4077032\n",
      "\tspeed: 0.0141s/iter; left time: 164.0609s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2276900\n",
      "\tspeed: 0.0141s/iter; left time: 162.6606s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2901473\n",
      "\tspeed: 0.0141s/iter; left time: 161.2683s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1349899\n",
      "\tspeed: 0.0141s/iter; left time: 159.7918s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1636490\n",
      "\tspeed: 0.0141s/iter; left time: 158.3646s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2848335\n",
      "\tspeed: 0.0141s/iter; left time: 157.0717s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1061540\n",
      "\tspeed: 0.0141s/iter; left time: 155.6201s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1824115\n",
      "\tspeed: 0.0141s/iter; left time: 154.2180s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1878522\n",
      "\tspeed: 0.0141s/iter; left time: 152.9805s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1878474\n",
      "\tspeed: 0.0141s/iter; left time: 151.2880s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0904252\n",
      "\tspeed: 0.0141s/iter; left time: 150.0378s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0542073\n",
      "\tspeed: 0.0141s/iter; left time: 148.4569s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1607954\n",
      "\tspeed: 0.0141s/iter; left time: 147.2266s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0310854\n",
      "\tspeed: 0.0141s/iter; left time: 145.7444s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2267053\n",
      "\tspeed: 0.0141s/iter; left time: 144.4280s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0824267\n",
      "\tspeed: 0.0141s/iter; left time: 142.9049s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0969501\n",
      "\tspeed: 0.0141s/iter; left time: 141.4736s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0843461\n",
      "\tspeed: 0.0141s/iter; left time: 140.0377s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1617526\n",
      "\tspeed: 0.0141s/iter; left time: 138.6461s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3581395\n",
      "\tspeed: 0.0141s/iter; left time: 137.3060s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3739244\n",
      "\tspeed: 0.0141s/iter; left time: 135.9007s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0748074\n",
      "\tspeed: 0.0141s/iter; left time: 134.4694s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0804443\n",
      "\tspeed: 0.0141s/iter; left time: 133.1421s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1978168\n",
      "\tspeed: 0.0140s/iter; left time: 131.5741s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0293587\n",
      "\tspeed: 0.0141s/iter; left time: 130.2285s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0607482\n",
      "\tspeed: 0.0140s/iter; left time: 128.7661s\n",
      "Epoch: 1 cost time: 65.19293665885925\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1760965 Vali Loss: 0.0375671 Test Loss: 0.1243711\n",
      "Validation loss decreased (inf --> 0.037567).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1023001\n",
      "\tspeed: 0.1297s/iter; left time: 1168.3238s\n",
      "\titers: 200, epoch: 2 | loss: 0.0443824\n",
      "\tspeed: 0.0140s/iter; left time: 125.1316s\n",
      "\titers: 300, epoch: 2 | loss: 0.0782190\n",
      "\tspeed: 0.0140s/iter; left time: 123.6996s\n",
      "\titers: 400, epoch: 2 | loss: 0.0906417\n",
      "\tspeed: 0.0141s/iter; left time: 122.4673s\n",
      "\titers: 500, epoch: 2 | loss: 0.3023317\n",
      "\tspeed: 0.0140s/iter; left time: 120.8885s\n",
      "\titers: 600, epoch: 2 | loss: 0.0732646\n",
      "\tspeed: 0.0140s/iter; left time: 119.3124s\n",
      "\titers: 700, epoch: 2 | loss: 0.0909683\n",
      "\tspeed: 0.0140s/iter; left time: 117.9174s\n",
      "\titers: 800, epoch: 2 | loss: 0.0453902\n",
      "\tspeed: 0.0140s/iter; left time: 116.4558s\n",
      "\titers: 900, epoch: 2 | loss: 0.0473101\n",
      "\tspeed: 0.0140s/iter; left time: 114.9414s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1697372\n",
      "\tspeed: 0.0140s/iter; left time: 113.6655s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2098863\n",
      "\tspeed: 0.0140s/iter; left time: 112.1954s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1173484\n",
      "\tspeed: 0.0140s/iter; left time: 110.7775s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0964068\n",
      "\tspeed: 0.0140s/iter; left time: 109.3372s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1164205\n",
      "\tspeed: 0.0140s/iter; left time: 107.9561s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0471032\n",
      "\tspeed: 0.0140s/iter; left time: 106.6108s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1479586\n",
      "\tspeed: 0.0140s/iter; left time: 105.1708s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2439645\n",
      "\tspeed: 0.0140s/iter; left time: 103.7612s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1059873\n",
      "\tspeed: 0.0140s/iter; left time: 102.2973s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1401853\n",
      "\tspeed: 0.0140s/iter; left time: 100.9093s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0535772\n",
      "\tspeed: 0.0140s/iter; left time: 99.5314s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0827046\n",
      "\tspeed: 0.0140s/iter; left time: 98.1003s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1374192\n",
      "\tspeed: 0.0140s/iter; left time: 96.7206s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1499075\n",
      "\tspeed: 0.0140s/iter; left time: 95.2811s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1197314\n",
      "\tspeed: 0.0140s/iter; left time: 93.9511s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0720138\n",
      "\tspeed: 0.0140s/iter; left time: 92.4663s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2874545\n",
      "\tspeed: 0.0140s/iter; left time: 91.1423s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1275534\n",
      "\tspeed: 0.0140s/iter; left time: 89.7192s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0577189\n",
      "\tspeed: 0.0140s/iter; left time: 88.3076s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0497496\n",
      "\tspeed: 0.0140s/iter; left time: 86.9141s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1213172\n",
      "\tspeed: 0.0140s/iter; left time: 85.4553s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0917065\n",
      "\tspeed: 0.0140s/iter; left time: 84.1136s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2026839\n",
      "\tspeed: 0.0140s/iter; left time: 82.6633s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0497320\n",
      "\tspeed: 0.0140s/iter; left time: 81.2876s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0741029\n",
      "\tspeed: 0.0140s/iter; left time: 79.8954s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1714709\n",
      "\tspeed: 0.0140s/iter; left time: 78.4610s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1354392\n",
      "\tspeed: 0.0140s/iter; left time: 77.0781s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1481443\n",
      "\tspeed: 0.0140s/iter; left time: 75.6937s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1425242\n",
      "\tspeed: 0.0140s/iter; left time: 74.3164s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2178237\n",
      "\tspeed: 0.0140s/iter; left time: 72.9650s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2526345\n",
      "\tspeed: 0.0140s/iter; left time: 71.5403s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0959773\n",
      "\tspeed: 0.0140s/iter; left time: 70.1498s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0774052\n",
      "\tspeed: 0.0140s/iter; left time: 68.7590s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1198923\n",
      "\tspeed: 0.0140s/iter; left time: 67.3487s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1138821\n",
      "\tspeed: 0.0140s/iter; left time: 65.9570s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0380464\n",
      "\tspeed: 0.0140s/iter; left time: 64.5505s\n",
      "Epoch: 2 cost time: 64.01629424095154\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1508490 Vali Loss: 0.0334156 Test Loss: 0.1120896\n",
      "Validation loss decreased (0.037567 --> 0.033416).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1158902\n",
      "\tspeed: 0.1239s/iter; left time: 551.8981s\n",
      "\titers: 200, epoch: 3 | loss: 0.0483347\n",
      "\tspeed: 0.0141s/iter; left time: 61.2255s\n",
      "\titers: 300, epoch: 3 | loss: 0.1121058\n",
      "\tspeed: 0.0141s/iter; left time: 59.8939s\n",
      "\titers: 400, epoch: 3 | loss: 0.1069300\n",
      "\tspeed: 0.0141s/iter; left time: 58.4497s\n",
      "\titers: 500, epoch: 3 | loss: 0.1013264\n",
      "\tspeed: 0.0141s/iter; left time: 57.0597s\n",
      "\titers: 600, epoch: 3 | loss: 0.0809711\n",
      "\tspeed: 0.0141s/iter; left time: 55.7032s\n",
      "\titers: 700, epoch: 3 | loss: 0.2910097\n",
      "\tspeed: 0.0141s/iter; left time: 54.2680s\n",
      "\titers: 800, epoch: 3 | loss: 0.1931276\n",
      "\tspeed: 0.0141s/iter; left time: 52.8301s\n",
      "\titers: 900, epoch: 3 | loss: 0.0564841\n",
      "\tspeed: 0.0141s/iter; left time: 51.3895s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1121366\n",
      "\tspeed: 0.0141s/iter; left time: 49.9966s\n",
      "\titers: 1100, epoch: 3 | loss: 0.2114139\n",
      "\tspeed: 0.0141s/iter; left time: 48.6703s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1515695\n",
      "\tspeed: 0.0141s/iter; left time: 47.2532s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2040596\n",
      "\tspeed: 0.0141s/iter; left time: 45.7865s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1711096\n",
      "\tspeed: 0.0141s/iter; left time: 44.3776s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2077705\n",
      "\tspeed: 0.0141s/iter; left time: 42.9449s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1979721\n",
      "\tspeed: 0.0141s/iter; left time: 41.5508s\n",
      "\titers: 1700, epoch: 3 | loss: 0.6470969\n",
      "\tspeed: 0.0141s/iter; left time: 40.2042s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0812857\n",
      "\tspeed: 0.0141s/iter; left time: 38.7997s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1738021\n",
      "\tspeed: 0.0141s/iter; left time: 37.3970s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2122969\n",
      "\tspeed: 0.0143s/iter; left time: 36.6240s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0888002\n",
      "\tspeed: 0.0141s/iter; left time: 34.6947s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0675884\n",
      "\tspeed: 0.0142s/iter; left time: 33.5270s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1076873\n",
      "\tspeed: 0.0142s/iter; left time: 32.1206s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0852173\n",
      "\tspeed: 0.0141s/iter; left time: 30.3116s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1263370\n",
      "\tspeed: 0.0141s/iter; left time: 28.9424s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0897379\n",
      "\tspeed: 0.0141s/iter; left time: 27.5190s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0478275\n",
      "\tspeed: 0.0140s/iter; left time: 26.0674s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1108397\n",
      "\tspeed: 0.0141s/iter; left time: 24.7227s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0762252\n",
      "\tspeed: 0.0141s/iter; left time: 23.3018s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0452943\n",
      "\tspeed: 0.0141s/iter; left time: 21.8859s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1515656\n",
      "\tspeed: 0.0141s/iter; left time: 20.4722s\n",
      "\titers: 3200, epoch: 3 | loss: 0.7125151\n",
      "\tspeed: 0.0141s/iter; left time: 19.0623s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0828298\n",
      "\tspeed: 0.0140s/iter; left time: 17.6413s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2642997\n",
      "\tspeed: 0.0140s/iter; left time: 16.2232s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0377675\n",
      "\tspeed: 0.0140s/iter; left time: 14.8147s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1297170\n",
      "\tspeed: 0.0140s/iter; left time: 13.4121s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1464807\n",
      "\tspeed: 0.0140s/iter; left time: 12.0121s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0711235\n",
      "\tspeed: 0.0140s/iter; left time: 10.6037s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1722317\n",
      "\tspeed: 0.0140s/iter; left time: 9.2030s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1867490\n",
      "\tspeed: 0.0140s/iter; left time: 7.8030s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2535416\n",
      "\tspeed: 0.0140s/iter; left time: 6.3938s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0618705\n",
      "\tspeed: 0.0140s/iter; left time: 4.9946s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0288661\n",
      "\tspeed: 0.0140s/iter; left time: 3.5917s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0905710\n",
      "\tspeed: 0.0140s/iter; left time: 2.1908s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0366086\n",
      "\tspeed: 0.0140s/iter; left time: 0.7867s\n",
      "Epoch: 3 cost time: 64.323646068573\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1292094 Vali Loss: 0.0335928 Test Loss: 0.1101938\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11211086064577103, mae:0.2069416046142578\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features S --predictor wind_forecast,total_load --enc_in 1 --dec_in 1 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce725e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 200, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 135, in train\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 222, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 167, in forecast\n",
      "    en_embed, n_vars = self.en_embedding(x_enc[:, :, -1].unsqueeze(-1).permute(0, 2, 1))\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 46, in forward\n",
      "    x = torch.cat([x, glb], dim=2)\n",
      "RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 3 for tensor number 1 in the list.\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features S --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e121b",
   "metadata": {},
   "source": [
    "#### MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1761947\n",
      "\tspeed: 0.0278s/iter; left time: 377.0971s\n",
      "\titers: 200, epoch: 1 | loss: 0.0938281\n",
      "\tspeed: 0.0162s/iter; left time: 217.8089s\n",
      "\titers: 300, epoch: 1 | loss: 0.0687024\n",
      "\tspeed: 0.0162s/iter; left time: 216.1327s\n",
      "\titers: 400, epoch: 1 | loss: 0.1241657\n",
      "\tspeed: 0.0162s/iter; left time: 214.3837s\n",
      "\titers: 500, epoch: 1 | loss: 0.0327702\n",
      "\tspeed: 0.0162s/iter; left time: 212.7032s\n",
      "\titers: 600, epoch: 1 | loss: 0.0715009\n",
      "\tspeed: 0.0162s/iter; left time: 211.2020s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451532\n",
      "\tspeed: 0.0161s/iter; left time: 209.3129s\n",
      "\titers: 800, epoch: 1 | loss: 0.5053760\n",
      "\tspeed: 0.0162s/iter; left time: 208.1143s\n",
      "\titers: 900, epoch: 1 | loss: 0.2560097\n",
      "\tspeed: 0.0162s/iter; left time: 206.6693s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0734115\n",
      "\tspeed: 0.0162s/iter; left time: 204.9248s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0425444\n",
      "\tspeed: 0.0160s/iter; left time: 201.2063s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0585851\n",
      "\tspeed: 0.0161s/iter; left time: 200.9863s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2605752\n",
      "\tspeed: 0.0161s/iter; left time: 199.2903s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0309507\n",
      "\tspeed: 0.0161s/iter; left time: 197.8686s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1115317\n",
      "\tspeed: 0.0161s/iter; left time: 196.3226s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1843507\n",
      "\tspeed: 0.0162s/iter; left time: 195.0959s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3890318\n",
      "\tspeed: 0.0161s/iter; left time: 193.2508s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0720876\n",
      "\tspeed: 0.0161s/iter; left time: 191.6066s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1775747\n",
      "\tspeed: 0.0161s/iter; left time: 189.8379s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3624939\n",
      "\tspeed: 0.0162s/iter; left time: 188.4372s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1508106\n",
      "\tspeed: 0.0161s/iter; left time: 186.6335s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3487170\n",
      "\tspeed: 0.0161s/iter; left time: 185.0331s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1623183\n",
      "\tspeed: 0.0161s/iter; left time: 183.4014s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1627411\n",
      "\tspeed: 0.0161s/iter; left time: 181.8000s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2751411\n",
      "\tspeed: 0.0161s/iter; left time: 180.1882s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1169626\n",
      "\tspeed: 0.0161s/iter; left time: 178.6108s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2052497\n",
      "\tspeed: 0.0161s/iter; left time: 176.9794s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2055999\n",
      "\tspeed: 0.0161s/iter; left time: 175.3710s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2338544\n",
      "\tspeed: 0.0161s/iter; left time: 173.7544s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0882175\n",
      "\tspeed: 0.0161s/iter; left time: 172.1576s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0823141\n",
      "\tspeed: 0.0161s/iter; left time: 170.4368s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1962520\n",
      "\tspeed: 0.0161s/iter; left time: 168.9372s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0351911\n",
      "\tspeed: 0.0161s/iter; left time: 167.1988s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4122079\n",
      "\tspeed: 0.0161s/iter; left time: 165.5160s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0856364\n",
      "\tspeed: 0.0161s/iter; left time: 163.8845s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1057309\n",
      "\tspeed: 0.0161s/iter; left time: 162.3751s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0931639\n",
      "\tspeed: 0.0161s/iter; left time: 160.7982s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1785975\n",
      "\tspeed: 0.0161s/iter; left time: 159.2275s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3997110\n",
      "\tspeed: 0.0161s/iter; left time: 157.6066s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2769776\n",
      "\tspeed: 0.0161s/iter; left time: 156.0245s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0955917\n",
      "\tspeed: 0.0161s/iter; left time: 154.3857s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0872184\n",
      "\tspeed: 0.0161s/iter; left time: 152.7328s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1506448\n",
      "\tspeed: 0.0161s/iter; left time: 151.1805s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0404093\n",
      "\tspeed: 0.0161s/iter; left time: 149.5334s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0716922\n",
      "\tspeed: 0.0161s/iter; left time: 147.9179s\n",
      "Epoch: 1 cost time: 74.73979902267456\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1763682 Vali Loss: 0.0384911 Test Loss: 0.1245387\n",
      "Validation loss decreased (inf --> 0.038491).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0965484\n",
      "\tspeed: 0.1293s/iter; left time: 1164.9784s\n",
      "\titers: 200, epoch: 2 | loss: 0.0394090\n",
      "\tspeed: 0.0162s/iter; left time: 144.3878s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977589\n",
      "\tspeed: 0.0162s/iter; left time: 142.7146s\n",
      "\titers: 400, epoch: 2 | loss: 0.1882427\n",
      "\tspeed: 0.0162s/iter; left time: 141.0924s\n",
      "\titers: 500, epoch: 2 | loss: 0.3373528\n",
      "\tspeed: 0.0161s/iter; left time: 139.0571s\n",
      "\titers: 600, epoch: 2 | loss: 0.1316223\n",
      "\tspeed: 0.0162s/iter; left time: 137.4828s\n",
      "\titers: 700, epoch: 2 | loss: 0.0733484\n",
      "\tspeed: 0.0161s/iter; left time: 135.8117s\n",
      "\titers: 800, epoch: 2 | loss: 0.0557397\n",
      "\tspeed: 0.0161s/iter; left time: 134.1777s\n",
      "\titers: 900, epoch: 2 | loss: 0.0551844\n",
      "\tspeed: 0.0161s/iter; left time: 132.6018s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1973986\n",
      "\tspeed: 0.0161s/iter; left time: 130.9640s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2021171\n",
      "\tspeed: 0.0162s/iter; left time: 129.3858s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1249596\n",
      "\tspeed: 0.0161s/iter; left time: 127.7484s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1480998\n",
      "\tspeed: 0.0161s/iter; left time: 126.1163s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1150100\n",
      "\tspeed: 0.0161s/iter; left time: 124.5127s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0475104\n",
      "\tspeed: 0.0161s/iter; left time: 122.8958s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1711187\n",
      "\tspeed: 0.0161s/iter; left time: 121.2671s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2710866\n",
      "\tspeed: 0.0161s/iter; left time: 119.6402s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1182006\n",
      "\tspeed: 0.0161s/iter; left time: 118.0277s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1678239\n",
      "\tspeed: 0.0161s/iter; left time: 116.4190s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0585374\n",
      "\tspeed: 0.0161s/iter; left time: 114.8109s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0376854\n",
      "\tspeed: 0.0161s/iter; left time: 113.1951s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177671\n",
      "\tspeed: 0.0161s/iter; left time: 111.5826s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1420685\n",
      "\tspeed: 0.0161s/iter; left time: 109.9327s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1342927\n",
      "\tspeed: 0.0161s/iter; left time: 108.3452s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0575067\n",
      "\tspeed: 0.0161s/iter; left time: 106.7017s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2137174\n",
      "\tspeed: 0.0161s/iter; left time: 105.0871s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1351328\n",
      "\tspeed: 0.0161s/iter; left time: 103.5100s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0589181\n",
      "\tspeed: 0.0161s/iter; left time: 101.8668s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0820708\n",
      "\tspeed: 0.0161s/iter; left time: 100.2390s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1777464\n",
      "\tspeed: 0.0161s/iter; left time: 98.6200s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0429570\n",
      "\tspeed: 0.0161s/iter; left time: 96.9885s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1527276\n",
      "\tspeed: 0.0161s/iter; left time: 95.4146s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0607781\n",
      "\tspeed: 0.0162s/iter; left time: 94.0244s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0930832\n",
      "\tspeed: 0.0162s/iter; left time: 92.2806s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1257465\n",
      "\tspeed: 0.0162s/iter; left time: 90.8421s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1400915\n",
      "\tspeed: 0.0162s/iter; left time: 89.0555s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1529161\n",
      "\tspeed: 0.0162s/iter; left time: 87.3883s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1372309\n",
      "\tspeed: 0.0143s/iter; left time: 75.9000s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1924893\n",
      "\tspeed: 0.0140s/iter; left time: 73.1098s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2247270\n",
      "\tspeed: 0.0140s/iter; left time: 71.7066s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171593\n",
      "\tspeed: 0.0140s/iter; left time: 70.3313s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0934797\n",
      "\tspeed: 0.0140s/iter; left time: 68.8631s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1451997\n",
      "\tspeed: 0.0140s/iter; left time: 67.4550s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1581813\n",
      "\tspeed: 0.0140s/iter; left time: 66.0426s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0497790\n",
      "\tspeed: 0.0140s/iter; left time: 64.6402s\n",
      "Epoch: 2 cost time: 72.03558230400085\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1446429 Vali Loss: 0.0337670 Test Loss: 0.1147559\n",
      "Validation loss decreased (0.038491 --> 0.033767).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0888895\n",
      "\tspeed: 0.1419s/iter; left time: 632.1907s\n",
      "\titers: 200, epoch: 3 | loss: 0.0433673\n",
      "\tspeed: 0.0142s/iter; left time: 61.7659s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213155\n",
      "\tspeed: 0.0151s/iter; left time: 64.4020s\n",
      "\titers: 400, epoch: 3 | loss: 0.0755301\n",
      "\tspeed: 0.0162s/iter; left time: 67.5303s\n",
      "\titers: 500, epoch: 3 | loss: 0.0728352\n",
      "\tspeed: 0.0163s/iter; left time: 65.9369s\n",
      "\titers: 600, epoch: 3 | loss: 0.0727856\n",
      "\tspeed: 0.0163s/iter; left time: 64.2897s\n",
      "\titers: 700, epoch: 3 | loss: 0.3633784\n",
      "\tspeed: 0.0163s/iter; left time: 62.6704s\n",
      "\titers: 800, epoch: 3 | loss: 0.0919219\n",
      "\tspeed: 0.0162s/iter; left time: 61.0158s\n",
      "\titers: 900, epoch: 3 | loss: 0.0787504\n",
      "\tspeed: 0.0162s/iter; left time: 59.3757s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1017697\n",
      "\tspeed: 0.0162s/iter; left time: 57.7604s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1278257\n",
      "\tspeed: 0.0162s/iter; left time: 56.1198s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1174518\n",
      "\tspeed: 0.0162s/iter; left time: 54.5051s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2299570\n",
      "\tspeed: 0.0162s/iter; left time: 52.8702s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1301254\n",
      "\tspeed: 0.0162s/iter; left time: 51.2499s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1960834\n",
      "\tspeed: 0.0162s/iter; left time: 49.6281s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1536359\n",
      "\tspeed: 0.0162s/iter; left time: 48.0088s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5119310\n",
      "\tspeed: 0.0162s/iter; left time: 46.3664s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0803905\n",
      "\tspeed: 0.0162s/iter; left time: 44.7550s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1686091\n",
      "\tspeed: 0.0162s/iter; left time: 43.1391s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2706951\n",
      "\tspeed: 0.0162s/iter; left time: 41.5093s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0878483\n",
      "\tspeed: 0.0162s/iter; left time: 39.8868s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0488082\n",
      "\tspeed: 0.0162s/iter; left time: 38.2558s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1471822\n",
      "\tspeed: 0.0162s/iter; left time: 36.6331s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1000195\n",
      "\tspeed: 0.0162s/iter; left time: 35.0105s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1408294\n",
      "\tspeed: 0.0162s/iter; left time: 33.3858s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0522313\n",
      "\tspeed: 0.0162s/iter; left time: 31.7793s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0433277\n",
      "\tspeed: 0.0163s/iter; left time: 30.1640s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1223619\n",
      "\tspeed: 0.0162s/iter; left time: 28.5257s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0673580\n",
      "\tspeed: 0.0162s/iter; left time: 26.8955s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0574459\n",
      "\tspeed: 0.0162s/iter; left time: 25.2769s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1538857\n",
      "\tspeed: 0.0163s/iter; left time: 23.6618s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2254446\n",
      "\tspeed: 0.0162s/iter; left time: 22.0249s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0779002\n",
      "\tspeed: 0.0162s/iter; left time: 20.4046s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1575561\n",
      "\tspeed: 0.0162s/iter; left time: 18.7719s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0331902\n",
      "\tspeed: 0.0162s/iter; left time: 17.1527s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1007734\n",
      "\tspeed: 0.0163s/iter; left time: 15.5357s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1122994\n",
      "\tspeed: 0.0162s/iter; left time: 13.9099s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0607264\n",
      "\tspeed: 0.0163s/iter; left time: 12.2922s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1815582\n",
      "\tspeed: 0.0162s/iter; left time: 10.6575s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1472391\n",
      "\tspeed: 0.0162s/iter; left time: 9.0310s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1834221\n",
      "\tspeed: 0.0162s/iter; left time: 7.4089s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0803711\n",
      "\tspeed: 0.0162s/iter; left time: 5.7843s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0216850\n",
      "\tspeed: 0.0162s/iter; left time: 4.1577s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0932727\n",
      "\tspeed: 0.0162s/iter; left time: 2.5347s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0153473\n",
      "\tspeed: 0.0162s/iter; left time: 0.9100s\n",
      "Epoch: 3 cost time: 73.83076882362366\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1149970 Vali Loss: 0.0354637 Test Loss: 0.1141755\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11478278785943985, mae:0.20999355614185333\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor wind_forecast,total_load --enc_in 1 --dec_in 1 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35ee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1761947\n",
      "\tspeed: 0.0273s/iter; left time: 370.8425s\n",
      "\titers: 200, epoch: 1 | loss: 0.0938281\n",
      "\tspeed: 0.0157s/iter; left time: 211.7778s\n",
      "\titers: 300, epoch: 1 | loss: 0.0687024\n",
      "\tspeed: 0.0157s/iter; left time: 209.9026s\n",
      "\titers: 400, epoch: 1 | loss: 0.1241657\n",
      "\tspeed: 0.0157s/iter; left time: 208.6773s\n",
      "\titers: 500, epoch: 1 | loss: 0.0327702\n",
      "\tspeed: 0.0157s/iter; left time: 206.7599s\n",
      "\titers: 600, epoch: 1 | loss: 0.0715009\n",
      "\tspeed: 0.0157s/iter; left time: 205.6079s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451532\n",
      "\tspeed: 0.0157s/iter; left time: 203.4265s\n",
      "\titers: 800, epoch: 1 | loss: 0.5053760\n",
      "\tspeed: 0.0157s/iter; left time: 202.1047s\n",
      "\titers: 900, epoch: 1 | loss: 0.2560097\n",
      "\tspeed: 0.0157s/iter; left time: 200.2594s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0734115\n",
      "\tspeed: 0.0157s/iter; left time: 198.6422s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0425444\n",
      "\tspeed: 0.0157s/iter; left time: 197.3046s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0585851\n",
      "\tspeed: 0.0157s/iter; left time: 195.5538s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2605752\n",
      "\tspeed: 0.0157s/iter; left time: 194.0811s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0309507\n",
      "\tspeed: 0.0157s/iter; left time: 192.3162s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1115317\n",
      "\tspeed: 0.0157s/iter; left time: 191.1325s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1843507\n",
      "\tspeed: 0.0157s/iter; left time: 189.1861s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3890318\n",
      "\tspeed: 0.0156s/iter; left time: 187.2368s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0720876\n",
      "\tspeed: 0.0157s/iter; left time: 185.7749s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1775747\n",
      "\tspeed: 0.0160s/iter; left time: 188.6732s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3624939\n",
      "\tspeed: 0.0162s/iter; left time: 188.5428s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1508106\n",
      "\tspeed: 0.0161s/iter; left time: 186.6992s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3487170\n",
      "\tspeed: 0.0158s/iter; left time: 181.5366s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1623183\n",
      "\tspeed: 0.0157s/iter; left time: 178.0541s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1627411\n",
      "\tspeed: 0.0157s/iter; left time: 176.5404s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2751411\n",
      "\tspeed: 0.0157s/iter; left time: 174.7594s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1169626\n",
      "\tspeed: 0.0157s/iter; left time: 173.2544s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2052497\n",
      "\tspeed: 0.0157s/iter; left time: 171.7768s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2055999\n",
      "\tspeed: 0.0157s/iter; left time: 170.2285s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2338544\n",
      "\tspeed: 0.0157s/iter; left time: 168.7095s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0882175\n",
      "\tspeed: 0.0157s/iter; left time: 167.1178s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0823141\n",
      "\tspeed: 0.0157s/iter; left time: 165.4468s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1962520\n",
      "\tspeed: 0.0157s/iter; left time: 163.8604s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0351911\n",
      "\tspeed: 0.0157s/iter; left time: 162.3671s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4122079\n",
      "\tspeed: 0.0157s/iter; left time: 160.7700s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0856364\n",
      "\tspeed: 0.0157s/iter; left time: 159.1680s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1057309\n",
      "\tspeed: 0.0157s/iter; left time: 157.6131s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0931639\n",
      "\tspeed: 0.0157s/iter; left time: 156.0668s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1785975\n",
      "\tspeed: 0.0157s/iter; left time: 154.4714s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3997110\n",
      "\tspeed: 0.0157s/iter; left time: 152.9991s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2769776\n",
      "\tspeed: 0.0157s/iter; left time: 151.3716s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0955917\n",
      "\tspeed: 0.0157s/iter; left time: 149.8403s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0872184\n",
      "\tspeed: 0.0157s/iter; left time: 148.2749s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1506448\n",
      "\tspeed: 0.0157s/iter; left time: 146.6557s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0404093\n",
      "\tspeed: 0.0157s/iter; left time: 145.1851s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0716922\n",
      "\tspeed: 0.0157s/iter; left time: 143.5943s\n",
      "Epoch: 1 cost time: 72.7674548625946\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1763682 Vali Loss: 0.0384911 Test Loss: 0.1245387\n",
      "Validation loss decreased (inf --> 0.038491).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0965484\n",
      "\tspeed: 0.1288s/iter; left time: 1160.2510s\n",
      "\titers: 200, epoch: 2 | loss: 0.0394090\n",
      "\tspeed: 0.0157s/iter; left time: 140.2874s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977589\n",
      "\tspeed: 0.0157s/iter; left time: 138.4587s\n",
      "\titers: 400, epoch: 2 | loss: 0.1882427\n",
      "\tspeed: 0.0157s/iter; left time: 136.7758s\n",
      "\titers: 500, epoch: 2 | loss: 0.3373528\n",
      "\tspeed: 0.0157s/iter; left time: 135.3175s\n",
      "\titers: 600, epoch: 2 | loss: 0.1316223\n",
      "\tspeed: 0.0157s/iter; left time: 133.6311s\n",
      "\titers: 700, epoch: 2 | loss: 0.0733484\n",
      "\tspeed: 0.0157s/iter; left time: 132.0904s\n",
      "\titers: 800, epoch: 2 | loss: 0.0557397\n",
      "\tspeed: 0.0157s/iter; left time: 130.4402s\n",
      "\titers: 900, epoch: 2 | loss: 0.0551844\n",
      "\tspeed: 0.0157s/iter; left time: 128.9730s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1973986\n",
      "\tspeed: 0.0157s/iter; left time: 127.3124s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2021171\n",
      "\tspeed: 0.0157s/iter; left time: 125.6587s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1249596\n",
      "\tspeed: 0.0157s/iter; left time: 124.1530s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1480998\n",
      "\tspeed: 0.0157s/iter; left time: 122.5995s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1150100\n",
      "\tspeed: 0.0157s/iter; left time: 120.9904s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0475104\n",
      "\tspeed: 0.0157s/iter; left time: 119.4362s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1711187\n",
      "\tspeed: 0.0157s/iter; left time: 117.8249s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2710866\n",
      "\tspeed: 0.0157s/iter; left time: 116.3074s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1182006\n",
      "\tspeed: 0.0157s/iter; left time: 114.7066s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1678239\n",
      "\tspeed: 0.0157s/iter; left time: 113.1287s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0585374\n",
      "\tspeed: 0.0157s/iter; left time: 111.5417s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0376854\n",
      "\tspeed: 0.0157s/iter; left time: 110.0087s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177671\n",
      "\tspeed: 0.0157s/iter; left time: 108.4298s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1420685\n",
      "\tspeed: 0.0157s/iter; left time: 106.8839s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1342927\n",
      "\tspeed: 0.0157s/iter; left time: 105.3081s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0575067\n",
      "\tspeed: 0.0157s/iter; left time: 103.7254s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2137174\n",
      "\tspeed: 0.0157s/iter; left time: 102.1351s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1351328\n",
      "\tspeed: 0.0157s/iter; left time: 100.6290s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0589181\n",
      "\tspeed: 0.0157s/iter; left time: 99.0475s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0820708\n",
      "\tspeed: 0.0157s/iter; left time: 97.5228s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1777464\n",
      "\tspeed: 0.0157s/iter; left time: 95.8022s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0429570\n",
      "\tspeed: 0.0157s/iter; left time: 94.2799s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1527276\n",
      "\tspeed: 0.0157s/iter; left time: 92.7218s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0607781\n",
      "\tspeed: 0.0157s/iter; left time: 91.1525s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0930832\n",
      "\tspeed: 0.0157s/iter; left time: 89.5937s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1257465\n",
      "\tspeed: 0.0157s/iter; left time: 88.0196s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1400915\n",
      "\tspeed: 0.0157s/iter; left time: 86.4194s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1529161\n",
      "\tspeed: 0.0157s/iter; left time: 84.8572s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1372309\n",
      "\tspeed: 0.0157s/iter; left time: 83.2639s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1924893\n",
      "\tspeed: 0.0157s/iter; left time: 81.7397s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2247270\n",
      "\tspeed: 0.0157s/iter; left time: 80.1670s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171593\n",
      "\tspeed: 0.0157s/iter; left time: 78.5824s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0934797\n",
      "\tspeed: 0.0157s/iter; left time: 77.0400s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1451997\n",
      "\tspeed: 0.0157s/iter; left time: 75.4590s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1581813\n",
      "\tspeed: 0.0157s/iter; left time: 73.9024s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0497790\n",
      "\tspeed: 0.0157s/iter; left time: 72.3118s\n",
      "Epoch: 2 cost time: 71.75486874580383\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1446429 Vali Loss: 0.0337670 Test Loss: 0.1147559\n",
      "Validation loss decreased (0.038491 --> 0.033767).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0888895\n",
      "\tspeed: 0.1274s/iter; left time: 567.8442s\n",
      "\titers: 200, epoch: 3 | loss: 0.0433673\n",
      "\tspeed: 0.0142s/iter; left time: 61.6707s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213155\n",
      "\tspeed: 0.0141s/iter; left time: 60.1973s\n",
      "\titers: 400, epoch: 3 | loss: 0.0755301\n",
      "\tspeed: 0.0141s/iter; left time: 58.7849s\n",
      "\titers: 500, epoch: 3 | loss: 0.0728352\n",
      "\tspeed: 0.0142s/iter; left time: 57.4076s\n",
      "\titers: 600, epoch: 3 | loss: 0.0727856\n",
      "\tspeed: 0.0141s/iter; left time: 55.9519s\n",
      "\titers: 700, epoch: 3 | loss: 0.3633784\n",
      "\tspeed: 0.0141s/iter; left time: 54.5151s\n",
      "\titers: 800, epoch: 3 | loss: 0.0919219\n",
      "\tspeed: 0.0141s/iter; left time: 53.0911s\n",
      "\titers: 900, epoch: 3 | loss: 0.0787504\n",
      "\tspeed: 0.0141s/iter; left time: 51.6741s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1017697\n",
      "\tspeed: 0.0141s/iter; left time: 50.2735s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1278257\n",
      "\tspeed: 0.0141s/iter; left time: 48.8898s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1174518\n",
      "\tspeed: 0.0141s/iter; left time: 47.4281s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2299570\n",
      "\tspeed: 0.0141s/iter; left time: 46.0580s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1301254\n",
      "\tspeed: 0.0144s/iter; left time: 45.5647s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1960834\n",
      "\tspeed: 0.0162s/iter; left time: 49.5433s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1536359\n",
      "\tspeed: 0.0162s/iter; left time: 47.8956s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5119310\n",
      "\tspeed: 0.0162s/iter; left time: 46.2884s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0803905\n",
      "\tspeed: 0.0162s/iter; left time: 44.6614s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1686091\n",
      "\tspeed: 0.0162s/iter; left time: 43.0572s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2706951\n",
      "\tspeed: 0.0162s/iter; left time: 41.4163s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0878483\n",
      "\tspeed: 0.0162s/iter; left time: 39.8006s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0488082\n",
      "\tspeed: 0.0162s/iter; left time: 38.1800s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1471822\n",
      "\tspeed: 0.0162s/iter; left time: 36.5859s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1000195\n",
      "\tspeed: 0.0162s/iter; left time: 34.9316s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1408294\n",
      "\tspeed: 0.0162s/iter; left time: 33.3205s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0522313\n",
      "\tspeed: 0.0162s/iter; left time: 31.6988s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0433277\n",
      "\tspeed: 0.0162s/iter; left time: 30.0790s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1223619\n",
      "\tspeed: 0.0162s/iter; left time: 28.4609s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0673580\n",
      "\tspeed: 0.0162s/iter; left time: 26.8292s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0574459\n",
      "\tspeed: 0.0162s/iter; left time: 25.2086s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1538857\n",
      "\tspeed: 0.0162s/iter; left time: 23.5969s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2254446\n",
      "\tspeed: 0.0162s/iter; left time: 21.9778s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0779002\n",
      "\tspeed: 0.0162s/iter; left time: 20.3511s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1575561\n",
      "\tspeed: 0.0162s/iter; left time: 18.7300s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0331902\n",
      "\tspeed: 0.0162s/iter; left time: 17.1091s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1007734\n",
      "\tspeed: 0.0162s/iter; left time: 15.4867s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1122994\n",
      "\tspeed: 0.0162s/iter; left time: 13.8652s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0607264\n",
      "\tspeed: 0.0162s/iter; left time: 12.2496s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1815582\n",
      "\tspeed: 0.0162s/iter; left time: 10.6257s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1472391\n",
      "\tspeed: 0.0162s/iter; left time: 9.0042s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1834221\n",
      "\tspeed: 0.0162s/iter; left time: 7.3867s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0803711\n",
      "\tspeed: 0.0162s/iter; left time: 5.7671s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0216850\n",
      "\tspeed: 0.0162s/iter; left time: 4.1488s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0932727\n",
      "\tspeed: 0.0162s/iter; left time: 2.5262s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0153473\n",
      "\tspeed: 0.0162s/iter; left time: 0.9069s\n",
      "Epoch: 3 cost time: 71.22443127632141\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1149970 Vali Loss: 0.0354637 Test Loss: 0.1141755\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11478278785943985, mae:0.20999355614185333\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e4699",
   "metadata": {},
   "source": [
    "#### M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f15e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 200, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 135, in train\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 219, in forward\n",
      "    dec_out = self.forecast_multi(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 197, in forecast_multi\n",
      "    en_embed, n_vars = self.en_embedding(x_enc.permute(0, 2, 1))\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 46, in forward\n",
      "    x = torch.cat([x, glb], dim=2)\n",
      "RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 3 but got size 1 for tensor number 1 in the list.\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features M --predictor wind_forecast,total_load --enc_in 1 --dec_in 1 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd4572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3632965\n",
      "\tspeed: 0.0279s/iter; left time: 378.6282s\n",
      "\titers: 200, epoch: 1 | loss: 0.3612089\n",
      "\tspeed: 0.0164s/iter; left time: 221.1442s\n",
      "\titers: 300, epoch: 1 | loss: 0.1663644\n",
      "\tspeed: 0.0164s/iter; left time: 219.3137s\n",
      "\titers: 400, epoch: 1 | loss: 0.3175196\n",
      "\tspeed: 0.0165s/iter; left time: 218.2545s\n",
      "\titers: 500, epoch: 1 | loss: 0.2269731\n",
      "\tspeed: 0.0164s/iter; left time: 216.2420s\n",
      "\titers: 600, epoch: 1 | loss: 0.2223674\n",
      "\tspeed: 0.0164s/iter; left time: 214.0410s\n",
      "\titers: 700, epoch: 1 | loss: 0.3031235\n",
      "\tspeed: 0.0164s/iter; left time: 212.6197s\n",
      "\titers: 800, epoch: 1 | loss: 0.1987855\n",
      "\tspeed: 0.0164s/iter; left time: 210.7128s\n",
      "\titers: 900, epoch: 1 | loss: 0.2818239\n",
      "\tspeed: 0.0164s/iter; left time: 209.1298s\n",
      "\titers: 1000, epoch: 1 | loss: 0.2000554\n",
      "\tspeed: 0.0164s/iter; left time: 207.6049s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2433306\n",
      "\tspeed: 0.0164s/iter; left time: 205.6955s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1458235\n",
      "\tspeed: 0.0164s/iter; left time: 204.1490s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1891625\n",
      "\tspeed: 0.0164s/iter; left time: 202.3235s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1622000\n",
      "\tspeed: 0.0164s/iter; left time: 201.1719s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1900355\n",
      "\tspeed: 0.0164s/iter; left time: 199.3857s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2153424\n",
      "\tspeed: 0.0164s/iter; left time: 197.3288s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3175794\n",
      "\tspeed: 0.0163s/iter; left time: 195.2743s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1359680\n",
      "\tspeed: 0.0163s/iter; left time: 193.6656s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2951876\n",
      "\tspeed: 0.0163s/iter; left time: 192.0945s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0921913\n",
      "\tspeed: 0.0163s/iter; left time: 190.3784s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2372821\n",
      "\tspeed: 0.0163s/iter; left time: 188.6682s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1914964\n",
      "\tspeed: 0.0163s/iter; left time: 187.0176s\n",
      "\titers: 2300, epoch: 1 | loss: 0.3013221\n",
      "\tspeed: 0.0163s/iter; left time: 185.3950s\n",
      "\titers: 2400, epoch: 1 | loss: 0.6709409\n",
      "\tspeed: 0.0163s/iter; left time: 183.9881s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2330123\n",
      "\tspeed: 0.0163s/iter; left time: 182.3352s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1482763\n",
      "\tspeed: 0.0163s/iter; left time: 180.5598s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1771284\n",
      "\tspeed: 0.0163s/iter; left time: 178.9543s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2025249\n",
      "\tspeed: 0.0163s/iter; left time: 177.3373s\n",
      "\titers: 2900, epoch: 1 | loss: 0.3192387\n",
      "\tspeed: 0.0163s/iter; left time: 175.8075s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2197634\n",
      "\tspeed: 0.0163s/iter; left time: 173.8873s\n",
      "\titers: 3100, epoch: 1 | loss: 0.2542244\n",
      "\tspeed: 0.0163s/iter; left time: 172.4733s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2812100\n",
      "\tspeed: 0.0163s/iter; left time: 170.7441s\n",
      "\titers: 3300, epoch: 1 | loss: 0.2572041\n",
      "\tspeed: 0.0163s/iter; left time: 169.0942s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1190821\n",
      "\tspeed: 0.0163s/iter; left time: 167.5077s\n",
      "\titers: 3500, epoch: 1 | loss: 0.4581592\n",
      "\tspeed: 0.0163s/iter; left time: 165.8238s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045532\n",
      "\tspeed: 0.0163s/iter; left time: 164.1337s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1117853\n",
      "\tspeed: 0.0163s/iter; left time: 162.5575s\n",
      "\titers: 3800, epoch: 1 | loss: 0.5856978\n",
      "\tspeed: 0.0163s/iter; left time: 160.9367s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1279238\n",
      "\tspeed: 0.0152s/iter; left time: 148.3714s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2664152\n",
      "\tspeed: 0.0157s/iter; left time: 151.6946s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1563135\n",
      "\tspeed: 0.0163s/iter; left time: 155.9413s\n",
      "\titers: 4200, epoch: 1 | loss: 0.2030613\n",
      "\tspeed: 0.0163s/iter; left time: 154.4227s\n",
      "\titers: 4300, epoch: 1 | loss: 0.2133383\n",
      "\tspeed: 0.0163s/iter; left time: 152.6971s\n",
      "\titers: 4400, epoch: 1 | loss: 0.3547910\n",
      "\tspeed: 0.0163s/iter; left time: 151.1360s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1626694\n",
      "\tspeed: 0.0163s/iter; left time: 149.4179s\n",
      "Epoch: 1 cost time: 75.46159172058105\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.2506176 Vali Loss: 0.1792732 Test Loss: 0.1749838\n",
      "Validation loss decreased (inf --> 0.179273).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2447494\n",
      "\tspeed: 0.1311s/iter; left time: 1181.0391s\n",
      "\titers: 200, epoch: 2 | loss: 0.1742973\n",
      "\tspeed: 0.0147s/iter; left time: 130.7076s\n",
      "\titers: 300, epoch: 2 | loss: 0.3313923\n",
      "\tspeed: 0.0147s/iter; left time: 129.1874s\n",
      "\titers: 400, epoch: 2 | loss: 0.2792679\n",
      "\tspeed: 0.0147s/iter; left time: 127.6480s\n",
      "\titers: 500, epoch: 2 | loss: 0.5619069\n",
      "\tspeed: 0.0146s/iter; left time: 126.0644s\n",
      "\titers: 600, epoch: 2 | loss: 0.1252872\n",
      "\tspeed: 0.0147s/iter; left time: 124.8745s\n",
      "\titers: 700, epoch: 2 | loss: 0.3338600\n",
      "\tspeed: 0.0147s/iter; left time: 123.3073s\n",
      "\titers: 800, epoch: 2 | loss: 0.1635914\n",
      "\tspeed: 0.0147s/iter; left time: 121.8683s\n",
      "\titers: 900, epoch: 2 | loss: 0.2267582\n",
      "\tspeed: 0.0146s/iter; left time: 120.2232s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2687585\n",
      "\tspeed: 0.0146s/iter; left time: 118.7330s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1283258\n",
      "\tspeed: 0.0147s/iter; left time: 117.4786s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2612851\n",
      "\tspeed: 0.0147s/iter; left time: 115.9391s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1672531\n",
      "\tspeed: 0.0146s/iter; left time: 114.4021s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1151724\n",
      "\tspeed: 0.0146s/iter; left time: 112.8752s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2623320\n",
      "\tspeed: 0.0146s/iter; left time: 111.4134s\n",
      "\titers: 1600, epoch: 2 | loss: 0.4514408\n",
      "\tspeed: 0.0146s/iter; left time: 109.9733s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3221964\n",
      "\tspeed: 0.0147s/iter; left time: 108.5821s\n",
      "\titers: 1800, epoch: 2 | loss: 0.3075443\n",
      "\tspeed: 0.0146s/iter; left time: 107.0587s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1465409\n",
      "\tspeed: 0.0146s/iter; left time: 105.6031s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1916947\n",
      "\tspeed: 0.0147s/iter; left time: 104.2067s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1685293\n",
      "\tspeed: 0.0146s/iter; left time: 102.7041s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1068334\n",
      "\tspeed: 0.0147s/iter; left time: 101.3571s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1138222\n",
      "\tspeed: 0.0147s/iter; left time: 99.8042s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2333193\n",
      "\tspeed: 0.0147s/iter; left time: 98.3636s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1435477\n",
      "\tspeed: 0.0147s/iter; left time: 96.8980s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2848657\n",
      "\tspeed: 0.0147s/iter; left time: 95.4913s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1357936\n",
      "\tspeed: 0.0147s/iter; left time: 94.0584s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1076243\n",
      "\tspeed: 0.0147s/iter; left time: 92.5671s\n",
      "\titers: 2900, epoch: 2 | loss: 0.2340539\n",
      "\tspeed: 0.0147s/iter; left time: 91.0689s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1995971\n",
      "\tspeed: 0.0147s/iter; left time: 89.5618s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1425870\n",
      "\tspeed: 0.0147s/iter; left time: 88.1130s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2513198\n",
      "\tspeed: 0.0147s/iter; left time: 86.6719s\n",
      "\titers: 3300, epoch: 2 | loss: 0.2660067\n",
      "\tspeed: 0.0147s/iter; left time: 85.1720s\n",
      "\titers: 3400, epoch: 2 | loss: 0.2678370\n",
      "\tspeed: 0.0147s/iter; left time: 83.6791s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1534656\n",
      "\tspeed: 0.0147s/iter; left time: 82.2437s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0933451\n",
      "\tspeed: 0.0147s/iter; left time: 80.7480s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2419136\n",
      "\tspeed: 0.0147s/iter; left time: 79.3220s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1625462\n",
      "\tspeed: 0.0146s/iter; left time: 77.8050s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2648199\n",
      "\tspeed: 0.0146s/iter; left time: 76.3371s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1442360\n",
      "\tspeed: 0.0146s/iter; left time: 74.8478s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1991904\n",
      "\tspeed: 0.0146s/iter; left time: 73.4050s\n",
      "\titers: 4200, epoch: 2 | loss: 0.2070115\n",
      "\tspeed: 0.0147s/iter; left time: 71.9813s\n",
      "\titers: 4300, epoch: 2 | loss: 0.3039147\n",
      "\tspeed: 0.0147s/iter; left time: 70.4860s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1379984\n",
      "\tspeed: 0.0147s/iter; left time: 69.0389s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1885291\n",
      "\tspeed: 0.0147s/iter; left time: 67.5797s\n",
      "Epoch: 2 cost time: 67.01384449005127\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.2085815 Vali Loss: 0.1740521 Test Loss: 0.1763017\n",
      "Validation loss decreased (0.179273 --> 0.174052).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2503781\n",
      "\tspeed: 0.1305s/iter; left time: 581.7061s\n",
      "\titers: 200, epoch: 3 | loss: 0.2045967\n",
      "\tspeed: 0.0148s/iter; left time: 64.5182s\n",
      "\titers: 300, epoch: 3 | loss: 0.1804852\n",
      "\tspeed: 0.0148s/iter; left time: 62.9629s\n",
      "\titers: 400, epoch: 3 | loss: 0.2779175\n",
      "\tspeed: 0.0148s/iter; left time: 61.5062s\n",
      "\titers: 500, epoch: 3 | loss: 0.1977433\n",
      "\tspeed: 0.0148s/iter; left time: 60.0529s\n",
      "\titers: 600, epoch: 3 | loss: 0.2751262\n",
      "\tspeed: 0.0148s/iter; left time: 58.4860s\n",
      "\titers: 700, epoch: 3 | loss: 0.1283241\n",
      "\tspeed: 0.0148s/iter; left time: 57.0188s\n",
      "\titers: 800, epoch: 3 | loss: 0.1195078\n",
      "\tspeed: 0.0148s/iter; left time: 55.6052s\n",
      "\titers: 900, epoch: 3 | loss: 0.1326087\n",
      "\tspeed: 0.0148s/iter; left time: 54.1300s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2927760\n",
      "\tspeed: 0.0148s/iter; left time: 52.5990s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0946229\n",
      "\tspeed: 0.0148s/iter; left time: 51.1249s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1599361\n",
      "\tspeed: 0.0148s/iter; left time: 49.6346s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1795194\n",
      "\tspeed: 0.0148s/iter; left time: 48.1371s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1515631\n",
      "\tspeed: 0.0148s/iter; left time: 46.6708s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1621780\n",
      "\tspeed: 0.0148s/iter; left time: 45.1731s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1284781\n",
      "\tspeed: 0.0148s/iter; left time: 43.6984s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1399402\n",
      "\tspeed: 0.0148s/iter; left time: 42.2042s\n",
      "\titers: 1800, epoch: 3 | loss: 0.3037611\n",
      "\tspeed: 0.0148s/iter; left time: 40.7520s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1341804\n",
      "\tspeed: 0.0148s/iter; left time: 39.2829s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2086841\n",
      "\tspeed: 0.0148s/iter; left time: 37.7928s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0556780\n",
      "\tspeed: 0.0148s/iter; left time: 36.2933s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1626754\n",
      "\tspeed: 0.0148s/iter; left time: 34.8222s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1540606\n",
      "\tspeed: 0.0148s/iter; left time: 33.3529s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1304726\n",
      "\tspeed: 0.0148s/iter; left time: 31.8811s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1162029\n",
      "\tspeed: 0.0148s/iter; left time: 30.3869s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2020690\n",
      "\tspeed: 0.0148s/iter; left time: 28.9207s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0991517\n",
      "\tspeed: 0.0148s/iter; left time: 27.4439s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1388047\n",
      "\tspeed: 0.0148s/iter; left time: 25.9508s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1469031\n",
      "\tspeed: 0.0148s/iter; left time: 24.4736s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1781340\n",
      "\tspeed: 0.0148s/iter; left time: 22.9960s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1358701\n",
      "\tspeed: 0.0148s/iter; left time: 21.5143s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1522381\n",
      "\tspeed: 0.0148s/iter; left time: 20.0512s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1845058\n",
      "\tspeed: 0.0148s/iter; left time: 18.5595s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1081519\n",
      "\tspeed: 0.0148s/iter; left time: 17.0857s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1954119\n",
      "\tspeed: 0.0148s/iter; left time: 15.6095s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1099689\n",
      "\tspeed: 0.0148s/iter; left time: 14.1287s\n",
      "\titers: 3700, epoch: 3 | loss: 0.2125370\n",
      "\tspeed: 0.0148s/iter; left time: 12.6516s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0912142\n",
      "\tspeed: 0.0148s/iter; left time: 11.1698s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0528369\n",
      "\tspeed: 0.0148s/iter; left time: 9.6946s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2848759\n",
      "\tspeed: 0.0148s/iter; left time: 8.2175s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0933764\n",
      "\tspeed: 0.0148s/iter; left time: 6.7372s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1051632\n",
      "\tspeed: 0.0148s/iter; left time: 5.2681s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1540013\n",
      "\tspeed: 0.0148s/iter; left time: 3.7936s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1135963\n",
      "\tspeed: 0.0148s/iter; left time: 2.3105s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1358993\n",
      "\tspeed: 0.0148s/iter; left time: 0.8297s\n",
      "Epoch: 3 cost time: 67.61999917030334\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1682378 Vali Loss: 0.1965955 Test Loss: 0.2002430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "mse:0.17627310752868652, mae:0.2662774920463562\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features M --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35597a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              3                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       3                   Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3632965\n",
      "\tspeed: 0.0264s/iter; left time: 358.1489s\n",
      "\titers: 200, epoch: 1 | loss: 0.3612089\n",
      "\tspeed: 0.0146s/iter; left time: 196.9570s\n",
      "\titers: 300, epoch: 1 | loss: 0.1663644\n",
      "\tspeed: 0.0146s/iter; left time: 195.0909s\n",
      "\titers: 400, epoch: 1 | loss: 0.3175196\n",
      "\tspeed: 0.0146s/iter; left time: 193.6423s\n",
      "\titers: 500, epoch: 1 | loss: 0.2269731\n",
      "\tspeed: 0.0146s/iter; left time: 192.0001s\n",
      "\titers: 600, epoch: 1 | loss: 0.2223674\n",
      "\tspeed: 0.0146s/iter; left time: 190.8427s\n",
      "\titers: 700, epoch: 1 | loss: 0.3031235\n",
      "\tspeed: 0.0146s/iter; left time: 189.0582s\n",
      "\titers: 800, epoch: 1 | loss: 0.1987855\n",
      "\tspeed: 0.0147s/iter; left time: 189.2614s\n",
      "\titers: 900, epoch: 1 | loss: 0.2818239\n",
      "\tspeed: 0.0148s/iter; left time: 188.5843s\n",
      "\titers: 1000, epoch: 1 | loss: 0.2000554\n",
      "\tspeed: 0.0148s/iter; left time: 187.4540s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2433306\n",
      "\tspeed: 0.0145s/iter; left time: 182.8148s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1458235\n",
      "\tspeed: 0.0146s/iter; left time: 181.6296s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1891625\n",
      "\tspeed: 0.0145s/iter; left time: 179.8345s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1622000\n",
      "\tspeed: 0.0146s/iter; left time: 178.6677s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1900355\n",
      "\tspeed: 0.0145s/iter; left time: 176.8428s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2153424\n",
      "\tspeed: 0.0145s/iter; left time: 175.4103s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3175794\n",
      "\tspeed: 0.0146s/iter; left time: 174.1263s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1359680\n",
      "\tspeed: 0.0145s/iter; left time: 172.5530s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2951876\n",
      "\tspeed: 0.0145s/iter; left time: 171.0503s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0921913\n",
      "\tspeed: 0.0145s/iter; left time: 169.5827s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2372821\n",
      "\tspeed: 0.0145s/iter; left time: 168.1282s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1914964\n",
      "\tspeed: 0.0145s/iter; left time: 166.6994s\n",
      "\titers: 2300, epoch: 1 | loss: 0.3013221\n",
      "\tspeed: 0.0145s/iter; left time: 165.2545s\n",
      "\titers: 2400, epoch: 1 | loss: 0.6709409\n",
      "\tspeed: 0.0145s/iter; left time: 163.7905s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2330123\n",
      "\tspeed: 0.0145s/iter; left time: 162.3244s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1482763\n",
      "\tspeed: 0.0145s/iter; left time: 160.9085s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1771284\n",
      "\tspeed: 0.0145s/iter; left time: 159.4759s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2025249\n",
      "\tspeed: 0.0145s/iter; left time: 158.0807s\n",
      "\titers: 2900, epoch: 1 | loss: 0.3192387\n",
      "\tspeed: 0.0145s/iter; left time: 156.5642s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2197634\n",
      "\tspeed: 0.0145s/iter; left time: 155.0996s\n",
      "\titers: 3100, epoch: 1 | loss: 0.2542244\n",
      "\tspeed: 0.0145s/iter; left time: 153.6898s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2812100\n",
      "\tspeed: 0.0145s/iter; left time: 152.2606s\n",
      "\titers: 3300, epoch: 1 | loss: 0.2572041\n",
      "\tspeed: 0.0145s/iter; left time: 150.8046s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1190821\n",
      "\tspeed: 0.0145s/iter; left time: 149.3470s\n",
      "\titers: 3500, epoch: 1 | loss: 0.4581592\n",
      "\tspeed: 0.0146s/iter; left time: 147.9209s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045532\n",
      "\tspeed: 0.0145s/iter; left time: 146.4372s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1117853\n",
      "\tspeed: 0.0146s/iter; left time: 145.3698s\n",
      "\titers: 3800, epoch: 1 | loss: 0.5856978\n",
      "\tspeed: 0.0146s/iter; left time: 143.8440s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1279238\n",
      "\tspeed: 0.0146s/iter; left time: 142.7016s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2664152\n",
      "\tspeed: 0.0146s/iter; left time: 140.7824s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1563135\n",
      "\tspeed: 0.0146s/iter; left time: 139.5136s\n",
      "\titers: 4200, epoch: 1 | loss: 0.2030613\n",
      "\tspeed: 0.0146s/iter; left time: 138.0524s\n",
      "\titers: 4300, epoch: 1 | loss: 0.2133383\n",
      "\tspeed: 0.0146s/iter; left time: 136.3848s\n",
      "\titers: 4400, epoch: 1 | loss: 0.3547910\n",
      "\tspeed: 0.0146s/iter; left time: 135.1739s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1626694\n",
      "\tspeed: 0.0146s/iter; left time: 133.4781s\n",
      "Epoch: 1 cost time: 67.61253595352173\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.2506176 Vali Loss: 0.1792732 Test Loss: 0.1749838\n",
      "Validation loss decreased (inf --> 0.179273).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2447494\n",
      "\tspeed: 0.1328s/iter; left time: 1196.3507s\n",
      "\titers: 200, epoch: 2 | loss: 0.1742973\n",
      "\tspeed: 0.0147s/iter; left time: 130.6289s\n",
      "\titers: 300, epoch: 2 | loss: 0.3313923\n",
      "\tspeed: 0.0147s/iter; left time: 129.2647s\n",
      "\titers: 400, epoch: 2 | loss: 0.2792679\n",
      "\tspeed: 0.0146s/iter; left time: 127.6160s\n",
      "\titers: 500, epoch: 2 | loss: 0.5619069\n",
      "\tspeed: 0.0146s/iter; left time: 126.1141s\n",
      "\titers: 600, epoch: 2 | loss: 0.1252872\n",
      "\tspeed: 0.0147s/iter; left time: 124.9487s\n",
      "\titers: 700, epoch: 2 | loss: 0.3338600\n",
      "\tspeed: 0.0147s/iter; left time: 123.3512s\n",
      "\titers: 800, epoch: 2 | loss: 0.1635914\n",
      "\tspeed: 0.0147s/iter; left time: 121.8615s\n",
      "\titers: 900, epoch: 2 | loss: 0.2267582\n",
      "\tspeed: 0.0146s/iter; left time: 120.2827s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2687585\n",
      "\tspeed: 0.0146s/iter; left time: 118.5284s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1283258\n",
      "\tspeed: 0.0146s/iter; left time: 117.0401s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2612851\n",
      "\tspeed: 0.0146s/iter; left time: 115.5450s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1672531\n",
      "\tspeed: 0.0146s/iter; left time: 114.1597s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1151724\n",
      "\tspeed: 0.0146s/iter; left time: 112.6765s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2623320\n",
      "\tspeed: 0.0146s/iter; left time: 111.1166s\n",
      "\titers: 1600, epoch: 2 | loss: 0.4514408\n",
      "\tspeed: 0.0146s/iter; left time: 109.6086s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3221964\n",
      "\tspeed: 0.0146s/iter; left time: 108.2526s\n",
      "\titers: 1800, epoch: 2 | loss: 0.3075443\n",
      "\tspeed: 0.0146s/iter; left time: 106.7694s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1465409\n",
      "\tspeed: 0.0146s/iter; left time: 105.3198s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1916947\n",
      "\tspeed: 0.0146s/iter; left time: 103.8734s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1685293\n",
      "\tspeed: 0.0146s/iter; left time: 102.4139s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1068334\n",
      "\tspeed: 0.0146s/iter; left time: 100.9721s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1138222\n",
      "\tspeed: 0.0146s/iter; left time: 99.4954s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2333193\n",
      "\tspeed: 0.0146s/iter; left time: 98.0176s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1435477\n",
      "\tspeed: 0.0146s/iter; left time: 96.5250s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2848657\n",
      "\tspeed: 0.0146s/iter; left time: 95.0823s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1357936\n",
      "\tspeed: 0.0146s/iter; left time: 93.6402s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1076243\n",
      "\tspeed: 0.0146s/iter; left time: 92.4226s\n",
      "\titers: 2900, epoch: 2 | loss: 0.2340539\n",
      "\tspeed: 0.0147s/iter; left time: 91.0730s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1995971\n",
      "\tspeed: 0.0147s/iter; left time: 89.6121s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1425870\n",
      "\tspeed: 0.0147s/iter; left time: 88.1329s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2513198\n",
      "\tspeed: 0.0147s/iter; left time: 86.7013s\n",
      "\titers: 3300, epoch: 2 | loss: 0.2660067\n",
      "\tspeed: 0.0147s/iter; left time: 85.2212s\n",
      "\titers: 3400, epoch: 2 | loss: 0.2678370\n",
      "\tspeed: 0.0147s/iter; left time: 83.7260s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1534656\n",
      "\tspeed: 0.0147s/iter; left time: 82.2286s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0933451\n",
      "\tspeed: 0.0146s/iter; left time: 80.7168s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2419136\n",
      "\tspeed: 0.0146s/iter; left time: 79.2682s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1625462\n",
      "\tspeed: 0.0147s/iter; left time: 77.8129s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2648199\n",
      "\tspeed: 0.0146s/iter; left time: 76.3342s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1442360\n",
      "\tspeed: 0.0146s/iter; left time: 74.7238s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1991904\n",
      "\tspeed: 0.0146s/iter; left time: 73.3165s\n",
      "\titers: 4200, epoch: 2 | loss: 0.2070115\n",
      "\tspeed: 0.0146s/iter; left time: 71.6685s\n",
      "\titers: 4300, epoch: 2 | loss: 0.3039147\n",
      "\tspeed: 0.0146s/iter; left time: 70.1247s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1379984\n",
      "\tspeed: 0.0145s/iter; left time: 68.4136s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1885291\n",
      "\tspeed: 0.0145s/iter; left time: 66.9478s\n",
      "Epoch: 2 cost time: 67.19622087478638\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.2085815 Vali Loss: 0.1740521 Test Loss: 0.1763017\n",
      "Validation loss decreased (0.179273 --> 0.174052).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2503781\n",
      "\tspeed: 0.1310s/iter; left time: 583.9453s\n",
      "\titers: 200, epoch: 3 | loss: 0.2045967\n",
      "\tspeed: 0.0146s/iter; left time: 63.7557s\n",
      "\titers: 300, epoch: 3 | loss: 0.1804852\n",
      "\tspeed: 0.0148s/iter; left time: 63.0603s\n",
      "\titers: 400, epoch: 3 | loss: 0.2779175\n",
      "\tspeed: 0.0147s/iter; left time: 60.9074s\n",
      "\titers: 500, epoch: 3 | loss: 0.1977433\n",
      "\tspeed: 0.0147s/iter; left time: 59.4734s\n",
      "\titers: 600, epoch: 3 | loss: 0.2751262\n",
      "\tspeed: 0.0147s/iter; left time: 58.0729s\n",
      "\titers: 700, epoch: 3 | loss: 0.1283241\n",
      "\tspeed: 0.0147s/iter; left time: 56.5029s\n",
      "\titers: 800, epoch: 3 | loss: 0.1195078\n",
      "\tspeed: 0.0146s/iter; left time: 54.9912s\n",
      "\titers: 900, epoch: 3 | loss: 0.1326087\n",
      "\tspeed: 0.0147s/iter; left time: 53.5844s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2927760\n",
      "\tspeed: 0.0147s/iter; left time: 52.1151s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0946229\n",
      "\tspeed: 0.0147s/iter; left time: 50.6994s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1599361\n",
      "\tspeed: 0.0146s/iter; left time: 49.1375s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1795194\n",
      "\tspeed: 0.0146s/iter; left time: 47.6766s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1515631\n",
      "\tspeed: 0.0146s/iter; left time: 46.2024s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1621780\n",
      "\tspeed: 0.0146s/iter; left time: 44.7688s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1284781\n",
      "\tspeed: 0.0147s/iter; left time: 43.3410s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1399402\n",
      "\tspeed: 0.0147s/iter; left time: 41.8460s\n",
      "\titers: 1800, epoch: 3 | loss: 0.3037611\n",
      "\tspeed: 0.0146s/iter; left time: 40.3677s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1341804\n",
      "\tspeed: 0.0146s/iter; left time: 38.9034s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2086841\n",
      "\tspeed: 0.0146s/iter; left time: 37.3985s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0556780\n",
      "\tspeed: 0.0146s/iter; left time: 35.8989s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1626754\n",
      "\tspeed: 0.0146s/iter; left time: 34.4555s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1540606\n",
      "\tspeed: 0.0146s/iter; left time: 33.0130s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1304726\n",
      "\tspeed: 0.0146s/iter; left time: 31.4856s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1162029\n",
      "\tspeed: 0.0146s/iter; left time: 30.0031s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2020690\n",
      "\tspeed: 0.0146s/iter; left time: 28.5431s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0991517\n",
      "\tspeed: 0.0146s/iter; left time: 27.0869s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1388047\n",
      "\tspeed: 0.0146s/iter; left time: 25.6338s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1469031\n",
      "\tspeed: 0.0146s/iter; left time: 24.1629s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1781340\n",
      "\tspeed: 0.0146s/iter; left time: 22.7249s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1358701\n",
      "\tspeed: 0.0146s/iter; left time: 21.2817s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1522381\n",
      "\tspeed: 0.0146s/iter; left time: 19.8237s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1845058\n",
      "\tspeed: 0.0146s/iter; left time: 18.3459s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1081519\n",
      "\tspeed: 0.0146s/iter; left time: 16.8826s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1954119\n",
      "\tspeed: 0.0146s/iter; left time: 15.4264s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1099689\n",
      "\tspeed: 0.0146s/iter; left time: 13.9613s\n",
      "\titers: 3700, epoch: 3 | loss: 0.2125370\n",
      "\tspeed: 0.0146s/iter; left time: 12.5059s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0912142\n",
      "\tspeed: 0.0146s/iter; left time: 11.0428s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0528369\n",
      "\tspeed: 0.0146s/iter; left time: 9.5789s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2848759\n",
      "\tspeed: 0.0146s/iter; left time: 8.1247s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0933764\n",
      "\tspeed: 0.0146s/iter; left time: 6.6525s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1051632\n",
      "\tspeed: 0.0146s/iter; left time: 5.1954s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1540013\n",
      "\tspeed: 0.0146s/iter; left time: 3.7376s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1135963\n",
      "\tspeed: 0.0146s/iter; left time: 2.2779s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1358993\n",
      "\tspeed: 0.0146s/iter; left time: 0.8176s\n",
      "Epoch: 3 cost time: 66.91784024238586\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1682378 Vali Loss: 0.1965955 Test Loss: 0.2002430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "mse:0.17627310752868652, mae:0.2662774920463562\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features M --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fc8d0",
   "metadata": {},
   "source": [
    "## 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de29391",
   "metadata": {},
   "source": [
    "### wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5aa0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1880594\n",
      "\tspeed: 0.0273s/iter; left time: 1241.6736s\n",
      "\titers: 200, epoch: 1 | loss: 0.1026268\n",
      "\tspeed: 0.0153s/iter; left time: 693.9870s\n",
      "\titers: 300, epoch: 1 | loss: 0.0646233\n",
      "\tspeed: 0.0142s/iter; left time: 642.7613s\n",
      "\titers: 400, epoch: 1 | loss: 0.1436839\n",
      "\tspeed: 0.0142s/iter; left time: 641.1864s\n",
      "\titers: 500, epoch: 1 | loss: 0.0337003\n",
      "\tspeed: 0.0142s/iter; left time: 638.2957s\n",
      "\titers: 600, epoch: 1 | loss: 0.0588938\n",
      "\tspeed: 0.0142s/iter; left time: 639.2321s\n",
      "\titers: 700, epoch: 1 | loss: 0.1358326\n",
      "\tspeed: 0.0142s/iter; left time: 636.5204s\n",
      "\titers: 800, epoch: 1 | loss: 0.5110136\n",
      "\tspeed: 0.0142s/iter; left time: 636.0602s\n",
      "\titers: 900, epoch: 1 | loss: 0.2601639\n",
      "\tspeed: 0.0142s/iter; left time: 633.4646s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0690488\n",
      "\tspeed: 0.0142s/iter; left time: 630.6430s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0552730\n",
      "\tspeed: 0.0142s/iter; left time: 629.0866s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0722296\n",
      "\tspeed: 0.0141s/iter; left time: 627.1344s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2892447\n",
      "\tspeed: 0.0142s/iter; left time: 627.0699s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0230870\n",
      "\tspeed: 0.0142s/iter; left time: 625.0468s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1299016\n",
      "\tspeed: 0.0142s/iter; left time: 623.4042s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1390953\n",
      "\tspeed: 0.0141s/iter; left time: 621.8815s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3027755\n",
      "\tspeed: 0.0149s/iter; left time: 652.2752s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0582245\n",
      "\tspeed: 0.0157s/iter; left time: 685.6951s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1958276\n",
      "\tspeed: 0.0157s/iter; left time: 685.5559s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4077032\n",
      "\tspeed: 0.0157s/iter; left time: 683.5023s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2276900\n",
      "\tspeed: 0.0157s/iter; left time: 681.3888s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2901473\n",
      "\tspeed: 0.0157s/iter; left time: 679.8344s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1349899\n",
      "\tspeed: 0.0157s/iter; left time: 678.1347s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1636490\n",
      "\tspeed: 0.0157s/iter; left time: 676.4527s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2848335\n",
      "\tspeed: 0.0157s/iter; left time: 674.6066s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1061540\n",
      "\tspeed: 0.0157s/iter; left time: 673.1303s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1824115\n",
      "\tspeed: 0.0157s/iter; left time: 671.3593s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1878522\n",
      "\tspeed: 0.0157s/iter; left time: 669.9447s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1878474\n",
      "\tspeed: 0.0157s/iter; left time: 668.7465s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0904252\n",
      "\tspeed: 0.0157s/iter; left time: 666.8061s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0542073\n",
      "\tspeed: 0.0157s/iter; left time: 665.4525s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1607954\n",
      "\tspeed: 0.0157s/iter; left time: 663.7374s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0310854\n",
      "\tspeed: 0.0157s/iter; left time: 662.2578s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2267053\n",
      "\tspeed: 0.0157s/iter; left time: 660.4803s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0824267\n",
      "\tspeed: 0.0157s/iter; left time: 659.0470s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0969501\n",
      "\tspeed: 0.0156s/iter; left time: 654.2408s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0843461\n",
      "\tspeed: 0.0141s/iter; left time: 592.0028s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1617526\n",
      "\tspeed: 0.0141s/iter; left time: 590.6852s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3581395\n",
      "\tspeed: 0.0141s/iter; left time: 589.3114s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3739244\n",
      "\tspeed: 0.0141s/iter; left time: 587.5685s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0748074\n",
      "\tspeed: 0.0141s/iter; left time: 586.3256s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0804443\n",
      "\tspeed: 0.0141s/iter; left time: 584.9888s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1978168\n",
      "\tspeed: 0.0141s/iter; left time: 583.5112s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0293587\n",
      "\tspeed: 0.0141s/iter; left time: 582.2319s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0607482\n",
      "\tspeed: 0.0141s/iter; left time: 580.7007s\n",
      "Epoch: 1 cost time: 68.93593215942383\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1760965 Vali Loss: 0.0375671 Test Loss: 0.1243711\n",
      "Validation loss decreased (inf --> 0.037567).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1023001\n",
      "\tspeed: 0.1425s/iter; left time: 5828.8012s\n",
      "\titers: 200, epoch: 2 | loss: 0.0443824\n",
      "\tspeed: 0.0163s/iter; left time: 666.1198s\n",
      "\titers: 300, epoch: 2 | loss: 0.0782190\n",
      "\tspeed: 0.0163s/iter; left time: 663.1441s\n",
      "\titers: 400, epoch: 2 | loss: 0.0906417\n",
      "\tspeed: 0.0163s/iter; left time: 662.0638s\n",
      "\titers: 500, epoch: 2 | loss: 0.3023317\n",
      "\tspeed: 0.0163s/iter; left time: 660.8344s\n",
      "\titers: 600, epoch: 2 | loss: 0.0732646\n",
      "\tspeed: 0.0163s/iter; left time: 659.5896s\n",
      "\titers: 700, epoch: 2 | loss: 0.0909683\n",
      "\tspeed: 0.0163s/iter; left time: 657.7419s\n",
      "\titers: 800, epoch: 2 | loss: 0.0453902\n",
      "\tspeed: 0.0163s/iter; left time: 655.1294s\n",
      "\titers: 900, epoch: 2 | loss: 0.0473101\n",
      "\tspeed: 0.0163s/iter; left time: 653.4053s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1697372\n",
      "\tspeed: 0.0163s/iter; left time: 651.6405s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2098863\n",
      "\tspeed: 0.0163s/iter; left time: 649.9832s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1173484\n",
      "\tspeed: 0.0163s/iter; left time: 648.8611s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0964068\n",
      "\tspeed: 0.0163s/iter; left time: 646.8487s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1164205\n",
      "\tspeed: 0.0163s/iter; left time: 645.0066s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0471032\n",
      "\tspeed: 0.0163s/iter; left time: 643.1754s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1479586\n",
      "\tspeed: 0.0163s/iter; left time: 641.5066s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2439645\n",
      "\tspeed: 0.0163s/iter; left time: 639.8629s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1059873\n",
      "\tspeed: 0.0163s/iter; left time: 639.1469s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1401853\n",
      "\tspeed: 0.0163s/iter; left time: 637.2248s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0535772\n",
      "\tspeed: 0.0163s/iter; left time: 635.6222s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0827046\n",
      "\tspeed: 0.0163s/iter; left time: 633.9168s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1374192\n",
      "\tspeed: 0.0163s/iter; left time: 632.2392s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1499075\n",
      "\tspeed: 0.0163s/iter; left time: 630.9288s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1197314\n",
      "\tspeed: 0.0163s/iter; left time: 629.0034s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0720138\n",
      "\tspeed: 0.0163s/iter; left time: 627.3206s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2874545\n",
      "\tspeed: 0.0163s/iter; left time: 625.8475s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1275534\n",
      "\tspeed: 0.0163s/iter; left time: 623.9087s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0577189\n",
      "\tspeed: 0.0159s/iter; left time: 606.8913s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0497496\n",
      "\tspeed: 0.0157s/iter; left time: 597.9487s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1213172\n",
      "\tspeed: 0.0157s/iter; left time: 596.9794s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0917065\n",
      "\tspeed: 0.0157s/iter; left time: 594.6768s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2026839\n",
      "\tspeed: 0.0157s/iter; left time: 592.2190s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0497320\n",
      "\tspeed: 0.0157s/iter; left time: 591.1656s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0741029\n",
      "\tspeed: 0.0157s/iter; left time: 588.9456s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1714709\n",
      "\tspeed: 0.0157s/iter; left time: 588.1791s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1354392\n",
      "\tspeed: 0.0157s/iter; left time: 585.8968s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1481443\n",
      "\tspeed: 0.0157s/iter; left time: 584.4107s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1425242\n",
      "\tspeed: 0.0157s/iter; left time: 583.0395s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2178237\n",
      "\tspeed: 0.0157s/iter; left time: 581.6489s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2526345\n",
      "\tspeed: 0.0157s/iter; left time: 580.2042s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0959773\n",
      "\tspeed: 0.0157s/iter; left time: 578.2770s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0774052\n",
      "\tspeed: 0.0157s/iter; left time: 577.1882s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1198923\n",
      "\tspeed: 0.0157s/iter; left time: 575.5314s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1138821\n",
      "\tspeed: 0.0157s/iter; left time: 574.2718s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0380464\n",
      "\tspeed: 0.0157s/iter; left time: 573.1837s\n",
      "Epoch: 2 cost time: 73.23544883728027\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1508490 Vali Loss: 0.0334156 Test Loss: 0.1120896\n",
      "Validation loss decreased (0.037567 --> 0.033416).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1158902\n",
      "\tspeed: 0.1286s/iter; left time: 4674.4953s\n",
      "\titers: 200, epoch: 3 | loss: 0.0483347\n",
      "\tspeed: 0.0142s/iter; left time: 515.6616s\n",
      "\titers: 300, epoch: 3 | loss: 0.1121058\n",
      "\tspeed: 0.0142s/iter; left time: 513.2721s\n",
      "\titers: 400, epoch: 3 | loss: 0.1069300\n",
      "\tspeed: 0.0142s/iter; left time: 512.2672s\n",
      "\titers: 500, epoch: 3 | loss: 0.1013264\n",
      "\tspeed: 0.0142s/iter; left time: 509.9626s\n",
      "\titers: 600, epoch: 3 | loss: 0.0809711\n",
      "\tspeed: 0.0142s/iter; left time: 508.6560s\n",
      "\titers: 700, epoch: 3 | loss: 0.2910097\n",
      "\tspeed: 0.0142s/iter; left time: 507.2957s\n",
      "\titers: 800, epoch: 3 | loss: 0.1931276\n",
      "\tspeed: 0.0142s/iter; left time: 505.6342s\n",
      "\titers: 900, epoch: 3 | loss: 0.0564841\n",
      "\tspeed: 0.0142s/iter; left time: 504.0997s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1121366\n",
      "\tspeed: 0.0142s/iter; left time: 502.5908s\n",
      "\titers: 1100, epoch: 3 | loss: 0.2114139\n",
      "\tspeed: 0.0142s/iter; left time: 501.8678s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1515695\n",
      "\tspeed: 0.0141s/iter; left time: 498.2809s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2040596\n",
      "\tspeed: 0.0141s/iter; left time: 496.2375s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1711096\n",
      "\tspeed: 0.0141s/iter; left time: 494.6890s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2077705\n",
      "\tspeed: 0.0142s/iter; left time: 494.4590s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1979721\n",
      "\tspeed: 0.0142s/iter; left time: 493.8427s\n",
      "\titers: 1700, epoch: 3 | loss: 0.6470969\n",
      "\tspeed: 0.0142s/iter; left time: 492.4780s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0812857\n",
      "\tspeed: 0.0142s/iter; left time: 491.0634s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1738021\n",
      "\tspeed: 0.0142s/iter; left time: 489.5875s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2122969\n",
      "\tspeed: 0.0142s/iter; left time: 487.9384s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0888002\n",
      "\tspeed: 0.0142s/iter; left time: 486.8754s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0675884\n",
      "\tspeed: 0.0142s/iter; left time: 485.4183s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1076873\n",
      "\tspeed: 0.0142s/iter; left time: 483.9176s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0852173\n",
      "\tspeed: 0.0142s/iter; left time: 482.5302s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1263370\n",
      "\tspeed: 0.0142s/iter; left time: 480.9162s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0897379\n",
      "\tspeed: 0.0142s/iter; left time: 480.2082s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0478275\n",
      "\tspeed: 0.0142s/iter; left time: 478.3425s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1108397\n",
      "\tspeed: 0.0142s/iter; left time: 477.0554s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0762252\n",
      "\tspeed: 0.0142s/iter; left time: 475.4317s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0452943\n",
      "\tspeed: 0.0142s/iter; left time: 474.3211s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1515656\n",
      "\tspeed: 0.0142s/iter; left time: 472.4362s\n",
      "\titers: 3200, epoch: 3 | loss: 0.7125151\n",
      "\tspeed: 0.0142s/iter; left time: 471.3647s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0828298\n",
      "\tspeed: 0.0142s/iter; left time: 469.6262s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2642997\n",
      "\tspeed: 0.0142s/iter; left time: 470.1172s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0377675\n",
      "\tspeed: 0.0142s/iter; left time: 468.2099s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1297170\n",
      "\tspeed: 0.0142s/iter; left time: 466.8077s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1464807\n",
      "\tspeed: 0.0142s/iter; left time: 465.3029s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0711235\n",
      "\tspeed: 0.0142s/iter; left time: 464.0967s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1722317\n",
      "\tspeed: 0.0142s/iter; left time: 462.5203s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1867490\n",
      "\tspeed: 0.0142s/iter; left time: 461.1942s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2535416\n",
      "\tspeed: 0.0142s/iter; left time: 459.6942s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0618705\n",
      "\tspeed: 0.0142s/iter; left time: 457.6654s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0288661\n",
      "\tspeed: 0.0142s/iter; left time: 455.5358s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0905710\n",
      "\tspeed: 0.0142s/iter; left time: 454.1805s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0366086\n",
      "\tspeed: 0.0142s/iter; left time: 452.6757s\n",
      "Epoch: 3 cost time: 64.90696454048157\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1292094 Vali Loss: 0.0335928 Test Loss: 0.1101938\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0715407\n",
      "\tspeed: 0.1270s/iter; left time: 4035.2597s\n",
      "\titers: 200, epoch: 4 | loss: 0.3183874\n",
      "\tspeed: 0.0143s/iter; left time: 451.7353s\n",
      "\titers: 300, epoch: 4 | loss: 0.0261297\n",
      "\tspeed: 0.0143s/iter; left time: 451.0664s\n",
      "\titers: 400, epoch: 4 | loss: 0.1463172\n",
      "\tspeed: 0.0143s/iter; left time: 448.6756s\n",
      "\titers: 500, epoch: 4 | loss: 0.1227192\n",
      "\tspeed: 0.0143s/iter; left time: 447.3951s\n",
      "\titers: 600, epoch: 4 | loss: 0.1024016\n",
      "\tspeed: 0.0142s/iter; left time: 445.1671s\n",
      "\titers: 700, epoch: 4 | loss: 0.1774553\n",
      "\tspeed: 0.0142s/iter; left time: 444.2353s\n",
      "\titers: 800, epoch: 4 | loss: 0.1642316\n",
      "\tspeed: 0.0143s/iter; left time: 443.0292s\n",
      "\titers: 900, epoch: 4 | loss: 0.0997953\n",
      "\tspeed: 0.0142s/iter; left time: 441.4592s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1123670\n",
      "\tspeed: 0.0142s/iter; left time: 439.3976s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1835231\n",
      "\tspeed: 0.0142s/iter; left time: 437.9901s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0433758\n",
      "\tspeed: 0.0142s/iter; left time: 436.6314s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0918214\n",
      "\tspeed: 0.0142s/iter; left time: 435.6427s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0537506\n",
      "\tspeed: 0.0142s/iter; left time: 433.9153s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1954989\n",
      "\tspeed: 0.0142s/iter; left time: 432.7770s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1014705\n",
      "\tspeed: 0.0142s/iter; left time: 431.2446s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1508603\n",
      "\tspeed: 0.0142s/iter; left time: 429.6953s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0381464\n",
      "\tspeed: 0.0142s/iter; left time: 428.0247s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0788325\n",
      "\tspeed: 0.0142s/iter; left time: 426.7416s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1285593\n",
      "\tspeed: 0.0142s/iter; left time: 425.2392s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0152580\n",
      "\tspeed: 0.0142s/iter; left time: 423.8976s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1971332\n",
      "\tspeed: 0.0142s/iter; left time: 422.3413s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1511163\n",
      "\tspeed: 0.0142s/iter; left time: 420.8162s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1008988\n",
      "\tspeed: 0.0142s/iter; left time: 419.2806s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0431851\n",
      "\tspeed: 0.0142s/iter; left time: 417.9273s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0438550\n",
      "\tspeed: 0.0142s/iter; left time: 416.4417s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5332378\n",
      "\tspeed: 0.0142s/iter; left time: 415.1137s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0207551\n",
      "\tspeed: 0.0142s/iter; left time: 413.9163s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0340281\n",
      "\tspeed: 0.0142s/iter; left time: 412.3761s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0954766\n",
      "\tspeed: 0.0142s/iter; left time: 410.9568s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1000658\n",
      "\tspeed: 0.0142s/iter; left time: 409.3981s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1199103\n",
      "\tspeed: 0.0142s/iter; left time: 408.1903s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0925681\n",
      "\tspeed: 0.0142s/iter; left time: 406.6107s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0569380\n",
      "\tspeed: 0.0142s/iter; left time: 405.2626s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0395989\n",
      "\tspeed: 0.0142s/iter; left time: 403.9360s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0399831\n",
      "\tspeed: 0.0142s/iter; left time: 402.4014s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2401575\n",
      "\tspeed: 0.0142s/iter; left time: 401.0915s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0371709\n",
      "\tspeed: 0.0142s/iter; left time: 399.8224s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0938381\n",
      "\tspeed: 0.0142s/iter; left time: 398.2862s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0754569\n",
      "\tspeed: 0.0142s/iter; left time: 396.9330s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0412428\n",
      "\tspeed: 0.0142s/iter; left time: 395.3759s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0795756\n",
      "\tspeed: 0.0142s/iter; left time: 394.0374s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3650653\n",
      "\tspeed: 0.0142s/iter; left time: 392.6134s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1153900\n",
      "\tspeed: 0.0142s/iter; left time: 391.0206s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0828952\n",
      "\tspeed: 0.0142s/iter; left time: 389.7254s\n",
      "Epoch: 4 cost time: 65.13966655731201\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1124171 Vali Loss: 0.0332162 Test Loss: 0.1152915\n",
      "Validation loss decreased (0.033416 --> 0.033216).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0760190\n",
      "\tspeed: 0.1427s/iter; left time: 3886.7113s\n",
      "\titers: 200, epoch: 5 | loss: 0.0606147\n",
      "\tspeed: 0.0142s/iter; left time: 385.2073s\n",
      "\titers: 300, epoch: 5 | loss: 0.0887262\n",
      "\tspeed: 0.0142s/iter; left time: 383.4358s\n",
      "\titers: 400, epoch: 5 | loss: 0.0528458\n",
      "\tspeed: 0.0142s/iter; left time: 381.0975s\n",
      "\titers: 500, epoch: 5 | loss: 0.1164466\n",
      "\tspeed: 0.0141s/iter; left time: 379.5317s\n",
      "\titers: 600, epoch: 5 | loss: 0.0153286\n",
      "\tspeed: 0.0141s/iter; left time: 378.0768s\n",
      "\titers: 700, epoch: 5 | loss: 0.0424965\n",
      "\tspeed: 0.0141s/iter; left time: 376.7870s\n",
      "\titers: 800, epoch: 5 | loss: 0.1152716\n",
      "\tspeed: 0.0142s/iter; left time: 375.6402s\n",
      "\titers: 900, epoch: 5 | loss: 0.0887059\n",
      "\tspeed: 0.0141s/iter; left time: 373.8611s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1392863\n",
      "\tspeed: 0.0141s/iter; left time: 372.5468s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0510879\n",
      "\tspeed: 0.0142s/iter; left time: 371.2721s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0901645\n",
      "\tspeed: 0.0141s/iter; left time: 368.7327s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0892580\n",
      "\tspeed: 0.0141s/iter; left time: 367.3141s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1008420\n",
      "\tspeed: 0.0141s/iter; left time: 365.8072s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1002330\n",
      "\tspeed: 0.0141s/iter; left time: 364.1826s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0562704\n",
      "\tspeed: 0.0141s/iter; left time: 362.7995s\n",
      "\titers: 1700, epoch: 5 | loss: 0.2267590\n",
      "\tspeed: 0.0141s/iter; left time: 361.5010s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0861740\n",
      "\tspeed: 0.0141s/iter; left time: 360.0673s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0315677\n",
      "\tspeed: 0.0141s/iter; left time: 358.5294s\n",
      "\titers: 2000, epoch: 5 | loss: 0.6385766\n",
      "\tspeed: 0.0141s/iter; left time: 357.1090s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0938682\n",
      "\tspeed: 0.0141s/iter; left time: 355.5722s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0757801\n",
      "\tspeed: 0.0141s/iter; left time: 354.1463s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0472356\n",
      "\tspeed: 0.0141s/iter; left time: 352.6664s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0841407\n",
      "\tspeed: 0.0141s/iter; left time: 351.2280s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0699744\n",
      "\tspeed: 0.0141s/iter; left time: 350.0789s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0792382\n",
      "\tspeed: 0.0141s/iter; left time: 348.6561s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1085688\n",
      "\tspeed: 0.0141s/iter; left time: 347.0125s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0712323\n",
      "\tspeed: 0.0141s/iter; left time: 345.6881s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0493310\n",
      "\tspeed: 0.0141s/iter; left time: 344.2285s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0688816\n",
      "\tspeed: 0.0141s/iter; left time: 342.8415s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0443238\n",
      "\tspeed: 0.0141s/iter; left time: 341.4638s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0271417\n",
      "\tspeed: 0.0141s/iter; left time: 340.0500s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0704094\n",
      "\tspeed: 0.0141s/iter; left time: 338.7366s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0721979\n",
      "\tspeed: 0.0141s/iter; left time: 337.2204s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0464406\n",
      "\tspeed: 0.0141s/iter; left time: 335.7039s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0807444\n",
      "\tspeed: 0.0141s/iter; left time: 334.4230s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0606233\n",
      "\tspeed: 0.0141s/iter; left time: 332.9660s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0988854\n",
      "\tspeed: 0.0141s/iter; left time: 331.6315s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0497513\n",
      "\tspeed: 0.0141s/iter; left time: 330.0936s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0444475\n",
      "\tspeed: 0.0141s/iter; left time: 328.7634s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0360230\n",
      "\tspeed: 0.0141s/iter; left time: 327.2674s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0605384\n",
      "\tspeed: 0.0141s/iter; left time: 325.8961s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1196653\n",
      "\tspeed: 0.0141s/iter; left time: 324.5739s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0210007\n",
      "\tspeed: 0.0141s/iter; left time: 323.0789s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0578803\n",
      "\tspeed: 0.0141s/iter; left time: 321.7980s\n",
      "Epoch: 5 cost time: 64.58784580230713\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1022209 Vali Loss: 0.0329238 Test Loss: 0.1117936\n",
      "Validation loss decreased (0.033216 --> 0.032924).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0450002\n",
      "\tspeed: 0.1414s/iter; left time: 3206.5174s\n",
      "\titers: 200, epoch: 6 | loss: 0.1656143\n",
      "\tspeed: 0.0141s/iter; left time: 318.0585s\n",
      "\titers: 300, epoch: 6 | loss: 0.1255506\n",
      "\tspeed: 0.0141s/iter; left time: 316.5317s\n",
      "\titers: 400, epoch: 6 | loss: 0.0901936\n",
      "\tspeed: 0.0141s/iter; left time: 315.2312s\n",
      "\titers: 500, epoch: 6 | loss: 0.1622164\n",
      "\tspeed: 0.0141s/iter; left time: 313.8549s\n",
      "\titers: 600, epoch: 6 | loss: 0.1800657\n",
      "\tspeed: 0.0141s/iter; left time: 312.4504s\n",
      "\titers: 700, epoch: 6 | loss: 0.0323191\n",
      "\tspeed: 0.0155s/iter; left time: 342.2608s\n",
      "\titers: 800, epoch: 6 | loss: 0.0680526\n",
      "\tspeed: 0.0155s/iter; left time: 341.0714s\n",
      "\titers: 900, epoch: 6 | loss: 0.0559860\n",
      "\tspeed: 0.0142s/iter; left time: 309.9763s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1562211\n",
      "\tspeed: 0.0142s/iter; left time: 308.7781s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0526574\n",
      "\tspeed: 0.0142s/iter; left time: 307.1531s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1110927\n",
      "\tspeed: 0.0145s/iter; left time: 311.9992s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1017463\n",
      "\tspeed: 0.0156s/iter; left time: 335.7491s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0743956\n",
      "\tspeed: 0.0157s/iter; left time: 334.9236s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0771593\n",
      "\tspeed: 0.0153s/iter; left time: 325.2028s\n",
      "\titers: 1600, epoch: 6 | loss: 0.2069089\n",
      "\tspeed: 0.0142s/iter; left time: 299.9003s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0555285\n",
      "\tspeed: 0.0142s/iter; left time: 298.9396s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2228368\n",
      "\tspeed: 0.0142s/iter; left time: 297.3984s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1397611\n",
      "\tspeed: 0.0142s/iter; left time: 295.8218s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1799869\n",
      "\tspeed: 0.0142s/iter; left time: 294.1059s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1252721\n",
      "\tspeed: 0.0142s/iter; left time: 292.7152s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1276691\n",
      "\tspeed: 0.0142s/iter; left time: 291.3096s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1206822\n",
      "\tspeed: 0.0142s/iter; left time: 290.1716s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0605967\n",
      "\tspeed: 0.0142s/iter; left time: 288.6232s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0665944\n",
      "\tspeed: 0.0142s/iter; left time: 287.1172s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0597489\n",
      "\tspeed: 0.0142s/iter; left time: 285.6611s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0679836\n",
      "\tspeed: 0.0142s/iter; left time: 284.2615s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0936124\n",
      "\tspeed: 0.0150s/iter; left time: 299.0570s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2157032\n",
      "\tspeed: 0.0156s/iter; left time: 310.5715s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0618645\n",
      "\tspeed: 0.0156s/iter; left time: 309.0150s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0866964\n",
      "\tspeed: 0.0156s/iter; left time: 307.5187s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0981589\n",
      "\tspeed: 0.0156s/iter; left time: 306.0466s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0786688\n",
      "\tspeed: 0.0156s/iter; left time: 304.4213s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0262990\n",
      "\tspeed: 0.0156s/iter; left time: 302.8345s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0154320\n",
      "\tspeed: 0.0156s/iter; left time: 301.3128s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1149451\n",
      "\tspeed: 0.0156s/iter; left time: 299.7820s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1622312\n",
      "\tspeed: 0.0156s/iter; left time: 298.2748s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1367102\n",
      "\tspeed: 0.0156s/iter; left time: 296.5084s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0466978\n",
      "\tspeed: 0.0156s/iter; left time: 294.8751s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1673133\n",
      "\tspeed: 0.0156s/iter; left time: 293.4621s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0167590\n",
      "\tspeed: 0.0156s/iter; left time: 291.9341s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0377559\n",
      "\tspeed: 0.0156s/iter; left time: 290.3710s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1641956\n",
      "\tspeed: 0.0156s/iter; left time: 288.7971s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0710560\n",
      "\tspeed: 0.0156s/iter; left time: 287.2298s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0925189\n",
      "\tspeed: 0.0156s/iter; left time: 285.6340s\n",
      "Epoch: 6 cost time: 68.15160250663757\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0976002 Vali Loss: 0.0325405 Test Loss: 0.1141897\n",
      "Validation loss decreased (0.032924 --> 0.032541).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1231600\n",
      "\tspeed: 0.1362s/iter; left time: 2468.9847s\n",
      "\titers: 200, epoch: 7 | loss: 0.2148099\n",
      "\tspeed: 0.0142s/iter; left time: 256.2044s\n",
      "\titers: 300, epoch: 7 | loss: 0.2253210\n",
      "\tspeed: 0.0142s/iter; left time: 253.9202s\n",
      "\titers: 400, epoch: 7 | loss: 0.0709067\n",
      "\tspeed: 0.0142s/iter; left time: 252.9877s\n",
      "\titers: 500, epoch: 7 | loss: 0.0464612\n",
      "\tspeed: 0.0142s/iter; left time: 251.3489s\n",
      "\titers: 600, epoch: 7 | loss: 0.0750231\n",
      "\tspeed: 0.0142s/iter; left time: 249.6842s\n",
      "\titers: 700, epoch: 7 | loss: 0.0134326\n",
      "\tspeed: 0.0142s/iter; left time: 248.6421s\n",
      "\titers: 800, epoch: 7 | loss: 0.0560909\n",
      "\tspeed: 0.0142s/iter; left time: 246.8705s\n",
      "\titers: 900, epoch: 7 | loss: 0.1757647\n",
      "\tspeed: 0.0142s/iter; left time: 245.5031s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0409092\n",
      "\tspeed: 0.0142s/iter; left time: 244.0046s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2195573\n",
      "\tspeed: 0.0142s/iter; left time: 242.7241s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0549210\n",
      "\tspeed: 0.0142s/iter; left time: 241.4118s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0869845\n",
      "\tspeed: 0.0142s/iter; left time: 240.1257s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0948825\n",
      "\tspeed: 0.0142s/iter; left time: 238.3832s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0828277\n",
      "\tspeed: 0.0142s/iter; left time: 236.8775s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0600317\n",
      "\tspeed: 0.0142s/iter; left time: 235.4473s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0466159\n",
      "\tspeed: 0.0142s/iter; left time: 234.6141s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0909457\n",
      "\tspeed: 0.0142s/iter; left time: 233.3860s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0338065\n",
      "\tspeed: 0.0142s/iter; left time: 231.8047s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0433758\n",
      "\tspeed: 0.0142s/iter; left time: 230.3421s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0520106\n",
      "\tspeed: 0.0142s/iter; left time: 229.0841s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0500247\n",
      "\tspeed: 0.0142s/iter; left time: 227.5849s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0494973\n",
      "\tspeed: 0.0142s/iter; left time: 226.1690s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0766317\n",
      "\tspeed: 0.0142s/iter; left time: 224.8048s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1893050\n",
      "\tspeed: 0.0142s/iter; left time: 223.3854s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1082218\n",
      "\tspeed: 0.0142s/iter; left time: 221.9359s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0427964\n",
      "\tspeed: 0.0142s/iter; left time: 220.3700s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0476387\n",
      "\tspeed: 0.0142s/iter; left time: 218.9113s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0976878\n",
      "\tspeed: 0.0142s/iter; left time: 217.4675s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0653234\n",
      "\tspeed: 0.0142s/iter; left time: 216.1019s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0167749\n",
      "\tspeed: 0.0142s/iter; left time: 214.6079s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1451631\n",
      "\tspeed: 0.0142s/iter; left time: 213.2880s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0716250\n",
      "\tspeed: 0.0142s/iter; left time: 211.8137s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0365382\n",
      "\tspeed: 0.0142s/iter; left time: 210.4570s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0504188\n",
      "\tspeed: 0.0142s/iter; left time: 208.9973s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0595517\n",
      "\tspeed: 0.0142s/iter; left time: 207.5161s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1771957\n",
      "\tspeed: 0.0142s/iter; left time: 206.1489s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0391880\n",
      "\tspeed: 0.0142s/iter; left time: 204.7508s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0750370\n",
      "\tspeed: 0.0142s/iter; left time: 203.2978s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0633304\n",
      "\tspeed: 0.0142s/iter; left time: 201.8767s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0733753\n",
      "\tspeed: 0.0142s/iter; left time: 200.4960s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0280967\n",
      "\tspeed: 0.0142s/iter; left time: 199.0136s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0214055\n",
      "\tspeed: 0.0153s/iter; left time: 213.4292s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0241183\n",
      "\tspeed: 0.0157s/iter; left time: 216.4280s\n",
      "\titers: 4500, epoch: 7 | loss: 0.1046705\n",
      "\tspeed: 0.0157s/iter; left time: 214.8975s\n",
      "Epoch: 7 cost time: 65.45573163032532\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0943563 Vali Loss: 0.0324630 Test Loss: 0.1145989\n",
      "Validation loss decreased (0.032541 --> 0.032463).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0848830\n",
      "\tspeed: 0.1433s/iter; left time: 1944.1465s\n",
      "\titers: 200, epoch: 8 | loss: 0.0747153\n",
      "\tspeed: 0.0142s/iter; left time: 190.9061s\n",
      "\titers: 300, epoch: 8 | loss: 0.1423734\n",
      "\tspeed: 0.0142s/iter; left time: 189.1311s\n",
      "\titers: 400, epoch: 8 | loss: 0.0448341\n",
      "\tspeed: 0.0142s/iter; left time: 188.8006s\n",
      "\titers: 500, epoch: 8 | loss: 0.1960470\n",
      "\tspeed: 0.0142s/iter; left time: 186.8802s\n",
      "\titers: 600, epoch: 8 | loss: 0.0745049\n",
      "\tspeed: 0.0142s/iter; left time: 185.4843s\n",
      "\titers: 700, epoch: 8 | loss: 0.1958699\n",
      "\tspeed: 0.0142s/iter; left time: 184.1407s\n",
      "\titers: 800, epoch: 8 | loss: 0.1754576\n",
      "\tspeed: 0.0142s/iter; left time: 182.5248s\n",
      "\titers: 900, epoch: 8 | loss: 0.0731587\n",
      "\tspeed: 0.0142s/iter; left time: 181.0931s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0623545\n",
      "\tspeed: 0.0142s/iter; left time: 179.7253s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0859997\n",
      "\tspeed: 0.0142s/iter; left time: 178.2358s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1315812\n",
      "\tspeed: 0.0142s/iter; left time: 176.8092s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3710501\n",
      "\tspeed: 0.0142s/iter; left time: 175.4491s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0372819\n",
      "\tspeed: 0.0142s/iter; left time: 173.9662s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0877636\n",
      "\tspeed: 0.0142s/iter; left time: 172.5432s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1333730\n",
      "\tspeed: 0.0142s/iter; left time: 171.1154s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0269792\n",
      "\tspeed: 0.0142s/iter; left time: 169.7792s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1270098\n",
      "\tspeed: 0.0142s/iter; left time: 168.3074s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1052443\n",
      "\tspeed: 0.0142s/iter; left time: 166.9098s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1125871\n",
      "\tspeed: 0.0142s/iter; left time: 165.4737s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0627683\n",
      "\tspeed: 0.0142s/iter; left time: 164.1043s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0578786\n",
      "\tspeed: 0.0142s/iter; left time: 162.5903s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1181847\n",
      "\tspeed: 0.0142s/iter; left time: 161.1772s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0324623\n",
      "\tspeed: 0.0142s/iter; left time: 159.6892s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0823186\n",
      "\tspeed: 0.0142s/iter; left time: 158.3475s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0565674\n",
      "\tspeed: 0.0142s/iter; left time: 156.8645s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0378775\n",
      "\tspeed: 0.0142s/iter; left time: 155.4567s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0499634\n",
      "\tspeed: 0.0142s/iter; left time: 154.0234s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0747446\n",
      "\tspeed: 0.0142s/iter; left time: 152.6581s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2936129\n",
      "\tspeed: 0.0142s/iter; left time: 151.2101s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1718150\n",
      "\tspeed: 0.0142s/iter; left time: 149.8039s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1334103\n",
      "\tspeed: 0.0142s/iter; left time: 148.3810s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0416132\n",
      "\tspeed: 0.0142s/iter; left time: 146.9647s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0750119\n",
      "\tspeed: 0.0142s/iter; left time: 145.5570s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1724584\n",
      "\tspeed: 0.0142s/iter; left time: 144.1460s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0334292\n",
      "\tspeed: 0.0142s/iter; left time: 142.4466s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2519407\n",
      "\tspeed: 0.0141s/iter; left time: 140.8786s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0665036\n",
      "\tspeed: 0.0141s/iter; left time: 139.4429s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0873349\n",
      "\tspeed: 0.0141s/iter; left time: 138.0436s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0500058\n",
      "\tspeed: 0.0141s/iter; left time: 136.5996s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0285778\n",
      "\tspeed: 0.0141s/iter; left time: 135.1458s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0990806\n",
      "\tspeed: 0.0141s/iter; left time: 133.7304s\n",
      "\titers: 4300, epoch: 8 | loss: 0.1072617\n",
      "\tspeed: 0.0141s/iter; left time: 132.3538s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1298561\n",
      "\tspeed: 0.0141s/iter; left time: 130.9177s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1279658\n",
      "\tspeed: 0.0141s/iter; left time: 129.5240s\n",
      "Epoch: 8 cost time: 64.8492362499237\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0942525 Vali Loss: 0.0323278 Test Loss: 0.1149161\n",
      "Validation loss decreased (0.032463 --> 0.032328).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0558414\n",
      "\tspeed: 0.1283s/iter; left time: 1155.7279s\n",
      "\titers: 200, epoch: 9 | loss: 0.1115082\n",
      "\tspeed: 0.0143s/iter; left time: 127.1159s\n",
      "\titers: 300, epoch: 9 | loss: 0.0498636\n",
      "\tspeed: 0.0142s/iter; left time: 125.2337s\n",
      "\titers: 400, epoch: 9 | loss: 0.1202771\n",
      "\tspeed: 0.0142s/iter; left time: 124.1127s\n",
      "\titers: 500, epoch: 9 | loss: 0.0728375\n",
      "\tspeed: 0.0142s/iter; left time: 122.5701s\n",
      "\titers: 600, epoch: 9 | loss: 0.0285222\n",
      "\tspeed: 0.0142s/iter; left time: 121.2151s\n",
      "\titers: 700, epoch: 9 | loss: 0.1468434\n",
      "\tspeed: 0.0142s/iter; left time: 119.6782s\n",
      "\titers: 800, epoch: 9 | loss: 0.0328571\n",
      "\tspeed: 0.0142s/iter; left time: 118.2601s\n",
      "\titers: 900, epoch: 9 | loss: 0.0442758\n",
      "\tspeed: 0.0142s/iter; left time: 116.8929s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0867593\n",
      "\tspeed: 0.0143s/iter; left time: 115.6031s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0509694\n",
      "\tspeed: 0.0143s/iter; left time: 114.1577s\n",
      "\titers: 1200, epoch: 9 | loss: 0.2063788\n",
      "\tspeed: 0.0142s/iter; left time: 112.5672s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0343674\n",
      "\tspeed: 0.0142s/iter; left time: 111.1529s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2436148\n",
      "\tspeed: 0.0142s/iter; left time: 109.7142s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1175912\n",
      "\tspeed: 0.0142s/iter; left time: 108.2695s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0440759\n",
      "\tspeed: 0.0142s/iter; left time: 106.9841s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0942924\n",
      "\tspeed: 0.0142s/iter; left time: 105.4594s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0417999\n",
      "\tspeed: 0.0142s/iter; left time: 104.0607s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0244172\n",
      "\tspeed: 0.0142s/iter; left time: 102.6109s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0554739\n",
      "\tspeed: 0.0142s/iter; left time: 101.2196s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0480510\n",
      "\tspeed: 0.0142s/iter; left time: 99.8196s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1111191\n",
      "\tspeed: 0.0142s/iter; left time: 98.4011s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1197103\n",
      "\tspeed: 0.0142s/iter; left time: 96.9218s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0301762\n",
      "\tspeed: 0.0142s/iter; left time: 95.4721s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0380156\n",
      "\tspeed: 0.0142s/iter; left time: 94.0607s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0372587\n",
      "\tspeed: 0.0142s/iter; left time: 92.6179s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0458427\n",
      "\tspeed: 0.0142s/iter; left time: 91.1898s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1336535\n",
      "\tspeed: 0.0142s/iter; left time: 89.7834s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0538796\n",
      "\tspeed: 0.0142s/iter; left time: 88.3521s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1160163\n",
      "\tspeed: 0.0142s/iter; left time: 86.9387s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0348445\n",
      "\tspeed: 0.0142s/iter; left time: 85.5063s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0721262\n",
      "\tspeed: 0.0142s/iter; left time: 84.0848s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2242307\n",
      "\tspeed: 0.0142s/iter; left time: 82.6594s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1041888\n",
      "\tspeed: 0.0142s/iter; left time: 81.2499s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0874932\n",
      "\tspeed: 0.0142s/iter; left time: 79.8211s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0480979\n",
      "\tspeed: 0.0142s/iter; left time: 78.4414s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0998791\n",
      "\tspeed: 0.0142s/iter; left time: 76.9741s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1642730\n",
      "\tspeed: 0.0142s/iter; left time: 75.5397s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0941878\n",
      "\tspeed: 0.0142s/iter; left time: 74.1050s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0933366\n",
      "\tspeed: 0.0142s/iter; left time: 72.6675s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0576156\n",
      "\tspeed: 0.0142s/iter; left time: 71.2741s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0647083\n",
      "\tspeed: 0.0142s/iter; left time: 69.9101s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0746929\n",
      "\tspeed: 0.0142s/iter; left time: 68.4304s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0418253\n",
      "\tspeed: 0.0142s/iter; left time: 66.9972s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0262634\n",
      "\tspeed: 0.0142s/iter; left time: 65.5647s\n",
      "Epoch: 9 cost time: 65.12616753578186\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0924521 Vali Loss: 0.0324080 Test Loss: 0.1145802\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0571351\n",
      "\tspeed: 0.1360s/iter; left time: 605.9326s\n",
      "\titers: 200, epoch: 10 | loss: 0.0641491\n",
      "\tspeed: 0.0157s/iter; left time: 68.3140s\n",
      "\titers: 300, epoch: 10 | loss: 0.0434622\n",
      "\tspeed: 0.0157s/iter; left time: 66.9367s\n",
      "\titers: 400, epoch: 10 | loss: 0.0506059\n",
      "\tspeed: 0.0157s/iter; left time: 65.1836s\n",
      "\titers: 500, epoch: 10 | loss: 0.0949381\n",
      "\tspeed: 0.0157s/iter; left time: 63.6549s\n",
      "\titers: 600, epoch: 10 | loss: 0.0799008\n",
      "\tspeed: 0.0157s/iter; left time: 62.0552s\n",
      "\titers: 700, epoch: 10 | loss: 0.0466813\n",
      "\tspeed: 0.0157s/iter; left time: 60.4935s\n",
      "\titers: 800, epoch: 10 | loss: 0.1733008\n",
      "\tspeed: 0.0157s/iter; left time: 58.9771s\n",
      "\titers: 900, epoch: 10 | loss: 0.1050396\n",
      "\tspeed: 0.0157s/iter; left time: 57.3985s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0793241\n",
      "\tspeed: 0.0157s/iter; left time: 55.7494s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0895304\n",
      "\tspeed: 0.0157s/iter; left time: 54.1762s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0637012\n",
      "\tspeed: 0.0157s/iter; left time: 52.6267s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0465128\n",
      "\tspeed: 0.0157s/iter; left time: 51.0730s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0738366\n",
      "\tspeed: 0.0157s/iter; left time: 49.5054s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2594577\n",
      "\tspeed: 0.0157s/iter; left time: 47.9018s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0961117\n",
      "\tspeed: 0.0157s/iter; left time: 46.3291s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0226820\n",
      "\tspeed: 0.0157s/iter; left time: 44.8197s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0990813\n",
      "\tspeed: 0.0157s/iter; left time: 43.2303s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1480603\n",
      "\tspeed: 0.0152s/iter; left time: 40.3168s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0763314\n",
      "\tspeed: 0.0142s/iter; left time: 36.3118s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0462448\n",
      "\tspeed: 0.0155s/iter; left time: 38.0198s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0668825\n",
      "\tspeed: 0.0157s/iter; left time: 36.9237s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1318017\n",
      "\tspeed: 0.0157s/iter; left time: 35.3620s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0600385\n",
      "\tspeed: 0.0157s/iter; left time: 33.7978s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2491900\n",
      "\tspeed: 0.0157s/iter; left time: 32.2287s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1195425\n",
      "\tspeed: 0.0157s/iter; left time: 30.6564s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0715698\n",
      "\tspeed: 0.0157s/iter; left time: 29.0824s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0682379\n",
      "\tspeed: 0.0157s/iter; left time: 27.5119s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0618594\n",
      "\tspeed: 0.0152s/iter; left time: 25.2069s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0510329\n",
      "\tspeed: 0.0142s/iter; left time: 22.1106s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0657394\n",
      "\tspeed: 0.0142s/iter; left time: 20.6853s\n",
      "\titers: 3200, epoch: 10 | loss: 0.1013830\n",
      "\tspeed: 0.0142s/iter; left time: 19.2704s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0883707\n",
      "\tspeed: 0.0148s/iter; left time: 18.5961s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1538824\n",
      "\tspeed: 0.0157s/iter; left time: 18.1219s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0263022\n",
      "\tspeed: 0.0157s/iter; left time: 16.5507s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1661120\n",
      "\tspeed: 0.0157s/iter; left time: 14.9835s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0855708\n",
      "\tspeed: 0.0157s/iter; left time: 13.4195s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0612805\n",
      "\tspeed: 0.0157s/iter; left time: 11.8464s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0270694\n",
      "\tspeed: 0.0157s/iter; left time: 10.2814s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0293280\n",
      "\tspeed: 0.0157s/iter; left time: 8.7156s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1422729\n",
      "\tspeed: 0.0157s/iter; left time: 7.1482s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0434198\n",
      "\tspeed: 0.0157s/iter; left time: 5.5736s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0309585\n",
      "\tspeed: 0.0157s/iter; left time: 4.0138s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0500935\n",
      "\tspeed: 0.0157s/iter; left time: 2.4451s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1392013\n",
      "\tspeed: 0.0157s/iter; left time: 0.8775s\n",
      "Epoch: 10 cost time: 70.92639064788818\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0928382 Vali Loss: 0.0324397 Test Loss: 0.1143354\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11495085060596466, mae:0.2000247836112976\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features S --predictor wind_forecast,total_load --enc_in 1 --dec_in 1 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1eaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1761947\n",
      "\tspeed: 0.0256s/iter; left time: 1162.0689s\n",
      "\titers: 200, epoch: 1 | loss: 0.0938281\n",
      "\tspeed: 0.0142s/iter; left time: 643.8166s\n",
      "\titers: 300, epoch: 1 | loss: 0.0687024\n",
      "\tspeed: 0.0142s/iter; left time: 641.1715s\n",
      "\titers: 400, epoch: 1 | loss: 0.1241657\n",
      "\tspeed: 0.0142s/iter; left time: 640.3368s\n",
      "\titers: 500, epoch: 1 | loss: 0.0327702\n",
      "\tspeed: 0.0142s/iter; left time: 638.2935s\n",
      "\titers: 600, epoch: 1 | loss: 0.0715009\n",
      "\tspeed: 0.0142s/iter; left time: 636.8428s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451532\n",
      "\tspeed: 0.0142s/iter; left time: 635.6327s\n",
      "\titers: 800, epoch: 1 | loss: 0.5053760\n",
      "\tspeed: 0.0142s/iter; left time: 634.5785s\n",
      "\titers: 900, epoch: 1 | loss: 0.2560097\n",
      "\tspeed: 0.0142s/iter; left time: 632.5826s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0734115\n",
      "\tspeed: 0.0142s/iter; left time: 630.9153s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0425444\n",
      "\tspeed: 0.0142s/iter; left time: 629.9946s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0585851\n",
      "\tspeed: 0.0148s/iter; left time: 656.1178s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2605752\n",
      "\tspeed: 0.0150s/iter; left time: 661.5576s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0309507\n",
      "\tspeed: 0.0142s/iter; left time: 625.9133s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1115317\n",
      "\tspeed: 0.0157s/iter; left time: 693.4967s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1843507\n",
      "\tspeed: 0.0157s/iter; left time: 690.3573s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3890318\n",
      "\tspeed: 0.0148s/iter; left time: 650.5447s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0720876\n",
      "\tspeed: 0.0143s/iter; left time: 624.9742s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1775747\n",
      "\tspeed: 0.0143s/iter; left time: 626.0445s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3624939\n",
      "\tspeed: 0.0142s/iter; left time: 616.6505s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1508106\n",
      "\tspeed: 0.0141s/iter; left time: 612.8255s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3487170\n",
      "\tspeed: 0.0141s/iter; left time: 611.2245s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1623183\n",
      "\tspeed: 0.0142s/iter; left time: 612.0613s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1627411\n",
      "\tspeed: 0.0142s/iter; left time: 614.4803s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2751411\n",
      "\tspeed: 0.0141s/iter; left time: 609.1468s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1169626\n",
      "\tspeed: 0.0141s/iter; left time: 607.5154s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2052497\n",
      "\tspeed: 0.0141s/iter; left time: 605.5739s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2055999\n",
      "\tspeed: 0.0141s/iter; left time: 603.2474s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2338544\n",
      "\tspeed: 0.0141s/iter; left time: 601.1792s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0882175\n",
      "\tspeed: 0.0161s/iter; left time: 683.2453s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0823141\n",
      "\tspeed: 0.0159s/iter; left time: 674.6061s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1962520\n",
      "\tspeed: 0.0141s/iter; left time: 598.5589s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0351911\n",
      "\tspeed: 0.0142s/iter; left time: 600.2780s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4122079\n",
      "\tspeed: 0.0142s/iter; left time: 597.3607s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0856364\n",
      "\tspeed: 0.0141s/iter; left time: 593.7274s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1057309\n",
      "\tspeed: 0.0142s/iter; left time: 596.7196s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0931639\n",
      "\tspeed: 0.0143s/iter; left time: 596.7161s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1785975\n",
      "\tspeed: 0.0156s/iter; left time: 650.5569s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3997110\n",
      "\tspeed: 0.0157s/iter; left time: 654.0041s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2769776\n",
      "\tspeed: 0.0157s/iter; left time: 652.2814s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0955917\n",
      "\tspeed: 0.0156s/iter; left time: 645.7972s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0872184\n",
      "\tspeed: 0.0153s/iter; left time: 632.0114s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1506448\n",
      "\tspeed: 0.0142s/iter; left time: 586.0194s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0404093\n",
      "\tspeed: 0.0142s/iter; left time: 583.3557s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0716922\n",
      "\tspeed: 0.0142s/iter; left time: 582.0669s\n",
      "Epoch: 1 cost time: 67.32950353622437\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1763682 Vali Loss: 0.0384911 Test Loss: 0.1245387\n",
      "Validation loss decreased (inf --> 0.038491).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0965484\n",
      "\tspeed: 0.1292s/iter; left time: 5285.6099s\n",
      "\titers: 200, epoch: 2 | loss: 0.0394090\n",
      "\tspeed: 0.0143s/iter; left time: 585.0050s\n",
      "\titers: 300, epoch: 2 | loss: 0.0977589\n",
      "\tspeed: 0.0143s/iter; left time: 582.2816s\n",
      "\titers: 400, epoch: 2 | loss: 0.1882427\n",
      "\tspeed: 0.0143s/iter; left time: 579.8757s\n",
      "\titers: 500, epoch: 2 | loss: 0.3373528\n",
      "\tspeed: 0.0143s/iter; left time: 577.6851s\n",
      "\titers: 600, epoch: 2 | loss: 0.1316223\n",
      "\tspeed: 0.0142s/iter; left time: 573.2442s\n",
      "\titers: 700, epoch: 2 | loss: 0.0733484\n",
      "\tspeed: 0.0142s/iter; left time: 572.7049s\n",
      "\titers: 800, epoch: 2 | loss: 0.0557397\n",
      "\tspeed: 0.0143s/iter; left time: 573.2632s\n",
      "\titers: 900, epoch: 2 | loss: 0.0551844\n",
      "\tspeed: 0.0143s/iter; left time: 572.0480s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1973986\n",
      "\tspeed: 0.0143s/iter; left time: 570.4861s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2021171\n",
      "\tspeed: 0.0143s/iter; left time: 569.6521s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1249596\n",
      "\tspeed: 0.0143s/iter; left time: 567.4027s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1480998\n",
      "\tspeed: 0.0143s/iter; left time: 565.8745s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1150100\n",
      "\tspeed: 0.0143s/iter; left time: 564.5012s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0475104\n",
      "\tspeed: 0.0142s/iter; left time: 562.8078s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1711187\n",
      "\tspeed: 0.0143s/iter; left time: 561.6603s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2710866\n",
      "\tspeed: 0.0143s/iter; left time: 560.1830s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1182006\n",
      "\tspeed: 0.0142s/iter; left time: 557.3887s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1678239\n",
      "\tspeed: 0.0142s/iter; left time: 556.3658s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0585374\n",
      "\tspeed: 0.0143s/iter; left time: 555.8119s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0376854\n",
      "\tspeed: 0.0143s/iter; left time: 554.3088s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1177671\n",
      "\tspeed: 0.0142s/iter; left time: 552.5227s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1420685\n",
      "\tspeed: 0.0142s/iter; left time: 550.9481s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1342927\n",
      "\tspeed: 0.0143s/iter; left time: 550.2488s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0575067\n",
      "\tspeed: 0.0142s/iter; left time: 548.3594s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2137174\n",
      "\tspeed: 0.0142s/iter; left time: 546.7460s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1351328\n",
      "\tspeed: 0.0142s/iter; left time: 545.3455s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0589181\n",
      "\tspeed: 0.0142s/iter; left time: 544.1335s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0820708\n",
      "\tspeed: 0.0142s/iter; left time: 542.8644s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1777464\n",
      "\tspeed: 0.0142s/iter; left time: 541.0968s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0429570\n",
      "\tspeed: 0.0142s/iter; left time: 539.6930s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1527276\n",
      "\tspeed: 0.0142s/iter; left time: 538.1707s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0607781\n",
      "\tspeed: 0.0143s/iter; left time: 538.4450s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0930832\n",
      "\tspeed: 0.0162s/iter; left time: 609.1355s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1257465\n",
      "\tspeed: 0.0141s/iter; left time: 529.9187s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1400915\n",
      "\tspeed: 0.0145s/iter; left time: 543.1991s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1529161\n",
      "\tspeed: 0.0163s/iter; left time: 607.0187s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1372309\n",
      "\tspeed: 0.0162s/iter; left time: 602.8622s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1924893\n",
      "\tspeed: 0.0152s/iter; left time: 565.6031s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2247270\n",
      "\tspeed: 0.0145s/iter; left time: 537.4256s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1171593\n",
      "\tspeed: 0.0142s/iter; left time: 524.1565s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0934797\n",
      "\tspeed: 0.0149s/iter; left time: 547.4064s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1451997\n",
      "\tspeed: 0.0157s/iter; left time: 576.8377s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1581813\n",
      "\tspeed: 0.0157s/iter; left time: 575.3406s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0497790\n",
      "\tspeed: 0.0157s/iter; left time: 573.9435s\n",
      "Epoch: 2 cost time: 66.47970581054688\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1446429 Vali Loss: 0.0337670 Test Loss: 0.1147559\n",
      "Validation loss decreased (0.038491 --> 0.033767).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0888895\n",
      "\tspeed: 0.1270s/iter; left time: 4616.2494s\n",
      "\titers: 200, epoch: 3 | loss: 0.0433673\n",
      "\tspeed: 0.0144s/iter; left time: 523.3660s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213155\n",
      "\tspeed: 0.0156s/iter; left time: 562.5897s\n",
      "\titers: 400, epoch: 3 | loss: 0.0755301\n",
      "\tspeed: 0.0142s/iter; left time: 512.8349s\n",
      "\titers: 500, epoch: 3 | loss: 0.0728352\n",
      "\tspeed: 0.0142s/iter; left time: 510.4029s\n",
      "\titers: 600, epoch: 3 | loss: 0.0727856\n",
      "\tspeed: 0.0142s/iter; left time: 509.7848s\n",
      "\titers: 700, epoch: 3 | loss: 0.3633784\n",
      "\tspeed: 0.0142s/iter; left time: 508.4349s\n",
      "\titers: 800, epoch: 3 | loss: 0.0919219\n",
      "\tspeed: 0.0144s/iter; left time: 511.6780s\n",
      "\titers: 900, epoch: 3 | loss: 0.0787504\n",
      "\tspeed: 0.0156s/iter; left time: 554.8147s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1017697\n",
      "\tspeed: 0.0158s/iter; left time: 558.7800s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1278257\n",
      "\tspeed: 0.0159s/iter; left time: 562.1968s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1174518\n",
      "\tspeed: 0.0163s/iter; left time: 573.5874s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2299570\n",
      "\tspeed: 0.0163s/iter; left time: 571.8067s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1301254\n",
      "\tspeed: 0.0163s/iter; left time: 570.0170s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1960834\n",
      "\tspeed: 0.0163s/iter; left time: 568.3707s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1536359\n",
      "\tspeed: 0.0163s/iter; left time: 566.7725s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5119310\n",
      "\tspeed: 0.0163s/iter; left time: 565.2197s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0803905\n",
      "\tspeed: 0.0163s/iter; left time: 563.6489s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1686091\n",
      "\tspeed: 0.0163s/iter; left time: 561.8714s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2706951\n",
      "\tspeed: 0.0163s/iter; left time: 561.0518s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0878483\n",
      "\tspeed: 0.0163s/iter; left time: 560.0775s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0488082\n",
      "\tspeed: 0.0163s/iter; left time: 558.4430s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1471822\n",
      "\tspeed: 0.0163s/iter; left time: 557.0622s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1000195\n",
      "\tspeed: 0.0163s/iter; left time: 555.5066s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1408294\n",
      "\tspeed: 0.0163s/iter; left time: 553.9156s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0522313\n",
      "\tspeed: 0.0163s/iter; left time: 550.6604s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0433277\n",
      "\tspeed: 0.0163s/iter; left time: 549.1276s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1223619\n",
      "\tspeed: 0.0153s/iter; left time: 515.3344s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0673580\n",
      "\tspeed: 0.0142s/iter; left time: 475.2681s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0574459\n",
      "\tspeed: 0.0141s/iter; left time: 472.8889s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1538857\n",
      "\tspeed: 0.0141s/iter; left time: 470.8254s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2254446\n",
      "\tspeed: 0.0141s/iter; left time: 469.3651s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0779002\n",
      "\tspeed: 0.0141s/iter; left time: 467.8396s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1575561\n",
      "\tspeed: 0.0141s/iter; left time: 466.4985s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0331902\n",
      "\tspeed: 0.0141s/iter; left time: 465.0400s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1007734\n",
      "\tspeed: 0.0141s/iter; left time: 463.2009s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1122994\n",
      "\tspeed: 0.0141s/iter; left time: 462.0191s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0607264\n",
      "\tspeed: 0.0141s/iter; left time: 460.5798s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1815582\n",
      "\tspeed: 0.0141s/iter; left time: 459.1704s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1472391\n",
      "\tspeed: 0.0141s/iter; left time: 458.0430s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1834221\n",
      "\tspeed: 0.0141s/iter; left time: 456.7520s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0803711\n",
      "\tspeed: 0.0141s/iter; left time: 455.2594s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0216850\n",
      "\tspeed: 0.0141s/iter; left time: 453.7534s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0932727\n",
      "\tspeed: 0.0141s/iter; left time: 452.4117s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0153473\n",
      "\tspeed: 0.0141s/iter; left time: 451.0301s\n",
      "Epoch: 3 cost time: 68.89491748809814\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1149970 Vali Loss: 0.0354637 Test Loss: 0.1141755\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0631109\n",
      "\tspeed: 0.1265s/iter; left time: 4022.3554s\n",
      "\titers: 200, epoch: 4 | loss: 0.2875030\n",
      "\tspeed: 0.0142s/iter; left time: 448.7944s\n",
      "\titers: 300, epoch: 4 | loss: 0.0210192\n",
      "\tspeed: 0.0142s/iter; left time: 447.1613s\n",
      "\titers: 400, epoch: 4 | loss: 0.0991706\n",
      "\tspeed: 0.0141s/iter; left time: 445.4145s\n",
      "\titers: 500, epoch: 4 | loss: 0.0960686\n",
      "\tspeed: 0.0141s/iter; left time: 443.9204s\n",
      "\titers: 600, epoch: 4 | loss: 0.1770177\n",
      "\tspeed: 0.0142s/iter; left time: 442.8115s\n",
      "\titers: 700, epoch: 4 | loss: 0.1712226\n",
      "\tspeed: 0.0141s/iter; left time: 441.1347s\n",
      "\titers: 800, epoch: 4 | loss: 0.1566384\n",
      "\tspeed: 0.0141s/iter; left time: 439.6282s\n",
      "\titers: 900, epoch: 4 | loss: 0.1039117\n",
      "\tspeed: 0.0141s/iter; left time: 438.2863s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0762816\n",
      "\tspeed: 0.0142s/iter; left time: 437.0717s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1103869\n",
      "\tspeed: 0.0142s/iter; left time: 435.8444s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0567773\n",
      "\tspeed: 0.0141s/iter; left time: 433.9523s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0957227\n",
      "\tspeed: 0.0141s/iter; left time: 432.5535s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0460843\n",
      "\tspeed: 0.0141s/iter; left time: 431.0396s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1783445\n",
      "\tspeed: 0.0141s/iter; left time: 429.7365s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0993816\n",
      "\tspeed: 0.0141s/iter; left time: 428.0676s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1247748\n",
      "\tspeed: 0.0141s/iter; left time: 426.5564s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0204686\n",
      "\tspeed: 0.0141s/iter; left time: 425.3107s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0617034\n",
      "\tspeed: 0.0141s/iter; left time: 423.8594s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1042527\n",
      "\tspeed: 0.0141s/iter; left time: 422.4554s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0174318\n",
      "\tspeed: 0.0141s/iter; left time: 421.3887s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1664865\n",
      "\tspeed: 0.0141s/iter; left time: 419.8017s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1293821\n",
      "\tspeed: 0.0141s/iter; left time: 418.2996s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0772202\n",
      "\tspeed: 0.0141s/iter; left time: 417.1841s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0164892\n",
      "\tspeed: 0.0142s/iter; left time: 416.7696s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0531578\n",
      "\tspeed: 0.0142s/iter; left time: 415.4174s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5676972\n",
      "\tspeed: 0.0142s/iter; left time: 413.9577s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0195277\n",
      "\tspeed: 0.0142s/iter; left time: 412.6831s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0262614\n",
      "\tspeed: 0.0142s/iter; left time: 411.0744s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1373415\n",
      "\tspeed: 0.0142s/iter; left time: 409.6723s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0994673\n",
      "\tspeed: 0.0142s/iter; left time: 408.3816s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1655515\n",
      "\tspeed: 0.0142s/iter; left time: 406.8307s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0791031\n",
      "\tspeed: 0.0142s/iter; left time: 405.1236s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0591687\n",
      "\tspeed: 0.0141s/iter; left time: 402.5855s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0567421\n",
      "\tspeed: 0.0141s/iter; left time: 401.2201s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0513307\n",
      "\tspeed: 0.0141s/iter; left time: 399.6145s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1378802\n",
      "\tspeed: 0.0141s/iter; left time: 398.5791s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0390188\n",
      "\tspeed: 0.0141s/iter; left time: 397.2331s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0773032\n",
      "\tspeed: 0.0141s/iter; left time: 395.5051s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0881940\n",
      "\tspeed: 0.0141s/iter; left time: 394.2960s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0252138\n",
      "\tspeed: 0.0141s/iter; left time: 392.8124s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0641742\n",
      "\tspeed: 0.0141s/iter; left time: 391.3620s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2728181\n",
      "\tspeed: 0.0141s/iter; left time: 389.8443s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0952311\n",
      "\tspeed: 0.0141s/iter; left time: 388.4573s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0663833\n",
      "\tspeed: 0.0142s/iter; left time: 388.1132s\n",
      "Epoch: 4 cost time: 64.69224429130554\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0959688 Vali Loss: 0.0351006 Test Loss: 0.1077463\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0373502\n",
      "\tspeed: 0.1407s/iter; left time: 3831.9695s\n",
      "\titers: 200, epoch: 5 | loss: 0.0524443\n",
      "\tspeed: 0.0158s/iter; left time: 427.5161s\n",
      "\titers: 300, epoch: 5 | loss: 0.0598457\n",
      "\tspeed: 0.0157s/iter; left time: 424.6670s\n",
      "\titers: 400, epoch: 5 | loss: 0.0432743\n",
      "\tspeed: 0.0157s/iter; left time: 421.8290s\n",
      "\titers: 500, epoch: 5 | loss: 0.1162245\n",
      "\tspeed: 0.0157s/iter; left time: 420.4605s\n",
      "\titers: 600, epoch: 5 | loss: 0.0336827\n",
      "\tspeed: 0.0157s/iter; left time: 419.9361s\n",
      "\titers: 700, epoch: 5 | loss: 0.0636778\n",
      "\tspeed: 0.0157s/iter; left time: 418.2041s\n",
      "\titers: 800, epoch: 5 | loss: 0.0793169\n",
      "\tspeed: 0.0157s/iter; left time: 416.5066s\n",
      "\titers: 900, epoch: 5 | loss: 0.0391933\n",
      "\tspeed: 0.0157s/iter; left time: 414.9601s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1438223\n",
      "\tspeed: 0.0157s/iter; left time: 413.4733s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0300231\n",
      "\tspeed: 0.0142s/iter; left time: 372.5661s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1026424\n",
      "\tspeed: 0.0141s/iter; left time: 369.4630s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0712850\n",
      "\tspeed: 0.0141s/iter; left time: 367.3483s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1095237\n",
      "\tspeed: 0.0141s/iter; left time: 365.8869s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1186156\n",
      "\tspeed: 0.0141s/iter; left time: 364.4912s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0570681\n",
      "\tspeed: 0.0155s/iter; left time: 398.2514s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1760297\n",
      "\tspeed: 0.0157s/iter; left time: 402.0403s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0911143\n",
      "\tspeed: 0.0157s/iter; left time: 400.3801s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0277795\n",
      "\tspeed: 0.0157s/iter; left time: 399.2721s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5396386\n",
      "\tspeed: 0.0157s/iter; left time: 397.5687s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0865949\n",
      "\tspeed: 0.0157s/iter; left time: 395.7232s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0422525\n",
      "\tspeed: 0.0157s/iter; left time: 393.7696s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0285333\n",
      "\tspeed: 0.0157s/iter; left time: 392.2637s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0772292\n",
      "\tspeed: 0.0157s/iter; left time: 390.7644s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0591693\n",
      "\tspeed: 0.0157s/iter; left time: 389.3788s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0688789\n",
      "\tspeed: 0.0157s/iter; left time: 387.5636s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0948981\n",
      "\tspeed: 0.0157s/iter; left time: 386.3694s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0655452\n",
      "\tspeed: 0.0157s/iter; left time: 384.9095s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0620498\n",
      "\tspeed: 0.0157s/iter; left time: 383.3397s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0421777\n",
      "\tspeed: 0.0157s/iter; left time: 381.7933s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0343712\n",
      "\tspeed: 0.0157s/iter; left time: 380.1716s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0124530\n",
      "\tspeed: 0.0157s/iter; left time: 378.3527s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0510148\n",
      "\tspeed: 0.0157s/iter; left time: 376.6639s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0607961\n",
      "\tspeed: 0.0157s/iter; left time: 375.0306s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0542052\n",
      "\tspeed: 0.0157s/iter; left time: 373.3761s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0656159\n",
      "\tspeed: 0.0157s/iter; left time: 371.7413s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0490740\n",
      "\tspeed: 0.0157s/iter; left time: 370.3239s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1356292\n",
      "\tspeed: 0.0157s/iter; left time: 368.6090s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0560427\n",
      "\tspeed: 0.0157s/iter; left time: 367.1353s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0542096\n",
      "\tspeed: 0.0141s/iter; left time: 329.5648s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0280492\n",
      "\tspeed: 0.0141s/iter; left time: 326.4998s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0362821\n",
      "\tspeed: 0.0141s/iter; left time: 325.1979s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1139971\n",
      "\tspeed: 0.0141s/iter; left time: 323.7138s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0214874\n",
      "\tspeed: 0.0149s/iter; left time: 340.6554s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0509721\n",
      "\tspeed: 0.0157s/iter; left time: 357.8187s\n",
      "Epoch: 5 cost time: 70.14928388595581\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0878741 Vali Loss: 0.0343938 Test Loss: 0.1081526\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11478278785943985, mae:0.20999355614185333\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3632965\n",
      "\tspeed: 0.0261s/iter; left time: 1186.6505s\n",
      "\titers: 200, epoch: 1 | loss: 0.3612089\n",
      "\tspeed: 0.0147s/iter; left time: 664.9969s\n",
      "\titers: 300, epoch: 1 | loss: 0.1663644\n",
      "\tspeed: 0.0146s/iter; left time: 662.3621s\n",
      "\titers: 400, epoch: 1 | loss: 0.3175196\n",
      "\tspeed: 0.0147s/iter; left time: 662.4291s\n",
      "\titers: 500, epoch: 1 | loss: 0.2269731\n",
      "\tspeed: 0.0147s/iter; left time: 660.9395s\n",
      "\titers: 600, epoch: 1 | loss: 0.2223674\n",
      "\tspeed: 0.0147s/iter; left time: 659.1292s\n",
      "\titers: 700, epoch: 1 | loss: 0.3031235\n",
      "\tspeed: 0.0146s/iter; left time: 656.1205s\n",
      "\titers: 800, epoch: 1 | loss: 0.1987855\n",
      "\tspeed: 0.0147s/iter; left time: 655.6245s\n",
      "\titers: 900, epoch: 1 | loss: 0.2818239\n",
      "\tspeed: 0.0146s/iter; left time: 653.3404s\n",
      "\titers: 1000, epoch: 1 | loss: 0.2000554\n",
      "\tspeed: 0.0146s/iter; left time: 651.4179s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2433306\n",
      "\tspeed: 0.0146s/iter; left time: 649.8999s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1458235\n",
      "\tspeed: 0.0146s/iter; left time: 646.7427s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1891625\n",
      "\tspeed: 0.0145s/iter; left time: 643.5476s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1622000\n",
      "\tspeed: 0.0146s/iter; left time: 642.9354s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1900355\n",
      "\tspeed: 0.0146s/iter; left time: 641.7295s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2153424\n",
      "\tspeed: 0.0146s/iter; left time: 639.7061s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3175794\n",
      "\tspeed: 0.0146s/iter; left time: 638.4239s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1359680\n",
      "\tspeed: 0.0145s/iter; left time: 635.0960s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2951876\n",
      "\tspeed: 0.0145s/iter; left time: 633.1857s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0921913\n",
      "\tspeed: 0.0145s/iter; left time: 632.1044s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2372821\n",
      "\tspeed: 0.0145s/iter; left time: 630.3598s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1914964\n",
      "\tspeed: 0.0145s/iter; left time: 628.7901s\n",
      "\titers: 2300, epoch: 1 | loss: 0.3013221\n",
      "\tspeed: 0.0145s/iter; left time: 628.3140s\n",
      "\titers: 2400, epoch: 1 | loss: 0.6709409\n",
      "\tspeed: 0.0145s/iter; left time: 627.7460s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2330123\n",
      "\tspeed: 0.0145s/iter; left time: 626.0507s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1482763\n",
      "\tspeed: 0.0145s/iter; left time: 624.8485s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1771284\n",
      "\tspeed: 0.0146s/iter; left time: 623.5639s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2025249\n",
      "\tspeed: 0.0145s/iter; left time: 621.4201s\n",
      "\titers: 2900, epoch: 1 | loss: 0.3192387\n",
      "\tspeed: 0.0145s/iter; left time: 620.2333s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2197634\n",
      "\tspeed: 0.0145s/iter; left time: 618.9304s\n",
      "\titers: 3100, epoch: 1 | loss: 0.2542244\n",
      "\tspeed: 0.0145s/iter; left time: 617.3894s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2812100\n",
      "\tspeed: 0.0145s/iter; left time: 616.0518s\n",
      "\titers: 3300, epoch: 1 | loss: 0.2572041\n",
      "\tspeed: 0.0146s/iter; left time: 614.8044s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1190821\n",
      "\tspeed: 0.0145s/iter; left time: 613.2819s\n",
      "\titers: 3500, epoch: 1 | loss: 0.4581592\n",
      "\tspeed: 0.0145s/iter; left time: 611.5413s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045532\n",
      "\tspeed: 0.0145s/iter; left time: 610.3175s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1117853\n",
      "\tspeed: 0.0146s/iter; left time: 609.2163s\n",
      "\titers: 3800, epoch: 1 | loss: 0.5856978\n",
      "\tspeed: 0.0146s/iter; left time: 607.7476s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1279238\n",
      "\tspeed: 0.0146s/iter; left time: 606.3317s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2664152\n",
      "\tspeed: 0.0146s/iter; left time: 605.0300s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1563135\n",
      "\tspeed: 0.0146s/iter; left time: 603.4653s\n",
      "\titers: 4200, epoch: 1 | loss: 0.2030613\n",
      "\tspeed: 0.0146s/iter; left time: 602.0917s\n",
      "\titers: 4300, epoch: 1 | loss: 0.2133383\n",
      "\tspeed: 0.0146s/iter; left time: 600.7082s\n",
      "\titers: 4400, epoch: 1 | loss: 0.3547910\n",
      "\tspeed: 0.0146s/iter; left time: 599.1291s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1626694\n",
      "\tspeed: 0.0146s/iter; left time: 597.9337s\n",
      "Epoch: 1 cost time: 67.5660662651062\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.2506176 Vali Loss: 0.1792732 Test Loss: 0.1749838\n",
      "Validation loss decreased (inf --> 0.179273).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2447494\n",
      "\tspeed: 0.1351s/iter; left time: 5523.7307s\n",
      "\titers: 200, epoch: 2 | loss: 0.1742973\n",
      "\tspeed: 0.0146s/iter; left time: 595.9150s\n",
      "\titers: 300, epoch: 2 | loss: 0.3313923\n",
      "\tspeed: 0.0146s/iter; left time: 594.8865s\n",
      "\titers: 400, epoch: 2 | loss: 0.2792679\n",
      "\tspeed: 0.0146s/iter; left time: 592.9796s\n",
      "\titers: 500, epoch: 2 | loss: 0.5619069\n",
      "\tspeed: 0.0146s/iter; left time: 592.1558s\n",
      "\titers: 600, epoch: 2 | loss: 0.1252872\n",
      "\tspeed: 0.0147s/iter; left time: 591.9209s\n",
      "\titers: 700, epoch: 2 | loss: 0.3338600\n",
      "\tspeed: 0.0147s/iter; left time: 590.8631s\n",
      "\titers: 800, epoch: 2 | loss: 0.1635914\n",
      "\tspeed: 0.0147s/iter; left time: 589.4157s\n",
      "\titers: 900, epoch: 2 | loss: 0.2267582\n",
      "\tspeed: 0.0146s/iter; left time: 586.5095s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2687585\n",
      "\tspeed: 0.0146s/iter; left time: 585.8555s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1283258\n",
      "\tspeed: 0.0146s/iter; left time: 584.1960s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2612851\n",
      "\tspeed: 0.0146s/iter; left time: 581.9774s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1672531\n",
      "\tspeed: 0.0146s/iter; left time: 580.3795s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1151724\n",
      "\tspeed: 0.0146s/iter; left time: 578.9681s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2623320\n",
      "\tspeed: 0.0146s/iter; left time: 577.6598s\n",
      "\titers: 1600, epoch: 2 | loss: 0.4514408\n",
      "\tspeed: 0.0146s/iter; left time: 576.4664s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3221964\n",
      "\tspeed: 0.0147s/iter; left time: 575.8656s\n",
      "\titers: 1800, epoch: 2 | loss: 0.3075443\n",
      "\tspeed: 0.0146s/iter; left time: 573.7703s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1465409\n",
      "\tspeed: 0.0146s/iter; left time: 572.0770s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1916947\n",
      "\tspeed: 0.0146s/iter; left time: 569.8141s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1685293\n",
      "\tspeed: 0.0146s/iter; left time: 568.1474s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1068334\n",
      "\tspeed: 0.0146s/iter; left time: 566.3065s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1138222\n",
      "\tspeed: 0.0146s/iter; left time: 564.3521s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2333193\n",
      "\tspeed: 0.0146s/iter; left time: 562.9080s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1435477\n",
      "\tspeed: 0.0146s/iter; left time: 561.5692s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2848657\n",
      "\tspeed: 0.0146s/iter; left time: 559.8073s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1357936\n",
      "\tspeed: 0.0146s/iter; left time: 558.7119s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1076243\n",
      "\tspeed: 0.0146s/iter; left time: 558.3419s\n",
      "\titers: 2900, epoch: 2 | loss: 0.2340539\n",
      "\tspeed: 0.0146s/iter; left time: 555.3694s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1995971\n",
      "\tspeed: 0.0146s/iter; left time: 554.0155s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1425870\n",
      "\tspeed: 0.0146s/iter; left time: 552.5015s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2513198\n",
      "\tspeed: 0.0146s/iter; left time: 552.8312s\n",
      "\titers: 3300, epoch: 2 | loss: 0.2660067\n",
      "\tspeed: 0.0146s/iter; left time: 552.2412s\n",
      "\titers: 3400, epoch: 2 | loss: 0.2678370\n",
      "\tspeed: 0.0146s/iter; left time: 550.0656s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1534656\n",
      "\tspeed: 0.0146s/iter; left time: 548.4737s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0933451\n",
      "\tspeed: 0.0146s/iter; left time: 546.8017s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2419136\n",
      "\tspeed: 0.0146s/iter; left time: 545.5382s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1625462\n",
      "\tspeed: 0.0146s/iter; left time: 543.9430s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2648199\n",
      "\tspeed: 0.0146s/iter; left time: 542.7837s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1442360\n",
      "\tspeed: 0.0146s/iter; left time: 540.8969s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1991904\n",
      "\tspeed: 0.0146s/iter; left time: 539.3948s\n",
      "\titers: 4200, epoch: 2 | loss: 0.2070115\n",
      "\tspeed: 0.0146s/iter; left time: 536.9710s\n",
      "\titers: 4300, epoch: 2 | loss: 0.3039147\n",
      "\tspeed: 0.0146s/iter; left time: 534.3572s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1379984\n",
      "\tspeed: 0.0145s/iter; left time: 531.9397s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1885291\n",
      "\tspeed: 0.0145s/iter; left time: 530.1278s\n",
      "Epoch: 2 cost time: 66.82407093048096\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.2085815 Vali Loss: 0.1740521 Test Loss: 0.1763017\n",
      "Validation loss decreased (0.179273 --> 0.174052).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2503781\n",
      "\tspeed: 0.1305s/iter; left time: 4741.5663s\n",
      "\titers: 200, epoch: 3 | loss: 0.2045967\n",
      "\tspeed: 0.0147s/iter; left time: 531.4833s\n",
      "\titers: 300, epoch: 3 | loss: 0.1804852\n",
      "\tspeed: 0.0146s/iter; left time: 529.1645s\n",
      "\titers: 400, epoch: 3 | loss: 0.2779175\n",
      "\tspeed: 0.0146s/iter; left time: 527.8335s\n",
      "\titers: 500, epoch: 3 | loss: 0.1977433\n",
      "\tspeed: 0.0146s/iter; left time: 525.9832s\n",
      "\titers: 600, epoch: 3 | loss: 0.2751262\n",
      "\tspeed: 0.0147s/iter; left time: 525.4340s\n",
      "\titers: 700, epoch: 3 | loss: 0.1283241\n",
      "\tspeed: 0.0147s/iter; left time: 523.8707s\n",
      "\titers: 800, epoch: 3 | loss: 0.1195078\n",
      "\tspeed: 0.0146s/iter; left time: 521.8202s\n",
      "\titers: 900, epoch: 3 | loss: 0.1326087\n",
      "\tspeed: 0.0146s/iter; left time: 520.3748s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2927760\n",
      "\tspeed: 0.0146s/iter; left time: 518.7033s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0946229\n",
      "\tspeed: 0.0146s/iter; left time: 517.1840s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1599361\n",
      "\tspeed: 0.0146s/iter; left time: 516.1302s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1795194\n",
      "\tspeed: 0.0147s/iter; left time: 514.8281s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1515631\n",
      "\tspeed: 0.0146s/iter; left time: 512.3202s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1621780\n",
      "\tspeed: 0.0146s/iter; left time: 510.0248s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1284781\n",
      "\tspeed: 0.0146s/iter; left time: 509.9402s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1399402\n",
      "\tspeed: 0.0146s/iter; left time: 508.4158s\n",
      "\titers: 1800, epoch: 3 | loss: 0.3037611\n",
      "\tspeed: 0.0146s/iter; left time: 506.8768s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1341804\n",
      "\tspeed: 0.0146s/iter; left time: 505.4634s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2086841\n",
      "\tspeed: 0.0146s/iter; left time: 503.9664s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0556780\n",
      "\tspeed: 0.0146s/iter; left time: 502.2491s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1626754\n",
      "\tspeed: 0.0146s/iter; left time: 500.8425s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1540606\n",
      "\tspeed: 0.0146s/iter; left time: 499.2954s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1304726\n",
      "\tspeed: 0.0146s/iter; left time: 497.9320s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1162029\n",
      "\tspeed: 0.0146s/iter; left time: 496.2980s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2020690\n",
      "\tspeed: 0.0146s/iter; left time: 494.7184s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0991517\n",
      "\tspeed: 0.0146s/iter; left time: 493.4591s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1388047\n",
      "\tspeed: 0.0146s/iter; left time: 491.9909s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1469031\n",
      "\tspeed: 0.0146s/iter; left time: 490.5386s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1781340\n",
      "\tspeed: 0.0146s/iter; left time: 488.9060s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1358701\n",
      "\tspeed: 0.0146s/iter; left time: 487.4701s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1522381\n",
      "\tspeed: 0.0146s/iter; left time: 486.0850s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1845058\n",
      "\tspeed: 0.0146s/iter; left time: 484.6913s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1081519\n",
      "\tspeed: 0.0146s/iter; left time: 483.2566s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1954119\n",
      "\tspeed: 0.0146s/iter; left time: 481.8980s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1099689\n",
      "\tspeed: 0.0146s/iter; left time: 479.8356s\n",
      "\titers: 3700, epoch: 3 | loss: 0.2125370\n",
      "\tspeed: 0.0147s/iter; left time: 479.7426s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0912142\n",
      "\tspeed: 0.0147s/iter; left time: 480.8417s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0528369\n",
      "\tspeed: 0.0146s/iter; left time: 475.9632s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2848759\n",
      "\tspeed: 0.0146s/iter; left time: 474.1883s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0933764\n",
      "\tspeed: 0.0146s/iter; left time: 472.7388s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1051632\n",
      "\tspeed: 0.0146s/iter; left time: 471.1804s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1540013\n",
      "\tspeed: 0.0146s/iter; left time: 470.0027s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1135963\n",
      "\tspeed: 0.0146s/iter; left time: 468.6130s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1358993\n",
      "\tspeed: 0.0152s/iter; left time: 484.2058s\n",
      "Epoch: 3 cost time: 67.07454705238342\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1682378 Vali Loss: 0.1965955 Test Loss: 0.2002430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1461346\n",
      "\tspeed: 0.1356s/iter; left time: 4310.5873s\n",
      "\titers: 200, epoch: 4 | loss: 0.1145547\n",
      "\tspeed: 0.0168s/iter; left time: 533.5617s\n",
      "\titers: 300, epoch: 4 | loss: 0.0721253\n",
      "\tspeed: 0.0168s/iter; left time: 530.9244s\n",
      "\titers: 400, epoch: 4 | loss: 0.2167036\n",
      "\tspeed: 0.0168s/iter; left time: 529.1318s\n",
      "\titers: 500, epoch: 4 | loss: 0.1483090\n",
      "\tspeed: 0.0168s/iter; left time: 527.4989s\n",
      "\titers: 600, epoch: 4 | loss: 0.1316151\n",
      "\tspeed: 0.0146s/iter; left time: 457.2735s\n",
      "\titers: 700, epoch: 4 | loss: 0.1183128\n",
      "\tspeed: 0.0154s/iter; left time: 481.7645s\n",
      "\titers: 800, epoch: 4 | loss: 0.0873641\n",
      "\tspeed: 0.0169s/iter; left time: 525.2907s\n",
      "\titers: 900, epoch: 4 | loss: 0.1386785\n",
      "\tspeed: 0.0169s/iter; left time: 522.8800s\n",
      "\titers: 1000, epoch: 4 | loss: 0.2361491\n",
      "\tspeed: 0.0155s/iter; left time: 480.1237s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1643889\n",
      "\tspeed: 0.0146s/iter; left time: 450.7671s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1380167\n",
      "\tspeed: 0.0147s/iter; left time: 449.6538s\n",
      "\titers: 1300, epoch: 4 | loss: 0.2204322\n",
      "\tspeed: 0.0157s/iter; left time: 479.1604s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0863345\n",
      "\tspeed: 0.0168s/iter; left time: 513.3331s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2711574\n",
      "\tspeed: 0.0168s/iter; left time: 511.6040s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1332587\n",
      "\tspeed: 0.0154s/iter; left time: 467.7451s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1343015\n",
      "\tspeed: 0.0146s/iter; left time: 439.7704s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1103061\n",
      "\tspeed: 0.0146s/iter; left time: 438.0409s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1064024\n",
      "\tspeed: 0.0146s/iter; left time: 438.4362s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1190815\n",
      "\tspeed: 0.0146s/iter; left time: 437.4373s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0609430\n",
      "\tspeed: 0.0147s/iter; left time: 436.3799s\n",
      "\titers: 2200, epoch: 4 | loss: 0.2330273\n",
      "\tspeed: 0.0147s/iter; left time: 435.0600s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0986790\n",
      "\tspeed: 0.0147s/iter; left time: 433.6606s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1223683\n",
      "\tspeed: 0.0147s/iter; left time: 432.1579s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1086090\n",
      "\tspeed: 0.0147s/iter; left time: 430.6939s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0950660\n",
      "\tspeed: 0.0147s/iter; left time: 429.2014s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0768292\n",
      "\tspeed: 0.0147s/iter; left time: 427.7606s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0920092\n",
      "\tspeed: 0.0147s/iter; left time: 426.4362s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1378216\n",
      "\tspeed: 0.0147s/iter; left time: 424.8198s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1757267\n",
      "\tspeed: 0.0146s/iter; left time: 421.7152s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0927222\n",
      "\tspeed: 0.0146s/iter; left time: 420.0934s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1780021\n",
      "\tspeed: 0.0146s/iter; left time: 419.1704s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1675545\n",
      "\tspeed: 0.0146s/iter; left time: 417.2432s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1389802\n",
      "\tspeed: 0.0146s/iter; left time: 415.4258s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0903400\n",
      "\tspeed: 0.0146s/iter; left time: 413.9447s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1004618\n",
      "\tspeed: 0.0146s/iter; left time: 413.1213s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2664283\n",
      "\tspeed: 0.0147s/iter; left time: 412.9721s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1019938\n",
      "\tspeed: 0.0146s/iter; left time: 411.0513s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1312030\n",
      "\tspeed: 0.0146s/iter; left time: 409.3870s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0829624\n",
      "\tspeed: 0.0146s/iter; left time: 408.0534s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0956842\n",
      "\tspeed: 0.0146s/iter; left time: 405.4256s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1223028\n",
      "\tspeed: 0.0146s/iter; left time: 403.9586s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1121927\n",
      "\tspeed: 0.0146s/iter; left time: 402.3505s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1367601\n",
      "\tspeed: 0.0146s/iter; left time: 400.7648s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1229346\n",
      "\tspeed: 0.0146s/iter; left time: 399.3260s\n",
      "Epoch: 4 cost time: 69.18385648727417\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1427174 Vali Loss: 0.1872938 Test Loss: 0.1927574\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1515499\n",
      "\tspeed: 0.1280s/iter; left time: 3486.6458s\n",
      "\titers: 200, epoch: 5 | loss: 0.1522538\n",
      "\tspeed: 0.0146s/iter; left time: 395.8111s\n",
      "\titers: 300, epoch: 5 | loss: 0.1391341\n",
      "\tspeed: 0.0146s/iter; left time: 394.0593s\n",
      "\titers: 400, epoch: 5 | loss: 0.1546288\n",
      "\tspeed: 0.0146s/iter; left time: 392.6679s\n",
      "\titers: 500, epoch: 5 | loss: 0.1187211\n",
      "\tspeed: 0.0146s/iter; left time: 390.8814s\n",
      "\titers: 600, epoch: 5 | loss: 0.1217946\n",
      "\tspeed: 0.0146s/iter; left time: 389.8033s\n",
      "\titers: 700, epoch: 5 | loss: 0.0978291\n",
      "\tspeed: 0.0146s/iter; left time: 388.1141s\n",
      "\titers: 800, epoch: 5 | loss: 0.1474133\n",
      "\tspeed: 0.0146s/iter; left time: 386.6307s\n",
      "\titers: 900, epoch: 5 | loss: 0.1081419\n",
      "\tspeed: 0.0146s/iter; left time: 385.0765s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0644245\n",
      "\tspeed: 0.0146s/iter; left time: 383.5161s\n",
      "\titers: 1100, epoch: 5 | loss: 0.2635769\n",
      "\tspeed: 0.0146s/iter; left time: 382.1964s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1900672\n",
      "\tspeed: 0.0146s/iter; left time: 380.9742s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1575088\n",
      "\tspeed: 0.0146s/iter; left time: 379.3379s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1263475\n",
      "\tspeed: 0.0146s/iter; left time: 377.4958s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0900358\n",
      "\tspeed: 0.0146s/iter; left time: 376.2473s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1326066\n",
      "\tspeed: 0.0146s/iter; left time: 374.8407s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0963128\n",
      "\tspeed: 0.0146s/iter; left time: 373.3912s\n",
      "\titers: 1800, epoch: 5 | loss: 0.2107437\n",
      "\tspeed: 0.0146s/iter; left time: 372.3680s\n",
      "\titers: 1900, epoch: 5 | loss: 0.2218291\n",
      "\tspeed: 0.0146s/iter; left time: 370.5559s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0546871\n",
      "\tspeed: 0.0147s/iter; left time: 371.2772s\n",
      "\titers: 2100, epoch: 5 | loss: 0.2167634\n",
      "\tspeed: 0.0146s/iter; left time: 367.8568s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0732133\n",
      "\tspeed: 0.0145s/iter; left time: 365.4532s\n",
      "\titers: 2300, epoch: 5 | loss: 0.2100748\n",
      "\tspeed: 0.0145s/iter; left time: 364.1533s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1457046\n",
      "\tspeed: 0.0145s/iter; left time: 362.6856s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1383481\n",
      "\tspeed: 0.0146s/iter; left time: 361.4923s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0799324\n",
      "\tspeed: 0.0145s/iter; left time: 359.6999s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0972785\n",
      "\tspeed: 0.0146s/iter; left time: 358.4843s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0734720\n",
      "\tspeed: 0.0146s/iter; left time: 357.5007s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1389921\n",
      "\tspeed: 0.0147s/iter; left time: 358.0505s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0674662\n",
      "\tspeed: 0.0146s/iter; left time: 356.2775s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1068149\n",
      "\tspeed: 0.0146s/iter; left time: 354.6733s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1337530\n",
      "\tspeed: 0.0146s/iter; left time: 353.3317s\n",
      "\titers: 3300, epoch: 5 | loss: 0.2249113\n",
      "\tspeed: 0.0146s/iter; left time: 351.8459s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1661278\n",
      "\tspeed: 0.0146s/iter; left time: 350.3680s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0746726\n",
      "\tspeed: 0.0147s/iter; left time: 349.2356s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1314172\n",
      "\tspeed: 0.0146s/iter; left time: 347.5124s\n",
      "\titers: 3700, epoch: 5 | loss: 0.2219796\n",
      "\tspeed: 0.0147s/iter; left time: 346.2124s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0755728\n",
      "\tspeed: 0.0148s/iter; left time: 347.4726s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1467164\n",
      "\tspeed: 0.0146s/iter; left time: 342.6211s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1136220\n",
      "\tspeed: 0.0146s/iter; left time: 341.1499s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1118259\n",
      "\tspeed: 0.0146s/iter; left time: 339.6382s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1088497\n",
      "\tspeed: 0.0146s/iter; left time: 338.3841s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1185541\n",
      "\tspeed: 0.0146s/iter; left time: 336.7365s\n",
      "\titers: 4400, epoch: 5 | loss: 0.1098725\n",
      "\tspeed: 0.0147s/iter; left time: 335.9848s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1839811\n",
      "\tspeed: 0.0146s/iter; left time: 333.1577s\n",
      "Epoch: 5 cost time: 66.7395761013031\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1302546 Vali Loss: 0.1956121 Test Loss: 0.1950452\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "mse:0.17627310752868652, mae:0.2662774920463562\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features M --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de08448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3632965\n",
      "\tspeed: 0.0263s/iter; left time: 1194.2309s\n",
      "\titers: 200, epoch: 1 | loss: 0.3612089\n",
      "\tspeed: 0.0146s/iter; left time: 663.4068s\n",
      "\titers: 300, epoch: 1 | loss: 0.1663644\n",
      "\tspeed: 0.0146s/iter; left time: 662.4377s\n",
      "\titers: 400, epoch: 1 | loss: 0.3175196\n",
      "\tspeed: 0.0146s/iter; left time: 659.7078s\n",
      "\titers: 500, epoch: 1 | loss: 0.2269731\n",
      "\tspeed: 0.0146s/iter; left time: 659.1608s\n",
      "\titers: 600, epoch: 1 | loss: 0.2223674\n",
      "\tspeed: 0.0147s/iter; left time: 658.6748s\n",
      "\titers: 700, epoch: 1 | loss: 0.3031235\n",
      "\tspeed: 0.0145s/iter; left time: 651.6939s\n",
      "\titers: 800, epoch: 1 | loss: 0.1987855\n",
      "\tspeed: 0.0145s/iter; left time: 650.9215s\n",
      "\titers: 900, epoch: 1 | loss: 0.2818239\n",
      "\tspeed: 0.0145s/iter; left time: 648.4327s\n",
      "\titers: 1000, epoch: 1 | loss: 0.2000554\n",
      "\tspeed: 0.0145s/iter; left time: 646.9017s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2433306\n",
      "\tspeed: 0.0145s/iter; left time: 645.9620s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1458235\n",
      "\tspeed: 0.0145s/iter; left time: 643.6103s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1891625\n",
      "\tspeed: 0.0146s/iter; left time: 644.7797s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1622000\n",
      "\tspeed: 0.0146s/iter; left time: 643.7183s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1900355\n",
      "\tspeed: 0.0146s/iter; left time: 641.8529s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2153424\n",
      "\tspeed: 0.0146s/iter; left time: 640.4407s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3175794\n",
      "\tspeed: 0.0146s/iter; left time: 640.5416s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1359680\n",
      "\tspeed: 0.0146s/iter; left time: 637.9492s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2951876\n",
      "\tspeed: 0.0146s/iter; left time: 636.2019s\n",
      "\titers: 2000, epoch: 1 | loss: 0.0921913\n",
      "\tspeed: 0.0146s/iter; left time: 634.5854s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2372821\n",
      "\tspeed: 0.0146s/iter; left time: 633.4404s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1914964\n",
      "\tspeed: 0.0146s/iter; left time: 631.3973s\n",
      "\titers: 2300, epoch: 1 | loss: 0.3013221\n",
      "\tspeed: 0.0146s/iter; left time: 629.9500s\n",
      "\titers: 2400, epoch: 1 | loss: 0.6709409\n",
      "\tspeed: 0.0146s/iter; left time: 629.4290s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2330123\n",
      "\tspeed: 0.0146s/iter; left time: 629.0384s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1482763\n",
      "\tspeed: 0.0146s/iter; left time: 627.1097s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1771284\n",
      "\tspeed: 0.0146s/iter; left time: 625.5989s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2025249\n",
      "\tspeed: 0.0146s/iter; left time: 623.9793s\n",
      "\titers: 2900, epoch: 1 | loss: 0.3192387\n",
      "\tspeed: 0.0146s/iter; left time: 622.6839s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2197634\n",
      "\tspeed: 0.0146s/iter; left time: 622.6980s\n",
      "\titers: 3100, epoch: 1 | loss: 0.2542244\n",
      "\tspeed: 0.0146s/iter; left time: 619.7515s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2812100\n",
      "\tspeed: 0.0146s/iter; left time: 618.3071s\n",
      "\titers: 3300, epoch: 1 | loss: 0.2572041\n",
      "\tspeed: 0.0146s/iter; left time: 616.7697s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1190821\n",
      "\tspeed: 0.0146s/iter; left time: 615.5158s\n",
      "\titers: 3500, epoch: 1 | loss: 0.4581592\n",
      "\tspeed: 0.0146s/iter; left time: 614.7180s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1045532\n",
      "\tspeed: 0.0146s/iter; left time: 613.3547s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1117853\n",
      "\tspeed: 0.0146s/iter; left time: 612.2320s\n",
      "\titers: 3800, epoch: 1 | loss: 0.5856978\n",
      "\tspeed: 0.0146s/iter; left time: 610.6289s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1279238\n",
      "\tspeed: 0.0146s/iter; left time: 608.8904s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2664152\n",
      "\tspeed: 0.0146s/iter; left time: 607.3129s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1563135\n",
      "\tspeed: 0.0146s/iter; left time: 606.0089s\n",
      "\titers: 4200, epoch: 1 | loss: 0.2030613\n",
      "\tspeed: 0.0146s/iter; left time: 604.7884s\n",
      "\titers: 4300, epoch: 1 | loss: 0.2133383\n",
      "\tspeed: 0.0146s/iter; left time: 603.1664s\n",
      "\titers: 4400, epoch: 1 | loss: 0.3547910\n",
      "\tspeed: 0.0146s/iter; left time: 601.6865s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1626694\n",
      "\tspeed: 0.0146s/iter; left time: 599.9909s\n",
      "Epoch: 1 cost time: 67.69497847557068\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.2506176 Vali Loss: 0.1792732 Test Loss: 0.1749838\n",
      "Validation loss decreased (inf --> 0.179273).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2447494\n",
      "\tspeed: 0.1445s/iter; left time: 5910.8736s\n",
      "\titers: 200, epoch: 2 | loss: 0.1742973\n",
      "\tspeed: 0.0145s/iter; left time: 592.7767s\n",
      "\titers: 300, epoch: 2 | loss: 0.3313923\n",
      "\tspeed: 0.0145s/iter; left time: 590.0924s\n",
      "\titers: 400, epoch: 2 | loss: 0.2792679\n",
      "\tspeed: 0.0145s/iter; left time: 588.4974s\n",
      "\titers: 500, epoch: 2 | loss: 0.5619069\n",
      "\tspeed: 0.0145s/iter; left time: 587.6677s\n",
      "\titers: 600, epoch: 2 | loss: 0.1252872\n",
      "\tspeed: 0.0145s/iter; left time: 586.3444s\n",
      "\titers: 700, epoch: 2 | loss: 0.3338600\n",
      "\tspeed: 0.0145s/iter; left time: 584.9719s\n",
      "\titers: 800, epoch: 2 | loss: 0.1635914\n",
      "\tspeed: 0.0145s/iter; left time: 583.3533s\n",
      "\titers: 900, epoch: 2 | loss: 0.2267582\n",
      "\tspeed: 0.0145s/iter; left time: 581.5725s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2687585\n",
      "\tspeed: 0.0145s/iter; left time: 580.1676s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1283258\n",
      "\tspeed: 0.0145s/iter; left time: 578.9348s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2612851\n",
      "\tspeed: 0.0146s/iter; left time: 581.0836s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1672531\n",
      "\tspeed: 0.0146s/iter; left time: 579.4501s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1151724\n",
      "\tspeed: 0.0146s/iter; left time: 578.0306s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2623320\n",
      "\tspeed: 0.0146s/iter; left time: 576.5856s\n",
      "\titers: 1600, epoch: 2 | loss: 0.4514408\n",
      "\tspeed: 0.0146s/iter; left time: 575.1478s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3221964\n",
      "\tspeed: 0.0146s/iter; left time: 573.7537s\n",
      "\titers: 1800, epoch: 2 | loss: 0.3075443\n",
      "\tspeed: 0.0146s/iter; left time: 572.2664s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1465409\n",
      "\tspeed: 0.0146s/iter; left time: 570.8631s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1916947\n",
      "\tspeed: 0.0146s/iter; left time: 569.2600s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1685293\n",
      "\tspeed: 0.0146s/iter; left time: 567.8441s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1068334\n",
      "\tspeed: 0.0146s/iter; left time: 566.4510s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1138222\n",
      "\tspeed: 0.0146s/iter; left time: 564.9096s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2333193\n",
      "\tspeed: 0.0146s/iter; left time: 563.5073s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1435477\n",
      "\tspeed: 0.0146s/iter; left time: 561.9389s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2848657\n",
      "\tspeed: 0.0146s/iter; left time: 560.3442s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1357936\n",
      "\tspeed: 0.0146s/iter; left time: 558.7705s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1076243\n",
      "\tspeed: 0.0146s/iter; left time: 557.6727s\n",
      "\titers: 2900, epoch: 2 | loss: 0.2340539\n",
      "\tspeed: 0.0146s/iter; left time: 556.1405s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1995971\n",
      "\tspeed: 0.0146s/iter; left time: 554.7131s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1425870\n",
      "\tspeed: 0.0146s/iter; left time: 553.3169s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2513198\n",
      "\tspeed: 0.0146s/iter; left time: 551.9998s\n",
      "\titers: 3300, epoch: 2 | loss: 0.2660067\n",
      "\tspeed: 0.0146s/iter; left time: 550.4241s\n",
      "\titers: 3400, epoch: 2 | loss: 0.2678370\n",
      "\tspeed: 0.0147s/iter; left time: 550.9646s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1534656\n",
      "\tspeed: 0.0147s/iter; left time: 549.6223s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0933451\n",
      "\tspeed: 0.0147s/iter; left time: 548.1624s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2419136\n",
      "\tspeed: 0.0147s/iter; left time: 546.6150s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1625462\n",
      "\tspeed: 0.0147s/iter; left time: 545.3009s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2648199\n",
      "\tspeed: 0.0147s/iter; left time: 543.6338s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1442360\n",
      "\tspeed: 0.0147s/iter; left time: 542.1943s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1991904\n",
      "\tspeed: 0.0146s/iter; left time: 540.4455s\n",
      "\titers: 4200, epoch: 2 | loss: 0.2070115\n",
      "\tspeed: 0.0147s/iter; left time: 539.2509s\n",
      "\titers: 4300, epoch: 2 | loss: 0.3039147\n",
      "\tspeed: 0.0147s/iter; left time: 537.7244s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1379984\n",
      "\tspeed: 0.0147s/iter; left time: 536.5287s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1885291\n",
      "\tspeed: 0.0147s/iter; left time: 534.8382s\n",
      "Epoch: 2 cost time: 66.72277069091797\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.2085815 Vali Loss: 0.1740521 Test Loss: 0.1763017\n",
      "Validation loss decreased (0.179273 --> 0.174052).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2503781\n",
      "\tspeed: 0.1287s/iter; left time: 4677.6752s\n",
      "\titers: 200, epoch: 3 | loss: 0.2045967\n",
      "\tspeed: 0.0163s/iter; left time: 590.6838s\n",
      "\titers: 300, epoch: 3 | loss: 0.1804852\n",
      "\tspeed: 0.0163s/iter; left time: 588.9457s\n",
      "\titers: 400, epoch: 3 | loss: 0.2779175\n",
      "\tspeed: 0.0163s/iter; left time: 586.3895s\n",
      "\titers: 500, epoch: 3 | loss: 0.1977433\n",
      "\tspeed: 0.0155s/iter; left time: 556.8098s\n",
      "\titers: 600, epoch: 3 | loss: 0.2751262\n",
      "\tspeed: 0.0146s/iter; left time: 522.4162s\n",
      "\titers: 700, epoch: 3 | loss: 0.1283241\n",
      "\tspeed: 0.0146s/iter; left time: 521.0165s\n",
      "\titers: 800, epoch: 3 | loss: 0.1195078\n",
      "\tspeed: 0.0146s/iter; left time: 519.6860s\n",
      "\titers: 900, epoch: 3 | loss: 0.1326087\n",
      "\tspeed: 0.0146s/iter; left time: 518.1768s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2927760\n",
      "\tspeed: 0.0146s/iter; left time: 516.4175s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0946229\n",
      "\tspeed: 0.0146s/iter; left time: 515.4350s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1599361\n",
      "\tspeed: 0.0146s/iter; left time: 515.0273s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1795194\n",
      "\tspeed: 0.0146s/iter; left time: 512.7515s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1515631\n",
      "\tspeed: 0.0146s/iter; left time: 511.3903s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1621780\n",
      "\tspeed: 0.0146s/iter; left time: 509.7489s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1284781\n",
      "\tspeed: 0.0146s/iter; left time: 508.0244s\n",
      "\titers: 1700, epoch: 3 | loss: 0.1399402\n",
      "\tspeed: 0.0146s/iter; left time: 506.5584s\n",
      "\titers: 1800, epoch: 3 | loss: 0.3037611\n",
      "\tspeed: 0.0146s/iter; left time: 505.1454s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1341804\n",
      "\tspeed: 0.0146s/iter; left time: 503.7421s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2086841\n",
      "\tspeed: 0.0146s/iter; left time: 502.2010s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0556780\n",
      "\tspeed: 0.0146s/iter; left time: 500.7194s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1626754\n",
      "\tspeed: 0.0146s/iter; left time: 499.2575s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1540606\n",
      "\tspeed: 0.0146s/iter; left time: 497.6953s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1304726\n",
      "\tspeed: 0.0146s/iter; left time: 496.2919s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1162029\n",
      "\tspeed: 0.0146s/iter; left time: 495.1057s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2020690\n",
      "\tspeed: 0.0146s/iter; left time: 493.7798s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0991517\n",
      "\tspeed: 0.0146s/iter; left time: 492.2556s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1388047\n",
      "\tspeed: 0.0146s/iter; left time: 490.7274s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1469031\n",
      "\tspeed: 0.0146s/iter; left time: 489.1271s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1781340\n",
      "\tspeed: 0.0146s/iter; left time: 487.5643s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1358701\n",
      "\tspeed: 0.0146s/iter; left time: 486.1894s\n",
      "\titers: 3200, epoch: 3 | loss: 0.1522381\n",
      "\tspeed: 0.0146s/iter; left time: 484.8829s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1845058\n",
      "\tspeed: 0.0146s/iter; left time: 483.4714s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1081519\n",
      "\tspeed: 0.0146s/iter; left time: 481.8588s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1954119\n",
      "\tspeed: 0.0146s/iter; left time: 480.2934s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1099689\n",
      "\tspeed: 0.0146s/iter; left time: 478.8436s\n",
      "\titers: 3700, epoch: 3 | loss: 0.2125370\n",
      "\tspeed: 0.0146s/iter; left time: 477.5959s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0912142\n",
      "\tspeed: 0.0146s/iter; left time: 475.8514s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0528369\n",
      "\tspeed: 0.0146s/iter; left time: 474.4950s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2848759\n",
      "\tspeed: 0.0146s/iter; left time: 472.9804s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0933764\n",
      "\tspeed: 0.0146s/iter; left time: 471.5709s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1051632\n",
      "\tspeed: 0.0146s/iter; left time: 470.3190s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1540013\n",
      "\tspeed: 0.0146s/iter; left time: 468.9895s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1135963\n",
      "\tspeed: 0.0150s/iter; left time: 480.8206s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1358993\n",
      "\tspeed: 0.0163s/iter; left time: 519.0546s\n",
      "Epoch: 3 cost time: 67.61356139183044\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1682378 Vali Loss: 0.1965955 Test Loss: 0.2002430\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1461346\n",
      "\tspeed: 0.1283s/iter; left time: 4077.8117s\n",
      "\titers: 200, epoch: 4 | loss: 0.1145547\n",
      "\tspeed: 0.0162s/iter; left time: 512.7139s\n",
      "\titers: 300, epoch: 4 | loss: 0.0721253\n",
      "\tspeed: 0.0163s/iter; left time: 513.6920s\n",
      "\titers: 400, epoch: 4 | loss: 0.2167036\n",
      "\tspeed: 0.0163s/iter; left time: 511.8990s\n",
      "\titers: 500, epoch: 4 | loss: 0.1483090\n",
      "\tspeed: 0.0163s/iter; left time: 510.4000s\n",
      "\titers: 600, epoch: 4 | loss: 0.1316151\n",
      "\tspeed: 0.0163s/iter; left time: 508.6455s\n",
      "\titers: 700, epoch: 4 | loss: 0.1183128\n",
      "\tspeed: 0.0163s/iter; left time: 506.9547s\n",
      "\titers: 800, epoch: 4 | loss: 0.0873641\n",
      "\tspeed: 0.0163s/iter; left time: 505.5828s\n",
      "\titers: 900, epoch: 4 | loss: 0.1386785\n",
      "\tspeed: 0.0163s/iter; left time: 503.9110s\n",
      "\titers: 1000, epoch: 4 | loss: 0.2361491\n",
      "\tspeed: 0.0163s/iter; left time: 502.1848s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1643889\n",
      "\tspeed: 0.0163s/iter; left time: 500.2906s\n",
      "\titers: 1200, epoch: 4 | loss: 0.1380167\n",
      "\tspeed: 0.0163s/iter; left time: 498.9097s\n",
      "\titers: 1300, epoch: 4 | loss: 0.2204322\n",
      "\tspeed: 0.0163s/iter; left time: 497.4195s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0863345\n",
      "\tspeed: 0.0163s/iter; left time: 495.6197s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2711574\n",
      "\tspeed: 0.0163s/iter; left time: 494.7270s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1332587\n",
      "\tspeed: 0.0163s/iter; left time: 492.8564s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1343015\n",
      "\tspeed: 0.0163s/iter; left time: 490.9755s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1103061\n",
      "\tspeed: 0.0163s/iter; left time: 489.2071s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1064024\n",
      "\tspeed: 0.0163s/iter; left time: 487.6523s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1190815\n",
      "\tspeed: 0.0163s/iter; left time: 485.8418s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0609430\n",
      "\tspeed: 0.0163s/iter; left time: 484.7658s\n",
      "\titers: 2200, epoch: 4 | loss: 0.2330273\n",
      "\tspeed: 0.0163s/iter; left time: 483.1803s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0986790\n",
      "\tspeed: 0.0163s/iter; left time: 481.1670s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1223683\n",
      "\tspeed: 0.0163s/iter; left time: 479.8232s\n",
      "\titers: 2500, epoch: 4 | loss: 0.1086090\n",
      "\tspeed: 0.0163s/iter; left time: 478.4131s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0950660\n",
      "\tspeed: 0.0163s/iter; left time: 476.7880s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0768292\n",
      "\tspeed: 0.0163s/iter; left time: 475.1111s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0920092\n",
      "\tspeed: 0.0163s/iter; left time: 473.4584s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1378216\n",
      "\tspeed: 0.0163s/iter; left time: 471.6714s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1757267\n",
      "\tspeed: 0.0163s/iter; left time: 469.9381s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0927222\n",
      "\tspeed: 0.0163s/iter; left time: 468.3733s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1780021\n",
      "\tspeed: 0.0163s/iter; left time: 466.5872s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1675545\n",
      "\tspeed: 0.0163s/iter; left time: 465.3309s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1389802\n",
      "\tspeed: 0.0163s/iter; left time: 463.3876s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0903400\n",
      "\tspeed: 0.0163s/iter; left time: 461.9213s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1004618\n",
      "\tspeed: 0.0163s/iter; left time: 460.0934s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2664283\n",
      "\tspeed: 0.0163s/iter; left time: 458.6143s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1019938\n",
      "\tspeed: 0.0163s/iter; left time: 457.3712s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1312030\n",
      "\tspeed: 0.0163s/iter; left time: 455.5622s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0829624\n",
      "\tspeed: 0.0163s/iter; left time: 454.1363s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0956842\n",
      "\tspeed: 0.0163s/iter; left time: 452.3952s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1223028\n",
      "\tspeed: 0.0163s/iter; left time: 451.0178s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1121927\n",
      "\tspeed: 0.0163s/iter; left time: 449.3082s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1367601\n",
      "\tspeed: 0.0163s/iter; left time: 447.5661s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1229346\n",
      "\tspeed: 0.0163s/iter; left time: 446.0816s\n",
      "Epoch: 4 cost time: 74.16883444786072\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1427174 Vali Loss: 0.1872938 Test Loss: 0.1927574\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1515499\n",
      "\tspeed: 0.1379s/iter; left time: 3755.8399s\n",
      "\titers: 200, epoch: 5 | loss: 0.1522538\n",
      "\tspeed: 0.0147s/iter; left time: 398.2869s\n",
      "\titers: 300, epoch: 5 | loss: 0.1391341\n",
      "\tspeed: 0.0147s/iter; left time: 397.3070s\n",
      "\titers: 400, epoch: 5 | loss: 0.1546288\n",
      "\tspeed: 0.0147s/iter; left time: 395.5562s\n",
      "\titers: 500, epoch: 5 | loss: 0.1187211\n",
      "\tspeed: 0.0147s/iter; left time: 394.1536s\n",
      "\titers: 600, epoch: 5 | loss: 0.1217946\n",
      "\tspeed: 0.0147s/iter; left time: 392.3484s\n",
      "\titers: 700, epoch: 5 | loss: 0.0978291\n",
      "\tspeed: 0.0147s/iter; left time: 390.8291s\n",
      "\titers: 800, epoch: 5 | loss: 0.1474133\n",
      "\tspeed: 0.0147s/iter; left time: 389.7636s\n",
      "\titers: 900, epoch: 5 | loss: 0.1081419\n",
      "\tspeed: 0.0147s/iter; left time: 387.8447s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0644245\n",
      "\tspeed: 0.0147s/iter; left time: 386.6079s\n",
      "\titers: 1100, epoch: 5 | loss: 0.2635769\n",
      "\tspeed: 0.0147s/iter; left time: 384.8452s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1900672\n",
      "\tspeed: 0.0147s/iter; left time: 383.4530s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1575088\n",
      "\tspeed: 0.0147s/iter; left time: 381.8836s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1263475\n",
      "\tspeed: 0.0147s/iter; left time: 380.3026s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0900358\n",
      "\tspeed: 0.0147s/iter; left time: 378.8955s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1326066\n",
      "\tspeed: 0.0147s/iter; left time: 377.1672s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0963128\n",
      "\tspeed: 0.0147s/iter; left time: 376.0194s\n",
      "\titers: 1800, epoch: 5 | loss: 0.2107437\n",
      "\tspeed: 0.0147s/iter; left time: 374.7813s\n",
      "\titers: 1900, epoch: 5 | loss: 0.2218291\n",
      "\tspeed: 0.0147s/iter; left time: 373.3216s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0546871\n",
      "\tspeed: 0.0147s/iter; left time: 371.6991s\n",
      "\titers: 2100, epoch: 5 | loss: 0.2167634\n",
      "\tspeed: 0.0147s/iter; left time: 370.1376s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0732133\n",
      "\tspeed: 0.0146s/iter; left time: 367.7250s\n",
      "\titers: 2300, epoch: 5 | loss: 0.2100748\n",
      "\tspeed: 0.0146s/iter; left time: 364.3229s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1457046\n",
      "\tspeed: 0.0146s/iter; left time: 362.7859s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1383481\n",
      "\tspeed: 0.0146s/iter; left time: 361.3299s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0799324\n",
      "\tspeed: 0.0146s/iter; left time: 359.9696s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0972785\n",
      "\tspeed: 0.0146s/iter; left time: 358.5598s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0734720\n",
      "\tspeed: 0.0146s/iter; left time: 357.0836s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1389921\n",
      "\tspeed: 0.0146s/iter; left time: 355.5342s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0674662\n",
      "\tspeed: 0.0145s/iter; left time: 353.9182s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1068149\n",
      "\tspeed: 0.0146s/iter; left time: 352.5694s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1337530\n",
      "\tspeed: 0.0145s/iter; left time: 351.0820s\n",
      "\titers: 3300, epoch: 5 | loss: 0.2249113\n",
      "\tspeed: 0.0146s/iter; left time: 349.6880s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1661278\n",
      "\tspeed: 0.0146s/iter; left time: 348.2257s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0746726\n",
      "\tspeed: 0.0146s/iter; left time: 346.7541s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1314172\n",
      "\tspeed: 0.0145s/iter; left time: 345.2452s\n",
      "\titers: 3700, epoch: 5 | loss: 0.2219796\n",
      "\tspeed: 0.0145s/iter; left time: 343.6991s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0755728\n",
      "\tspeed: 0.0145s/iter; left time: 342.2510s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1467164\n",
      "\tspeed: 0.0146s/iter; left time: 341.0123s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1136220\n",
      "\tspeed: 0.0145s/iter; left time: 339.4128s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1118259\n",
      "\tspeed: 0.0145s/iter; left time: 337.9258s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1088497\n",
      "\tspeed: 0.0145s/iter; left time: 336.4747s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1185541\n",
      "\tspeed: 0.0145s/iter; left time: 335.0885s\n",
      "\titers: 4400, epoch: 5 | loss: 0.1098725\n",
      "\tspeed: 0.0145s/iter; left time: 333.6027s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1839811\n",
      "\tspeed: 0.0145s/iter; left time: 332.0484s\n",
      "Epoch: 5 cost time: 66.8033094406128\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1302546 Vali Loss: 0.1956121 Test Loss: 0.1950452\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0974626\n",
      "\tspeed: 0.1292s/iter; left time: 2929.7453s\n",
      "\titers: 200, epoch: 6 | loss: 0.1915258\n",
      "\tspeed: 0.0145s/iter; left time: 327.6543s\n",
      "\titers: 300, epoch: 6 | loss: 0.1264053\n",
      "\tspeed: 0.0145s/iter; left time: 326.0559s\n",
      "\titers: 400, epoch: 6 | loss: 0.1126189\n",
      "\tspeed: 0.0152s/iter; left time: 339.7079s\n",
      "\titers: 500, epoch: 6 | loss: 0.1475457\n",
      "\tspeed: 0.0163s/iter; left time: 362.0565s\n",
      "\titers: 600, epoch: 6 | loss: 0.0907727\n",
      "\tspeed: 0.0163s/iter; left time: 361.0064s\n",
      "\titers: 700, epoch: 6 | loss: 0.1255901\n",
      "\tspeed: 0.0163s/iter; left time: 359.2997s\n",
      "\titers: 800, epoch: 6 | loss: 0.1361752\n",
      "\tspeed: 0.0163s/iter; left time: 357.6270s\n",
      "\titers: 900, epoch: 6 | loss: 0.0805942\n",
      "\tspeed: 0.0162s/iter; left time: 355.4301s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1233077\n",
      "\tspeed: 0.0163s/iter; left time: 354.8609s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1200426\n",
      "\tspeed: 0.0163s/iter; left time: 352.5746s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1243140\n",
      "\tspeed: 0.0151s/iter; left time: 325.0959s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0843925\n",
      "\tspeed: 0.0157s/iter; left time: 336.4599s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1083739\n",
      "\tspeed: 0.0163s/iter; left time: 347.3615s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0997034\n",
      "\tspeed: 0.0160s/iter; left time: 341.2456s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1574220\n",
      "\tspeed: 0.0145s/iter; left time: 306.6488s\n",
      "\titers: 1700, epoch: 6 | loss: 0.2105803\n",
      "\tspeed: 0.0159s/iter; left time: 335.3874s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1666127\n",
      "\tspeed: 0.0162s/iter; left time: 340.3463s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1489685\n",
      "\tspeed: 0.0162s/iter; left time: 338.6994s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1299897\n",
      "\tspeed: 0.0162s/iter; left time: 337.0161s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1200618\n",
      "\tspeed: 0.0162s/iter; left time: 335.3369s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1507327\n",
      "\tspeed: 0.0162s/iter; left time: 333.7412s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0790934\n",
      "\tspeed: 0.0162s/iter; left time: 332.2200s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0739198\n",
      "\tspeed: 0.0161s/iter; left time: 327.3138s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1049544\n",
      "\tspeed: 0.0146s/iter; left time: 295.6591s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1227250\n",
      "\tspeed: 0.0146s/iter; left time: 294.1300s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1376776\n",
      "\tspeed: 0.0145s/iter; left time: 291.9089s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0779197\n",
      "\tspeed: 0.0145s/iter; left time: 290.1375s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1760439\n",
      "\tspeed: 0.0145s/iter; left time: 288.3935s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0997921\n",
      "\tspeed: 0.0145s/iter; left time: 287.0355s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1150519\n",
      "\tspeed: 0.0145s/iter; left time: 285.3221s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1161397\n",
      "\tspeed: 0.0145s/iter; left time: 283.8201s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1823420\n",
      "\tspeed: 0.0145s/iter; left time: 282.4394s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1440495\n",
      "\tspeed: 0.0145s/iter; left time: 281.0306s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1222128\n",
      "\tspeed: 0.0145s/iter; left time: 279.4174s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1714308\n",
      "\tspeed: 0.0145s/iter; left time: 277.6636s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0951071\n",
      "\tspeed: 0.0145s/iter; left time: 276.2468s\n",
      "\titers: 3800, epoch: 6 | loss: 0.3225097\n",
      "\tspeed: 0.0145s/iter; left time: 274.9357s\n",
      "\titers: 3900, epoch: 6 | loss: 0.2353079\n",
      "\tspeed: 0.0145s/iter; left time: 273.4047s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1300678\n",
      "\tspeed: 0.0145s/iter; left time: 271.9022s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0781664\n",
      "\tspeed: 0.0146s/iter; left time: 272.0295s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1776518\n",
      "\tspeed: 0.0146s/iter; left time: 270.7278s\n",
      "\titers: 4300, epoch: 6 | loss: 0.2048515\n",
      "\tspeed: 0.0146s/iter; left time: 269.3866s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1536963\n",
      "\tspeed: 0.0146s/iter; left time: 267.7953s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0894376\n",
      "\tspeed: 0.0146s/iter; left time: 266.3002s\n",
      "Epoch: 6 cost time: 69.67527604103088\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.1254101 Vali Loss: 0.1948203 Test Loss: 0.1948870\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1036821\n",
      "\tspeed: 0.1300s/iter; left time: 2356.0342s\n",
      "\titers: 200, epoch: 7 | loss: 0.1207518\n",
      "\tspeed: 0.0146s/iter; left time: 262.9657s\n",
      "\titers: 300, epoch: 7 | loss: 0.1047385\n",
      "\tspeed: 0.0146s/iter; left time: 261.8770s\n",
      "\titers: 400, epoch: 7 | loss: 0.1157430\n",
      "\tspeed: 0.0146s/iter; left time: 260.4297s\n",
      "\titers: 500, epoch: 7 | loss: 0.1086809\n",
      "\tspeed: 0.0146s/iter; left time: 258.5249s\n",
      "\titers: 600, epoch: 7 | loss: 0.1396945\n",
      "\tspeed: 0.0146s/iter; left time: 257.0711s\n",
      "\titers: 700, epoch: 7 | loss: 0.1439156\n",
      "\tspeed: 0.0146s/iter; left time: 255.3293s\n",
      "\titers: 800, epoch: 7 | loss: 0.1250606\n",
      "\tspeed: 0.0146s/iter; left time: 253.9384s\n",
      "\titers: 900, epoch: 7 | loss: 0.1108047\n",
      "\tspeed: 0.0146s/iter; left time: 252.4291s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0979106\n",
      "\tspeed: 0.0146s/iter; left time: 250.9886s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2156012\n",
      "\tspeed: 0.0146s/iter; left time: 249.6982s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1245021\n",
      "\tspeed: 0.0146s/iter; left time: 248.1341s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1515636\n",
      "\tspeed: 0.0146s/iter; left time: 247.6531s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0979920\n",
      "\tspeed: 0.0146s/iter; left time: 246.1666s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1152729\n",
      "\tspeed: 0.0146s/iter; left time: 244.4288s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1495885\n",
      "\tspeed: 0.0146s/iter; left time: 243.0009s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1660379\n",
      "\tspeed: 0.0146s/iter; left time: 241.3451s\n",
      "\titers: 1800, epoch: 7 | loss: 0.1470367\n",
      "\tspeed: 0.0146s/iter; left time: 239.9501s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0782897\n",
      "\tspeed: 0.0146s/iter; left time: 238.4328s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0857420\n",
      "\tspeed: 0.0146s/iter; left time: 236.8201s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0605544\n",
      "\tspeed: 0.0146s/iter; left time: 235.4129s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1274098\n",
      "\tspeed: 0.0146s/iter; left time: 234.1563s\n",
      "\titers: 2300, epoch: 7 | loss: 0.1153464\n",
      "\tspeed: 0.0146s/iter; left time: 232.9055s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1657291\n",
      "\tspeed: 0.0146s/iter; left time: 231.2867s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1466566\n",
      "\tspeed: 0.0147s/iter; left time: 230.4822s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0608111\n",
      "\tspeed: 0.0147s/iter; left time: 228.9500s\n",
      "\titers: 2700, epoch: 7 | loss: 0.1040281\n",
      "\tspeed: 0.0146s/iter; left time: 226.7290s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0797585\n",
      "\tspeed: 0.0146s/iter; left time: 225.4524s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0975097\n",
      "\tspeed: 0.0146s/iter; left time: 223.9053s\n",
      "\titers: 3000, epoch: 7 | loss: 0.1017438\n",
      "\tspeed: 0.0146s/iter; left time: 222.4137s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0833483\n",
      "\tspeed: 0.0147s/iter; left time: 221.8325s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0942332\n",
      "\tspeed: 0.0147s/iter; left time: 220.5804s\n",
      "\titers: 3300, epoch: 7 | loss: 0.1195173\n",
      "\tspeed: 0.0147s/iter; left time: 219.0723s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1258307\n",
      "\tspeed: 0.0147s/iter; left time: 217.6961s\n",
      "\titers: 3500, epoch: 7 | loss: 0.3660537\n",
      "\tspeed: 0.0147s/iter; left time: 216.2313s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0867909\n",
      "\tspeed: 0.0147s/iter; left time: 214.6621s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1350448\n",
      "\tspeed: 0.0147s/iter; left time: 213.2754s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1550996\n",
      "\tspeed: 0.0147s/iter; left time: 211.7790s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0911356\n",
      "\tspeed: 0.0147s/iter; left time: 210.2743s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1220208\n",
      "\tspeed: 0.0147s/iter; left time: 208.8291s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0783033\n",
      "\tspeed: 0.0147s/iter; left time: 207.3448s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1116060\n",
      "\tspeed: 0.0147s/iter; left time: 205.8638s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1071366\n",
      "\tspeed: 0.0147s/iter; left time: 204.3897s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0748180\n",
      "\tspeed: 0.0147s/iter; left time: 203.0730s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0999266\n",
      "\tspeed: 0.0147s/iter; left time: 201.4611s\n",
      "Epoch: 7 cost time: 66.90370655059814\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.1233644 Vali Loss: 0.1948174 Test Loss: 0.1980146\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "mse:0.17627310752868652, mae:0.2662774920463562\n"
     ]
    }
   ],
   "source": [
    "# patience = 5\n",
    "!bash ./scripts/TimeXer.sh --features M --predictor wind_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f649366",
   "metadata": {},
   "source": [
    "### solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1880594\n",
      "\tspeed: 0.0275s/iter; left time: 1248.6292s\n",
      "\titers: 200, epoch: 1 | loss: 0.1026268\n",
      "\tspeed: 0.0156s/iter; left time: 708.7919s\n",
      "\titers: 300, epoch: 1 | loss: 0.0646233\n",
      "\tspeed: 0.0156s/iter; left time: 705.6853s\n",
      "\titers: 400, epoch: 1 | loss: 0.1436839\n",
      "\tspeed: 0.0156s/iter; left time: 705.5070s\n",
      "\titers: 500, epoch: 1 | loss: 0.0337003\n",
      "\tspeed: 0.0156s/iter; left time: 702.9494s\n",
      "\titers: 600, epoch: 1 | loss: 0.0588938\n",
      "\tspeed: 0.0156s/iter; left time: 701.7160s\n",
      "\titers: 700, epoch: 1 | loss: 0.1358326\n",
      "\tspeed: 0.0156s/iter; left time: 700.8568s\n",
      "\titers: 800, epoch: 1 | loss: 0.5110136\n",
      "\tspeed: 0.0144s/iter; left time: 646.4893s\n",
      "\titers: 900, epoch: 1 | loss: 0.2601639\n",
      "\tspeed: 0.0141s/iter; left time: 630.0140s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0690488\n",
      "\tspeed: 0.0141s/iter; left time: 627.0330s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0552730\n",
      "\tspeed: 0.0141s/iter; left time: 626.1040s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0722296\n",
      "\tspeed: 0.0141s/iter; left time: 624.7468s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2892447\n",
      "\tspeed: 0.0141s/iter; left time: 622.6923s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0230870\n",
      "\tspeed: 0.0141s/iter; left time: 620.6505s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1299016\n",
      "\tspeed: 0.0141s/iter; left time: 619.9806s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1390953\n",
      "\tspeed: 0.0141s/iter; left time: 619.0121s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3027755\n",
      "\tspeed: 0.0141s/iter; left time: 617.7583s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0582245\n",
      "\tspeed: 0.0141s/iter; left time: 615.9644s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1958276\n",
      "\tspeed: 0.0141s/iter; left time: 613.6658s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4077032\n",
      "\tspeed: 0.0141s/iter; left time: 612.6056s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2276900\n",
      "\tspeed: 0.0141s/iter; left time: 611.1859s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2901473\n",
      "\tspeed: 0.0141s/iter; left time: 609.3646s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1349899\n",
      "\tspeed: 0.0141s/iter; left time: 609.1047s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1636490\n",
      "\tspeed: 0.0141s/iter; left time: 609.1700s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2848335\n",
      "\tspeed: 0.0142s/iter; left time: 611.1973s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1061540\n",
      "\tspeed: 0.0141s/iter; left time: 604.3719s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1824115\n",
      "\tspeed: 0.0141s/iter; left time: 602.6987s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1878522\n",
      "\tspeed: 0.0141s/iter; left time: 601.1332s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1878474\n",
      "\tspeed: 0.0141s/iter; left time: 600.1420s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0904252\n",
      "\tspeed: 0.0141s/iter; left time: 599.3667s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0542073\n",
      "\tspeed: 0.0141s/iter; left time: 597.4081s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1607954\n",
      "\tspeed: 0.0141s/iter; left time: 595.7384s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0310854\n",
      "\tspeed: 0.0141s/iter; left time: 594.3359s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2267053\n",
      "\tspeed: 0.0141s/iter; left time: 592.9299s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0824267\n",
      "\tspeed: 0.0140s/iter; left time: 590.4385s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0969501\n",
      "\tspeed: 0.0141s/iter; left time: 591.6716s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0843461\n",
      "\tspeed: 0.0141s/iter; left time: 590.4930s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1617526\n",
      "\tspeed: 0.0141s/iter; left time: 588.6186s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3581395\n",
      "\tspeed: 0.0141s/iter; left time: 587.4137s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3739244\n",
      "\tspeed: 0.0141s/iter; left time: 586.2920s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0748074\n",
      "\tspeed: 0.0141s/iter; left time: 584.7506s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0804443\n",
      "\tspeed: 0.0141s/iter; left time: 583.0831s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1978168\n",
      "\tspeed: 0.0141s/iter; left time: 581.6874s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0293587\n",
      "\tspeed: 0.0141s/iter; left time: 580.1808s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0607482\n",
      "\tspeed: 0.0141s/iter; left time: 578.6031s\n",
      "Epoch: 1 cost time: 66.50196671485901\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1760965 Vali Loss: 0.0375671 Test Loss: 0.1243711\n",
      "Validation loss decreased (inf --> 0.037567).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1023001\n",
      "\tspeed: 0.1300s/iter; left time: 5317.1910s\n",
      "\titers: 200, epoch: 2 | loss: 0.0443824\n",
      "\tspeed: 0.0141s/iter; left time: 574.8172s\n",
      "\titers: 300, epoch: 2 | loss: 0.0782190\n",
      "\tspeed: 0.0141s/iter; left time: 574.3139s\n",
      "\titers: 400, epoch: 2 | loss: 0.0906417\n",
      "\tspeed: 0.0141s/iter; left time: 571.8968s\n",
      "\titers: 500, epoch: 2 | loss: 0.3023317\n",
      "\tspeed: 0.0141s/iter; left time: 570.2989s\n",
      "\titers: 600, epoch: 2 | loss: 0.0732646\n",
      "\tspeed: 0.0140s/iter; left time: 566.6011s\n",
      "\titers: 700, epoch: 2 | loss: 0.0909683\n",
      "\tspeed: 0.0140s/iter; left time: 564.1419s\n",
      "\titers: 800, epoch: 2 | loss: 0.0453902\n",
      "\tspeed: 0.0140s/iter; left time: 563.6103s\n",
      "\titers: 900, epoch: 2 | loss: 0.0473101\n",
      "\tspeed: 0.0140s/iter; left time: 561.6135s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1697372\n",
      "\tspeed: 0.0140s/iter; left time: 559.9297s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2098863\n",
      "\tspeed: 0.0140s/iter; left time: 560.0308s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1173484\n",
      "\tspeed: 0.0140s/iter; left time: 558.1006s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0964068\n",
      "\tspeed: 0.0140s/iter; left time: 556.8443s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1164205\n",
      "\tspeed: 0.0140s/iter; left time: 555.3369s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0471032\n",
      "\tspeed: 0.0140s/iter; left time: 553.8773s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1479586\n",
      "\tspeed: 0.0140s/iter; left time: 552.7717s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2439645\n",
      "\tspeed: 0.0140s/iter; left time: 551.1637s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1059873\n",
      "\tspeed: 0.0140s/iter; left time: 549.7013s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1401853\n",
      "\tspeed: 0.0140s/iter; left time: 548.4010s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0535772\n",
      "\tspeed: 0.0140s/iter; left time: 547.1667s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0827046\n",
      "\tspeed: 0.0140s/iter; left time: 545.6945s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1374192\n",
      "\tspeed: 0.0140s/iter; left time: 544.1460s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1499075\n",
      "\tspeed: 0.0140s/iter; left time: 542.7179s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1197314\n",
      "\tspeed: 0.0140s/iter; left time: 541.5109s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0720138\n",
      "\tspeed: 0.0140s/iter; left time: 540.0956s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2874545\n",
      "\tspeed: 0.0140s/iter; left time: 538.7043s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1275534\n",
      "\tspeed: 0.0140s/iter; left time: 537.3605s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0577189\n",
      "\tspeed: 0.0140s/iter; left time: 535.9178s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0497496\n",
      "\tspeed: 0.0140s/iter; left time: 534.3935s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1213172\n",
      "\tspeed: 0.0140s/iter; left time: 532.9818s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0917065\n",
      "\tspeed: 0.0140s/iter; left time: 531.5484s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2026839\n",
      "\tspeed: 0.0140s/iter; left time: 530.1944s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0497320\n",
      "\tspeed: 0.0140s/iter; left time: 528.8898s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0741029\n",
      "\tspeed: 0.0140s/iter; left time: 527.5206s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1714709\n",
      "\tspeed: 0.0140s/iter; left time: 526.0206s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1354392\n",
      "\tspeed: 0.0140s/iter; left time: 524.5420s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1481443\n",
      "\tspeed: 0.0150s/iter; left time: 560.9033s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1425242\n",
      "\tspeed: 0.0156s/iter; left time: 579.5926s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2178237\n",
      "\tspeed: 0.0156s/iter; left time: 577.8024s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2526345\n",
      "\tspeed: 0.0156s/iter; left time: 576.4737s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0959773\n",
      "\tspeed: 0.0156s/iter; left time: 574.7081s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0774052\n",
      "\tspeed: 0.0156s/iter; left time: 573.1140s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1198923\n",
      "\tspeed: 0.0156s/iter; left time: 571.5024s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1138821\n",
      "\tspeed: 0.0156s/iter; left time: 570.1370s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0380464\n",
      "\tspeed: 0.0156s/iter; left time: 568.3767s\n",
      "Epoch: 2 cost time: 65.61089420318604\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1508490 Vali Loss: 0.0334156 Test Loss: 0.1120896\n",
      "Validation loss decreased (0.037567 --> 0.033416).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1158902\n",
      "\tspeed: 0.1250s/iter; left time: 4542.8333s\n",
      "\titers: 200, epoch: 3 | loss: 0.0483347\n",
      "\tspeed: 0.0141s/iter; left time: 510.7399s\n",
      "\titers: 300, epoch: 3 | loss: 0.1121058\n",
      "\tspeed: 0.0141s/iter; left time: 509.6280s\n",
      "\titers: 400, epoch: 3 | loss: 0.1069300\n",
      "\tspeed: 0.0141s/iter; left time: 507.2340s\n",
      "\titers: 500, epoch: 3 | loss: 0.1013264\n",
      "\tspeed: 0.0141s/iter; left time: 506.7855s\n",
      "\titers: 600, epoch: 3 | loss: 0.0809711\n",
      "\tspeed: 0.0141s/iter; left time: 505.2021s\n",
      "\titers: 700, epoch: 3 | loss: 0.2910097\n",
      "\tspeed: 0.0140s/iter; left time: 501.2955s\n",
      "\titers: 800, epoch: 3 | loss: 0.1931276\n",
      "\tspeed: 0.0140s/iter; left time: 499.2007s\n",
      "\titers: 900, epoch: 3 | loss: 0.0564841\n",
      "\tspeed: 0.0140s/iter; left time: 497.5289s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1121366\n",
      "\tspeed: 0.0140s/iter; left time: 496.2771s\n",
      "\titers: 1100, epoch: 3 | loss: 0.2114139\n",
      "\tspeed: 0.0140s/iter; left time: 495.0346s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1515695\n",
      "\tspeed: 0.0140s/iter; left time: 493.7691s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2040596\n",
      "\tspeed: 0.0140s/iter; left time: 491.8207s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1711096\n",
      "\tspeed: 0.0140s/iter; left time: 490.7356s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2077705\n",
      "\tspeed: 0.0140s/iter; left time: 489.0921s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1979721\n",
      "\tspeed: 0.0140s/iter; left time: 488.4421s\n",
      "\titers: 1700, epoch: 3 | loss: 0.6470969\n",
      "\tspeed: 0.0140s/iter; left time: 486.3669s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0812857\n",
      "\tspeed: 0.0141s/iter; left time: 487.3118s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1738021\n",
      "\tspeed: 0.0141s/iter; left time: 486.6940s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2122969\n",
      "\tspeed: 0.0141s/iter; left time: 485.0172s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0888002\n",
      "\tspeed: 0.0141s/iter; left time: 483.2868s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0675884\n",
      "\tspeed: 0.0141s/iter; left time: 481.7603s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1076873\n",
      "\tspeed: 0.0141s/iter; left time: 480.3514s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0852173\n",
      "\tspeed: 0.0141s/iter; left time: 479.1208s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1263370\n",
      "\tspeed: 0.0141s/iter; left time: 477.2774s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0897379\n",
      "\tspeed: 0.0141s/iter; left time: 475.6419s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0478275\n",
      "\tspeed: 0.0141s/iter; left time: 474.3883s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1108397\n",
      "\tspeed: 0.0141s/iter; left time: 472.9988s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0762252\n",
      "\tspeed: 0.0141s/iter; left time: 472.4256s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0452943\n",
      "\tspeed: 0.0141s/iter; left time: 470.6264s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1515656\n",
      "\tspeed: 0.0140s/iter; left time: 468.0735s\n",
      "\titers: 3200, epoch: 3 | loss: 0.7125151\n",
      "\tspeed: 0.0140s/iter; left time: 466.8205s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0828298\n",
      "\tspeed: 0.0141s/iter; left time: 468.7993s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2642997\n",
      "\tspeed: 0.0140s/iter; left time: 462.7496s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0377675\n",
      "\tspeed: 0.0140s/iter; left time: 461.2807s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1297170\n",
      "\tspeed: 0.0140s/iter; left time: 459.7758s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1464807\n",
      "\tspeed: 0.0140s/iter; left time: 458.4737s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0711235\n",
      "\tspeed: 0.0149s/iter; left time: 485.0738s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1722317\n",
      "\tspeed: 0.0156s/iter; left time: 508.6697s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1867490\n",
      "\tspeed: 0.0140s/iter; left time: 455.7240s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2535416\n",
      "\tspeed: 0.0140s/iter; left time: 452.9061s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0618705\n",
      "\tspeed: 0.0140s/iter; left time: 450.3501s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0288661\n",
      "\tspeed: 0.0140s/iter; left time: 448.7555s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0905710\n",
      "\tspeed: 0.0140s/iter; left time: 448.9727s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0366086\n",
      "\tspeed: 0.0140s/iter; left time: 448.2050s\n",
      "Epoch: 3 cost time: 64.45743942260742\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1292094 Vali Loss: 0.0335928 Test Loss: 0.1101938\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0715407\n",
      "\tspeed: 0.1389s/iter; left time: 4413.5346s\n",
      "\titers: 200, epoch: 4 | loss: 0.3183874\n",
      "\tspeed: 0.0140s/iter; left time: 442.8530s\n",
      "\titers: 300, epoch: 4 | loss: 0.0261297\n",
      "\tspeed: 0.0140s/iter; left time: 441.4684s\n",
      "\titers: 400, epoch: 4 | loss: 0.1463172\n",
      "\tspeed: 0.0140s/iter; left time: 439.6093s\n",
      "\titers: 500, epoch: 4 | loss: 0.1227192\n",
      "\tspeed: 0.0140s/iter; left time: 438.4344s\n",
      "\titers: 600, epoch: 4 | loss: 0.1024016\n",
      "\tspeed: 0.0140s/iter; left time: 436.9741s\n",
      "\titers: 700, epoch: 4 | loss: 0.1774553\n",
      "\tspeed: 0.0140s/iter; left time: 435.6858s\n",
      "\titers: 800, epoch: 4 | loss: 0.1642316\n",
      "\tspeed: 0.0140s/iter; left time: 434.9632s\n",
      "\titers: 900, epoch: 4 | loss: 0.0997953\n",
      "\tspeed: 0.0140s/iter; left time: 433.4883s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1123670\n",
      "\tspeed: 0.0140s/iter; left time: 432.1392s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1835231\n",
      "\tspeed: 0.0140s/iter; left time: 430.7726s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0433758\n",
      "\tspeed: 0.0140s/iter; left time: 429.4251s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0918214\n",
      "\tspeed: 0.0140s/iter; left time: 428.2169s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0537506\n",
      "\tspeed: 0.0140s/iter; left time: 426.7631s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1954989\n",
      "\tspeed: 0.0140s/iter; left time: 425.4513s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1014705\n",
      "\tspeed: 0.0140s/iter; left time: 423.9923s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1508603\n",
      "\tspeed: 0.0140s/iter; left time: 422.6031s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0381464\n",
      "\tspeed: 0.0140s/iter; left time: 420.8505s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0788325\n",
      "\tspeed: 0.0140s/iter; left time: 419.5577s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1285593\n",
      "\tspeed: 0.0140s/iter; left time: 418.0661s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0152580\n",
      "\tspeed: 0.0140s/iter; left time: 418.1781s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1971332\n",
      "\tspeed: 0.0155s/iter; left time: 460.6573s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1511163\n",
      "\tspeed: 0.0155s/iter; left time: 459.0114s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1008988\n",
      "\tspeed: 0.0155s/iter; left time: 458.1359s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0431851\n",
      "\tspeed: 0.0156s/iter; left time: 457.3417s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0438550\n",
      "\tspeed: 0.0156s/iter; left time: 456.4446s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5332378\n",
      "\tspeed: 0.0142s/iter; left time: 413.4657s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0207551\n",
      "\tspeed: 0.0141s/iter; left time: 411.0058s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0340281\n",
      "\tspeed: 0.0141s/iter; left time: 410.1362s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0954766\n",
      "\tspeed: 0.0141s/iter; left time: 408.4626s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1000658\n",
      "\tspeed: 0.0141s/iter; left time: 406.9127s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1199103\n",
      "\tspeed: 0.0141s/iter; left time: 404.3893s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0925681\n",
      "\tspeed: 0.0141s/iter; left time: 403.0882s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0569380\n",
      "\tspeed: 0.0141s/iter; left time: 400.5436s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0395989\n",
      "\tspeed: 0.0141s/iter; left time: 398.9951s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0399831\n",
      "\tspeed: 0.0141s/iter; left time: 398.2686s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2401575\n",
      "\tspeed: 0.0141s/iter; left time: 397.4295s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0371709\n",
      "\tspeed: 0.0141s/iter; left time: 395.9459s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0938381\n",
      "\tspeed: 0.0141s/iter; left time: 394.2886s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0754569\n",
      "\tspeed: 0.0141s/iter; left time: 392.9107s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0412428\n",
      "\tspeed: 0.0141s/iter; left time: 391.7074s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0795756\n",
      "\tspeed: 0.0141s/iter; left time: 390.4351s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3650653\n",
      "\tspeed: 0.0141s/iter; left time: 389.4570s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1153900\n",
      "\tspeed: 0.0141s/iter; left time: 388.6663s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0828952\n",
      "\tspeed: 0.0141s/iter; left time: 387.2627s\n",
      "Epoch: 4 cost time: 65.00205492973328\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1124171 Vali Loss: 0.0332162 Test Loss: 0.1152915\n",
      "Validation loss decreased (0.033416 --> 0.033216).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0760190\n",
      "\tspeed: 0.1287s/iter; left time: 3503.7020s\n",
      "\titers: 200, epoch: 5 | loss: 0.0606147\n",
      "\tspeed: 0.0141s/iter; left time: 383.3160s\n",
      "\titers: 300, epoch: 5 | loss: 0.0887262\n",
      "\tspeed: 0.0141s/iter; left time: 382.1917s\n",
      "\titers: 400, epoch: 5 | loss: 0.0528458\n",
      "\tspeed: 0.0141s/iter; left time: 380.2149s\n",
      "\titers: 500, epoch: 5 | loss: 0.1164466\n",
      "\tspeed: 0.0141s/iter; left time: 378.5217s\n",
      "\titers: 600, epoch: 5 | loss: 0.0153286\n",
      "\tspeed: 0.0141s/iter; left time: 376.1896s\n",
      "\titers: 700, epoch: 5 | loss: 0.0424965\n",
      "\tspeed: 0.0141s/iter; left time: 374.5268s\n",
      "\titers: 800, epoch: 5 | loss: 0.1152716\n",
      "\tspeed: 0.0141s/iter; left time: 374.3391s\n",
      "\titers: 900, epoch: 5 | loss: 0.0887059\n",
      "\tspeed: 0.0141s/iter; left time: 372.9212s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1392863\n",
      "\tspeed: 0.0141s/iter; left time: 370.6381s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0510879\n",
      "\tspeed: 0.0141s/iter; left time: 368.6709s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0901645\n",
      "\tspeed: 0.0141s/iter; left time: 367.4877s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0892580\n",
      "\tspeed: 0.0141s/iter; left time: 366.4038s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1008420\n",
      "\tspeed: 0.0140s/iter; left time: 364.2730s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1002330\n",
      "\tspeed: 0.0140s/iter; left time: 362.8022s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0562704\n",
      "\tspeed: 0.0140s/iter; left time: 361.3732s\n",
      "\titers: 1700, epoch: 5 | loss: 0.2267590\n",
      "\tspeed: 0.0140s/iter; left time: 360.0644s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0861740\n",
      "\tspeed: 0.0140s/iter; left time: 358.5252s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0315677\n",
      "\tspeed: 0.0140s/iter; left time: 357.2077s\n",
      "\titers: 2000, epoch: 5 | loss: 0.6385766\n",
      "\tspeed: 0.0140s/iter; left time: 355.7843s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0938682\n",
      "\tspeed: 0.0140s/iter; left time: 354.3951s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0757801\n",
      "\tspeed: 0.0140s/iter; left time: 353.0275s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0472356\n",
      "\tspeed: 0.0141s/iter; left time: 352.0766s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0841407\n",
      "\tspeed: 0.0141s/iter; left time: 351.6509s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0699744\n",
      "\tspeed: 0.0141s/iter; left time: 350.1224s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0792382\n",
      "\tspeed: 0.0148s/iter; left time: 365.3862s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1085688\n",
      "\tspeed: 0.0141s/iter; left time: 346.5267s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0712323\n",
      "\tspeed: 0.0141s/iter; left time: 345.0546s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0493310\n",
      "\tspeed: 0.0141s/iter; left time: 343.4614s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0688816\n",
      "\tspeed: 0.0141s/iter; left time: 342.3841s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0443238\n",
      "\tspeed: 0.0141s/iter; left time: 341.0082s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0271417\n",
      "\tspeed: 0.0141s/iter; left time: 339.5289s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0704094\n",
      "\tspeed: 0.0141s/iter; left time: 338.1490s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0721979\n",
      "\tspeed: 0.0141s/iter; left time: 336.9085s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0464406\n",
      "\tspeed: 0.0141s/iter; left time: 335.5199s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0807444\n",
      "\tspeed: 0.0141s/iter; left time: 334.0727s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0606233\n",
      "\tspeed: 0.0141s/iter; left time: 332.6110s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0988854\n",
      "\tspeed: 0.0148s/iter; left time: 348.7252s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0497513\n",
      "\tspeed: 0.0156s/iter; left time: 365.3154s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0444475\n",
      "\tspeed: 0.0156s/iter; left time: 363.5592s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0360230\n",
      "\tspeed: 0.0156s/iter; left time: 362.0471s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0605384\n",
      "\tspeed: 0.0156s/iter; left time: 360.5753s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1196653\n",
      "\tspeed: 0.0156s/iter; left time: 358.8328s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0210007\n",
      "\tspeed: 0.0156s/iter; left time: 357.3766s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0578803\n",
      "\tspeed: 0.0156s/iter; left time: 355.8036s\n",
      "Epoch: 5 cost time: 65.66826152801514\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1022209 Vali Loss: 0.0329238 Test Loss: 0.1117936\n",
      "Validation loss decreased (0.033216 --> 0.032924).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0450002\n",
      "\tspeed: 0.1470s/iter; left time: 3333.2805s\n",
      "\titers: 200, epoch: 6 | loss: 0.1656143\n",
      "\tspeed: 0.0156s/iter; left time: 351.9768s\n",
      "\titers: 300, epoch: 6 | loss: 0.1255506\n",
      "\tspeed: 0.0156s/iter; left time: 350.4914s\n",
      "\titers: 400, epoch: 6 | loss: 0.0901936\n",
      "\tspeed: 0.0156s/iter; left time: 349.1035s\n",
      "\titers: 500, epoch: 6 | loss: 0.1622164\n",
      "\tspeed: 0.0156s/iter; left time: 347.7459s\n",
      "\titers: 600, epoch: 6 | loss: 0.1800657\n",
      "\tspeed: 0.0156s/iter; left time: 346.3562s\n",
      "\titers: 700, epoch: 6 | loss: 0.0323191\n",
      "\tspeed: 0.0156s/iter; left time: 344.7947s\n",
      "\titers: 800, epoch: 6 | loss: 0.0680526\n",
      "\tspeed: 0.0156s/iter; left time: 343.3357s\n",
      "\titers: 900, epoch: 6 | loss: 0.0559860\n",
      "\tspeed: 0.0156s/iter; left time: 341.4412s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1562211\n",
      "\tspeed: 0.0156s/iter; left time: 339.6795s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0526574\n",
      "\tspeed: 0.0156s/iter; left time: 338.1024s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1110927\n",
      "\tspeed: 0.0156s/iter; left time: 336.6703s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1017463\n",
      "\tspeed: 0.0156s/iter; left time: 335.0289s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0743956\n",
      "\tspeed: 0.0156s/iter; left time: 333.3573s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0771593\n",
      "\tspeed: 0.0156s/iter; left time: 332.0639s\n",
      "\titers: 1600, epoch: 6 | loss: 0.2069089\n",
      "\tspeed: 0.0156s/iter; left time: 330.2213s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0555285\n",
      "\tspeed: 0.0142s/iter; left time: 299.6150s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2228368\n",
      "\tspeed: 0.0140s/iter; left time: 294.0962s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1397611\n",
      "\tspeed: 0.0140s/iter; left time: 292.7424s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1799869\n",
      "\tspeed: 0.0140s/iter; left time: 291.2920s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1252721\n",
      "\tspeed: 0.0140s/iter; left time: 289.9017s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1276691\n",
      "\tspeed: 0.0140s/iter; left time: 288.4229s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1206822\n",
      "\tspeed: 0.0140s/iter; left time: 286.9453s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0605967\n",
      "\tspeed: 0.0140s/iter; left time: 285.6854s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0665944\n",
      "\tspeed: 0.0140s/iter; left time: 284.4141s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0597489\n",
      "\tspeed: 0.0140s/iter; left time: 282.8643s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0679836\n",
      "\tspeed: 0.0149s/iter; left time: 299.3377s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0936124\n",
      "\tspeed: 0.0155s/iter; left time: 309.7154s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2157032\n",
      "\tspeed: 0.0155s/iter; left time: 308.4663s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0618645\n",
      "\tspeed: 0.0155s/iter; left time: 306.5562s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0866964\n",
      "\tspeed: 0.0144s/iter; left time: 283.1791s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0981589\n",
      "\tspeed: 0.0140s/iter; left time: 274.1211s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0786688\n",
      "\tspeed: 0.0140s/iter; left time: 272.7884s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0262990\n",
      "\tspeed: 0.0140s/iter; left time: 271.2789s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0154320\n",
      "\tspeed: 0.0140s/iter; left time: 270.0281s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1149451\n",
      "\tspeed: 0.0140s/iter; left time: 268.5190s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1622312\n",
      "\tspeed: 0.0140s/iter; left time: 267.0255s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1367102\n",
      "\tspeed: 0.0140s/iter; left time: 265.6524s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0466978\n",
      "\tspeed: 0.0140s/iter; left time: 264.3235s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1673133\n",
      "\tspeed: 0.0140s/iter; left time: 262.7915s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0167590\n",
      "\tspeed: 0.0140s/iter; left time: 261.4807s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0377559\n",
      "\tspeed: 0.0140s/iter; left time: 260.1084s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1641956\n",
      "\tspeed: 0.0140s/iter; left time: 258.7252s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0710560\n",
      "\tspeed: 0.0140s/iter; left time: 257.4130s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0925189\n",
      "\tspeed: 0.0140s/iter; left time: 256.0318s\n",
      "Epoch: 6 cost time: 67.2397928237915\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0976002 Vali Loss: 0.0325405 Test Loss: 0.1141897\n",
      "Validation loss decreased (0.032924 --> 0.032541).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1231600\n",
      "\tspeed: 0.1273s/iter; left time: 2307.1736s\n",
      "\titers: 200, epoch: 7 | loss: 0.2148099\n",
      "\tspeed: 0.0156s/iter; left time: 281.7591s\n",
      "\titers: 300, epoch: 7 | loss: 0.2253210\n",
      "\tspeed: 0.0146s/iter; left time: 261.5754s\n",
      "\titers: 400, epoch: 7 | loss: 0.0709067\n",
      "\tspeed: 0.0141s/iter; left time: 251.6136s\n",
      "\titers: 500, epoch: 7 | loss: 0.0464612\n",
      "\tspeed: 0.0141s/iter; left time: 249.9168s\n",
      "\titers: 600, epoch: 7 | loss: 0.0750231\n",
      "\tspeed: 0.0141s/iter; left time: 248.9543s\n",
      "\titers: 700, epoch: 7 | loss: 0.0134326\n",
      "\tspeed: 0.0141s/iter; left time: 247.2077s\n",
      "\titers: 800, epoch: 7 | loss: 0.0560909\n",
      "\tspeed: 0.0141s/iter; left time: 245.5522s\n",
      "\titers: 900, epoch: 7 | loss: 0.1757647\n",
      "\tspeed: 0.0141s/iter; left time: 244.2459s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0409092\n",
      "\tspeed: 0.0141s/iter; left time: 242.7093s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2195573\n",
      "\tspeed: 0.0141s/iter; left time: 241.4224s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0549210\n",
      "\tspeed: 0.0141s/iter; left time: 239.9214s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0869845\n",
      "\tspeed: 0.0141s/iter; left time: 238.5832s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0948825\n",
      "\tspeed: 0.0141s/iter; left time: 237.3885s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0828277\n",
      "\tspeed: 0.0141s/iter; left time: 235.7797s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0600317\n",
      "\tspeed: 0.0141s/iter; left time: 233.9854s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0466159\n",
      "\tspeed: 0.0141s/iter; left time: 232.2168s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0909457\n",
      "\tspeed: 0.0141s/iter; left time: 231.6062s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0338065\n",
      "\tspeed: 0.0141s/iter; left time: 229.8805s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0433758\n",
      "\tspeed: 0.0141s/iter; left time: 228.4594s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0520106\n",
      "\tspeed: 0.0140s/iter; left time: 226.0225s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0500247\n",
      "\tspeed: 0.0140s/iter; left time: 224.4005s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0494973\n",
      "\tspeed: 0.0140s/iter; left time: 222.9138s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0766317\n",
      "\tspeed: 0.0140s/iter; left time: 221.5104s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1893050\n",
      "\tspeed: 0.0140s/iter; left time: 220.3723s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1082218\n",
      "\tspeed: 0.0140s/iter; left time: 219.0463s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0427964\n",
      "\tspeed: 0.0140s/iter; left time: 217.4639s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0476387\n",
      "\tspeed: 0.0140s/iter; left time: 215.9571s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0976878\n",
      "\tspeed: 0.0140s/iter; left time: 214.5425s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0653234\n",
      "\tspeed: 0.0140s/iter; left time: 213.0316s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0167749\n",
      "\tspeed: 0.0140s/iter; left time: 211.8765s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1451631\n",
      "\tspeed: 0.0140s/iter; left time: 210.5870s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0716250\n",
      "\tspeed: 0.0140s/iter; left time: 209.2531s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0365382\n",
      "\tspeed: 0.0140s/iter; left time: 207.7623s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0504188\n",
      "\tspeed: 0.0140s/iter; left time: 206.3924s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0595517\n",
      "\tspeed: 0.0140s/iter; left time: 204.9538s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1771957\n",
      "\tspeed: 0.0140s/iter; left time: 203.5658s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0391880\n",
      "\tspeed: 0.0140s/iter; left time: 202.1249s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0750370\n",
      "\tspeed: 0.0140s/iter; left time: 200.6985s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0633304\n",
      "\tspeed: 0.0140s/iter; left time: 199.3768s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0733753\n",
      "\tspeed: 0.0140s/iter; left time: 197.9406s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0280967\n",
      "\tspeed: 0.0140s/iter; left time: 196.4342s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0214055\n",
      "\tspeed: 0.0140s/iter; left time: 194.9664s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0241183\n",
      "\tspeed: 0.0140s/iter; left time: 193.6389s\n",
      "\titers: 4500, epoch: 7 | loss: 0.1046705\n",
      "\tspeed: 0.0140s/iter; left time: 192.2203s\n",
      "Epoch: 7 cost time: 64.6275475025177\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0943563 Vali Loss: 0.0324630 Test Loss: 0.1145989\n",
      "Validation loss decreased (0.032541 --> 0.032463).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0848830\n",
      "\tspeed: 0.1248s/iter; left time: 1693.6446s\n",
      "\titers: 200, epoch: 8 | loss: 0.0747153\n",
      "\tspeed: 0.0141s/iter; left time: 189.9007s\n",
      "\titers: 300, epoch: 8 | loss: 0.1423734\n",
      "\tspeed: 0.0141s/iter; left time: 188.3113s\n",
      "\titers: 400, epoch: 8 | loss: 0.0448341\n",
      "\tspeed: 0.0141s/iter; left time: 186.8873s\n",
      "\titers: 500, epoch: 8 | loss: 0.1960470\n",
      "\tspeed: 0.0141s/iter; left time: 185.5022s\n",
      "\titers: 600, epoch: 8 | loss: 0.0745049\n",
      "\tspeed: 0.0141s/iter; left time: 183.9845s\n",
      "\titers: 700, epoch: 8 | loss: 0.1958699\n",
      "\tspeed: 0.0141s/iter; left time: 182.5643s\n",
      "\titers: 800, epoch: 8 | loss: 0.1754576\n",
      "\tspeed: 0.0141s/iter; left time: 181.2591s\n",
      "\titers: 900, epoch: 8 | loss: 0.0731587\n",
      "\tspeed: 0.0141s/iter; left time: 179.9163s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0623545\n",
      "\tspeed: 0.0141s/iter; left time: 178.5158s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0859997\n",
      "\tspeed: 0.0141s/iter; left time: 177.0647s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1315812\n",
      "\tspeed: 0.0141s/iter; left time: 175.7210s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3710501\n",
      "\tspeed: 0.0141s/iter; left time: 174.3712s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0372819\n",
      "\tspeed: 0.0141s/iter; left time: 172.9153s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0877636\n",
      "\tspeed: 0.0141s/iter; left time: 171.4105s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1333730\n",
      "\tspeed: 0.0141s/iter; left time: 170.0289s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0269792\n",
      "\tspeed: 0.0141s/iter; left time: 168.5523s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1270098\n",
      "\tspeed: 0.0141s/iter; left time: 167.1651s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1052443\n",
      "\tspeed: 0.0141s/iter; left time: 165.6610s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1125871\n",
      "\tspeed: 0.0141s/iter; left time: 164.3720s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0627683\n",
      "\tspeed: 0.0141s/iter; left time: 162.9806s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0578786\n",
      "\tspeed: 0.0141s/iter; left time: 161.5925s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1181847\n",
      "\tspeed: 0.0141s/iter; left time: 160.1501s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0324623\n",
      "\tspeed: 0.0141s/iter; left time: 158.6159s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0823186\n",
      "\tspeed: 0.0141s/iter; left time: 157.1621s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0565674\n",
      "\tspeed: 0.0141s/iter; left time: 155.7515s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0378775\n",
      "\tspeed: 0.0141s/iter; left time: 154.3770s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0499634\n",
      "\tspeed: 0.0141s/iter; left time: 152.9773s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0747446\n",
      "\tspeed: 0.0141s/iter; left time: 151.5798s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2936129\n",
      "\tspeed: 0.0141s/iter; left time: 150.2032s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1718150\n",
      "\tspeed: 0.0141s/iter; left time: 148.7939s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1334103\n",
      "\tspeed: 0.0141s/iter; left time: 147.3169s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0416132\n",
      "\tspeed: 0.0141s/iter; left time: 145.9686s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0750119\n",
      "\tspeed: 0.0141s/iter; left time: 144.4941s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1724584\n",
      "\tspeed: 0.0141s/iter; left time: 143.1038s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0334292\n",
      "\tspeed: 0.0141s/iter; left time: 141.7084s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2519407\n",
      "\tspeed: 0.0141s/iter; left time: 140.4445s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0665036\n",
      "\tspeed: 0.0141s/iter; left time: 138.9090s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0873349\n",
      "\tspeed: 0.0141s/iter; left time: 137.4483s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0500058\n",
      "\tspeed: 0.0141s/iter; left time: 136.1155s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0285778\n",
      "\tspeed: 0.0141s/iter; left time: 134.7047s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0990806\n",
      "\tspeed: 0.0141s/iter; left time: 133.2325s\n",
      "\titers: 4300, epoch: 8 | loss: 0.1072617\n",
      "\tspeed: 0.0141s/iter; left time: 131.8301s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1298561\n",
      "\tspeed: 0.0141s/iter; left time: 130.7289s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1279658\n",
      "\tspeed: 0.0141s/iter; left time: 129.5604s\n",
      "Epoch: 8 cost time: 64.43225264549255\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0942525 Vali Loss: 0.0323278 Test Loss: 0.1149161\n",
      "Validation loss decreased (0.032463 --> 0.032328).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0558414\n",
      "\tspeed: 0.1299s/iter; left time: 1170.4769s\n",
      "\titers: 200, epoch: 9 | loss: 0.1115082\n",
      "\tspeed: 0.0162s/iter; left time: 143.9443s\n",
      "\titers: 300, epoch: 9 | loss: 0.0498636\n",
      "\tspeed: 0.0162s/iter; left time: 142.4163s\n",
      "\titers: 400, epoch: 9 | loss: 0.1202771\n",
      "\tspeed: 0.0161s/iter; left time: 140.4012s\n",
      "\titers: 500, epoch: 9 | loss: 0.0728375\n",
      "\tspeed: 0.0161s/iter; left time: 138.7395s\n",
      "\titers: 600, epoch: 9 | loss: 0.0285222\n",
      "\tspeed: 0.0161s/iter; left time: 137.1422s\n",
      "\titers: 700, epoch: 9 | loss: 0.1468434\n",
      "\tspeed: 0.0161s/iter; left time: 135.4213s\n",
      "\titers: 800, epoch: 9 | loss: 0.0328571\n",
      "\tspeed: 0.0161s/iter; left time: 133.8331s\n",
      "\titers: 900, epoch: 9 | loss: 0.0442758\n",
      "\tspeed: 0.0161s/iter; left time: 132.1438s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0867593\n",
      "\tspeed: 0.0161s/iter; left time: 130.6283s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0509694\n",
      "\tspeed: 0.0161s/iter; left time: 129.0621s\n",
      "\titers: 1200, epoch: 9 | loss: 0.2063788\n",
      "\tspeed: 0.0161s/iter; left time: 127.4540s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0343674\n",
      "\tspeed: 0.0161s/iter; left time: 126.0292s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2436148\n",
      "\tspeed: 0.0161s/iter; left time: 124.2987s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1175912\n",
      "\tspeed: 0.0161s/iter; left time: 122.5980s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0440759\n",
      "\tspeed: 0.0161s/iter; left time: 120.9674s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0942924\n",
      "\tspeed: 0.0161s/iter; left time: 119.4502s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0417999\n",
      "\tspeed: 0.0161s/iter; left time: 117.8747s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0244172\n",
      "\tspeed: 0.0161s/iter; left time: 116.2124s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0554739\n",
      "\tspeed: 0.0161s/iter; left time: 114.5570s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0480510\n",
      "\tspeed: 0.0161s/iter; left time: 112.9981s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1111191\n",
      "\tspeed: 0.0161s/iter; left time: 111.3429s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1197103\n",
      "\tspeed: 0.0161s/iter; left time: 109.7121s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0301762\n",
      "\tspeed: 0.0161s/iter; left time: 108.1538s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0380156\n",
      "\tspeed: 0.0161s/iter; left time: 106.5709s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0372587\n",
      "\tspeed: 0.0161s/iter; left time: 104.9707s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0458427\n",
      "\tspeed: 0.0161s/iter; left time: 103.3576s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1336535\n",
      "\tspeed: 0.0161s/iter; left time: 101.7719s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0538796\n",
      "\tspeed: 0.0161s/iter; left time: 100.1608s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1160163\n",
      "\tspeed: 0.0161s/iter; left time: 98.4699s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0348445\n",
      "\tspeed: 0.0161s/iter; left time: 96.6334s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0721262\n",
      "\tspeed: 0.0161s/iter; left time: 94.9187s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2242307\n",
      "\tspeed: 0.0160s/iter; left time: 93.2137s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1041888\n",
      "\tspeed: 0.0160s/iter; left time: 91.5685s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0874932\n",
      "\tspeed: 0.0160s/iter; left time: 89.9376s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0480979\n",
      "\tspeed: 0.0160s/iter; left time: 88.3442s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0998791\n",
      "\tspeed: 0.0160s/iter; left time: 86.7629s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1642730\n",
      "\tspeed: 0.0160s/iter; left time: 85.2032s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0941878\n",
      "\tspeed: 0.0156s/iter; left time: 81.1587s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0933366\n",
      "\tspeed: 0.0143s/iter; left time: 72.8805s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0576156\n",
      "\tspeed: 0.0156s/iter; left time: 78.0594s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0647083\n",
      "\tspeed: 0.0156s/iter; left time: 76.4135s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0746929\n",
      "\tspeed: 0.0150s/iter; left time: 72.0763s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0418253\n",
      "\tspeed: 0.0151s/iter; left time: 71.2279s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0262634\n",
      "\tspeed: 0.0156s/iter; left time: 71.7442s\n",
      "Epoch: 9 cost time: 72.90247130393982\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0924521 Vali Loss: 0.0324080 Test Loss: 0.1145802\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0571351\n",
      "\tspeed: 0.1324s/iter; left time: 589.9950s\n",
      "\titers: 200, epoch: 10 | loss: 0.0641491\n",
      "\tspeed: 0.0142s/iter; left time: 61.6976s\n",
      "\titers: 300, epoch: 10 | loss: 0.0434622\n",
      "\tspeed: 0.0142s/iter; left time: 60.3069s\n",
      "\titers: 400, epoch: 10 | loss: 0.0506059\n",
      "\tspeed: 0.0142s/iter; left time: 58.8448s\n",
      "\titers: 500, epoch: 10 | loss: 0.0949381\n",
      "\tspeed: 0.0141s/iter; left time: 57.3550s\n",
      "\titers: 600, epoch: 10 | loss: 0.0799008\n",
      "\tspeed: 0.0141s/iter; left time: 55.7610s\n",
      "\titers: 700, epoch: 10 | loss: 0.0466813\n",
      "\tspeed: 0.0141s/iter; left time: 54.3023s\n",
      "\titers: 800, epoch: 10 | loss: 0.1733008\n",
      "\tspeed: 0.0141s/iter; left time: 52.8312s\n",
      "\titers: 900, epoch: 10 | loss: 0.1050396\n",
      "\tspeed: 0.0141s/iter; left time: 51.4405s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0793241\n",
      "\tspeed: 0.0141s/iter; left time: 50.0341s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0895304\n",
      "\tspeed: 0.0141s/iter; left time: 48.6143s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0637012\n",
      "\tspeed: 0.0141s/iter; left time: 47.2171s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0465128\n",
      "\tspeed: 0.0141s/iter; left time: 45.8484s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0738366\n",
      "\tspeed: 0.0141s/iter; left time: 44.4130s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2594577\n",
      "\tspeed: 0.0141s/iter; left time: 43.0075s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0961117\n",
      "\tspeed: 0.0141s/iter; left time: 41.6559s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0226820\n",
      "\tspeed: 0.0141s/iter; left time: 40.1741s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0990813\n",
      "\tspeed: 0.0141s/iter; left time: 38.7600s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1480603\n",
      "\tspeed: 0.0141s/iter; left time: 37.3400s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0763314\n",
      "\tspeed: 0.0141s/iter; left time: 35.9570s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0462448\n",
      "\tspeed: 0.0141s/iter; left time: 34.5552s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0668825\n",
      "\tspeed: 0.0141s/iter; left time: 33.1721s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1318017\n",
      "\tspeed: 0.0141s/iter; left time: 31.7517s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0600385\n",
      "\tspeed: 0.0141s/iter; left time: 30.3366s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2491900\n",
      "\tspeed: 0.0141s/iter; left time: 28.9228s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1195425\n",
      "\tspeed: 0.0141s/iter; left time: 27.5452s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0715698\n",
      "\tspeed: 0.0141s/iter; left time: 26.1535s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0682379\n",
      "\tspeed: 0.0141s/iter; left time: 24.7047s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0618594\n",
      "\tspeed: 0.0141s/iter; left time: 23.3386s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0510329\n",
      "\tspeed: 0.0141s/iter; left time: 21.9211s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0657394\n",
      "\tspeed: 0.0141s/iter; left time: 20.5167s\n",
      "\titers: 3200, epoch: 10 | loss: 0.1013830\n",
      "\tspeed: 0.0141s/iter; left time: 19.0944s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0883707\n",
      "\tspeed: 0.0141s/iter; left time: 17.6920s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1538824\n",
      "\tspeed: 0.0141s/iter; left time: 16.2909s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0263022\n",
      "\tspeed: 0.0141s/iter; left time: 14.8718s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1661120\n",
      "\tspeed: 0.0141s/iter; left time: 13.4755s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0855708\n",
      "\tspeed: 0.0141s/iter; left time: 12.0655s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0612805\n",
      "\tspeed: 0.0141s/iter; left time: 10.6560s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0270694\n",
      "\tspeed: 0.0141s/iter; left time: 9.2439s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0293280\n",
      "\tspeed: 0.0141s/iter; left time: 7.8409s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1422729\n",
      "\tspeed: 0.0141s/iter; left time: 6.4247s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0434198\n",
      "\tspeed: 0.0141s/iter; left time: 5.0166s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0309585\n",
      "\tspeed: 0.0141s/iter; left time: 3.6071s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0500935\n",
      "\tspeed: 0.0141s/iter; left time: 2.1984s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1392013\n",
      "\tspeed: 0.0141s/iter; left time: 0.7890s\n",
      "Epoch: 10 cost time: 64.43008661270142\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0928382 Vali Loss: 0.0324397 Test Loss: 0.1143354\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11495085060596466, mae:0.2000247836112976\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features S --predictor solar_forecast,total_load --enc_in 1 --dec_in 1 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef073f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1753043\n",
      "\tspeed: 0.0273s/iter; left time: 1240.6208s\n",
      "\titers: 200, epoch: 1 | loss: 0.1077347\n",
      "\tspeed: 0.0156s/iter; left time: 709.4396s\n",
      "\titers: 300, epoch: 1 | loss: 0.0663826\n",
      "\tspeed: 0.0156s/iter; left time: 706.7930s\n",
      "\titers: 400, epoch: 1 | loss: 0.1280175\n",
      "\tspeed: 0.0156s/iter; left time: 703.7254s\n",
      "\titers: 500, epoch: 1 | loss: 0.0312991\n",
      "\tspeed: 0.0156s/iter; left time: 702.9810s\n",
      "\titers: 600, epoch: 1 | loss: 0.0673031\n",
      "\tspeed: 0.0156s/iter; left time: 700.2021s\n",
      "\titers: 700, epoch: 1 | loss: 0.1459749\n",
      "\tspeed: 0.0156s/iter; left time: 697.5809s\n",
      "\titers: 800, epoch: 1 | loss: 0.4970564\n",
      "\tspeed: 0.0156s/iter; left time: 697.0027s\n",
      "\titers: 900, epoch: 1 | loss: 0.2613052\n",
      "\tspeed: 0.0155s/iter; left time: 692.0671s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0759784\n",
      "\tspeed: 0.0141s/iter; left time: 629.2200s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0833589\n",
      "\tspeed: 0.0141s/iter; left time: 626.2048s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0704285\n",
      "\tspeed: 0.0141s/iter; left time: 624.8351s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2727672\n",
      "\tspeed: 0.0148s/iter; left time: 654.2007s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0231198\n",
      "\tspeed: 0.0144s/iter; left time: 634.4163s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1133188\n",
      "\tspeed: 0.0141s/iter; left time: 621.6538s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1999876\n",
      "\tspeed: 0.0141s/iter; left time: 617.9404s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4395665\n",
      "\tspeed: 0.0141s/iter; left time: 616.5160s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0567457\n",
      "\tspeed: 0.0141s/iter; left time: 616.0350s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1737622\n",
      "\tspeed: 0.0141s/iter; left time: 615.0150s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3616043\n",
      "\tspeed: 0.0141s/iter; left time: 614.5627s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2101596\n",
      "\tspeed: 0.0141s/iter; left time: 611.4624s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3242244\n",
      "\tspeed: 0.0140s/iter; left time: 608.6606s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1569029\n",
      "\tspeed: 0.0141s/iter; left time: 607.9811s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1768864\n",
      "\tspeed: 0.0140s/iter; left time: 606.1095s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2277374\n",
      "\tspeed: 0.0142s/iter; left time: 610.9368s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1055813\n",
      "\tspeed: 0.0141s/iter; left time: 605.5982s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1563688\n",
      "\tspeed: 0.0141s/iter; left time: 604.1556s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2034613\n",
      "\tspeed: 0.0141s/iter; left time: 602.9796s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1688126\n",
      "\tspeed: 0.0141s/iter; left time: 601.2667s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0908880\n",
      "\tspeed: 0.0141s/iter; left time: 598.0565s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0868602\n",
      "\tspeed: 0.0141s/iter; left time: 598.3182s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1677474\n",
      "\tspeed: 0.0141s/iter; left time: 596.8629s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0260000\n",
      "\tspeed: 0.0141s/iter; left time: 595.4025s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2454062\n",
      "\tspeed: 0.0141s/iter; left time: 594.3529s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0880173\n",
      "\tspeed: 0.0141s/iter; left time: 592.6894s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1130646\n",
      "\tspeed: 0.0141s/iter; left time: 590.8258s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0845899\n",
      "\tspeed: 0.0141s/iter; left time: 589.2756s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1550690\n",
      "\tspeed: 0.0140s/iter; left time: 585.9940s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3135510\n",
      "\tspeed: 0.0140s/iter; left time: 584.2801s\n",
      "\titers: 4000, epoch: 1 | loss: 0.4224209\n",
      "\tspeed: 0.0140s/iter; left time: 583.1517s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0906006\n",
      "\tspeed: 0.0140s/iter; left time: 581.8337s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0793019\n",
      "\tspeed: 0.0140s/iter; left time: 580.3118s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1526469\n",
      "\tspeed: 0.0140s/iter; left time: 578.6570s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0259536\n",
      "\tspeed: 0.0140s/iter; left time: 577.6425s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0581182\n",
      "\tspeed: 0.0140s/iter; left time: 576.0062s\n",
      "Epoch: 1 cost time: 66.79062938690186\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1786124 Vali Loss: 0.0385811 Test Loss: 0.1275217\n",
      "Validation loss decreased (inf --> 0.038581).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0790856\n",
      "\tspeed: 0.1288s/iter; left time: 5268.6842s\n",
      "\titers: 200, epoch: 2 | loss: 0.0379522\n",
      "\tspeed: 0.0141s/iter; left time: 574.1539s\n",
      "\titers: 300, epoch: 2 | loss: 0.0849036\n",
      "\tspeed: 0.0144s/iter; left time: 584.9434s\n",
      "\titers: 400, epoch: 2 | loss: 0.0969812\n",
      "\tspeed: 0.0156s/iter; left time: 633.5860s\n",
      "\titers: 500, epoch: 2 | loss: 0.3665311\n",
      "\tspeed: 0.0142s/iter; left time: 573.3401s\n",
      "\titers: 600, epoch: 2 | loss: 0.1241603\n",
      "\tspeed: 0.0140s/iter; left time: 566.2809s\n",
      "\titers: 700, epoch: 2 | loss: 0.0816992\n",
      "\tspeed: 0.0141s/iter; left time: 567.1177s\n",
      "\titers: 800, epoch: 2 | loss: 0.0563953\n",
      "\tspeed: 0.0141s/iter; left time: 567.9072s\n",
      "\titers: 900, epoch: 2 | loss: 0.0453718\n",
      "\tspeed: 0.0141s/iter; left time: 565.8886s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1969409\n",
      "\tspeed: 0.0141s/iter; left time: 563.7429s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1826231\n",
      "\tspeed: 0.0142s/iter; left time: 567.0978s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1227199\n",
      "\tspeed: 0.0141s/iter; left time: 561.1137s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1078722\n",
      "\tspeed: 0.0141s/iter; left time: 561.0556s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1170693\n",
      "\tspeed: 0.0141s/iter; left time: 559.2095s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0553412\n",
      "\tspeed: 0.0156s/iter; left time: 614.7036s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1090526\n",
      "\tspeed: 0.0156s/iter; left time: 614.9309s\n",
      "\titers: 1700, epoch: 2 | loss: 0.4210885\n",
      "\tspeed: 0.0156s/iter; left time: 614.4691s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1200532\n",
      "\tspeed: 0.0156s/iter; left time: 612.0972s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1354636\n",
      "\tspeed: 0.0156s/iter; left time: 608.4310s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0677640\n",
      "\tspeed: 0.0142s/iter; left time: 551.8653s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0688940\n",
      "\tspeed: 0.0141s/iter; left time: 550.1502s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1914614\n",
      "\tspeed: 0.0141s/iter; left time: 548.4017s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1282514\n",
      "\tspeed: 0.0141s/iter; left time: 546.6307s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1180599\n",
      "\tspeed: 0.0141s/iter; left time: 545.3736s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0794819\n",
      "\tspeed: 0.0141s/iter; left time: 543.5997s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2328810\n",
      "\tspeed: 0.0141s/iter; left time: 542.3329s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1043982\n",
      "\tspeed: 0.0141s/iter; left time: 540.0977s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0455933\n",
      "\tspeed: 0.0141s/iter; left time: 539.2802s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0651997\n",
      "\tspeed: 0.0141s/iter; left time: 537.4520s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0983221\n",
      "\tspeed: 0.0141s/iter; left time: 535.9468s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1045333\n",
      "\tspeed: 0.0141s/iter; left time: 534.0947s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1798421\n",
      "\tspeed: 0.0141s/iter; left time: 532.4346s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0568886\n",
      "\tspeed: 0.0141s/iter; left time: 531.3800s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0764247\n",
      "\tspeed: 0.0141s/iter; left time: 529.9513s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1899243\n",
      "\tspeed: 0.0141s/iter; left time: 528.4329s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1324636\n",
      "\tspeed: 0.0141s/iter; left time: 526.7334s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2148753\n",
      "\tspeed: 0.0141s/iter; left time: 525.3237s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1371614\n",
      "\tspeed: 0.0141s/iter; left time: 524.2068s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1989816\n",
      "\tspeed: 0.0141s/iter; left time: 523.1747s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2433214\n",
      "\tspeed: 0.0141s/iter; left time: 521.4752s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0918373\n",
      "\tspeed: 0.0141s/iter; left time: 520.1071s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0785700\n",
      "\tspeed: 0.0141s/iter; left time: 518.6650s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1248799\n",
      "\tspeed: 0.0141s/iter; left time: 517.2980s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1390696\n",
      "\tspeed: 0.0141s/iter; left time: 515.9154s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0406284\n",
      "\tspeed: 0.0141s/iter; left time: 514.4003s\n",
      "Epoch: 2 cost time: 65.59350490570068\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1521022 Vali Loss: 0.0345455 Test Loss: 0.1193821\n",
      "Validation loss decreased (0.038581 --> 0.034546).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0997194\n",
      "\tspeed: 0.1370s/iter; left time: 4979.5048s\n",
      "\titers: 200, epoch: 3 | loss: 0.0525237\n",
      "\tspeed: 0.0141s/iter; left time: 509.8201s\n",
      "\titers: 300, epoch: 3 | loss: 0.0964992\n",
      "\tspeed: 0.0141s/iter; left time: 509.5186s\n",
      "\titers: 400, epoch: 3 | loss: 0.1255521\n",
      "\tspeed: 0.0141s/iter; left time: 508.8376s\n",
      "\titers: 500, epoch: 3 | loss: 0.0975445\n",
      "\tspeed: 0.0141s/iter; left time: 507.2015s\n",
      "\titers: 600, epoch: 3 | loss: 0.1105032\n",
      "\tspeed: 0.0141s/iter; left time: 504.6175s\n",
      "\titers: 700, epoch: 3 | loss: 0.2843919\n",
      "\tspeed: 0.0141s/iter; left time: 503.1963s\n",
      "\titers: 800, epoch: 3 | loss: 0.1513453\n",
      "\tspeed: 0.0141s/iter; left time: 501.6505s\n",
      "\titers: 900, epoch: 3 | loss: 0.0568057\n",
      "\tspeed: 0.0141s/iter; left time: 500.2208s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0950229\n",
      "\tspeed: 0.0141s/iter; left time: 499.1015s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1467626\n",
      "\tspeed: 0.0145s/iter; left time: 511.0323s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1541756\n",
      "\tspeed: 0.0156s/iter; left time: 550.2056s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2895573\n",
      "\tspeed: 0.0156s/iter; left time: 548.2742s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1408771\n",
      "\tspeed: 0.0156s/iter; left time: 546.8245s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2322203\n",
      "\tspeed: 0.0159s/iter; left time: 555.1811s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1393859\n",
      "\tspeed: 0.0162s/iter; left time: 564.2928s\n",
      "\titers: 1700, epoch: 3 | loss: 0.6099322\n",
      "\tspeed: 0.0162s/iter; left time: 562.5910s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1088450\n",
      "\tspeed: 0.0162s/iter; left time: 561.0116s\n",
      "\titers: 1900, epoch: 3 | loss: 0.2049181\n",
      "\tspeed: 0.0162s/iter; left time: 559.6016s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2307991\n",
      "\tspeed: 0.0162s/iter; left time: 557.8109s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1079544\n",
      "\tspeed: 0.0162s/iter; left time: 556.1315s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0673332\n",
      "\tspeed: 0.0162s/iter; left time: 554.6229s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1312494\n",
      "\tspeed: 0.0162s/iter; left time: 553.0259s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1002727\n",
      "\tspeed: 0.0162s/iter; left time: 551.7042s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1334111\n",
      "\tspeed: 0.0162s/iter; left time: 549.6276s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1158223\n",
      "\tspeed: 0.0162s/iter; left time: 548.1809s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0509393\n",
      "\tspeed: 0.0162s/iter; left time: 546.4098s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1000815\n",
      "\tspeed: 0.0162s/iter; left time: 544.8494s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0876475\n",
      "\tspeed: 0.0162s/iter; left time: 543.6898s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0417903\n",
      "\tspeed: 0.0162s/iter; left time: 542.0767s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1384540\n",
      "\tspeed: 0.0162s/iter; left time: 540.2556s\n",
      "\titers: 3200, epoch: 3 | loss: 0.5688301\n",
      "\tspeed: 0.0162s/iter; left time: 538.8581s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0893088\n",
      "\tspeed: 0.0162s/iter; left time: 538.3879s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2971028\n",
      "\tspeed: 0.0162s/iter; left time: 536.0009s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0404648\n",
      "\tspeed: 0.0162s/iter; left time: 534.0312s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1201640\n",
      "\tspeed: 0.0162s/iter; left time: 532.3583s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1489238\n",
      "\tspeed: 0.0162s/iter; left time: 530.6638s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0727765\n",
      "\tspeed: 0.0162s/iter; left time: 529.8993s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1715078\n",
      "\tspeed: 0.0162s/iter; left time: 528.1252s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2004449\n",
      "\tspeed: 0.0162s/iter; left time: 525.9943s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2037411\n",
      "\tspeed: 0.0162s/iter; left time: 524.8099s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0610477\n",
      "\tspeed: 0.0162s/iter; left time: 522.7026s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0275362\n",
      "\tspeed: 0.0162s/iter; left time: 520.9620s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0782880\n",
      "\tspeed: 0.0162s/iter; left time: 519.8347s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0308673\n",
      "\tspeed: 0.0154s/iter; left time: 491.8565s\n",
      "Epoch: 3 cost time: 71.46896648406982\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1274727 Vali Loss: 0.0343468 Test Loss: 0.1166713\n",
      "Validation loss decreased (0.034546 --> 0.034347).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0576389\n",
      "\tspeed: 0.1273s/iter; left time: 4047.4663s\n",
      "\titers: 200, epoch: 4 | loss: 0.2752374\n",
      "\tspeed: 0.0141s/iter; left time: 448.1244s\n",
      "\titers: 300, epoch: 4 | loss: 0.0287914\n",
      "\tspeed: 0.0151s/iter; left time: 477.7151s\n",
      "\titers: 400, epoch: 4 | loss: 0.1751598\n",
      "\tspeed: 0.0147s/iter; left time: 463.4476s\n",
      "\titers: 500, epoch: 4 | loss: 0.1396465\n",
      "\tspeed: 0.0147s/iter; left time: 461.8867s\n",
      "\titers: 600, epoch: 4 | loss: 0.1032840\n",
      "\tspeed: 0.0152s/iter; left time: 474.1035s\n",
      "\titers: 700, epoch: 4 | loss: 0.0747593\n",
      "\tspeed: 0.0141s/iter; left time: 440.8579s\n",
      "\titers: 800, epoch: 4 | loss: 0.1653925\n",
      "\tspeed: 0.0141s/iter; left time: 438.5087s\n",
      "\titers: 900, epoch: 4 | loss: 0.1157264\n",
      "\tspeed: 0.0143s/iter; left time: 442.7288s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1395583\n",
      "\tspeed: 0.0141s/iter; left time: 435.6662s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1273917\n",
      "\tspeed: 0.0141s/iter; left time: 434.7045s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0411865\n",
      "\tspeed: 0.0142s/iter; left time: 434.2534s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0974761\n",
      "\tspeed: 0.0142s/iter; left time: 432.9319s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0506707\n",
      "\tspeed: 0.0142s/iter; left time: 431.4746s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1848618\n",
      "\tspeed: 0.0141s/iter; left time: 428.7957s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0760100\n",
      "\tspeed: 0.0141s/iter; left time: 428.2910s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1537352\n",
      "\tspeed: 0.0141s/iter; left time: 426.9071s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0287306\n",
      "\tspeed: 0.0142s/iter; left time: 425.9738s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0946563\n",
      "\tspeed: 0.0142s/iter; left time: 424.6471s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1493269\n",
      "\tspeed: 0.0141s/iter; left time: 422.8503s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0160322\n",
      "\tspeed: 0.0141s/iter; left time: 421.2433s\n",
      "\titers: 2200, epoch: 4 | loss: 0.2072097\n",
      "\tspeed: 0.0141s/iter; left time: 419.6542s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1288340\n",
      "\tspeed: 0.0141s/iter; left time: 418.1207s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1190259\n",
      "\tspeed: 0.0141s/iter; left time: 416.7590s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0509818\n",
      "\tspeed: 0.0141s/iter; left time: 415.5668s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0380869\n",
      "\tspeed: 0.0141s/iter; left time: 414.1531s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5603182\n",
      "\tspeed: 0.0142s/iter; left time: 413.6676s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0248226\n",
      "\tspeed: 0.0141s/iter; left time: 411.1690s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0351221\n",
      "\tspeed: 0.0141s/iter; left time: 407.7907s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1171454\n",
      "\tspeed: 0.0141s/iter; left time: 406.0112s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1115870\n",
      "\tspeed: 0.0141s/iter; left time: 405.6959s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1424984\n",
      "\tspeed: 0.0141s/iter; left time: 403.9839s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0709415\n",
      "\tspeed: 0.0141s/iter; left time: 402.3015s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0610360\n",
      "\tspeed: 0.0141s/iter; left time: 400.6064s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0664296\n",
      "\tspeed: 0.0141s/iter; left time: 399.0618s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0657415\n",
      "\tspeed: 0.0141s/iter; left time: 397.6535s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2103222\n",
      "\tspeed: 0.0141s/iter; left time: 396.5075s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0405048\n",
      "\tspeed: 0.0141s/iter; left time: 394.8614s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1097481\n",
      "\tspeed: 0.0140s/iter; left time: 392.3299s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0850392\n",
      "\tspeed: 0.0140s/iter; left time: 390.9471s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0461364\n",
      "\tspeed: 0.0140s/iter; left time: 389.4950s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1007793\n",
      "\tspeed: 0.0140s/iter; left time: 388.2081s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3017981\n",
      "\tspeed: 0.0140s/iter; left time: 387.0671s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0936708\n",
      "\tspeed: 0.0140s/iter; left time: 385.5101s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1077963\n",
      "\tspeed: 0.0140s/iter; left time: 384.0013s\n",
      "Epoch: 4 cost time: 64.96328163146973\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1103817 Vali Loss: 0.0338532 Test Loss: 0.1128421\n",
      "Validation loss decreased (0.034347 --> 0.033853).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0785104\n",
      "\tspeed: 0.1335s/iter; left time: 3635.9025s\n",
      "\titers: 200, epoch: 5 | loss: 0.0603779\n",
      "\tspeed: 0.0141s/iter; left time: 383.3870s\n",
      "\titers: 300, epoch: 5 | loss: 0.0673461\n",
      "\tspeed: 0.0141s/iter; left time: 380.5133s\n",
      "\titers: 400, epoch: 5 | loss: 0.0562438\n",
      "\tspeed: 0.0141s/iter; left time: 379.6237s\n",
      "\titers: 500, epoch: 5 | loss: 0.1110560\n",
      "\tspeed: 0.0141s/iter; left time: 377.9587s\n",
      "\titers: 600, epoch: 5 | loss: 0.0206359\n",
      "\tspeed: 0.0141s/iter; left time: 377.4898s\n",
      "\titers: 700, epoch: 5 | loss: 0.0403416\n",
      "\tspeed: 0.0141s/iter; left time: 376.7084s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935556\n",
      "\tspeed: 0.0141s/iter; left time: 374.9985s\n",
      "\titers: 900, epoch: 5 | loss: 0.0640305\n",
      "\tspeed: 0.0141s/iter; left time: 373.4482s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1415762\n",
      "\tspeed: 0.0141s/iter; left time: 371.9673s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0272753\n",
      "\tspeed: 0.0141s/iter; left time: 370.7547s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0947147\n",
      "\tspeed: 0.0141s/iter; left time: 369.2230s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0735672\n",
      "\tspeed: 0.0141s/iter; left time: 367.6022s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0991888\n",
      "\tspeed: 0.0141s/iter; left time: 366.1455s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1163013\n",
      "\tspeed: 0.0141s/iter; left time: 365.1226s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0541945\n",
      "\tspeed: 0.0141s/iter; left time: 363.5745s\n",
      "\titers: 1700, epoch: 5 | loss: 0.2281307\n",
      "\tspeed: 0.0141s/iter; left time: 362.0911s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1138795\n",
      "\tspeed: 0.0141s/iter; left time: 360.7984s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0326305\n",
      "\tspeed: 0.0141s/iter; left time: 358.9957s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5340424\n",
      "\tspeed: 0.0141s/iter; left time: 357.6560s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0946761\n",
      "\tspeed: 0.0141s/iter; left time: 356.2213s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0630995\n",
      "\tspeed: 0.0141s/iter; left time: 354.8351s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0433988\n",
      "\tspeed: 0.0141s/iter; left time: 353.8130s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0820433\n",
      "\tspeed: 0.0141s/iter; left time: 351.8189s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0811024\n",
      "\tspeed: 0.0141s/iter; left time: 350.6452s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0739033\n",
      "\tspeed: 0.0141s/iter; left time: 349.3639s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1003244\n",
      "\tspeed: 0.0141s/iter; left time: 347.8788s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0556785\n",
      "\tspeed: 0.0141s/iter; left time: 345.3623s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0445681\n",
      "\tspeed: 0.0141s/iter; left time: 343.8023s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0500327\n",
      "\tspeed: 0.0141s/iter; left time: 342.3304s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0351428\n",
      "\tspeed: 0.0141s/iter; left time: 340.9631s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0171011\n",
      "\tspeed: 0.0141s/iter; left time: 339.7943s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0872114\n",
      "\tspeed: 0.0141s/iter; left time: 338.3538s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0506429\n",
      "\tspeed: 0.0141s/iter; left time: 336.8649s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0529571\n",
      "\tspeed: 0.0141s/iter; left time: 335.4813s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0670077\n",
      "\tspeed: 0.0141s/iter; left time: 334.0278s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0593882\n",
      "\tspeed: 0.0141s/iter; left time: 332.4095s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0870268\n",
      "\tspeed: 0.0141s/iter; left time: 331.2265s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0545563\n",
      "\tspeed: 0.0141s/iter; left time: 329.7915s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0549904\n",
      "\tspeed: 0.0141s/iter; left time: 328.5517s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0207501\n",
      "\tspeed: 0.0141s/iter; left time: 327.7753s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0344409\n",
      "\tspeed: 0.0141s/iter; left time: 326.6571s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1144929\n",
      "\tspeed: 0.0141s/iter; left time: 325.1806s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0145858\n",
      "\tspeed: 0.0141s/iter; left time: 323.7411s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0490095\n",
      "\tspeed: 0.0141s/iter; left time: 322.4529s\n",
      "Epoch: 5 cost time: 64.52818775177002\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1001827 Vali Loss: 0.0331114 Test Loss: 0.1135699\n",
      "Validation loss decreased (0.033853 --> 0.033111).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0289830\n",
      "\tspeed: 0.1263s/iter; left time: 2863.1940s\n",
      "\titers: 200, epoch: 6 | loss: 0.1611261\n",
      "\tspeed: 0.0141s/iter; left time: 318.8335s\n",
      "\titers: 300, epoch: 6 | loss: 0.1635068\n",
      "\tspeed: 0.0141s/iter; left time: 316.4113s\n",
      "\titers: 400, epoch: 6 | loss: 0.0940145\n",
      "\tspeed: 0.0146s/iter; left time: 327.7514s\n",
      "\titers: 500, epoch: 6 | loss: 0.1440269\n",
      "\tspeed: 0.0156s/iter; left time: 348.4169s\n",
      "\titers: 600, epoch: 6 | loss: 0.1533241\n",
      "\tspeed: 0.0142s/iter; left time: 314.2943s\n",
      "\titers: 700, epoch: 6 | loss: 0.0402026\n",
      "\tspeed: 0.0142s/iter; left time: 312.9888s\n",
      "\titers: 800, epoch: 6 | loss: 0.0591984\n",
      "\tspeed: 0.0142s/iter; left time: 312.5573s\n",
      "\titers: 900, epoch: 6 | loss: 0.0640327\n",
      "\tspeed: 0.0141s/iter; left time: 309.4203s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1813027\n",
      "\tspeed: 0.0141s/iter; left time: 307.8932s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0330169\n",
      "\tspeed: 0.0141s/iter; left time: 306.3586s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1211472\n",
      "\tspeed: 0.0141s/iter; left time: 304.9297s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1289187\n",
      "\tspeed: 0.0141s/iter; left time: 303.4884s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1124306\n",
      "\tspeed: 0.0141s/iter; left time: 302.2221s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0523304\n",
      "\tspeed: 0.0141s/iter; left time: 300.7556s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1770798\n",
      "\tspeed: 0.0141s/iter; left time: 299.4075s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0525290\n",
      "\tspeed: 0.0141s/iter; left time: 297.9724s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2427282\n",
      "\tspeed: 0.0141s/iter; left time: 296.5375s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1466224\n",
      "\tspeed: 0.0141s/iter; left time: 295.1214s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1722656\n",
      "\tspeed: 0.0141s/iter; left time: 293.6230s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0973565\n",
      "\tspeed: 0.0142s/iter; left time: 292.7555s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0982651\n",
      "\tspeed: 0.0142s/iter; left time: 291.5640s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1670761\n",
      "\tspeed: 0.0142s/iter; left time: 290.1630s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0614178\n",
      "\tspeed: 0.0142s/iter; left time: 288.5239s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0990296\n",
      "\tspeed: 0.0141s/iter; left time: 286.8658s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0630544\n",
      "\tspeed: 0.0142s/iter; left time: 285.7396s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0764563\n",
      "\tspeed: 0.0141s/iter; left time: 283.9235s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0984251\n",
      "\tspeed: 0.0141s/iter; left time: 281.2570s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2259469\n",
      "\tspeed: 0.0141s/iter; left time: 279.7675s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0337761\n",
      "\tspeed: 0.0141s/iter; left time: 278.5700s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1085882\n",
      "\tspeed: 0.0141s/iter; left time: 277.0875s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1164428\n",
      "\tspeed: 0.0141s/iter; left time: 275.2990s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0689914\n",
      "\tspeed: 0.0141s/iter; left time: 273.9765s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0299434\n",
      "\tspeed: 0.0141s/iter; left time: 273.1039s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0147749\n",
      "\tspeed: 0.0141s/iter; left time: 271.2649s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0851746\n",
      "\tspeed: 0.0141s/iter; left time: 269.9892s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1055528\n",
      "\tspeed: 0.0141s/iter; left time: 268.2119s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1835547\n",
      "\tspeed: 0.0141s/iter; left time: 266.7430s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0599276\n",
      "\tspeed: 0.0141s/iter; left time: 265.2842s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1783794\n",
      "\tspeed: 0.0141s/iter; left time: 263.9550s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0205125\n",
      "\tspeed: 0.0141s/iter; left time: 262.4311s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0307689\n",
      "\tspeed: 0.0141s/iter; left time: 261.1116s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1395018\n",
      "\tspeed: 0.0140s/iter; left time: 259.5874s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1043912\n",
      "\tspeed: 0.0141s/iter; left time: 258.4696s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0789568\n",
      "\tspeed: 0.0141s/iter; left time: 256.9029s\n",
      "Epoch: 6 cost time: 64.75584244728088\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0953701 Vali Loss: 0.0329509 Test Loss: 0.1174688\n",
      "Validation loss decreased (0.033111 --> 0.032951).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1099629\n",
      "\tspeed: 0.1273s/iter; left time: 2307.3758s\n",
      "\titers: 200, epoch: 7 | loss: 0.1983022\n",
      "\tspeed: 0.0147s/iter; left time: 265.2007s\n",
      "\titers: 300, epoch: 7 | loss: 0.2044026\n",
      "\tspeed: 0.0156s/iter; left time: 279.1186s\n",
      "\titers: 400, epoch: 7 | loss: 0.0523858\n",
      "\tspeed: 0.0156s/iter; left time: 277.4324s\n",
      "\titers: 500, epoch: 7 | loss: 0.0523795\n",
      "\tspeed: 0.0156s/iter; left time: 276.0544s\n",
      "\titers: 600, epoch: 7 | loss: 0.0645101\n",
      "\tspeed: 0.0145s/iter; left time: 255.5963s\n",
      "\titers: 700, epoch: 7 | loss: 0.0158990\n",
      "\tspeed: 0.0141s/iter; left time: 246.8418s\n",
      "\titers: 800, epoch: 7 | loss: 0.0501764\n",
      "\tspeed: 0.0141s/iter; left time: 245.2274s\n",
      "\titers: 900, epoch: 7 | loss: 0.2272983\n",
      "\tspeed: 0.0141s/iter; left time: 243.7328s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0393277\n",
      "\tspeed: 0.0141s/iter; left time: 242.3991s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2189463\n",
      "\tspeed: 0.0141s/iter; left time: 241.4063s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0656718\n",
      "\tspeed: 0.0141s/iter; left time: 240.0423s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1074567\n",
      "\tspeed: 0.0141s/iter; left time: 238.5787s\n",
      "\titers: 1400, epoch: 7 | loss: 0.1027504\n",
      "\tspeed: 0.0141s/iter; left time: 237.2630s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0823208\n",
      "\tspeed: 0.0141s/iter; left time: 235.8029s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0757674\n",
      "\tspeed: 0.0141s/iter; left time: 234.6258s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0475362\n",
      "\tspeed: 0.0141s/iter; left time: 233.7020s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0532699\n",
      "\tspeed: 0.0141s/iter; left time: 231.2631s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0399863\n",
      "\tspeed: 0.0141s/iter; left time: 230.4108s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0485556\n",
      "\tspeed: 0.0141s/iter; left time: 228.7441s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0464956\n",
      "\tspeed: 0.0141s/iter; left time: 226.9618s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0618479\n",
      "\tspeed: 0.0141s/iter; left time: 226.1821s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0385197\n",
      "\tspeed: 0.0141s/iter; left time: 224.2571s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0497813\n",
      "\tspeed: 0.0141s/iter; left time: 222.6384s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1650084\n",
      "\tspeed: 0.0141s/iter; left time: 221.1425s\n",
      "\titers: 2600, epoch: 7 | loss: 0.1035766\n",
      "\tspeed: 0.0141s/iter; left time: 219.8616s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0367910\n",
      "\tspeed: 0.0141s/iter; left time: 218.5432s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0380885\n",
      "\tspeed: 0.0141s/iter; left time: 217.2029s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0708064\n",
      "\tspeed: 0.0141s/iter; left time: 215.7219s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0722591\n",
      "\tspeed: 0.0141s/iter; left time: 214.1708s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0153503\n",
      "\tspeed: 0.0141s/iter; left time: 212.6993s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1690415\n",
      "\tspeed: 0.0141s/iter; left time: 211.0969s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0604177\n",
      "\tspeed: 0.0141s/iter; left time: 209.6628s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0354910\n",
      "\tspeed: 0.0141s/iter; left time: 208.3605s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0404593\n",
      "\tspeed: 0.0141s/iter; left time: 206.9562s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1022016\n",
      "\tspeed: 0.0141s/iter; left time: 205.5362s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1732755\n",
      "\tspeed: 0.0141s/iter; left time: 204.2497s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0473625\n",
      "\tspeed: 0.0141s/iter; left time: 202.8702s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0481395\n",
      "\tspeed: 0.0141s/iter; left time: 201.7660s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0492032\n",
      "\tspeed: 0.0141s/iter; left time: 200.3023s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0823483\n",
      "\tspeed: 0.0141s/iter; left time: 198.9727s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0353368\n",
      "\tspeed: 0.0141s/iter; left time: 197.4499s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0261534\n",
      "\tspeed: 0.0140s/iter; left time: 195.4200s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0425033\n",
      "\tspeed: 0.0140s/iter; left time: 193.9604s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0789986\n",
      "\tspeed: 0.0140s/iter; left time: 192.5515s\n",
      "Epoch: 7 cost time: 65.11302876472473\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0916367 Vali Loss: 0.0328634 Test Loss: 0.1175968\n",
      "Validation loss decreased (0.032951 --> 0.032863).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0646106\n",
      "\tspeed: 0.1427s/iter; left time: 1935.6314s\n",
      "\titers: 200, epoch: 8 | loss: 0.0365684\n",
      "\tspeed: 0.0156s/iter; left time: 210.5267s\n",
      "\titers: 300, epoch: 8 | loss: 0.1116534\n",
      "\tspeed: 0.0156s/iter; left time: 209.0216s\n",
      "\titers: 400, epoch: 8 | loss: 0.0461679\n",
      "\tspeed: 0.0156s/iter; left time: 206.4213s\n",
      "\titers: 500, epoch: 8 | loss: 0.2314401\n",
      "\tspeed: 0.0156s/iter; left time: 204.9434s\n",
      "\titers: 600, epoch: 8 | loss: 0.0839031\n",
      "\tspeed: 0.0156s/iter; left time: 203.4092s\n",
      "\titers: 700, epoch: 8 | loss: 0.1920541\n",
      "\tspeed: 0.0156s/iter; left time: 201.8243s\n",
      "\titers: 800, epoch: 8 | loss: 0.1616083\n",
      "\tspeed: 0.0155s/iter; left time: 199.4524s\n",
      "\titers: 900, epoch: 8 | loss: 0.0827668\n",
      "\tspeed: 0.0141s/iter; left time: 180.0178s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0735827\n",
      "\tspeed: 0.0141s/iter; left time: 178.5547s\n",
      "\titers: 1100, epoch: 8 | loss: 0.1058869\n",
      "\tspeed: 0.0141s/iter; left time: 177.2034s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1269293\n",
      "\tspeed: 0.0141s/iter; left time: 176.2879s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3649760\n",
      "\tspeed: 0.0143s/iter; left time: 176.4734s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0339007\n",
      "\tspeed: 0.0141s/iter; left time: 173.3384s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0779374\n",
      "\tspeed: 0.0141s/iter; left time: 171.7455s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1486876\n",
      "\tspeed: 0.0141s/iter; left time: 170.1185s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0225649\n",
      "\tspeed: 0.0141s/iter; left time: 168.7450s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1194144\n",
      "\tspeed: 0.0140s/iter; left time: 166.6111s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1004532\n",
      "\tspeed: 0.0140s/iter; left time: 165.0135s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1020600\n",
      "\tspeed: 0.0140s/iter; left time: 163.6337s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0428479\n",
      "\tspeed: 0.0140s/iter; left time: 162.4513s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0516911\n",
      "\tspeed: 0.0140s/iter; left time: 161.0422s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1383002\n",
      "\tspeed: 0.0140s/iter; left time: 159.6111s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0410548\n",
      "\tspeed: 0.0140s/iter; left time: 158.2092s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0609355\n",
      "\tspeed: 0.0141s/iter; left time: 156.9177s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0593496\n",
      "\tspeed: 0.0141s/iter; left time: 155.5135s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0326984\n",
      "\tspeed: 0.0140s/iter; left time: 154.0461s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0495661\n",
      "\tspeed: 0.0141s/iter; left time: 152.8177s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0984135\n",
      "\tspeed: 0.0141s/iter; left time: 151.4303s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2793420\n",
      "\tspeed: 0.0141s/iter; left time: 149.8668s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1072215\n",
      "\tspeed: 0.0141s/iter; left time: 148.4813s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1355446\n",
      "\tspeed: 0.0141s/iter; left time: 147.0937s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0680997\n",
      "\tspeed: 0.0141s/iter; left time: 145.6671s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0551747\n",
      "\tspeed: 0.0140s/iter; left time: 144.2100s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1548904\n",
      "\tspeed: 0.0140s/iter; left time: 142.8285s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0308465\n",
      "\tspeed: 0.0141s/iter; left time: 141.6016s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2271353\n",
      "\tspeed: 0.0141s/iter; left time: 140.6266s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0655918\n",
      "\tspeed: 0.0141s/iter; left time: 139.1386s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0590320\n",
      "\tspeed: 0.0141s/iter; left time: 137.7377s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0414677\n",
      "\tspeed: 0.0141s/iter; left time: 136.2421s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0348895\n",
      "\tspeed: 0.0141s/iter; left time: 134.8953s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0957698\n",
      "\tspeed: 0.0141s/iter; left time: 133.4326s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0716058\n",
      "\tspeed: 0.0141s/iter; left time: 132.0488s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1349273\n",
      "\tspeed: 0.0141s/iter; left time: 130.5655s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1439957\n",
      "\tspeed: 0.0140s/iter; left time: 128.3282s\n",
      "Epoch: 8 cost time: 65.60127067565918\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0920218 Vali Loss: 0.0331228 Test Loss: 0.1174829\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0535825\n",
      "\tspeed: 0.1263s/iter; left time: 1137.7699s\n",
      "\titers: 200, epoch: 9 | loss: 0.1106407\n",
      "\tspeed: 0.0141s/iter; left time: 125.4441s\n",
      "\titers: 300, epoch: 9 | loss: 0.0347949\n",
      "\tspeed: 0.0141s/iter; left time: 123.9013s\n",
      "\titers: 400, epoch: 9 | loss: 0.1138489\n",
      "\tspeed: 0.0140s/iter; left time: 122.2158s\n",
      "\titers: 500, epoch: 9 | loss: 0.0886443\n",
      "\tspeed: 0.0140s/iter; left time: 120.8354s\n",
      "\titers: 600, epoch: 9 | loss: 0.0302212\n",
      "\tspeed: 0.0141s/iter; left time: 119.8297s\n",
      "\titers: 700, epoch: 9 | loss: 0.0968511\n",
      "\tspeed: 0.0141s/iter; left time: 118.4488s\n",
      "\titers: 800, epoch: 9 | loss: 0.0317942\n",
      "\tspeed: 0.0141s/iter; left time: 116.9612s\n",
      "\titers: 900, epoch: 9 | loss: 0.0531709\n",
      "\tspeed: 0.0141s/iter; left time: 115.5943s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0905531\n",
      "\tspeed: 0.0141s/iter; left time: 114.4239s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0539447\n",
      "\tspeed: 0.0141s/iter; left time: 113.1068s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1935368\n",
      "\tspeed: 0.0141s/iter; left time: 111.6414s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0276913\n",
      "\tspeed: 0.0141s/iter; left time: 110.1508s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2232114\n",
      "\tspeed: 0.0141s/iter; left time: 108.7562s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0570007\n",
      "\tspeed: 0.0141s/iter; left time: 107.0070s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0444966\n",
      "\tspeed: 0.0140s/iter; left time: 105.5059s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0703715\n",
      "\tspeed: 0.0140s/iter; left time: 104.0911s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0350657\n",
      "\tspeed: 0.0140s/iter; left time: 102.6658s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0234423\n",
      "\tspeed: 0.0140s/iter; left time: 101.2945s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0483236\n",
      "\tspeed: 0.0140s/iter; left time: 99.8677s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0578080\n",
      "\tspeed: 0.0140s/iter; left time: 98.4292s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1185361\n",
      "\tspeed: 0.0140s/iter; left time: 97.0417s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1000079\n",
      "\tspeed: 0.0140s/iter; left time: 95.6589s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0301648\n",
      "\tspeed: 0.0140s/iter; left time: 94.2481s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0218247\n",
      "\tspeed: 0.0140s/iter; left time: 92.8363s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0637089\n",
      "\tspeed: 0.0140s/iter; left time: 91.3961s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0423516\n",
      "\tspeed: 0.0140s/iter; left time: 90.0235s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1393107\n",
      "\tspeed: 0.0140s/iter; left time: 88.6390s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0422159\n",
      "\tspeed: 0.0140s/iter; left time: 87.2312s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1203136\n",
      "\tspeed: 0.0140s/iter; left time: 85.7908s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0312540\n",
      "\tspeed: 0.0140s/iter; left time: 84.4087s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0746162\n",
      "\tspeed: 0.0140s/iter; left time: 82.9996s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2612969\n",
      "\tspeed: 0.0140s/iter; left time: 81.6115s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0971725\n",
      "\tspeed: 0.0140s/iter; left time: 80.1685s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0960424\n",
      "\tspeed: 0.0140s/iter; left time: 78.7538s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0499137\n",
      "\tspeed: 0.0140s/iter; left time: 77.3573s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0694873\n",
      "\tspeed: 0.0140s/iter; left time: 75.9362s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1424918\n",
      "\tspeed: 0.0140s/iter; left time: 74.5528s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0801451\n",
      "\tspeed: 0.0140s/iter; left time: 73.1321s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0869360\n",
      "\tspeed: 0.0140s/iter; left time: 71.7244s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0581198\n",
      "\tspeed: 0.0140s/iter; left time: 70.3212s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0757257\n",
      "\tspeed: 0.0140s/iter; left time: 68.9452s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0744628\n",
      "\tspeed: 0.0140s/iter; left time: 67.5315s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0329054\n",
      "\tspeed: 0.0140s/iter; left time: 66.0906s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0298722\n",
      "\tspeed: 0.0140s/iter; left time: 64.7410s\n",
      "Epoch: 9 cost time: 64.2761459350586\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0899772 Vali Loss: 0.0330352 Test Loss: 0.1174494\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0631996\n",
      "\tspeed: 0.1338s/iter; left time: 596.3334s\n",
      "\titers: 200, epoch: 10 | loss: 0.0888136\n",
      "\tspeed: 0.0141s/iter; left time: 61.5812s\n",
      "\titers: 300, epoch: 10 | loss: 0.0344665\n",
      "\tspeed: 0.0141s/iter; left time: 60.0528s\n",
      "\titers: 400, epoch: 10 | loss: 0.0750812\n",
      "\tspeed: 0.0141s/iter; left time: 58.6217s\n",
      "\titers: 500, epoch: 10 | loss: 0.0558594\n",
      "\tspeed: 0.0141s/iter; left time: 57.1031s\n",
      "\titers: 600, epoch: 10 | loss: 0.0876804\n",
      "\tspeed: 0.0141s/iter; left time: 55.7594s\n",
      "\titers: 700, epoch: 10 | loss: 0.0662050\n",
      "\tspeed: 0.0141s/iter; left time: 54.3292s\n",
      "\titers: 800, epoch: 10 | loss: 0.2021831\n",
      "\tspeed: 0.0141s/iter; left time: 53.0269s\n",
      "\titers: 900, epoch: 10 | loss: 0.0844504\n",
      "\tspeed: 0.0141s/iter; left time: 51.5449s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0459830\n",
      "\tspeed: 0.0141s/iter; left time: 50.0829s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0626340\n",
      "\tspeed: 0.0141s/iter; left time: 48.6630s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0457475\n",
      "\tspeed: 0.0141s/iter; left time: 47.2502s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0496145\n",
      "\tspeed: 0.0141s/iter; left time: 45.8854s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0630570\n",
      "\tspeed: 0.0141s/iter; left time: 44.5258s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2365202\n",
      "\tspeed: 0.0141s/iter; left time: 43.1106s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0952215\n",
      "\tspeed: 0.0141s/iter; left time: 41.6564s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0266585\n",
      "\tspeed: 0.0141s/iter; left time: 40.2740s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0776278\n",
      "\tspeed: 0.0141s/iter; left time: 38.8513s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1086182\n",
      "\tspeed: 0.0141s/iter; left time: 37.4350s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0596949\n",
      "\tspeed: 0.0141s/iter; left time: 36.0519s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0564876\n",
      "\tspeed: 0.0162s/iter; left time: 39.7074s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0702155\n",
      "\tspeed: 0.0162s/iter; left time: 38.0890s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1480383\n",
      "\tspeed: 0.0162s/iter; left time: 36.5126s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0790886\n",
      "\tspeed: 0.0162s/iter; left time: 34.8815s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2116390\n",
      "\tspeed: 0.0162s/iter; left time: 33.2482s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1041928\n",
      "\tspeed: 0.0162s/iter; left time: 31.6386s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0744508\n",
      "\tspeed: 0.0162s/iter; left time: 30.0272s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0650729\n",
      "\tspeed: 0.0150s/iter; left time: 26.2696s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0580915\n",
      "\tspeed: 0.0140s/iter; left time: 23.2643s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0443436\n",
      "\tspeed: 0.0140s/iter; left time: 21.8612s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0541247\n",
      "\tspeed: 0.0140s/iter; left time: 20.4519s\n",
      "\titers: 3200, epoch: 10 | loss: 0.1113902\n",
      "\tspeed: 0.0140s/iter; left time: 19.0454s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0853350\n",
      "\tspeed: 0.0140s/iter; left time: 17.6346s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1607841\n",
      "\tspeed: 0.0141s/iter; left time: 16.2453s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0213557\n",
      "\tspeed: 0.0140s/iter; left time: 14.8326s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1540724\n",
      "\tspeed: 0.0140s/iter; left time: 13.4290s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0910978\n",
      "\tspeed: 0.0140s/iter; left time: 12.0197s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0684156\n",
      "\tspeed: 0.0140s/iter; left time: 10.6178s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0224562\n",
      "\tspeed: 0.0140s/iter; left time: 9.2081s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0542614\n",
      "\tspeed: 0.0140s/iter; left time: 7.8066s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1472813\n",
      "\tspeed: 0.0140s/iter; left time: 6.4048s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0513240\n",
      "\tspeed: 0.0140s/iter; left time: 4.9968s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0382041\n",
      "\tspeed: 0.0140s/iter; left time: 3.5916s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0476256\n",
      "\tspeed: 0.0140s/iter; left time: 2.1882s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1135630\n",
      "\tspeed: 0.0140s/iter; left time: 0.7854s\n",
      "Epoch: 10 cost time: 65.89383220672607\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0902528 Vali Loss: 0.0330214 Test Loss: 0.1175839\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11762779206037521, mae:0.20591463148593903\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor solar_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f989b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1835497\n",
      "\tspeed: 0.0268s/iter; left time: 1217.9047s\n",
      "\titers: 200, epoch: 1 | loss: 0.1153142\n",
      "\tspeed: 0.0146s/iter; left time: 663.2923s\n",
      "\titers: 300, epoch: 1 | loss: 0.1916097\n",
      "\tspeed: 0.0146s/iter; left time: 660.4108s\n",
      "\titers: 400, epoch: 1 | loss: 0.1894591\n",
      "\tspeed: 0.0146s/iter; left time: 660.2043s\n",
      "\titers: 500, epoch: 1 | loss: 0.1693326\n",
      "\tspeed: 0.0146s/iter; left time: 658.8215s\n",
      "\titers: 600, epoch: 1 | loss: 0.1171933\n",
      "\tspeed: 0.0146s/iter; left time: 658.0113s\n",
      "\titers: 700, epoch: 1 | loss: 0.1451544\n",
      "\tspeed: 0.0146s/iter; left time: 653.9561s\n",
      "\titers: 800, epoch: 1 | loss: 0.1126527\n",
      "\tspeed: 0.0153s/iter; left time: 686.8434s\n",
      "\titers: 900, epoch: 1 | loss: 0.1244560\n",
      "\tspeed: 0.0146s/iter; left time: 651.1116s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1651168\n",
      "\tspeed: 0.0146s/iter; left time: 649.8383s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1482705\n",
      "\tspeed: 0.0146s/iter; left time: 648.5071s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1323465\n",
      "\tspeed: 0.0146s/iter; left time: 646.6811s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2229661\n",
      "\tspeed: 0.0146s/iter; left time: 644.9112s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1464737\n",
      "\tspeed: 0.0146s/iter; left time: 643.4498s\n",
      "\titers: 1500, epoch: 1 | loss: 0.0776521\n",
      "\tspeed: 0.0146s/iter; left time: 641.5653s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1507899\n",
      "\tspeed: 0.0146s/iter; left time: 641.8657s\n",
      "\titers: 1700, epoch: 1 | loss: 0.1011101\n",
      "\tspeed: 0.0156s/iter; left time: 686.1613s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1450867\n",
      "\tspeed: 0.0162s/iter; left time: 709.6869s\n",
      "\titers: 1900, epoch: 1 | loss: 0.3305567\n",
      "\tspeed: 0.0153s/iter; left time: 669.0590s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1160365\n",
      "\tspeed: 0.0167s/iter; left time: 729.1287s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1068159\n",
      "\tspeed: 0.0167s/iter; left time: 726.7814s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1328086\n",
      "\tspeed: 0.0167s/iter; left time: 724.8204s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1062998\n",
      "\tspeed: 0.0167s/iter; left time: 723.8193s\n",
      "\titers: 2400, epoch: 1 | loss: 0.3595074\n",
      "\tspeed: 0.0164s/iter; left time: 707.3384s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1744194\n",
      "\tspeed: 0.0163s/iter; left time: 699.6572s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1474980\n",
      "\tspeed: 0.0162s/iter; left time: 697.7069s\n",
      "\titers: 2700, epoch: 1 | loss: 0.0937831\n",
      "\tspeed: 0.0162s/iter; left time: 694.4354s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2184623\n",
      "\tspeed: 0.0162s/iter; left time: 690.8203s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1529506\n",
      "\tspeed: 0.0161s/iter; left time: 687.9521s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0590975\n",
      "\tspeed: 0.0161s/iter; left time: 686.4000s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0855486\n",
      "\tspeed: 0.0162s/iter; left time: 685.7340s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1507652\n",
      "\tspeed: 0.0161s/iter; left time: 683.2233s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1902263\n",
      "\tspeed: 0.0161s/iter; left time: 681.5828s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1578428\n",
      "\tspeed: 0.0161s/iter; left time: 679.5321s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1384976\n",
      "\tspeed: 0.0148s/iter; left time: 621.9494s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1221287\n",
      "\tspeed: 0.0145s/iter; left time: 609.2045s\n",
      "\titers: 3700, epoch: 1 | loss: 0.1167372\n",
      "\tspeed: 0.0145s/iter; left time: 607.6400s\n",
      "\titers: 3800, epoch: 1 | loss: 0.2298422\n",
      "\tspeed: 0.0159s/iter; left time: 662.5003s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1026639\n",
      "\tspeed: 0.0161s/iter; left time: 671.8202s\n",
      "\titers: 4000, epoch: 1 | loss: 0.1765046\n",
      "\tspeed: 0.0161s/iter; left time: 669.9275s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1406819\n",
      "\tspeed: 0.0156s/iter; left time: 646.9331s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1274727\n",
      "\tspeed: 0.0145s/iter; left time: 599.6221s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1364025\n",
      "\tspeed: 0.0164s/iter; left time: 678.2566s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1974474\n",
      "\tspeed: 0.0167s/iter; left time: 688.0218s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0846185\n",
      "\tspeed: 0.0167s/iter; left time: 686.0580s\n",
      "Epoch: 1 cost time: 72.0291314125061\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1706725 Vali Loss: 0.0736029 Test Loss: 0.0914102\n",
      "Validation loss decreased (inf --> 0.073603).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1685341\n",
      "\tspeed: 0.1335s/iter; left time: 5459.0140s\n",
      "\titers: 200, epoch: 2 | loss: 0.1408903\n",
      "\tspeed: 0.0170s/iter; left time: 693.9261s\n",
      "\titers: 300, epoch: 2 | loss: 0.1952439\n",
      "\tspeed: 0.0170s/iter; left time: 691.7547s\n",
      "\titers: 400, epoch: 2 | loss: 0.1452914\n",
      "\tspeed: 0.0170s/iter; left time: 689.3004s\n",
      "\titers: 500, epoch: 2 | loss: 0.1664198\n",
      "\tspeed: 0.0170s/iter; left time: 687.7037s\n",
      "\titers: 600, epoch: 2 | loss: 0.1740848\n",
      "\tspeed: 0.0170s/iter; left time: 685.6105s\n",
      "\titers: 700, epoch: 2 | loss: 0.2172345\n",
      "\tspeed: 0.0170s/iter; left time: 684.2398s\n",
      "\titers: 800, epoch: 2 | loss: 0.0882750\n",
      "\tspeed: 0.0170s/iter; left time: 682.2943s\n",
      "\titers: 900, epoch: 2 | loss: 0.1143197\n",
      "\tspeed: 0.0170s/iter; left time: 680.4972s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1582641\n",
      "\tspeed: 0.0170s/iter; left time: 678.6340s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1356126\n",
      "\tspeed: 0.0170s/iter; left time: 677.1493s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1227969\n",
      "\tspeed: 0.0170s/iter; left time: 675.3358s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1380577\n",
      "\tspeed: 0.0170s/iter; left time: 673.6281s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0903574\n",
      "\tspeed: 0.0170s/iter; left time: 671.9824s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2111163\n",
      "\tspeed: 0.0160s/iter; left time: 629.9953s\n",
      "\titers: 1600, epoch: 2 | loss: 0.2305768\n",
      "\tspeed: 0.0146s/iter; left time: 577.0348s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2302151\n",
      "\tspeed: 0.0146s/iter; left time: 575.4939s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1184028\n",
      "\tspeed: 0.0163s/iter; left time: 637.7145s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0814160\n",
      "\tspeed: 0.0170s/iter; left time: 663.3167s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1361259\n",
      "\tspeed: 0.0170s/iter; left time: 661.5338s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1527939\n",
      "\tspeed: 0.0170s/iter; left time: 659.8710s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0811446\n",
      "\tspeed: 0.0170s/iter; left time: 658.6280s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1963743\n",
      "\tspeed: 0.0170s/iter; left time: 656.9452s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1024969\n",
      "\tspeed: 0.0170s/iter; left time: 655.3958s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0811993\n",
      "\tspeed: 0.0170s/iter; left time: 653.6001s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1305210\n",
      "\tspeed: 0.0163s/iter; left time: 625.8571s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0777922\n",
      "\tspeed: 0.0147s/iter; left time: 563.7368s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1406435\n",
      "\tspeed: 0.0147s/iter; left time: 562.2347s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1390175\n",
      "\tspeed: 0.0147s/iter; left time: 560.4518s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1143527\n",
      "\tspeed: 0.0147s/iter; left time: 559.2715s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1569093\n",
      "\tspeed: 0.0147s/iter; left time: 557.6355s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2626752\n",
      "\tspeed: 0.0147s/iter; left time: 556.1151s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1914656\n",
      "\tspeed: 0.0147s/iter; left time: 554.7350s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1442455\n",
      "\tspeed: 0.0147s/iter; left time: 553.3315s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0962215\n",
      "\tspeed: 0.0147s/iter; left time: 551.6553s\n",
      "\titers: 3600, epoch: 2 | loss: 0.0784452\n",
      "\tspeed: 0.0147s/iter; left time: 550.1692s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1026973\n",
      "\tspeed: 0.0147s/iter; left time: 549.0079s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0938692\n",
      "\tspeed: 0.0147s/iter; left time: 547.4049s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1268843\n",
      "\tspeed: 0.0147s/iter; left time: 545.1770s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1141753\n",
      "\tspeed: 0.0147s/iter; left time: 543.6005s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1039462\n",
      "\tspeed: 0.0147s/iter; left time: 543.3201s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1155951\n",
      "\tspeed: 0.0147s/iter; left time: 541.1212s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1503512\n",
      "\tspeed: 0.0147s/iter; left time: 540.5387s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1256710\n",
      "\tspeed: 0.0147s/iter; left time: 538.3746s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0616412\n",
      "\tspeed: 0.0147s/iter; left time: 536.7733s\n",
      "Epoch: 2 cost time: 72.44048738479614\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1416156 Vali Loss: 0.0705284 Test Loss: 0.0882831\n",
      "Validation loss decreased (0.073603 --> 0.070528).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2076444\n",
      "\tspeed: 0.1323s/iter; left time: 4807.0813s\n",
      "\titers: 200, epoch: 3 | loss: 0.1206173\n",
      "\tspeed: 0.0147s/iter; left time: 532.2436s\n",
      "\titers: 300, epoch: 3 | loss: 0.1300245\n",
      "\tspeed: 0.0147s/iter; left time: 531.0605s\n",
      "\titers: 400, epoch: 3 | loss: 0.1121544\n",
      "\tspeed: 0.0147s/iter; left time: 529.4380s\n",
      "\titers: 500, epoch: 3 | loss: 0.1187190\n",
      "\tspeed: 0.0147s/iter; left time: 528.8672s\n",
      "\titers: 600, epoch: 3 | loss: 0.1955557\n",
      "\tspeed: 0.0147s/iter; left time: 527.7406s\n",
      "\titers: 700, epoch: 3 | loss: 0.0896333\n",
      "\tspeed: 0.0147s/iter; left time: 525.6546s\n",
      "\titers: 800, epoch: 3 | loss: 0.1293100\n",
      "\tspeed: 0.0147s/iter; left time: 524.9329s\n",
      "\titers: 900, epoch: 3 | loss: 0.1069431\n",
      "\tspeed: 0.0147s/iter; left time: 522.8022s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1430923\n",
      "\tspeed: 0.0147s/iter; left time: 521.4913s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1434563\n",
      "\tspeed: 0.0147s/iter; left time: 519.6546s\n",
      "\titers: 1200, epoch: 3 | loss: 0.0989282\n",
      "\tspeed: 0.0147s/iter; left time: 518.6062s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1284610\n",
      "\tspeed: 0.0147s/iter; left time: 516.7513s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1228089\n",
      "\tspeed: 0.0147s/iter; left time: 515.6138s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1053697\n",
      "\tspeed: 0.0147s/iter; left time: 514.2595s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1538387\n",
      "\tspeed: 0.0147s/iter; left time: 512.7587s\n",
      "\titers: 1700, epoch: 3 | loss: 0.0847474\n",
      "\tspeed: 0.0147s/iter; left time: 510.9779s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1803394\n",
      "\tspeed: 0.0147s/iter; left time: 510.1912s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0738180\n",
      "\tspeed: 0.0147s/iter; left time: 508.5670s\n",
      "\titers: 2000, epoch: 3 | loss: 0.1080962\n",
      "\tspeed: 0.0147s/iter; left time: 506.9182s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0741946\n",
      "\tspeed: 0.0147s/iter; left time: 505.5955s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0929136\n",
      "\tspeed: 0.0147s/iter; left time: 504.1381s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1887815\n",
      "\tspeed: 0.0147s/iter; left time: 502.6356s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1330007\n",
      "\tspeed: 0.0147s/iter; left time: 501.5413s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0777819\n",
      "\tspeed: 0.0147s/iter; left time: 499.7742s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1148708\n",
      "\tspeed: 0.0147s/iter; left time: 498.6066s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0894905\n",
      "\tspeed: 0.0147s/iter; left time: 497.0297s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1278787\n",
      "\tspeed: 0.0147s/iter; left time: 495.5109s\n",
      "\titers: 2900, epoch: 3 | loss: 0.1174078\n",
      "\tspeed: 0.0147s/iter; left time: 494.0517s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1663690\n",
      "\tspeed: 0.0147s/iter; left time: 492.4971s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1134720\n",
      "\tspeed: 0.0147s/iter; left time: 489.9849s\n",
      "\titers: 3200, epoch: 3 | loss: 0.0919659\n",
      "\tspeed: 0.0147s/iter; left time: 487.9217s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1346410\n",
      "\tspeed: 0.0147s/iter; left time: 486.6607s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1435977\n",
      "\tspeed: 0.0147s/iter; left time: 485.1292s\n",
      "\titers: 3500, epoch: 3 | loss: 0.1649803\n",
      "\tspeed: 0.0147s/iter; left time: 483.4681s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1071629\n",
      "\tspeed: 0.0147s/iter; left time: 481.7588s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1634591\n",
      "\tspeed: 0.0147s/iter; left time: 480.5632s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0894507\n",
      "\tspeed: 0.0147s/iter; left time: 479.4053s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0702681\n",
      "\tspeed: 0.0147s/iter; left time: 478.3505s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1284213\n",
      "\tspeed: 0.0146s/iter; left time: 475.2395s\n",
      "\titers: 4100, epoch: 3 | loss: 0.0869000\n",
      "\tspeed: 0.0146s/iter; left time: 472.2090s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0891982\n",
      "\tspeed: 0.0146s/iter; left time: 470.8298s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1132093\n",
      "\tspeed: 0.0146s/iter; left time: 469.2480s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1695403\n",
      "\tspeed: 0.0146s/iter; left time: 467.9474s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1036632\n",
      "\tspeed: 0.0146s/iter; left time: 466.3338s\n",
      "Epoch: 3 cost time: 67.173663854599\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1243915 Vali Loss: 0.0660020 Test Loss: 0.0876061\n",
      "Validation loss decreased (0.070528 --> 0.066002).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2082417\n",
      "\tspeed: 0.1293s/iter; left time: 4108.6519s\n",
      "\titers: 200, epoch: 4 | loss: 0.0879470\n",
      "\tspeed: 0.0146s/iter; left time: 462.5940s\n",
      "\titers: 300, epoch: 4 | loss: 0.0844748\n",
      "\tspeed: 0.0146s/iter; left time: 461.7881s\n",
      "\titers: 400, epoch: 4 | loss: 0.0571169\n",
      "\tspeed: 0.0146s/iter; left time: 459.7922s\n",
      "\titers: 500, epoch: 4 | loss: 0.1313982\n",
      "\tspeed: 0.0146s/iter; left time: 458.2207s\n",
      "\titers: 600, epoch: 4 | loss: 0.0538558\n",
      "\tspeed: 0.0146s/iter; left time: 456.5701s\n",
      "\titers: 700, epoch: 4 | loss: 0.0985864\n",
      "\tspeed: 0.0146s/iter; left time: 455.1443s\n",
      "\titers: 800, epoch: 4 | loss: 0.0826372\n",
      "\tspeed: 0.0146s/iter; left time: 453.6338s\n",
      "\titers: 900, epoch: 4 | loss: 0.1546814\n",
      "\tspeed: 0.0146s/iter; left time: 452.1367s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1964880\n",
      "\tspeed: 0.0146s/iter; left time: 450.7694s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0809737\n",
      "\tspeed: 0.0146s/iter; left time: 449.1601s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0824177\n",
      "\tspeed: 0.0146s/iter; left time: 447.1775s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1521258\n",
      "\tspeed: 0.0146s/iter; left time: 446.3395s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1076525\n",
      "\tspeed: 0.0146s/iter; left time: 444.7689s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1242884\n",
      "\tspeed: 0.0146s/iter; left time: 443.4053s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1456586\n",
      "\tspeed: 0.0146s/iter; left time: 441.1027s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1359025\n",
      "\tspeed: 0.0146s/iter; left time: 439.6090s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1271009\n",
      "\tspeed: 0.0146s/iter; left time: 437.9716s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1163095\n",
      "\tspeed: 0.0146s/iter; left time: 436.5902s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0998775\n",
      "\tspeed: 0.0145s/iter; left time: 434.8404s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1244938\n",
      "\tspeed: 0.0145s/iter; left time: 433.3753s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1313602\n",
      "\tspeed: 0.0146s/iter; left time: 431.9761s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1287825\n",
      "\tspeed: 0.0146s/iter; left time: 430.6621s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0953010\n",
      "\tspeed: 0.0146s/iter; left time: 429.2027s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0837687\n",
      "\tspeed: 0.0146s/iter; left time: 427.5968s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0952145\n",
      "\tspeed: 0.0145s/iter; left time: 426.0376s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0418774\n",
      "\tspeed: 0.0145s/iter; left time: 424.5668s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1078189\n",
      "\tspeed: 0.0146s/iter; left time: 423.2706s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1114410\n",
      "\tspeed: 0.0146s/iter; left time: 421.8873s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1259274\n",
      "\tspeed: 0.0146s/iter; left time: 420.3183s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1017504\n",
      "\tspeed: 0.0145s/iter; left time: 418.8117s\n",
      "\titers: 3200, epoch: 4 | loss: 0.0830042\n",
      "\tspeed: 0.0146s/iter; left time: 417.8880s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1550459\n",
      "\tspeed: 0.0146s/iter; left time: 416.4845s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1215862\n",
      "\tspeed: 0.0146s/iter; left time: 415.1277s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0617131\n",
      "\tspeed: 0.0146s/iter; left time: 413.5346s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0773357\n",
      "\tspeed: 0.0146s/iter; left time: 412.1676s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2605830\n",
      "\tspeed: 0.0146s/iter; left time: 410.6153s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1460643\n",
      "\tspeed: 0.0146s/iter; left time: 409.2536s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0593700\n",
      "\tspeed: 0.0146s/iter; left time: 407.8013s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0639021\n",
      "\tspeed: 0.0146s/iter; left time: 406.4531s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0791090\n",
      "\tspeed: 0.0146s/iter; left time: 404.8920s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1184223\n",
      "\tspeed: 0.0146s/iter; left time: 403.4590s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1046427\n",
      "\tspeed: 0.0146s/iter; left time: 402.1220s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1162956\n",
      "\tspeed: 0.0146s/iter; left time: 400.7050s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0984423\n",
      "\tspeed: 0.0146s/iter; left time: 399.1857s\n",
      "Epoch: 4 cost time: 66.66235446929932\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1151214 Vali Loss: 0.0669468 Test Loss: 0.0887885\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1009906\n",
      "\tspeed: 0.1365s/iter; left time: 3718.1232s\n",
      "\titers: 200, epoch: 5 | loss: 0.0777524\n",
      "\tspeed: 0.0148s/iter; left time: 400.5590s\n",
      "\titers: 300, epoch: 5 | loss: 0.1353675\n",
      "\tspeed: 0.0148s/iter; left time: 399.0090s\n",
      "\titers: 400, epoch: 5 | loss: 0.0968147\n",
      "\tspeed: 0.0147s/iter; left time: 396.5940s\n",
      "\titers: 500, epoch: 5 | loss: 0.0950768\n",
      "\tspeed: 0.0147s/iter; left time: 395.0995s\n",
      "\titers: 600, epoch: 5 | loss: 0.1142523\n",
      "\tspeed: 0.0148s/iter; left time: 394.4520s\n",
      "\titers: 700, epoch: 5 | loss: 0.0550596\n",
      "\tspeed: 0.0147s/iter; left time: 392.7098s\n",
      "\titers: 800, epoch: 5 | loss: 0.1476153\n",
      "\tspeed: 0.0148s/iter; left time: 391.7773s\n",
      "\titers: 900, epoch: 5 | loss: 0.0811148\n",
      "\tspeed: 0.0147s/iter; left time: 389.7185s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0831191\n",
      "\tspeed: 0.0147s/iter; left time: 388.2670s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1377074\n",
      "\tspeed: 0.0148s/iter; left time: 387.1766s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1203765\n",
      "\tspeed: 0.0148s/iter; left time: 385.4804s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1446262\n",
      "\tspeed: 0.0148s/iter; left time: 384.0133s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1223935\n",
      "\tspeed: 0.0148s/iter; left time: 382.8194s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0934269\n",
      "\tspeed: 0.0148s/iter; left time: 381.1419s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1294899\n",
      "\tspeed: 0.0147s/iter; left time: 379.4200s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0639271\n",
      "\tspeed: 0.0147s/iter; left time: 377.9323s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0859756\n",
      "\tspeed: 0.0148s/iter; left time: 376.8806s\n",
      "\titers: 1900, epoch: 5 | loss: 0.2059999\n",
      "\tspeed: 0.0147s/iter; left time: 375.0365s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0671764\n",
      "\tspeed: 0.0148s/iter; left time: 373.6646s\n",
      "\titers: 2100, epoch: 5 | loss: 0.2054187\n",
      "\tspeed: 0.0147s/iter; left time: 371.9318s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0821730\n",
      "\tspeed: 0.0147s/iter; left time: 370.2978s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0920942\n",
      "\tspeed: 0.0147s/iter; left time: 368.9628s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1508584\n",
      "\tspeed: 0.0147s/iter; left time: 367.5138s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0996743\n",
      "\tspeed: 0.0147s/iter; left time: 366.1788s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1005093\n",
      "\tspeed: 0.0147s/iter; left time: 364.7027s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0800282\n",
      "\tspeed: 0.0147s/iter; left time: 363.0435s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1182793\n",
      "\tspeed: 0.0147s/iter; left time: 361.6695s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0986310\n",
      "\tspeed: 0.0147s/iter; left time: 359.9408s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0844812\n",
      "\tspeed: 0.0147s/iter; left time: 358.4391s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1785090\n",
      "\tspeed: 0.0147s/iter; left time: 356.9942s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1082306\n",
      "\tspeed: 0.0147s/iter; left time: 355.4748s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0951147\n",
      "\tspeed: 0.0147s/iter; left time: 354.0375s\n",
      "\titers: 3400, epoch: 5 | loss: 0.1191685\n",
      "\tspeed: 0.0147s/iter; left time: 352.8816s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0991600\n",
      "\tspeed: 0.0147s/iter; left time: 350.4699s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0785601\n",
      "\tspeed: 0.0147s/iter; left time: 347.8918s\n",
      "\titers: 3700, epoch: 5 | loss: 0.1352019\n",
      "\tspeed: 0.0147s/iter; left time: 346.2826s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0827713\n",
      "\tspeed: 0.0147s/iter; left time: 344.8059s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1440006\n",
      "\tspeed: 0.0146s/iter; left time: 343.2371s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0901359\n",
      "\tspeed: 0.0147s/iter; left time: 341.8375s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0983312\n",
      "\tspeed: 0.0146s/iter; left time: 340.3121s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0835969\n",
      "\tspeed: 0.0147s/iter; left time: 338.9848s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1077833\n",
      "\tspeed: 0.0166s/iter; left time: 382.5861s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0909086\n",
      "\tspeed: 0.0165s/iter; left time: 378.3483s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1491358\n",
      "\tspeed: 0.0147s/iter; left time: 335.8665s\n",
      "Epoch: 5 cost time: 67.6796464920044\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1099477 Vali Loss: 0.0653416 Test Loss: 0.0872289\n",
      "Validation loss decreased (0.066002 --> 0.065342).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0840790\n",
      "\tspeed: 0.1298s/iter; left time: 2943.7712s\n",
      "\titers: 200, epoch: 6 | loss: 0.0682715\n",
      "\tspeed: 0.0147s/iter; left time: 332.9192s\n",
      "\titers: 300, epoch: 6 | loss: 0.0777930\n",
      "\tspeed: 0.0147s/iter; left time: 331.2218s\n",
      "\titers: 400, epoch: 6 | loss: 0.1050574\n",
      "\tspeed: 0.0147s/iter; left time: 329.9478s\n",
      "\titers: 500, epoch: 6 | loss: 0.1035563\n",
      "\tspeed: 0.0147s/iter; left time: 327.3574s\n",
      "\titers: 600, epoch: 6 | loss: 0.1057217\n",
      "\tspeed: 0.0146s/iter; left time: 324.4851s\n",
      "\titers: 700, epoch: 6 | loss: 0.0925243\n",
      "\tspeed: 0.0146s/iter; left time: 323.0174s\n",
      "\titers: 800, epoch: 6 | loss: 0.0897140\n",
      "\tspeed: 0.0146s/iter; left time: 321.4936s\n",
      "\titers: 900, epoch: 6 | loss: 0.0808573\n",
      "\tspeed: 0.0146s/iter; left time: 320.1443s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0709313\n",
      "\tspeed: 0.0146s/iter; left time: 318.4079s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1130559\n",
      "\tspeed: 0.0146s/iter; left time: 317.1740s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0651264\n",
      "\tspeed: 0.0146s/iter; left time: 315.7045s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1058737\n",
      "\tspeed: 0.0146s/iter; left time: 314.5473s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0639276\n",
      "\tspeed: 0.0146s/iter; left time: 312.7597s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0786646\n",
      "\tspeed: 0.0146s/iter; left time: 311.3843s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1710585\n",
      "\tspeed: 0.0146s/iter; left time: 309.9464s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1003297\n",
      "\tspeed: 0.0146s/iter; left time: 308.6421s\n",
      "\titers: 1800, epoch: 6 | loss: 0.0898466\n",
      "\tspeed: 0.0146s/iter; left time: 307.0035s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0957313\n",
      "\tspeed: 0.0146s/iter; left time: 305.3207s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1510548\n",
      "\tspeed: 0.0146s/iter; left time: 304.0013s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1156729\n",
      "\tspeed: 0.0146s/iter; left time: 302.6574s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0716318\n",
      "\tspeed: 0.0147s/iter; left time: 301.5104s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0978160\n",
      "\tspeed: 0.0147s/iter; left time: 300.1317s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0787583\n",
      "\tspeed: 0.0147s/iter; left time: 298.6034s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0632978\n",
      "\tspeed: 0.0147s/iter; left time: 297.2046s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0600352\n",
      "\tspeed: 0.0146s/iter; left time: 295.5095s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1230741\n",
      "\tspeed: 0.0146s/iter; left time: 294.0700s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0512644\n",
      "\tspeed: 0.0146s/iter; left time: 292.5587s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0788324\n",
      "\tspeed: 0.0147s/iter; left time: 291.2933s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0699454\n",
      "\tspeed: 0.0146s/iter; left time: 289.7150s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1063982\n",
      "\tspeed: 0.0146s/iter; left time: 288.1825s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1146473\n",
      "\tspeed: 0.0146s/iter; left time: 286.7600s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1368030\n",
      "\tspeed: 0.0147s/iter; left time: 285.3644s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0741549\n",
      "\tspeed: 0.0146s/iter; left time: 283.7450s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0589319\n",
      "\tspeed: 0.0146s/iter; left time: 282.2970s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1086436\n",
      "\tspeed: 0.0146s/iter; left time: 280.7882s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1096720\n",
      "\tspeed: 0.0146s/iter; left time: 279.3345s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1756727\n",
      "\tspeed: 0.0147s/iter; left time: 278.0750s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0765743\n",
      "\tspeed: 0.0147s/iter; left time: 276.5926s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1247278\n",
      "\tspeed: 0.0146s/iter; left time: 274.8531s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1016554\n",
      "\tspeed: 0.0146s/iter; left time: 273.4022s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1100100\n",
      "\tspeed: 0.0146s/iter; left time: 271.7474s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1156157\n",
      "\tspeed: 0.0146s/iter; left time: 270.4830s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0884752\n",
      "\tspeed: 0.0146s/iter; left time: 268.9981s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0928946\n",
      "\tspeed: 0.0146s/iter; left time: 267.4689s\n",
      "Epoch: 6 cost time: 67.0081536769867\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.1072982 Vali Loss: 0.0663171 Test Loss: 0.0908449\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0657965\n",
      "\tspeed: 0.1371s/iter; left time: 2485.0123s\n",
      "\titers: 200, epoch: 7 | loss: 0.0988712\n",
      "\tspeed: 0.0148s/iter; left time: 267.3820s\n",
      "\titers: 300, epoch: 7 | loss: 0.1269787\n",
      "\tspeed: 0.0148s/iter; left time: 266.1233s\n",
      "\titers: 400, epoch: 7 | loss: 0.0901025\n",
      "\tspeed: 0.0148s/iter; left time: 263.9933s\n",
      "\titers: 500, epoch: 7 | loss: 0.0984829\n",
      "\tspeed: 0.0148s/iter; left time: 262.0333s\n",
      "\titers: 600, epoch: 7 | loss: 0.2235246\n",
      "\tspeed: 0.0148s/iter; left time: 260.6317s\n",
      "\titers: 700, epoch: 7 | loss: 0.1385790\n",
      "\tspeed: 0.0148s/iter; left time: 259.1024s\n",
      "\titers: 800, epoch: 7 | loss: 0.1290635\n",
      "\tspeed: 0.0148s/iter; left time: 257.3727s\n",
      "\titers: 900, epoch: 7 | loss: 0.0908079\n",
      "\tspeed: 0.0148s/iter; left time: 255.7354s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0594939\n",
      "\tspeed: 0.0148s/iter; left time: 254.1960s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1778095\n",
      "\tspeed: 0.0152s/iter; left time: 260.9509s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1196685\n",
      "\tspeed: 0.0163s/iter; left time: 277.8671s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0851565\n",
      "\tspeed: 0.0163s/iter; left time: 276.5040s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0633190\n",
      "\tspeed: 0.0163s/iter; left time: 274.6971s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1687127\n",
      "\tspeed: 0.0163s/iter; left time: 273.0375s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1145796\n",
      "\tspeed: 0.0163s/iter; left time: 271.3866s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0912381\n",
      "\tspeed: 0.0164s/iter; left time: 270.1911s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0964783\n",
      "\tspeed: 0.0164s/iter; left time: 268.6408s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0938445\n",
      "\tspeed: 0.0164s/iter; left time: 267.0403s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0532764\n",
      "\tspeed: 0.0164s/iter; left time: 265.3612s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0595038\n",
      "\tspeed: 0.0164s/iter; left time: 263.6667s\n",
      "\titers: 2200, epoch: 7 | loss: 0.1614323\n",
      "\tspeed: 0.0157s/iter; left time: 250.9034s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0794722\n",
      "\tspeed: 0.0148s/iter; left time: 235.4274s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1633375\n",
      "\tspeed: 0.0148s/iter; left time: 233.8188s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1133566\n",
      "\tspeed: 0.0148s/iter; left time: 232.2917s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0887792\n",
      "\tspeed: 0.0148s/iter; left time: 230.8670s\n",
      "\titers: 2700, epoch: 7 | loss: 0.1432370\n",
      "\tspeed: 0.0148s/iter; left time: 229.3821s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0714819\n",
      "\tspeed: 0.0148s/iter; left time: 228.0109s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1141355\n",
      "\tspeed: 0.0148s/iter; left time: 226.3781s\n",
      "\titers: 3000, epoch: 7 | loss: 0.1135379\n",
      "\tspeed: 0.0148s/iter; left time: 224.9126s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0924033\n",
      "\tspeed: 0.0148s/iter; left time: 223.7004s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0805888\n",
      "\tspeed: 0.0148s/iter; left time: 222.4679s\n",
      "\titers: 3300, epoch: 7 | loss: 0.1004262\n",
      "\tspeed: 0.0148s/iter; left time: 221.1355s\n",
      "\titers: 3400, epoch: 7 | loss: 0.1299303\n",
      "\tspeed: 0.0153s/iter; left time: 226.3631s\n",
      "\titers: 3500, epoch: 7 | loss: 0.1917235\n",
      "\tspeed: 0.0164s/iter; left time: 240.9842s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0454723\n",
      "\tspeed: 0.0164s/iter; left time: 239.5614s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1126813\n",
      "\tspeed: 0.0163s/iter; left time: 237.0071s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1342876\n",
      "\tspeed: 0.0163s/iter; left time: 235.7496s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0649522\n",
      "\tspeed: 0.0152s/iter; left time: 218.1269s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1152801\n",
      "\tspeed: 0.0147s/iter; left time: 209.7398s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0809043\n",
      "\tspeed: 0.0148s/iter; left time: 208.3962s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1248428\n",
      "\tspeed: 0.0147s/iter; left time: 206.6552s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0984467\n",
      "\tspeed: 0.0147s/iter; left time: 205.1770s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0777029\n",
      "\tspeed: 0.0147s/iter; left time: 203.8104s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0851263\n",
      "\tspeed: 0.0147s/iter; left time: 202.3783s\n",
      "Epoch: 7 cost time: 70.06169700622559\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.1069643 Vali Loss: 0.0661995 Test Loss: 0.0902857\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1070758\n",
      "\tspeed: 0.1351s/iter; left time: 1833.1249s\n",
      "\titers: 200, epoch: 8 | loss: 0.1145796\n",
      "\tspeed: 0.0147s/iter; left time: 198.0797s\n",
      "\titers: 300, epoch: 8 | loss: 0.0953521\n",
      "\tspeed: 0.0148s/iter; left time: 197.2590s\n",
      "\titers: 400, epoch: 8 | loss: 0.0840584\n",
      "\tspeed: 0.0147s/iter; left time: 195.5012s\n",
      "\titers: 500, epoch: 8 | loss: 0.1449923\n",
      "\tspeed: 0.0147s/iter; left time: 194.0540s\n",
      "\titers: 600, epoch: 8 | loss: 0.1074190\n",
      "\tspeed: 0.0148s/iter; left time: 192.7455s\n",
      "\titers: 700, epoch: 8 | loss: 0.1119742\n",
      "\tspeed: 0.0148s/iter; left time: 191.2770s\n",
      "\titers: 800, epoch: 8 | loss: 0.1322702\n",
      "\tspeed: 0.0147s/iter; left time: 189.6375s\n",
      "\titers: 900, epoch: 8 | loss: 0.1035301\n",
      "\tspeed: 0.0147s/iter; left time: 187.8280s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1473969\n",
      "\tspeed: 0.0147s/iter; left time: 186.2902s\n",
      "\titers: 1100, epoch: 8 | loss: 0.1768054\n",
      "\tspeed: 0.0147s/iter; left time: 184.8415s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1141749\n",
      "\tspeed: 0.0147s/iter; left time: 183.5880s\n",
      "\titers: 1300, epoch: 8 | loss: 0.1154452\n",
      "\tspeed: 0.0147s/iter; left time: 182.3357s\n",
      "\titers: 1400, epoch: 8 | loss: 0.1266104\n",
      "\tspeed: 0.0147s/iter; left time: 180.7696s\n",
      "\titers: 1500, epoch: 8 | loss: 0.1545639\n",
      "\tspeed: 0.0147s/iter; left time: 179.1427s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0909835\n",
      "\tspeed: 0.0147s/iter; left time: 177.7484s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0786316\n",
      "\tspeed: 0.0147s/iter; left time: 176.0471s\n",
      "\titers: 1800, epoch: 8 | loss: 0.2185486\n",
      "\tspeed: 0.0147s/iter; left time: 174.6819s\n",
      "\titers: 1900, epoch: 8 | loss: 0.1291891\n",
      "\tspeed: 0.0147s/iter; left time: 173.1471s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1761144\n",
      "\tspeed: 0.0147s/iter; left time: 171.4565s\n",
      "\titers: 2100, epoch: 8 | loss: 0.1588298\n",
      "\tspeed: 0.0147s/iter; left time: 170.3150s\n",
      "\titers: 2200, epoch: 8 | loss: 0.1429343\n",
      "\tspeed: 0.0147s/iter; left time: 168.5521s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0873736\n",
      "\tspeed: 0.0147s/iter; left time: 167.1970s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1052906\n",
      "\tspeed: 0.0147s/iter; left time: 165.8809s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0790414\n",
      "\tspeed: 0.0147s/iter; left time: 164.2269s\n",
      "\titers: 2600, epoch: 8 | loss: 0.1140674\n",
      "\tspeed: 0.0147s/iter; left time: 162.8159s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1633579\n",
      "\tspeed: 0.0147s/iter; left time: 161.2504s\n",
      "\titers: 2800, epoch: 8 | loss: 0.1175348\n",
      "\tspeed: 0.0147s/iter; left time: 159.7462s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0923500\n",
      "\tspeed: 0.0147s/iter; left time: 158.3661s\n",
      "\titers: 3000, epoch: 8 | loss: 0.1072185\n",
      "\tspeed: 0.0147s/iter; left time: 156.8043s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0845513\n",
      "\tspeed: 0.0147s/iter; left time: 155.3084s\n",
      "\titers: 3200, epoch: 8 | loss: 0.2268294\n",
      "\tspeed: 0.0147s/iter; left time: 153.8163s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0661287\n",
      "\tspeed: 0.0147s/iter; left time: 152.3854s\n",
      "\titers: 3400, epoch: 8 | loss: 0.1394139\n",
      "\tspeed: 0.0147s/iter; left time: 150.8252s\n",
      "\titers: 3500, epoch: 8 | loss: 0.0928707\n",
      "\tspeed: 0.0147s/iter; left time: 149.3558s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0885622\n",
      "\tspeed: 0.0147s/iter; left time: 147.8900s\n",
      "\titers: 3700, epoch: 8 | loss: 0.0795707\n",
      "\tspeed: 0.0147s/iter; left time: 146.4929s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0776304\n",
      "\tspeed: 0.0147s/iter; left time: 145.0636s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0835515\n",
      "\tspeed: 0.0147s/iter; left time: 143.5508s\n",
      "\titers: 4000, epoch: 8 | loss: 0.1084633\n",
      "\tspeed: 0.0147s/iter; left time: 142.0943s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0988553\n",
      "\tspeed: 0.0147s/iter; left time: 140.5878s\n",
      "\titers: 4200, epoch: 8 | loss: 0.2052981\n",
      "\tspeed: 0.0147s/iter; left time: 139.0745s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0742925\n",
      "\tspeed: 0.0147s/iter; left time: 137.5748s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1400330\n",
      "\tspeed: 0.0147s/iter; left time: 135.8758s\n",
      "\titers: 4500, epoch: 8 | loss: 0.0734369\n",
      "\tspeed: 0.0147s/iter; left time: 134.3929s\n",
      "Epoch: 8 cost time: 67.28078627586365\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.1059544 Vali Loss: 0.0657462 Test Loss: 0.0895326\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "test shape: (5237, 24, 3) (5237, 24, 3)\n",
      "mse:0.08726100623607635, mae:0.16931110620498657\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features M --predictor solar_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3366a",
   "metadata": {},
   "source": [
    "### solar + wind (patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1750453\n",
      "\tspeed: 0.0281s/iter; left time: 1277.3561s\n",
      "\titers: 200, epoch: 1 | loss: 0.0986526\n",
      "\tspeed: 0.0162s/iter; left time: 736.4332s\n",
      "\titers: 300, epoch: 1 | loss: 0.0693140\n",
      "\tspeed: 0.0159s/iter; left time: 721.3194s\n",
      "\titers: 400, epoch: 1 | loss: 0.1302993\n",
      "\tspeed: 0.0157s/iter; left time: 707.6695s\n",
      "\titers: 500, epoch: 1 | loss: 0.0357760\n",
      "\tspeed: 0.0157s/iter; left time: 705.4777s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 600, epoch: 1 | loss: 0.0710588\n",
      "\tspeed: 0.0157s/iter; left time: 705.4997s\n",
      "\titers: 700, epoch: 1 | loss: 0.1424736\n",
      "\tspeed: 0.0156s/iter; left time: 701.2364s\n",
      "\titers: 800, epoch: 1 | loss: 0.4935403\n",
      "\tspeed: 0.0156s/iter; left time: 700.2918s\n",
      "\titers: 900, epoch: 1 | loss: 0.2640402\n",
      "\tspeed: 0.0156s/iter; left time: 698.3474s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0780762\n",
      "\tspeed: 0.0156s/iter; left time: 696.0853s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0592716\n",
      "\tspeed: 0.0156s/iter; left time: 694.0913s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0613589\n",
      "\tspeed: 0.0156s/iter; left time: 692.2755s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2604851\n",
      "\tspeed: 0.0156s/iter; left time: 690.9597s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0300129\n",
      "\tspeed: 0.0156s/iter; left time: 689.4656s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1288587\n",
      "\tspeed: 0.0156s/iter; left time: 687.5744s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1958280\n",
      "\tspeed: 0.0157s/iter; left time: 690.6457s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4338458\n",
      "\tspeed: 0.0162s/iter; left time: 708.9923s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0614051\n",
      "\tspeed: 0.0162s/iter; left time: 707.0306s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1824192\n",
      "\tspeed: 0.0161s/iter; left time: 704.9547s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3970298\n",
      "\tspeed: 0.0162s/iter; left time: 703.4574s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1622045\n",
      "\tspeed: 0.0162s/iter; left time: 702.1250s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3438289\n",
      "\tspeed: 0.0161s/iter; left time: 699.8810s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1691433\n",
      "\tspeed: 0.0145s/iter; left time: 626.4946s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1882255\n",
      "\tspeed: 0.0141s/iter; left time: 609.8151s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2796445\n",
      "\tspeed: 0.0156s/iter; left time: 669.9963s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1148230\n",
      "\tspeed: 0.0162s/iter; left time: 693.6842s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1674909\n",
      "\tspeed: 0.0162s/iter; left time: 692.8657s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1961166\n",
      "\tspeed: 0.0161s/iter; left time: 687.2937s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2504083\n",
      "\tspeed: 0.0161s/iter; left time: 688.7417s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0981979\n",
      "\tspeed: 0.0162s/iter; left time: 687.7057s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0790760\n",
      "\tspeed: 0.0162s/iter; left time: 686.1627s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2204227\n",
      "\tspeed: 0.0162s/iter; left time: 684.2061s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0367366\n",
      "\tspeed: 0.0162s/iter; left time: 682.5965s\n",
      "\titers: 3400, epoch: 1 | loss: 0.3772850\n",
      "\tspeed: 0.0162s/iter; left time: 680.8792s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0860394\n",
      "\tspeed: 0.0162s/iter; left time: 679.7822s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1037896\n",
      "\tspeed: 0.0162s/iter; left time: 677.9396s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0836873\n",
      "\tspeed: 0.0162s/iter; left time: 676.5675s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1524573\n",
      "\tspeed: 0.0162s/iter; left time: 674.4147s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3999090\n",
      "\tspeed: 0.0161s/iter; left time: 672.4637s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2680612\n",
      "\tspeed: 0.0162s/iter; left time: 671.1504s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0791855\n",
      "\tspeed: 0.0161s/iter; left time: 668.2368s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1020011\n",
      "\tspeed: 0.0157s/iter; left time: 648.0302s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1590880\n",
      "\tspeed: 0.0140s/iter; left time: 578.6476s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0435403\n",
      "\tspeed: 0.0140s/iter; left time: 577.5237s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0729997\n",
      "\tspeed: 0.0140s/iter; left time: 576.1269s\n",
      "Epoch: 1 cost time: 72.91723012924194\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1798450 Vali Loss: 0.0385564 Test Loss: 0.1259200\n",
      "Validation loss decreased (inf --> 0.038556).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1084619\n",
      "\tspeed: 0.1269s/iter; left time: 5190.1950s\n",
      "\titers: 200, epoch: 2 | loss: 0.0454620\n",
      "\tspeed: 0.0141s/iter; left time: 574.8379s\n",
      "\titers: 300, epoch: 2 | loss: 0.0867445\n",
      "\tspeed: 0.0141s/iter; left time: 573.2861s\n",
      "\titers: 400, epoch: 2 | loss: 0.1204490\n",
      "\tspeed: 0.0141s/iter; left time: 571.7053s\n",
      "\titers: 500, epoch: 2 | loss: 0.2254681\n",
      "\tspeed: 0.0141s/iter; left time: 569.8515s\n",
      "\titers: 600, epoch: 2 | loss: 0.1112931\n",
      "\tspeed: 0.0141s/iter; left time: 568.2397s\n",
      "\titers: 700, epoch: 2 | loss: 0.0973362\n",
      "\tspeed: 0.0141s/iter; left time: 566.8624s\n",
      "\titers: 800, epoch: 2 | loss: 0.0474080\n",
      "\tspeed: 0.0141s/iter; left time: 565.2566s\n",
      "\titers: 900, epoch: 2 | loss: 0.0510080\n",
      "\tspeed: 0.0141s/iter; left time: 564.1876s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1940967\n",
      "\tspeed: 0.0141s/iter; left time: 562.8126s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2158491\n",
      "\tspeed: 0.0141s/iter; left time: 561.2273s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1620346\n",
      "\tspeed: 0.0141s/iter; left time: 560.1907s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1147979\n",
      "\tspeed: 0.0141s/iter; left time: 558.6512s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1281408\n",
      "\tspeed: 0.0141s/iter; left time: 556.9996s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0488797\n",
      "\tspeed: 0.0141s/iter; left time: 555.4993s\n",
      "\titers: 1600, epoch: 2 | loss: 0.0824104\n",
      "\tspeed: 0.0141s/iter; left time: 553.5404s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2018268\n",
      "\tspeed: 0.0141s/iter; left time: 552.5380s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0919800\n",
      "\tspeed: 0.0141s/iter; left time: 551.9441s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1202603\n",
      "\tspeed: 0.0141s/iter; left time: 552.2684s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0692922\n",
      "\tspeed: 0.0141s/iter; left time: 550.5546s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0546693\n",
      "\tspeed: 0.0141s/iter; left time: 549.3261s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1325690\n",
      "\tspeed: 0.0141s/iter; left time: 547.4364s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1530341\n",
      "\tspeed: 0.0141s/iter; left time: 546.2722s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1340312\n",
      "\tspeed: 0.0141s/iter; left time: 544.6762s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0621151\n",
      "\tspeed: 0.0141s/iter; left time: 543.4892s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1817942\n",
      "\tspeed: 0.0141s/iter; left time: 541.9821s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1099020\n",
      "\tspeed: 0.0141s/iter; left time: 541.3126s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0707040\n",
      "\tspeed: 0.0141s/iter; left time: 539.9587s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0805482\n",
      "\tspeed: 0.0144s/iter; left time: 549.7355s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1291864\n",
      "\tspeed: 0.0161s/iter; left time: 611.6595s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0454051\n",
      "\tspeed: 0.0161s/iter; left time: 610.0882s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1567767\n",
      "\tspeed: 0.0161s/iter; left time: 608.1276s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0504156\n",
      "\tspeed: 0.0161s/iter; left time: 606.6838s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0766628\n",
      "\tspeed: 0.0161s/iter; left time: 605.1358s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1834736\n",
      "\tspeed: 0.0161s/iter; left time: 603.5777s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1622138\n",
      "\tspeed: 0.0161s/iter; left time: 601.8779s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1311818\n",
      "\tspeed: 0.0161s/iter; left time: 600.4151s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1210568\n",
      "\tspeed: 0.0161s/iter; left time: 598.6895s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2345525\n",
      "\tspeed: 0.0161s/iter; left time: 597.1561s\n",
      "\titers: 4000, epoch: 2 | loss: 0.3807611\n",
      "\tspeed: 0.0161s/iter; left time: 596.1331s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1053217\n",
      "\tspeed: 0.0161s/iter; left time: 594.4771s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1064705\n",
      "\tspeed: 0.0161s/iter; left time: 592.9358s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1125944\n",
      "\tspeed: 0.0161s/iter; left time: 591.2927s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1500449\n",
      "\tspeed: 0.0161s/iter; left time: 589.7014s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0639796\n",
      "\tspeed: 0.0161s/iter; left time: 587.6267s\n",
      "Epoch: 2 cost time: 67.79405760765076\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1435090 Vali Loss: 0.0360805 Test Loss: 0.1242945\n",
      "Validation loss decreased (0.038556 --> 0.036080).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1127197\n",
      "\tspeed: 0.1352s/iter; left time: 4913.1376s\n",
      "\titers: 200, epoch: 3 | loss: 0.0349977\n",
      "\tspeed: 0.0163s/iter; left time: 589.9003s\n",
      "\titers: 300, epoch: 3 | loss: 0.0958721\n",
      "\tspeed: 0.0163s/iter; left time: 587.7636s\n",
      "\titers: 400, epoch: 3 | loss: 0.0667603\n",
      "\tspeed: 0.0163s/iter; left time: 586.0087s\n",
      "\titers: 500, epoch: 3 | loss: 0.0830596\n",
      "\tspeed: 0.0163s/iter; left time: 584.2391s\n",
      "\titers: 600, epoch: 3 | loss: 0.1104254\n",
      "\tspeed: 0.0163s/iter; left time: 582.7630s\n",
      "\titers: 700, epoch: 3 | loss: 0.2730089\n",
      "\tspeed: 0.0162s/iter; left time: 580.0516s\n",
      "\titers: 800, epoch: 3 | loss: 0.1221922\n",
      "\tspeed: 0.0162s/iter; left time: 577.8611s\n",
      "\titers: 900, epoch: 3 | loss: 0.0698011\n",
      "\tspeed: 0.0162s/iter; left time: 576.5165s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0932118\n",
      "\tspeed: 0.0162s/iter; left time: 573.9523s\n",
      "\titers: 1100, epoch: 3 | loss: 0.0853688\n",
      "\tspeed: 0.0162s/iter; left time: 572.2665s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1320890\n",
      "\tspeed: 0.0162s/iter; left time: 570.5186s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2777387\n",
      "\tspeed: 0.0158s/iter; left time: 555.4665s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1123153\n",
      "\tspeed: 0.0156s/iter; left time: 546.6854s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2199660\n",
      "\tspeed: 0.0156s/iter; left time: 545.0588s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1516347\n",
      "\tspeed: 0.0156s/iter; left time: 543.6176s\n",
      "\titers: 1700, epoch: 3 | loss: 0.3804110\n",
      "\tspeed: 0.0156s/iter; left time: 542.3851s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0781511\n",
      "\tspeed: 0.0156s/iter; left time: 540.7252s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1622395\n",
      "\tspeed: 0.0156s/iter; left time: 539.1911s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2034501\n",
      "\tspeed: 0.0156s/iter; left time: 538.2411s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0788254\n",
      "\tspeed: 0.0156s/iter; left time: 536.3495s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0639623\n",
      "\tspeed: 0.0156s/iter; left time: 534.8872s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1209723\n",
      "\tspeed: 0.0156s/iter; left time: 533.0329s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1469334\n",
      "\tspeed: 0.0156s/iter; left time: 530.9639s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1264428\n",
      "\tspeed: 0.0156s/iter; left time: 529.3354s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0313640\n",
      "\tspeed: 0.0156s/iter; left time: 527.8171s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0427104\n",
      "\tspeed: 0.0156s/iter; left time: 526.3446s\n",
      "\titers: 2800, epoch: 3 | loss: 0.0939123\n",
      "\tspeed: 0.0156s/iter; left time: 524.8901s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0605273\n",
      "\tspeed: 0.0156s/iter; left time: 523.0368s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0471309\n",
      "\tspeed: 0.0156s/iter; left time: 521.5660s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1472992\n",
      "\tspeed: 0.0143s/iter; left time: 476.0817s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2923927\n",
      "\tspeed: 0.0141s/iter; left time: 469.6442s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0755630\n",
      "\tspeed: 0.0141s/iter; left time: 467.7285s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1332128\n",
      "\tspeed: 0.0141s/iter; left time: 466.7228s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0291694\n",
      "\tspeed: 0.0141s/iter; left time: 465.0784s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1216315\n",
      "\tspeed: 0.0141s/iter; left time: 464.0658s\n",
      "\titers: 3700, epoch: 3 | loss: 0.0922807\n",
      "\tspeed: 0.0141s/iter; left time: 462.3173s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0714464\n",
      "\tspeed: 0.0141s/iter; left time: 460.8030s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1765684\n",
      "\tspeed: 0.0141s/iter; left time: 459.2790s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1284116\n",
      "\tspeed: 0.0141s/iter; left time: 458.1948s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1956069\n",
      "\tspeed: 0.0141s/iter; left time: 456.6030s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0650602\n",
      "\tspeed: 0.0141s/iter; left time: 455.5095s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0283783\n",
      "\tspeed: 0.0141s/iter; left time: 453.7717s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0794239\n",
      "\tspeed: 0.0141s/iter; left time: 452.6073s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0271310\n",
      "\tspeed: 0.0141s/iter; left time: 451.0673s\n",
      "Epoch: 3 cost time: 69.83837151527405\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1117065 Vali Loss: 0.0327506 Test Loss: 0.1157892\n",
      "Validation loss decreased (0.036080 --> 0.032751).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0694353\n",
      "\tspeed: 0.1262s/iter; left time: 4012.1657s\n",
      "\titers: 200, epoch: 4 | loss: 0.2536150\n",
      "\tspeed: 0.0141s/iter; left time: 447.2305s\n",
      "\titers: 300, epoch: 4 | loss: 0.0392176\n",
      "\tspeed: 0.0141s/iter; left time: 445.3370s\n",
      "\titers: 400, epoch: 4 | loss: 0.1150045\n",
      "\tspeed: 0.0141s/iter; left time: 443.1721s\n",
      "\titers: 500, epoch: 4 | loss: 0.0615782\n",
      "\tspeed: 0.0141s/iter; left time: 441.5360s\n",
      "\titers: 600, epoch: 4 | loss: 0.1069522\n",
      "\tspeed: 0.0141s/iter; left time: 440.4107s\n",
      "\titers: 700, epoch: 4 | loss: 0.1127013\n",
      "\tspeed: 0.0141s/iter; left time: 439.2406s\n",
      "\titers: 800, epoch: 4 | loss: 0.1477412\n",
      "\tspeed: 0.0141s/iter; left time: 437.9182s\n",
      "\titers: 900, epoch: 4 | loss: 0.0798545\n",
      "\tspeed: 0.0141s/iter; left time: 436.6425s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1506715\n",
      "\tspeed: 0.0141s/iter; left time: 435.1763s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0802182\n",
      "\tspeed: 0.0141s/iter; left time: 433.5898s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0664146\n",
      "\tspeed: 0.0141s/iter; left time: 432.0821s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0803368\n",
      "\tspeed: 0.0141s/iter; left time: 431.0369s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0496724\n",
      "\tspeed: 0.0141s/iter; left time: 429.7233s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2626966\n",
      "\tspeed: 0.0141s/iter; left time: 428.1789s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0849125\n",
      "\tspeed: 0.0141s/iter; left time: 427.0878s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1040561\n",
      "\tspeed: 0.0141s/iter; left time: 425.2934s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0233057\n",
      "\tspeed: 0.0141s/iter; left time: 423.9972s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0372018\n",
      "\tspeed: 0.0141s/iter; left time: 422.8814s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1155394\n",
      "\tspeed: 0.0141s/iter; left time: 421.3954s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0230513\n",
      "\tspeed: 0.0141s/iter; left time: 419.7834s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1827137\n",
      "\tspeed: 0.0141s/iter; left time: 418.4023s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1269017\n",
      "\tspeed: 0.0141s/iter; left time: 416.8594s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0962540\n",
      "\tspeed: 0.0141s/iter; left time: 415.2824s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0248383\n",
      "\tspeed: 0.0141s/iter; left time: 413.7035s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0576897\n",
      "\tspeed: 0.0141s/iter; left time: 412.4842s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5715809\n",
      "\tspeed: 0.0141s/iter; left time: 410.9874s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0196525\n",
      "\tspeed: 0.0141s/iter; left time: 408.7407s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0338509\n",
      "\tspeed: 0.0141s/iter; left time: 407.3654s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1261639\n",
      "\tspeed: 0.0140s/iter; left time: 405.5941s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0930139\n",
      "\tspeed: 0.0140s/iter; left time: 404.3830s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1146367\n",
      "\tspeed: 0.0140s/iter; left time: 402.8605s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0792971\n",
      "\tspeed: 0.0141s/iter; left time: 402.0806s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0708127\n",
      "\tspeed: 0.0140s/iter; left time: 400.1756s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0343159\n",
      "\tspeed: 0.0141s/iter; left time: 398.9259s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0482372\n",
      "\tspeed: 0.0140s/iter; left time: 397.2686s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1357945\n",
      "\tspeed: 0.0141s/iter; left time: 396.1045s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0493133\n",
      "\tspeed: 0.0141s/iter; left time: 394.6448s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0908506\n",
      "\tspeed: 0.0141s/iter; left time: 393.2830s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0717595\n",
      "\tspeed: 0.0140s/iter; left time: 391.7773s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0392473\n",
      "\tspeed: 0.0140s/iter; left time: 390.3441s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0571737\n",
      "\tspeed: 0.0140s/iter; left time: 388.7797s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2541614\n",
      "\tspeed: 0.0141s/iter; left time: 387.6038s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1019018\n",
      "\tspeed: 0.0140s/iter; left time: 385.9069s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0682563\n",
      "\tspeed: 0.0140s/iter; left time: 384.6956s\n",
      "Epoch: 4 cost time: 64.35810160636902\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0936633 Vali Loss: 0.0315414 Test Loss: 0.1140375\n",
      "Validation loss decreased (0.032751 --> 0.031541).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0413422\n",
      "\tspeed: 0.1274s/iter; left time: 3468.3320s\n",
      "\titers: 200, epoch: 5 | loss: 0.0818925\n",
      "\tspeed: 0.0155s/iter; left time: 421.7964s\n",
      "\titers: 300, epoch: 5 | loss: 0.0684421\n",
      "\tspeed: 0.0147s/iter; left time: 398.3251s\n",
      "\titers: 400, epoch: 5 | loss: 0.0359451\n",
      "\tspeed: 0.0142s/iter; left time: 381.6974s\n",
      "\titers: 500, epoch: 5 | loss: 0.1169416\n",
      "\tspeed: 0.0141s/iter; left time: 379.4499s\n",
      "\titers: 600, epoch: 5 | loss: 0.0232839\n",
      "\tspeed: 0.0141s/iter; left time: 376.5122s\n",
      "\titers: 700, epoch: 5 | loss: 0.0647462\n",
      "\tspeed: 0.0141s/iter; left time: 375.3931s\n",
      "\titers: 800, epoch: 5 | loss: 0.0709734\n",
      "\tspeed: 0.0141s/iter; left time: 373.0025s\n",
      "\titers: 900, epoch: 5 | loss: 0.0345730\n",
      "\tspeed: 0.0141s/iter; left time: 371.7076s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1239711\n",
      "\tspeed: 0.0141s/iter; left time: 370.7526s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0356770\n",
      "\tspeed: 0.0141s/iter; left time: 369.4255s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1087995\n",
      "\tspeed: 0.0141s/iter; left time: 367.6409s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0727367\n",
      "\tspeed: 0.0141s/iter; left time: 366.2627s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0736063\n",
      "\tspeed: 0.0141s/iter; left time: 364.6609s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1336058\n",
      "\tspeed: 0.0141s/iter; left time: 363.2450s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0620684\n",
      "\tspeed: 0.0141s/iter; left time: 361.7874s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1242147\n",
      "\tspeed: 0.0141s/iter; left time: 360.4571s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0738285\n",
      "\tspeed: 0.0141s/iter; left time: 359.1472s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0302699\n",
      "\tspeed: 0.0141s/iter; left time: 358.4328s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5253404\n",
      "\tspeed: 0.0141s/iter; left time: 356.9609s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0542336\n",
      "\tspeed: 0.0141s/iter; left time: 355.4739s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0362175\n",
      "\tspeed: 0.0141s/iter; left time: 353.9643s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0311085\n",
      "\tspeed: 0.0141s/iter; left time: 352.6770s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0869372\n",
      "\tspeed: 0.0141s/iter; left time: 351.1631s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0726962\n",
      "\tspeed: 0.0141s/iter; left time: 349.5492s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0443350\n",
      "\tspeed: 0.0141s/iter; left time: 348.3540s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0865461\n",
      "\tspeed: 0.0141s/iter; left time: 346.9643s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0853011\n",
      "\tspeed: 0.0141s/iter; left time: 345.5954s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0755313\n",
      "\tspeed: 0.0141s/iter; left time: 344.1144s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0338696\n",
      "\tspeed: 0.0141s/iter; left time: 342.8017s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0368937\n",
      "\tspeed: 0.0141s/iter; left time: 341.2485s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0192847\n",
      "\tspeed: 0.0141s/iter; left time: 339.8612s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0663951\n",
      "\tspeed: 0.0141s/iter; left time: 339.3409s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0780979\n",
      "\tspeed: 0.0141s/iter; left time: 337.8573s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0400156\n",
      "\tspeed: 0.0141s/iter; left time: 336.3427s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0894562\n",
      "\tspeed: 0.0141s/iter; left time: 334.9250s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0557314\n",
      "\tspeed: 0.0141s/iter; left time: 333.5793s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1218853\n",
      "\tspeed: 0.0141s/iter; left time: 332.0532s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0491604\n",
      "\tspeed: 0.0141s/iter; left time: 330.8296s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0466786\n",
      "\tspeed: 0.0141s/iter; left time: 329.4645s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0269382\n",
      "\tspeed: 0.0141s/iter; left time: 328.0981s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0488806\n",
      "\tspeed: 0.0141s/iter; left time: 326.5903s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1023692\n",
      "\tspeed: 0.0141s/iter; left time: 325.1819s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0168385\n",
      "\tspeed: 0.0141s/iter; left time: 323.8212s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0587420\n",
      "\tspeed: 0.0141s/iter; left time: 322.5061s\n",
      "Epoch: 5 cost time: 64.83392333984375\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0851896 Vali Loss: 0.0325436 Test Loss: 0.1158307\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0325069\n",
      "\tspeed: 0.1260s/iter; left time: 2857.9750s\n",
      "\titers: 200, epoch: 6 | loss: 0.1262978\n",
      "\tspeed: 0.0141s/iter; left time: 318.9925s\n",
      "\titers: 300, epoch: 6 | loss: 0.0947527\n",
      "\tspeed: 0.0141s/iter; left time: 317.6348s\n",
      "\titers: 400, epoch: 6 | loss: 0.0830778\n",
      "\tspeed: 0.0141s/iter; left time: 316.0008s\n",
      "\titers: 500, epoch: 6 | loss: 0.0918976\n",
      "\tspeed: 0.0141s/iter; left time: 314.6035s\n",
      "\titers: 600, epoch: 6 | loss: 0.1526328\n",
      "\tspeed: 0.0141s/iter; left time: 312.9713s\n",
      "\titers: 700, epoch: 6 | loss: 0.0318347\n",
      "\tspeed: 0.0141s/iter; left time: 311.6333s\n",
      "\titers: 800, epoch: 6 | loss: 0.0659793\n",
      "\tspeed: 0.0141s/iter; left time: 310.3746s\n",
      "\titers: 900, epoch: 6 | loss: 0.0426042\n",
      "\tspeed: 0.0141s/iter; left time: 309.0026s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1309268\n",
      "\tspeed: 0.0141s/iter; left time: 307.2597s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0334980\n",
      "\tspeed: 0.0141s/iter; left time: 305.8929s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0603199\n",
      "\tspeed: 0.0141s/iter; left time: 304.7216s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1014504\n",
      "\tspeed: 0.0141s/iter; left time: 302.9798s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0505050\n",
      "\tspeed: 0.0141s/iter; left time: 301.5256s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0776042\n",
      "\tspeed: 0.0141s/iter; left time: 300.0822s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1313531\n",
      "\tspeed: 0.0141s/iter; left time: 298.7413s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0501765\n",
      "\tspeed: 0.0141s/iter; left time: 297.9888s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1875056\n",
      "\tspeed: 0.0141s/iter; left time: 296.6649s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1033079\n",
      "\tspeed: 0.0141s/iter; left time: 295.1866s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1389752\n",
      "\tspeed: 0.0141s/iter; left time: 293.7289s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1033946\n",
      "\tspeed: 0.0141s/iter; left time: 291.9278s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1036058\n",
      "\tspeed: 0.0141s/iter; left time: 289.9781s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0978907\n",
      "\tspeed: 0.0141s/iter; left time: 288.7131s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0535899\n",
      "\tspeed: 0.0141s/iter; left time: 287.0099s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0456146\n",
      "\tspeed: 0.0141s/iter; left time: 285.5387s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0584717\n",
      "\tspeed: 0.0141s/iter; left time: 284.1954s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0662157\n",
      "\tspeed: 0.0141s/iter; left time: 282.7202s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0763167\n",
      "\tspeed: 0.0141s/iter; left time: 281.2366s\n",
      "\titers: 2900, epoch: 6 | loss: 0.0953692\n",
      "\tspeed: 0.0141s/iter; left time: 279.9000s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0272528\n",
      "\tspeed: 0.0141s/iter; left time: 278.4689s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0548477\n",
      "\tspeed: 0.0141s/iter; left time: 277.1784s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0522385\n",
      "\tspeed: 0.0141s/iter; left time: 275.7659s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0645773\n",
      "\tspeed: 0.0141s/iter; left time: 274.4595s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0235564\n",
      "\tspeed: 0.0141s/iter; left time: 273.0262s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0109253\n",
      "\tspeed: 0.0141s/iter; left time: 271.7010s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0894742\n",
      "\tspeed: 0.0141s/iter; left time: 270.2830s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0941710\n",
      "\tspeed: 0.0141s/iter; left time: 268.8957s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1207255\n",
      "\tspeed: 0.0141s/iter; left time: 267.4578s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0381158\n",
      "\tspeed: 0.0141s/iter; left time: 266.3315s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0858604\n",
      "\tspeed: 0.0141s/iter; left time: 264.7268s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0139142\n",
      "\tspeed: 0.0141s/iter; left time: 263.2218s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0356113\n",
      "\tspeed: 0.0141s/iter; left time: 262.0562s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1043971\n",
      "\tspeed: 0.0141s/iter; left time: 260.3596s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0624197\n",
      "\tspeed: 0.0141s/iter; left time: 259.2009s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0674458\n",
      "\tspeed: 0.0141s/iter; left time: 257.5920s\n",
      "Epoch: 6 cost time: 64.49332022666931\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0817312 Vali Loss: 0.0320862 Test Loss: 0.1143577\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0805206\n",
      "\tspeed: 0.1260s/iter; left time: 2282.4830s\n",
      "\titers: 200, epoch: 7 | loss: 0.1106957\n",
      "\tspeed: 0.0141s/iter; left time: 253.5991s\n",
      "\titers: 300, epoch: 7 | loss: 0.1473908\n",
      "\tspeed: 0.0140s/iter; left time: 251.5055s\n",
      "\titers: 400, epoch: 7 | loss: 0.0644306\n",
      "\tspeed: 0.0140s/iter; left time: 250.0935s\n",
      "\titers: 500, epoch: 7 | loss: 0.0351286\n",
      "\tspeed: 0.0140s/iter; left time: 248.6921s\n",
      "\titers: 600, epoch: 7 | loss: 0.0507353\n",
      "\tspeed: 0.0140s/iter; left time: 247.5216s\n",
      "\titers: 700, epoch: 7 | loss: 0.0232976\n",
      "\tspeed: 0.0140s/iter; left time: 245.9662s\n",
      "\titers: 800, epoch: 7 | loss: 0.0555681\n",
      "\tspeed: 0.0141s/iter; left time: 244.8401s\n",
      "\titers: 900, epoch: 7 | loss: 0.1560813\n",
      "\tspeed: 0.0140s/iter; left time: 243.1311s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0276720\n",
      "\tspeed: 0.0141s/iter; left time: 243.5023s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1840050\n",
      "\tspeed: 0.0141s/iter; left time: 241.9978s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0463805\n",
      "\tspeed: 0.0141s/iter; left time: 240.4831s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0815297\n",
      "\tspeed: 0.0141s/iter; left time: 239.1666s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0965929\n",
      "\tspeed: 0.0141s/iter; left time: 237.5993s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1013292\n",
      "\tspeed: 0.0141s/iter; left time: 236.3509s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0438934\n",
      "\tspeed: 0.0141s/iter; left time: 234.7858s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0436664\n",
      "\tspeed: 0.0141s/iter; left time: 233.3934s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0541283\n",
      "\tspeed: 0.0141s/iter; left time: 232.0564s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0263246\n",
      "\tspeed: 0.0141s/iter; left time: 230.7051s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0462369\n",
      "\tspeed: 0.0141s/iter; left time: 229.2206s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0580633\n",
      "\tspeed: 0.0141s/iter; left time: 227.6978s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0383869\n",
      "\tspeed: 0.0141s/iter; left time: 226.4270s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0495071\n",
      "\tspeed: 0.0141s/iter; left time: 224.8894s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0527458\n",
      "\tspeed: 0.0141s/iter; left time: 223.6351s\n",
      "\titers: 2500, epoch: 7 | loss: 0.0646458\n",
      "\tspeed: 0.0141s/iter; left time: 222.0855s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0607015\n",
      "\tspeed: 0.0141s/iter; left time: 220.6394s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0285556\n",
      "\tspeed: 0.0141s/iter; left time: 219.1884s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0314296\n",
      "\tspeed: 0.0141s/iter; left time: 217.8027s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0751430\n",
      "\tspeed: 0.0141s/iter; left time: 216.3920s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0600111\n",
      "\tspeed: 0.0141s/iter; left time: 215.0160s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0205084\n",
      "\tspeed: 0.0141s/iter; left time: 213.6278s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1222879\n",
      "\tspeed: 0.0141s/iter; left time: 212.2061s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0561661\n",
      "\tspeed: 0.0141s/iter; left time: 210.7715s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0345161\n",
      "\tspeed: 0.0141s/iter; left time: 209.3421s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0358787\n",
      "\tspeed: 0.0141s/iter; left time: 207.9521s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0444971\n",
      "\tspeed: 0.0141s/iter; left time: 206.5115s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1544201\n",
      "\tspeed: 0.0141s/iter; left time: 205.0973s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0277495\n",
      "\tspeed: 0.0141s/iter; left time: 203.8066s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0450809\n",
      "\tspeed: 0.0141s/iter; left time: 202.4557s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0490745\n",
      "\tspeed: 0.0141s/iter; left time: 200.9806s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0659482\n",
      "\tspeed: 0.0141s/iter; left time: 199.5017s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0262400\n",
      "\tspeed: 0.0141s/iter; left time: 198.0443s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0208854\n",
      "\tspeed: 0.0141s/iter; left time: 196.4972s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0193085\n",
      "\tspeed: 0.0140s/iter; left time: 193.9625s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0992975\n",
      "\tspeed: 0.0140s/iter; left time: 192.5475s\n",
      "Epoch: 7 cost time: 64.50654363632202\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0787467 Vali Loss: 0.0318654 Test Loss: 0.1146283\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0678457\n",
      "\tspeed: 0.1278s/iter; left time: 1734.3877s\n",
      "\titers: 200, epoch: 8 | loss: 0.0406131\n",
      "\tspeed: 0.0146s/iter; left time: 196.3913s\n",
      "\titers: 300, epoch: 8 | loss: 0.1035504\n",
      "\tspeed: 0.0141s/iter; left time: 188.2481s\n",
      "\titers: 400, epoch: 8 | loss: 0.0365796\n",
      "\tspeed: 0.0155s/iter; left time: 205.1653s\n",
      "\titers: 500, epoch: 8 | loss: 0.1780364\n",
      "\tspeed: 0.0156s/iter; left time: 205.2116s\n",
      "\titers: 600, epoch: 8 | loss: 0.0787749\n",
      "\tspeed: 0.0156s/iter; left time: 203.9432s\n",
      "\titers: 700, epoch: 8 | loss: 0.1211747\n",
      "\tspeed: 0.0156s/iter; left time: 202.3252s\n",
      "\titers: 800, epoch: 8 | loss: 0.1432823\n",
      "\tspeed: 0.0156s/iter; left time: 200.7412s\n",
      "\titers: 900, epoch: 8 | loss: 0.0638641\n",
      "\tspeed: 0.0156s/iter; left time: 199.1643s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0473557\n",
      "\tspeed: 0.0148s/iter; left time: 187.9340s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0948041\n",
      "\tspeed: 0.0141s/iter; left time: 176.7698s\n",
      "\titers: 1200, epoch: 8 | loss: 0.2043074\n",
      "\tspeed: 0.0141s/iter; left time: 175.4261s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3638110\n",
      "\tspeed: 0.0141s/iter; left time: 174.0140s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0798790\n",
      "\tspeed: 0.0141s/iter; left time: 172.9455s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0681412\n",
      "\tspeed: 0.0141s/iter; left time: 171.4711s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0991010\n",
      "\tspeed: 0.0147s/iter; left time: 176.8632s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0233468\n",
      "\tspeed: 0.0156s/iter; left time: 186.9055s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0899133\n",
      "\tspeed: 0.0156s/iter; left time: 185.3565s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0727243\n",
      "\tspeed: 0.0156s/iter; left time: 183.8622s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0745683\n",
      "\tspeed: 0.0156s/iter; left time: 182.1887s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0410042\n",
      "\tspeed: 0.0156s/iter; left time: 180.7583s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0447275\n",
      "\tspeed: 0.0156s/iter; left time: 179.1751s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0817338\n",
      "\tspeed: 0.0142s/iter; left time: 161.5538s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0614929\n",
      "\tspeed: 0.0141s/iter; left time: 158.8502s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0624712\n",
      "\tspeed: 0.0141s/iter; left time: 157.4024s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0325403\n",
      "\tspeed: 0.0141s/iter; left time: 156.0021s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0444778\n",
      "\tspeed: 0.0151s/iter; left time: 165.9348s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0481964\n",
      "\tspeed: 0.0153s/iter; left time: 166.4648s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0880119\n",
      "\tspeed: 0.0156s/iter; left time: 168.2053s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2584287\n",
      "\tspeed: 0.0147s/iter; left time: 157.3088s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0743446\n",
      "\tspeed: 0.0141s/iter; left time: 148.9339s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1352849\n",
      "\tspeed: 0.0141s/iter; left time: 147.5621s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0373918\n",
      "\tspeed: 0.0141s/iter; left time: 146.0992s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0545849\n",
      "\tspeed: 0.0141s/iter; left time: 144.7025s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1283219\n",
      "\tspeed: 0.0141s/iter; left time: 143.2687s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0289706\n",
      "\tspeed: 0.0141s/iter; left time: 141.8052s\n",
      "\titers: 3700, epoch: 8 | loss: 0.1404278\n",
      "\tspeed: 0.0141s/iter; left time: 140.3531s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0609376\n",
      "\tspeed: 0.0141s/iter; left time: 139.0627s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0819000\n",
      "\tspeed: 0.0141s/iter; left time: 137.6434s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0499514\n",
      "\tspeed: 0.0141s/iter; left time: 136.2149s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0224955\n",
      "\tspeed: 0.0141s/iter; left time: 134.6633s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0812120\n",
      "\tspeed: 0.0141s/iter; left time: 133.2040s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0768979\n",
      "\tspeed: 0.0141s/iter; left time: 131.8144s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1324289\n",
      "\tspeed: 0.0141s/iter; left time: 130.4172s\n",
      "\titers: 4500, epoch: 8 | loss: 0.0886921\n",
      "\tspeed: 0.0141s/iter; left time: 129.0221s\n",
      "Epoch: 8 cost time: 67.05374050140381\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0791331 Vali Loss: 0.0319320 Test Loss: 0.1143092\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0478899\n",
      "\tspeed: 0.1401s/iter; left time: 1262.5694s\n",
      "\titers: 200, epoch: 9 | loss: 0.0411773\n",
      "\tspeed: 0.0141s/iter; left time: 125.2273s\n",
      "\titers: 300, epoch: 9 | loss: 0.0353844\n",
      "\tspeed: 0.0141s/iter; left time: 124.0030s\n",
      "\titers: 400, epoch: 9 | loss: 0.0977097\n",
      "\tspeed: 0.0141s/iter; left time: 122.5101s\n",
      "\titers: 500, epoch: 9 | loss: 0.0714036\n",
      "\tspeed: 0.0141s/iter; left time: 121.0287s\n",
      "\titers: 600, epoch: 9 | loss: 0.0311396\n",
      "\tspeed: 0.0141s/iter; left time: 119.5798s\n",
      "\titers: 700, epoch: 9 | loss: 0.1078035\n",
      "\tspeed: 0.0140s/iter; left time: 118.1695s\n",
      "\titers: 800, epoch: 9 | loss: 0.0298331\n",
      "\tspeed: 0.0141s/iter; left time: 117.4182s\n",
      "\titers: 900, epoch: 9 | loss: 0.0420769\n",
      "\tspeed: 0.0141s/iter; left time: 116.1173s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0681224\n",
      "\tspeed: 0.0141s/iter; left time: 114.7149s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0620087\n",
      "\tspeed: 0.0141s/iter; left time: 113.3105s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1952560\n",
      "\tspeed: 0.0141s/iter; left time: 111.8963s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0333421\n",
      "\tspeed: 0.0141s/iter; left time: 110.4671s\n",
      "\titers: 1400, epoch: 9 | loss: 0.1897581\n",
      "\tspeed: 0.0141s/iter; left time: 109.0144s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0630544\n",
      "\tspeed: 0.0141s/iter; left time: 107.6053s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0572921\n",
      "\tspeed: 0.0141s/iter; left time: 105.6442s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0579260\n",
      "\tspeed: 0.0140s/iter; left time: 104.0507s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0517140\n",
      "\tspeed: 0.0140s/iter; left time: 102.6384s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0202033\n",
      "\tspeed: 0.0140s/iter; left time: 101.2394s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0519464\n",
      "\tspeed: 0.0140s/iter; left time: 99.8678s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0364485\n",
      "\tspeed: 0.0140s/iter; left time: 98.4814s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1159452\n",
      "\tspeed: 0.0141s/iter; left time: 97.1996s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0949825\n",
      "\tspeed: 0.0141s/iter; left time: 95.7169s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0318688\n",
      "\tspeed: 0.0140s/iter; left time: 94.2744s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0413194\n",
      "\tspeed: 0.0140s/iter; left time: 92.8341s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0463185\n",
      "\tspeed: 0.0140s/iter; left time: 91.3993s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0408158\n",
      "\tspeed: 0.0141s/iter; left time: 90.0942s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1229174\n",
      "\tspeed: 0.0141s/iter; left time: 88.7820s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0545473\n",
      "\tspeed: 0.0141s/iter; left time: 87.3657s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0676728\n",
      "\tspeed: 0.0141s/iter; left time: 85.9862s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0278492\n",
      "\tspeed: 0.0141s/iter; left time: 84.7254s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0564936\n",
      "\tspeed: 0.0141s/iter; left time: 83.2456s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2099865\n",
      "\tspeed: 0.0141s/iter; left time: 81.8354s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0911413\n",
      "\tspeed: 0.0141s/iter; left time: 80.5112s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0692234\n",
      "\tspeed: 0.0141s/iter; left time: 79.0782s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0329029\n",
      "\tspeed: 0.0141s/iter; left time: 77.6344s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0511283\n",
      "\tspeed: 0.0141s/iter; left time: 76.3005s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1436139\n",
      "\tspeed: 0.0141s/iter; left time: 74.8083s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0497653\n",
      "\tspeed: 0.0141s/iter; left time: 73.4507s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0787350\n",
      "\tspeed: 0.0141s/iter; left time: 71.9911s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0475103\n",
      "\tspeed: 0.0141s/iter; left time: 70.6321s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0682015\n",
      "\tspeed: 0.0141s/iter; left time: 69.2261s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0620698\n",
      "\tspeed: 0.0141s/iter; left time: 67.8017s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0348480\n",
      "\tspeed: 0.0141s/iter; left time: 66.3694s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0356965\n",
      "\tspeed: 0.0141s/iter; left time: 64.9652s\n",
      "Epoch: 9 cost time: 64.40155601501465\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0774516 Vali Loss: 0.0319157 Test Loss: 0.1143249\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11406522244215012, mae:0.20758919417858124\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor solar_forecast,wind_forecast,total_load --enc_in 4 --dec_in 4 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b79254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1482732\n",
      "\tspeed: 0.0264s/iter; left time: 1199.2928s\n",
      "\titers: 200, epoch: 1 | loss: 0.3056500\n",
      "\tspeed: 0.0146s/iter; left time: 662.9411s\n",
      "\titers: 300, epoch: 1 | loss: 0.3614666\n",
      "\tspeed: 0.0146s/iter; left time: 660.5590s\n",
      "\titers: 400, epoch: 1 | loss: 0.2376823\n",
      "\tspeed: 0.0147s/iter; left time: 663.3985s\n",
      "\titers: 500, epoch: 1 | loss: 0.3005523\n",
      "\tspeed: 0.0147s/iter; left time: 662.5863s\n",
      "\titers: 600, epoch: 1 | loss: 0.2208910\n",
      "\tspeed: 0.0148s/iter; left time: 663.3881s\n",
      "\titers: 700, epoch: 1 | loss: 0.1622673\n",
      "\tspeed: 0.0147s/iter; left time: 658.1907s\n",
      "\titers: 800, epoch: 1 | loss: 0.2873693\n",
      "\tspeed: 0.0147s/iter; left time: 655.8134s\n",
      "\titers: 900, epoch: 1 | loss: 0.2124687\n",
      "\tspeed: 0.0147s/iter; left time: 655.0695s\n",
      "\titers: 1000, epoch: 1 | loss: 0.2004651\n",
      "\tspeed: 0.0147s/iter; left time: 653.9281s\n",
      "\titers: 1100, epoch: 1 | loss: 0.3035830\n",
      "\tspeed: 0.0147s/iter; left time: 651.2683s\n",
      "\titers: 1200, epoch: 1 | loss: 0.3066354\n",
      "\tspeed: 0.0146s/iter; left time: 649.0837s\n",
      "\titers: 1300, epoch: 1 | loss: 0.1835537\n",
      "\tspeed: 0.0146s/iter; left time: 648.0143s\n",
      "\titers: 1400, epoch: 1 | loss: 0.3819258\n",
      "\tspeed: 0.0146s/iter; left time: 645.6921s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1448227\n",
      "\tspeed: 0.0147s/iter; left time: 645.8527s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2658688\n",
      "\tspeed: 0.0146s/iter; left time: 643.5893s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4097134\n",
      "\tspeed: 0.0146s/iter; left time: 641.7655s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1671428\n",
      "\tspeed: 0.0146s/iter; left time: 640.0251s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1719536\n",
      "\tspeed: 0.0146s/iter; left time: 639.0702s\n",
      "\titers: 2000, epoch: 1 | loss: 0.2062026\n",
      "\tspeed: 0.0146s/iter; left time: 637.0763s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1454974\n",
      "\tspeed: 0.0146s/iter; left time: 635.4741s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2687164\n",
      "\tspeed: 0.0146s/iter; left time: 632.7908s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1835314\n",
      "\tspeed: 0.0145s/iter; left time: 628.1787s\n",
      "\titers: 2400, epoch: 1 | loss: 0.2309533\n",
      "\tspeed: 0.0145s/iter; left time: 626.5316s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1681151\n",
      "\tspeed: 0.0145s/iter; left time: 625.1795s\n",
      "\titers: 2600, epoch: 1 | loss: 0.2447085\n",
      "\tspeed: 0.0145s/iter; left time: 624.1305s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2588028\n",
      "\tspeed: 0.0145s/iter; left time: 622.3155s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2295492\n",
      "\tspeed: 0.0145s/iter; left time: 620.9184s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1916413\n",
      "\tspeed: 0.0145s/iter; left time: 619.4713s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2271618\n",
      "\tspeed: 0.0145s/iter; left time: 617.2019s\n",
      "\titers: 3100, epoch: 1 | loss: 0.4091228\n",
      "\tspeed: 0.0145s/iter; left time: 616.4519s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2479726\n",
      "\tspeed: 0.0145s/iter; left time: 615.2378s\n",
      "\titers: 3300, epoch: 1 | loss: 0.2521783\n",
      "\tspeed: 0.0145s/iter; left time: 613.7278s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1459562\n",
      "\tspeed: 0.0145s/iter; left time: 611.8831s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1987698\n",
      "\tspeed: 0.0145s/iter; left time: 610.4368s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1969016\n",
      "\tspeed: 0.0145s/iter; left time: 608.7411s\n",
      "\titers: 3700, epoch: 1 | loss: 0.2151227\n",
      "\tspeed: 0.0145s/iter; left time: 607.2882s\n",
      "\titers: 3800, epoch: 1 | loss: 0.2422468\n",
      "\tspeed: 0.0145s/iter; left time: 606.3215s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3762795\n",
      "\tspeed: 0.0145s/iter; left time: 604.1550s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3276313\n",
      "\tspeed: 0.0145s/iter; left time: 602.8246s\n",
      "\titers: 4100, epoch: 1 | loss: 0.1426593\n",
      "\tspeed: 0.0145s/iter; left time: 601.2844s\n",
      "\titers: 4200, epoch: 1 | loss: 0.2129460\n",
      "\tspeed: 0.0145s/iter; left time: 600.0445s\n",
      "\titers: 4300, epoch: 1 | loss: 0.3126874\n",
      "\tspeed: 0.0145s/iter; left time: 598.4161s\n",
      "\titers: 4400, epoch: 1 | loss: 0.1671873\n",
      "\tspeed: 0.0145s/iter; left time: 596.8389s\n",
      "\titers: 4500, epoch: 1 | loss: 0.1673822\n",
      "\tspeed: 0.0145s/iter; left time: 595.3926s\n",
      "Epoch: 1 cost time: 67.63712692260742\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.2329371 Vali Loss: 0.1604693 Test Loss: 0.1637938\n",
      "Validation loss decreased (inf --> 0.160469).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2494518\n",
      "\tspeed: 0.1287s/iter; left time: 5263.3008s\n",
      "\titers: 200, epoch: 2 | loss: 0.3370044\n",
      "\tspeed: 0.0163s/iter; left time: 663.3747s\n",
      "\titers: 300, epoch: 2 | loss: 0.2866796\n",
      "\tspeed: 0.0163s/iter; left time: 662.9795s\n",
      "\titers: 400, epoch: 2 | loss: 0.1909690\n",
      "\tspeed: 0.0162s/iter; left time: 659.3149s\n",
      "\titers: 500, epoch: 2 | loss: 0.2141520\n",
      "\tspeed: 0.0158s/iter; left time: 641.3874s\n",
      "\titers: 600, epoch: 2 | loss: 0.2021720\n",
      "\tspeed: 0.0161s/iter; left time: 649.2864s\n",
      "\titers: 700, epoch: 2 | loss: 0.1596829\n",
      "\tspeed: 0.0162s/iter; left time: 653.3673s\n",
      "\titers: 800, epoch: 2 | loss: 0.1124944\n",
      "\tspeed: 0.0163s/iter; left time: 653.9467s\n",
      "\titers: 900, epoch: 2 | loss: 0.2056488\n",
      "\tspeed: 0.0162s/iter; left time: 651.3607s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2250295\n",
      "\tspeed: 0.0163s/iter; left time: 650.0758s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2469537\n",
      "\tspeed: 0.0162s/iter; left time: 647.3865s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2216826\n",
      "\tspeed: 0.0162s/iter; left time: 645.9597s\n",
      "\titers: 1300, epoch: 2 | loss: 0.2915415\n",
      "\tspeed: 0.0162s/iter; left time: 644.4178s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1941592\n",
      "\tspeed: 0.0162s/iter; left time: 643.3170s\n",
      "\titers: 1500, epoch: 2 | loss: 0.1273992\n",
      "\tspeed: 0.0162s/iter; left time: 640.9936s\n",
      "\titers: 1600, epoch: 2 | loss: 0.2205631\n",
      "\tspeed: 0.0162s/iter; left time: 639.1032s\n",
      "\titers: 1700, epoch: 2 | loss: 0.1828490\n",
      "\tspeed: 0.0162s/iter; left time: 637.0586s\n",
      "\titers: 1800, epoch: 2 | loss: 0.2197598\n",
      "\tspeed: 0.0162s/iter; left time: 635.6642s\n",
      "\titers: 1900, epoch: 2 | loss: 0.2154557\n",
      "\tspeed: 0.0162s/iter; left time: 633.9155s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1802035\n",
      "\tspeed: 0.0162s/iter; left time: 632.3100s\n",
      "\titers: 2100, epoch: 2 | loss: 0.1942134\n",
      "\tspeed: 0.0162s/iter; left time: 630.7428s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1733727\n",
      "\tspeed: 0.0150s/iter; left time: 580.1195s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1423124\n",
      "\tspeed: 0.0146s/iter; left time: 563.9846s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1388792\n",
      "\tspeed: 0.0146s/iter; left time: 562.3581s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1817766\n",
      "\tspeed: 0.0146s/iter; left time: 560.9798s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1470755\n",
      "\tspeed: 0.0146s/iter; left time: 559.5902s\n",
      "\titers: 2700, epoch: 2 | loss: 0.0960657\n",
      "\tspeed: 0.0146s/iter; left time: 558.2720s\n",
      "\titers: 2800, epoch: 2 | loss: 0.1793707\n",
      "\tspeed: 0.0146s/iter; left time: 556.7846s\n",
      "\titers: 2900, epoch: 2 | loss: 0.2154475\n",
      "\tspeed: 0.0146s/iter; left time: 555.4220s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1407404\n",
      "\tspeed: 0.0146s/iter; left time: 553.3246s\n",
      "\titers: 3100, epoch: 2 | loss: 0.2012191\n",
      "\tspeed: 0.0145s/iter; left time: 550.3767s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1083248\n",
      "\tspeed: 0.0145s/iter; left time: 548.7718s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1767366\n",
      "\tspeed: 0.0145s/iter; left time: 547.4158s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1455093\n",
      "\tspeed: 0.0145s/iter; left time: 546.0598s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1415934\n",
      "\tspeed: 0.0145s/iter; left time: 544.5883s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1887666\n",
      "\tspeed: 0.0145s/iter; left time: 543.2314s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1349013\n",
      "\tspeed: 0.0145s/iter; left time: 541.4334s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1587325\n",
      "\tspeed: 0.0145s/iter; left time: 539.9982s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1377392\n",
      "\tspeed: 0.0145s/iter; left time: 538.5125s\n",
      "\titers: 4000, epoch: 2 | loss: 0.1679386\n",
      "\tspeed: 0.0145s/iter; left time: 536.9284s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1548477\n",
      "\tspeed: 0.0145s/iter; left time: 535.7056s\n",
      "\titers: 4200, epoch: 2 | loss: 0.1182676\n",
      "\tspeed: 0.0145s/iter; left time: 534.1596s\n",
      "\titers: 4300, epoch: 2 | loss: 0.2218968\n",
      "\tspeed: 0.0145s/iter; left time: 532.5135s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1239753\n",
      "\tspeed: 0.0145s/iter; left time: 531.2376s\n",
      "\titers: 4500, epoch: 2 | loss: 0.3366104\n",
      "\tspeed: 0.0145s/iter; left time: 529.8891s\n",
      "Epoch: 2 cost time: 69.93576335906982\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1929715 Vali Loss: 0.1734458 Test Loss: 0.1681689\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2444387\n",
      "\tspeed: 0.1297s/iter; left time: 4712.3673s\n",
      "\titers: 200, epoch: 3 | loss: 0.1899634\n",
      "\tspeed: 0.0147s/iter; left time: 532.6926s\n",
      "\titers: 300, epoch: 3 | loss: 0.1654747\n",
      "\tspeed: 0.0147s/iter; left time: 532.2266s\n",
      "\titers: 400, epoch: 3 | loss: 0.1345528\n",
      "\tspeed: 0.0147s/iter; left time: 530.5448s\n",
      "\titers: 500, epoch: 3 | loss: 0.1505862\n",
      "\tspeed: 0.0147s/iter; left time: 527.5768s\n",
      "\titers: 600, epoch: 3 | loss: 0.1564683\n",
      "\tspeed: 0.0147s/iter; left time: 528.6010s\n",
      "\titers: 700, epoch: 3 | loss: 0.1069406\n",
      "\tspeed: 0.0147s/iter; left time: 526.1376s\n",
      "\titers: 800, epoch: 3 | loss: 0.2168484\n",
      "\tspeed: 0.0148s/iter; left time: 525.7498s\n",
      "\titers: 900, epoch: 3 | loss: 0.2664617\n",
      "\tspeed: 0.0147s/iter; left time: 522.8633s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1428763\n",
      "\tspeed: 0.0147s/iter; left time: 520.8694s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1447126\n",
      "\tspeed: 0.0147s/iter; left time: 519.2721s\n",
      "\titers: 1200, epoch: 3 | loss: 0.2733040\n",
      "\tspeed: 0.0147s/iter; left time: 518.2413s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1253953\n",
      "\tspeed: 0.0147s/iter; left time: 516.6345s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1082279\n",
      "\tspeed: 0.0147s/iter; left time: 516.0641s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2396155\n",
      "\tspeed: 0.0147s/iter; left time: 514.5836s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1506806\n",
      "\tspeed: 0.0147s/iter; left time: 512.6693s\n",
      "\titers: 1700, epoch: 3 | loss: 0.2059536\n",
      "\tspeed: 0.0147s/iter; left time: 511.1273s\n",
      "\titers: 1800, epoch: 3 | loss: 0.1294733\n",
      "\tspeed: 0.0147s/iter; left time: 509.5587s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1689831\n",
      "\tspeed: 0.0147s/iter; left time: 507.7086s\n",
      "\titers: 2000, epoch: 3 | loss: 0.0831051\n",
      "\tspeed: 0.0147s/iter; left time: 506.0970s\n",
      "\titers: 2100, epoch: 3 | loss: 0.1447430\n",
      "\tspeed: 0.0147s/iter; left time: 505.0096s\n",
      "\titers: 2200, epoch: 3 | loss: 0.2359949\n",
      "\tspeed: 0.0147s/iter; left time: 503.2595s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1481593\n",
      "\tspeed: 0.0147s/iter; left time: 502.7008s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1379936\n",
      "\tspeed: 0.0147s/iter; left time: 501.0212s\n",
      "\titers: 2500, epoch: 3 | loss: 0.0834344\n",
      "\tspeed: 0.0147s/iter; left time: 499.1521s\n",
      "\titers: 2600, epoch: 3 | loss: 0.1407903\n",
      "\tspeed: 0.0147s/iter; left time: 497.2233s\n",
      "\titers: 2700, epoch: 3 | loss: 0.1272321\n",
      "\tspeed: 0.0147s/iter; left time: 495.8658s\n",
      "\titers: 2800, epoch: 3 | loss: 0.2102586\n",
      "\tspeed: 0.0147s/iter; left time: 494.8132s\n",
      "\titers: 2900, epoch: 3 | loss: 0.2009628\n",
      "\tspeed: 0.0147s/iter; left time: 493.2022s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1348968\n",
      "\tspeed: 0.0147s/iter; left time: 491.4304s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1041845\n",
      "\tspeed: 0.0147s/iter; left time: 490.4726s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2050215\n",
      "\tspeed: 0.0147s/iter; left time: 488.8995s\n",
      "\titers: 3300, epoch: 3 | loss: 0.3107644\n",
      "\tspeed: 0.0147s/iter; left time: 486.5261s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2295838\n",
      "\tspeed: 0.0146s/iter; left time: 483.8399s\n",
      "\titers: 3500, epoch: 3 | loss: 0.3363081\n",
      "\tspeed: 0.0146s/iter; left time: 479.7769s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1098320\n",
      "\tspeed: 0.0146s/iter; left time: 478.6655s\n",
      "\titers: 3700, epoch: 3 | loss: 0.2125494\n",
      "\tspeed: 0.0146s/iter; left time: 477.4236s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0961848\n",
      "\tspeed: 0.0146s/iter; left time: 475.7776s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1356835\n",
      "\tspeed: 0.0146s/iter; left time: 475.1533s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1639344\n",
      "\tspeed: 0.0146s/iter; left time: 473.4824s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1472913\n",
      "\tspeed: 0.0146s/iter; left time: 472.3699s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1102188\n",
      "\tspeed: 0.0146s/iter; left time: 470.9960s\n",
      "\titers: 4300, epoch: 3 | loss: 0.1015065\n",
      "\tspeed: 0.0146s/iter; left time: 469.4328s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1191620\n",
      "\tspeed: 0.0146s/iter; left time: 468.1169s\n",
      "\titers: 4500, epoch: 3 | loss: 0.1881049\n",
      "\tspeed: 0.0146s/iter; left time: 466.9662s\n",
      "Epoch: 3 cost time: 67.1140809059143\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1590236 Vali Loss: 0.1683879 Test Loss: 0.1611778\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1012475\n",
      "\tspeed: 0.1288s/iter; left time: 4093.6738s\n",
      "\titers: 200, epoch: 4 | loss: 0.1646736\n",
      "\tspeed: 0.0146s/iter; left time: 464.1848s\n",
      "\titers: 300, epoch: 4 | loss: 0.1645684\n",
      "\tspeed: 0.0146s/iter; left time: 462.2773s\n",
      "\titers: 400, epoch: 4 | loss: 0.1206798\n",
      "\tspeed: 0.0146s/iter; left time: 460.4670s\n",
      "\titers: 500, epoch: 4 | loss: 0.1425237\n",
      "\tspeed: 0.0146s/iter; left time: 459.7574s\n",
      "\titers: 600, epoch: 4 | loss: 0.1426224\n",
      "\tspeed: 0.0146s/iter; left time: 457.3676s\n",
      "\titers: 700, epoch: 4 | loss: 0.1692563\n",
      "\tspeed: 0.0147s/iter; left time: 457.0356s\n",
      "\titers: 800, epoch: 4 | loss: 0.1536964\n",
      "\tspeed: 0.0146s/iter; left time: 454.5679s\n",
      "\titers: 900, epoch: 4 | loss: 0.1662820\n",
      "\tspeed: 0.0146s/iter; left time: 452.9871s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1281092\n",
      "\tspeed: 0.0146s/iter; left time: 451.6831s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0559125\n",
      "\tspeed: 0.0146s/iter; left time: 449.7695s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0968387\n",
      "\tspeed: 0.0146s/iter; left time: 448.0485s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1333388\n",
      "\tspeed: 0.0146s/iter; left time: 446.6784s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1098405\n",
      "\tspeed: 0.0146s/iter; left time: 445.2332s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1151561\n",
      "\tspeed: 0.0146s/iter; left time: 443.9108s\n",
      "\titers: 1600, epoch: 4 | loss: 0.1821272\n",
      "\tspeed: 0.0146s/iter; left time: 442.6140s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1730076\n",
      "\tspeed: 0.0146s/iter; left time: 441.3653s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1202855\n",
      "\tspeed: 0.0146s/iter; left time: 439.7319s\n",
      "\titers: 1900, epoch: 4 | loss: 0.1301807\n",
      "\tspeed: 0.0146s/iter; left time: 438.4252s\n",
      "\titers: 2000, epoch: 4 | loss: 0.0953304\n",
      "\tspeed: 0.0146s/iter; left time: 436.6687s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1019158\n",
      "\tspeed: 0.0146s/iter; left time: 434.8787s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1592135\n",
      "\tspeed: 0.0146s/iter; left time: 433.4102s\n",
      "\titers: 2300, epoch: 4 | loss: 0.0990298\n",
      "\tspeed: 0.0146s/iter; left time: 432.0642s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1415903\n",
      "\tspeed: 0.0146s/iter; left time: 430.4130s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0509912\n",
      "\tspeed: 0.0146s/iter; left time: 428.8729s\n",
      "\titers: 2600, epoch: 4 | loss: 0.1051929\n",
      "\tspeed: 0.0146s/iter; left time: 427.4884s\n",
      "\titers: 2700, epoch: 4 | loss: 0.0764490\n",
      "\tspeed: 0.0146s/iter; left time: 425.9351s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1367291\n",
      "\tspeed: 0.0146s/iter; left time: 424.4451s\n",
      "\titers: 2900, epoch: 4 | loss: 0.1231387\n",
      "\tspeed: 0.0146s/iter; left time: 422.7870s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0974427\n",
      "\tspeed: 0.0146s/iter; left time: 421.5124s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0955617\n",
      "\tspeed: 0.0146s/iter; left time: 419.9592s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1169614\n",
      "\tspeed: 0.0146s/iter; left time: 418.6532s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1319787\n",
      "\tspeed: 0.0146s/iter; left time: 417.0073s\n",
      "\titers: 3400, epoch: 4 | loss: 0.1515111\n",
      "\tspeed: 0.0146s/iter; left time: 415.7048s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0774313\n",
      "\tspeed: 0.0146s/iter; left time: 414.0564s\n",
      "\titers: 3600, epoch: 4 | loss: 0.1363832\n",
      "\tspeed: 0.0146s/iter; left time: 412.6525s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1360534\n",
      "\tspeed: 0.0146s/iter; left time: 411.3444s\n",
      "\titers: 3800, epoch: 4 | loss: 0.1400785\n",
      "\tspeed: 0.0146s/iter; left time: 409.7087s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1656272\n",
      "\tspeed: 0.0146s/iter; left time: 408.5256s\n",
      "\titers: 4000, epoch: 4 | loss: 0.2009505\n",
      "\tspeed: 0.0146s/iter; left time: 406.9086s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1065340\n",
      "\tspeed: 0.0146s/iter; left time: 405.4492s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1480855\n",
      "\tspeed: 0.0146s/iter; left time: 403.8472s\n",
      "\titers: 4300, epoch: 4 | loss: 0.1024904\n",
      "\tspeed: 0.0146s/iter; left time: 402.6059s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1274885\n",
      "\tspeed: 0.0146s/iter; left time: 400.9651s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1100308\n",
      "\tspeed: 0.0146s/iter; left time: 399.6914s\n",
      "Epoch: 4 cost time: 66.79635620117188\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1395727 Vali Loss: 0.1688482 Test Loss: 0.1654386\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1226516\n",
      "\tspeed: 0.1339s/iter; left time: 3647.3580s\n",
      "\titers: 200, epoch: 5 | loss: 0.1437890\n",
      "\tspeed: 0.0146s/iter; left time: 396.8521s\n",
      "\titers: 300, epoch: 5 | loss: 0.1067203\n",
      "\tspeed: 0.0146s/iter; left time: 395.4377s\n",
      "\titers: 400, epoch: 5 | loss: 0.1498516\n",
      "\tspeed: 0.0146s/iter; left time: 393.4405s\n",
      "\titers: 500, epoch: 5 | loss: 0.1380797\n",
      "\tspeed: 0.0146s/iter; left time: 392.4068s\n",
      "\titers: 600, epoch: 5 | loss: 0.1449442\n",
      "\tspeed: 0.0146s/iter; left time: 390.6526s\n",
      "\titers: 700, epoch: 5 | loss: 0.0958706\n",
      "\tspeed: 0.0146s/iter; left time: 389.0616s\n",
      "\titers: 800, epoch: 5 | loss: 0.1533191\n",
      "\tspeed: 0.0146s/iter; left time: 387.8850s\n",
      "\titers: 900, epoch: 5 | loss: 0.1048619\n",
      "\tspeed: 0.0146s/iter; left time: 386.3073s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1337347\n",
      "\tspeed: 0.0146s/iter; left time: 385.0299s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0950139\n",
      "\tspeed: 0.0146s/iter; left time: 383.8215s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1019517\n",
      "\tspeed: 0.0146s/iter; left time: 382.3802s\n",
      "\titers: 1300, epoch: 5 | loss: 0.2618865\n",
      "\tspeed: 0.0146s/iter; left time: 380.8045s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0821686\n",
      "\tspeed: 0.0146s/iter; left time: 379.3386s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0903065\n",
      "\tspeed: 0.0146s/iter; left time: 377.9742s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1039251\n",
      "\tspeed: 0.0146s/iter; left time: 376.6345s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0854441\n",
      "\tspeed: 0.0146s/iter; left time: 375.1904s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1287813\n",
      "\tspeed: 0.0146s/iter; left time: 373.6542s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1496496\n",
      "\tspeed: 0.0146s/iter; left time: 372.2382s\n",
      "\titers: 2000, epoch: 5 | loss: 0.0932090\n",
      "\tspeed: 0.0146s/iter; left time: 370.6814s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1664349\n",
      "\tspeed: 0.0146s/iter; left time: 369.1750s\n",
      "\titers: 2200, epoch: 5 | loss: 0.1270646\n",
      "\tspeed: 0.0146s/iter; left time: 367.5662s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1930762\n",
      "\tspeed: 0.0146s/iter; left time: 365.9425s\n",
      "\titers: 2400, epoch: 5 | loss: 0.1044890\n",
      "\tspeed: 0.0146s/iter; left time: 364.5395s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1094200\n",
      "\tspeed: 0.0146s/iter; left time: 363.0033s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1609405\n",
      "\tspeed: 0.0146s/iter; left time: 361.7281s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0950716\n",
      "\tspeed: 0.0146s/iter; left time: 360.2014s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0721826\n",
      "\tspeed: 0.0146s/iter; left time: 358.7444s\n",
      "\titers: 2900, epoch: 5 | loss: 0.1531657\n",
      "\tspeed: 0.0146s/iter; left time: 357.2207s\n",
      "\titers: 3000, epoch: 5 | loss: 0.1173939\n",
      "\tspeed: 0.0146s/iter; left time: 355.7547s\n",
      "\titers: 3100, epoch: 5 | loss: 0.1106961\n",
      "\tspeed: 0.0146s/iter; left time: 354.3790s\n",
      "\titers: 3200, epoch: 5 | loss: 0.1279214\n",
      "\tspeed: 0.0146s/iter; left time: 352.8740s\n",
      "\titers: 3300, epoch: 5 | loss: 0.1376185\n",
      "\tspeed: 0.0146s/iter; left time: 351.2494s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0995441\n",
      "\tspeed: 0.0146s/iter; left time: 349.9287s\n",
      "\titers: 3500, epoch: 5 | loss: 0.1110943\n",
      "\tspeed: 0.0146s/iter; left time: 348.4895s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1339159\n",
      "\tspeed: 0.0146s/iter; left time: 346.9992s\n",
      "\titers: 3700, epoch: 5 | loss: 0.2023411\n",
      "\tspeed: 0.0146s/iter; left time: 345.4698s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0918992\n",
      "\tspeed: 0.0146s/iter; left time: 344.0660s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1336188\n",
      "\tspeed: 0.0146s/iter; left time: 342.6009s\n",
      "\titers: 4000, epoch: 5 | loss: 0.1175198\n",
      "\tspeed: 0.0146s/iter; left time: 340.9743s\n",
      "\titers: 4100, epoch: 5 | loss: 0.1231361\n",
      "\tspeed: 0.0146s/iter; left time: 339.7031s\n",
      "\titers: 4200, epoch: 5 | loss: 0.1470902\n",
      "\tspeed: 0.0146s/iter; left time: 338.3069s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1264062\n",
      "\tspeed: 0.0146s/iter; left time: 336.4738s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0997995\n",
      "\tspeed: 0.0145s/iter; left time: 333.5940s\n",
      "\titers: 4500, epoch: 5 | loss: 0.1866920\n",
      "\tspeed: 0.0145s/iter; left time: 332.0422s\n",
      "Epoch: 5 cost time: 66.8429262638092\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.1306818 Vali Loss: 0.1705802 Test Loss: 0.1642129\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0914899\n",
      "\tspeed: 0.1335s/iter; left time: 3027.8474s\n",
      "\titers: 200, epoch: 6 | loss: 0.1319403\n",
      "\tspeed: 0.0146s/iter; left time: 328.5101s\n",
      "\titers: 300, epoch: 6 | loss: 0.1013137\n",
      "\tspeed: 0.0146s/iter; left time: 327.6397s\n",
      "\titers: 400, epoch: 6 | loss: 0.1395806\n",
      "\tspeed: 0.0145s/iter; left time: 325.5156s\n",
      "\titers: 500, epoch: 6 | loss: 0.1319958\n",
      "\tspeed: 0.0146s/iter; left time: 324.2060s\n",
      "\titers: 600, epoch: 6 | loss: 0.0926603\n",
      "\tspeed: 0.0145s/iter; left time: 322.5944s\n",
      "\titers: 700, epoch: 6 | loss: 0.0921494\n",
      "\tspeed: 0.0146s/iter; left time: 321.5017s\n",
      "\titers: 800, epoch: 6 | loss: 0.1409616\n",
      "\tspeed: 0.0146s/iter; left time: 320.6587s\n",
      "\titers: 900, epoch: 6 | loss: 0.0782563\n",
      "\tspeed: 0.0146s/iter; left time: 318.8150s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1368436\n",
      "\tspeed: 0.0145s/iter; left time: 316.7748s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1421293\n",
      "\tspeed: 0.0146s/iter; left time: 315.4011s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1454203\n",
      "\tspeed: 0.0146s/iter; left time: 313.9494s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1489857\n",
      "\tspeed: 0.0145s/iter; left time: 312.3604s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1402982\n",
      "\tspeed: 0.0146s/iter; left time: 311.5297s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1043792\n",
      "\tspeed: 0.0146s/iter; left time: 309.6525s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1225039\n",
      "\tspeed: 0.0146s/iter; left time: 308.7730s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1586867\n",
      "\tspeed: 0.0146s/iter; left time: 307.1448s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1799810\n",
      "\tspeed: 0.0146s/iter; left time: 305.4775s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0962154\n",
      "\tspeed: 0.0146s/iter; left time: 304.1206s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1172384\n",
      "\tspeed: 0.0146s/iter; left time: 302.3745s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1133705\n",
      "\tspeed: 0.0145s/iter; left time: 300.6657s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1064674\n",
      "\tspeed: 0.0146s/iter; left time: 299.6212s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1242828\n",
      "\tspeed: 0.0146s/iter; left time: 298.2017s\n",
      "\titers: 2400, epoch: 6 | loss: 0.1046641\n",
      "\tspeed: 0.0146s/iter; left time: 297.6712s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1044027\n",
      "\tspeed: 0.0146s/iter; left time: 295.9028s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1020114\n",
      "\tspeed: 0.0146s/iter; left time: 294.5481s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1173358\n",
      "\tspeed: 0.0146s/iter; left time: 293.6867s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0793056\n",
      "\tspeed: 0.0146s/iter; left time: 291.6903s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1152395\n",
      "\tspeed: 0.0146s/iter; left time: 290.3595s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1019234\n",
      "\tspeed: 0.0146s/iter; left time: 288.6805s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1600710\n",
      "\tspeed: 0.0146s/iter; left time: 287.0311s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1098400\n",
      "\tspeed: 0.0146s/iter; left time: 285.7273s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1926805\n",
      "\tspeed: 0.0146s/iter; left time: 284.1743s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0947982\n",
      "\tspeed: 0.0146s/iter; left time: 282.7300s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1308616\n",
      "\tspeed: 0.0146s/iter; left time: 281.4899s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1248509\n",
      "\tspeed: 0.0146s/iter; left time: 280.1390s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0920699\n",
      "\tspeed: 0.0146s/iter; left time: 278.4906s\n",
      "\titers: 3800, epoch: 6 | loss: 0.0965928\n",
      "\tspeed: 0.0146s/iter; left time: 276.9247s\n",
      "\titers: 3900, epoch: 6 | loss: 0.1200774\n",
      "\tspeed: 0.0146s/iter; left time: 275.3806s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0846738\n",
      "\tspeed: 0.0146s/iter; left time: 274.2966s\n",
      "\titers: 4100, epoch: 6 | loss: 0.1480327\n",
      "\tspeed: 0.0146s/iter; left time: 272.7379s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0646326\n",
      "\tspeed: 0.0146s/iter; left time: 271.1579s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1269039\n",
      "\tspeed: 0.0146s/iter; left time: 269.6909s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0828842\n",
      "\tspeed: 0.0146s/iter; left time: 268.5467s\n",
      "\titers: 4500, epoch: 6 | loss: 0.1353167\n",
      "\tspeed: 0.0146s/iter; left time: 266.9524s\n",
      "Epoch: 6 cost time: 66.68588066101074\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.1263753 Vali Loss: 0.1696082 Test Loss: 0.1668956\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 4) (5237, 24, 4)\n",
      "test shape: (5237, 24, 4) (5237, 24, 4)\n",
      "mse:0.1637289971113205, mae:0.25978586077690125\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features M --predictor solar_forecast,wind_forecast,total_load --enc_in 4 --dec_in 4 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e8e7d",
   "metadata": {},
   "source": [
    "## other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd324a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       gas_price           \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             2                   Dec In:             2                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1874164\n",
      "\tspeed: 0.0259s/iter; left time: 1177.0571s\n",
      "\titers: 200, epoch: 1 | loss: 0.0996964\n",
      "\tspeed: 0.0143s/iter; left time: 646.4153s\n",
      "\titers: 300, epoch: 1 | loss: 0.0642987\n",
      "\tspeed: 0.0142s/iter; left time: 642.4344s\n",
      "\titers: 400, epoch: 1 | loss: 0.1646833\n",
      "\tspeed: 0.0142s/iter; left time: 639.6573s\n",
      "\titers: 500, epoch: 1 | loss: 0.0343205\n",
      "\tspeed: 0.0141s/iter; left time: 636.8637s\n",
      "\titers: 600, epoch: 1 | loss: 0.0754719\n",
      "\tspeed: 0.0142s/iter; left time: 636.4444s\n",
      "\titers: 700, epoch: 1 | loss: 0.1261388\n",
      "\tspeed: 0.0142s/iter; left time: 636.0420s\n",
      "\titers: 800, epoch: 1 | loss: 0.5586543\n",
      "\tspeed: 0.0141s/iter; left time: 633.0630s\n",
      "\titers: 900, epoch: 1 | loss: 0.2091444\n",
      "\tspeed: 0.0141s/iter; left time: 630.4715s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0767377\n",
      "\tspeed: 0.0141s/iter; left time: 630.3590s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0581852\n",
      "\tspeed: 0.0141s/iter; left time: 627.6228s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0694833\n",
      "\tspeed: 0.0141s/iter; left time: 626.2811s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2380905\n",
      "\tspeed: 0.0141s/iter; left time: 624.9105s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0254721\n",
      "\tspeed: 0.0141s/iter; left time: 623.4910s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1571978\n",
      "\tspeed: 0.0141s/iter; left time: 622.5142s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1965792\n",
      "\tspeed: 0.0142s/iter; left time: 623.6963s\n",
      "\titers: 1700, epoch: 1 | loss: 0.2722510\n",
      "\tspeed: 0.0142s/iter; left time: 622.3187s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0580130\n",
      "\tspeed: 0.0142s/iter; left time: 620.6804s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1597717\n",
      "\tspeed: 0.0142s/iter; left time: 618.0772s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4418003\n",
      "\tspeed: 0.0142s/iter; left time: 617.3194s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2740053\n",
      "\tspeed: 0.0142s/iter; left time: 615.6771s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3112937\n",
      "\tspeed: 0.0142s/iter; left time: 614.3832s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1561617\n",
      "\tspeed: 0.0142s/iter; left time: 612.4140s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1724921\n",
      "\tspeed: 0.0142s/iter; left time: 611.6363s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2954292\n",
      "\tspeed: 0.0142s/iter; left time: 610.4370s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1039652\n",
      "\tspeed: 0.0142s/iter; left time: 608.3071s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1776437\n",
      "\tspeed: 0.0142s/iter; left time: 606.6555s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1824704\n",
      "\tspeed: 0.0142s/iter; left time: 605.5818s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2412692\n",
      "\tspeed: 0.0142s/iter; left time: 603.6139s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1334136\n",
      "\tspeed: 0.0142s/iter; left time: 602.4989s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0732290\n",
      "\tspeed: 0.0142s/iter; left time: 601.4009s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2198445\n",
      "\tspeed: 0.0142s/iter; left time: 600.4249s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0199656\n",
      "\tspeed: 0.0142s/iter; left time: 598.3356s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2470104\n",
      "\tspeed: 0.0142s/iter; left time: 596.7022s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1147464\n",
      "\tspeed: 0.0142s/iter; left time: 595.7375s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1584618\n",
      "\tspeed: 0.0142s/iter; left time: 593.7465s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0629789\n",
      "\tspeed: 0.0142s/iter; left time: 592.6342s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1725723\n",
      "\tspeed: 0.0142s/iter; left time: 591.1876s\n",
      "\titers: 3900, epoch: 1 | loss: 0.1187043\n",
      "\tspeed: 0.0142s/iter; left time: 589.9239s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3491612\n",
      "\tspeed: 0.0141s/iter; left time: 587.8805s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0832524\n",
      "\tspeed: 0.0142s/iter; left time: 586.9011s\n",
      "\titers: 4200, epoch: 1 | loss: 0.1045229\n",
      "\tspeed: 0.0142s/iter; left time: 585.5393s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1385456\n",
      "\tspeed: 0.0142s/iter; left time: 584.2249s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0504982\n",
      "\tspeed: 0.0142s/iter; left time: 582.9569s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0697189\n",
      "\tspeed: 0.0142s/iter; left time: 581.7338s\n",
      "Epoch: 1 cost time: 65.73166728019714\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1790623 Vali Loss: 0.0389262 Test Loss: 0.1316267\n",
      "Validation loss decreased (inf --> 0.038926).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0754416\n",
      "\tspeed: 0.1267s/iter; left time: 5179.7895s\n",
      "\titers: 200, epoch: 2 | loss: 0.0471980\n",
      "\tspeed: 0.0142s/iter; left time: 579.4940s\n",
      "\titers: 300, epoch: 2 | loss: 0.0986616\n",
      "\tspeed: 0.0141s/iter; left time: 575.6480s\n",
      "\titers: 400, epoch: 2 | loss: 0.1116458\n",
      "\tspeed: 0.0142s/iter; left time: 574.5761s\n",
      "\titers: 500, epoch: 2 | loss: 0.1641512\n",
      "\tspeed: 0.0141s/iter; left time: 572.1326s\n",
      "\titers: 600, epoch: 2 | loss: 0.0960478\n",
      "\tspeed: 0.0150s/iter; left time: 606.3644s\n",
      "\titers: 700, epoch: 2 | loss: 0.1070306\n",
      "\tspeed: 0.0157s/iter; left time: 631.9875s\n",
      "\titers: 800, epoch: 2 | loss: 0.0550093\n",
      "\tspeed: 0.0157s/iter; left time: 632.0789s\n",
      "\titers: 900, epoch: 2 | loss: 0.0448621\n",
      "\tspeed: 0.0157s/iter; left time: 629.6592s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1450810\n",
      "\tspeed: 0.0157s/iter; left time: 628.1492s\n",
      "\titers: 1100, epoch: 2 | loss: 0.2344576\n",
      "\tspeed: 0.0153s/iter; left time: 611.9443s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1792488\n",
      "\tspeed: 0.0157s/iter; left time: 624.2627s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1007891\n",
      "\tspeed: 0.0157s/iter; left time: 622.4167s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1157571\n",
      "\tspeed: 0.0157s/iter; left time: 621.6390s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0396512\n",
      "\tspeed: 0.0157s/iter; left time: 619.9737s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1051240\n",
      "\tspeed: 0.0157s/iter; left time: 617.7212s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2360380\n",
      "\tspeed: 0.0157s/iter; left time: 616.4621s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1353546\n",
      "\tspeed: 0.0157s/iter; left time: 615.7603s\n",
      "\titers: 1900, epoch: 2 | loss: 0.0796287\n",
      "\tspeed: 0.0157s/iter; left time: 613.8487s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0539659\n",
      "\tspeed: 0.0157s/iter; left time: 612.2177s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0582912\n",
      "\tspeed: 0.0157s/iter; left time: 610.5824s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1220618\n",
      "\tspeed: 0.0157s/iter; left time: 608.9415s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1629577\n",
      "\tspeed: 0.0157s/iter; left time: 607.6674s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1238086\n",
      "\tspeed: 0.0157s/iter; left time: 605.7176s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0676154\n",
      "\tspeed: 0.0157s/iter; left time: 604.7337s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2309626\n",
      "\tspeed: 0.0157s/iter; left time: 602.5880s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1626542\n",
      "\tspeed: 0.0157s/iter; left time: 601.1345s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0958964\n",
      "\tspeed: 0.0157s/iter; left time: 599.9557s\n",
      "\titers: 2900, epoch: 2 | loss: 0.1151819\n",
      "\tspeed: 0.0157s/iter; left time: 597.8771s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0974249\n",
      "\tspeed: 0.0157s/iter; left time: 596.6504s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0671584\n",
      "\tspeed: 0.0157s/iter; left time: 594.7857s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1733702\n",
      "\tspeed: 0.0149s/iter; left time: 562.6721s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0467012\n",
      "\tspeed: 0.0148s/iter; left time: 559.2347s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1079641\n",
      "\tspeed: 0.0157s/iter; left time: 589.9180s\n",
      "\titers: 3500, epoch: 2 | loss: 0.2211758\n",
      "\tspeed: 0.0157s/iter; left time: 588.2440s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1250477\n",
      "\tspeed: 0.0157s/iter; left time: 586.9748s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2028959\n",
      "\tspeed: 0.0157s/iter; left time: 585.5190s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0938816\n",
      "\tspeed: 0.0157s/iter; left time: 583.6168s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1691197\n",
      "\tspeed: 0.0157s/iter; left time: 582.2357s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2163981\n",
      "\tspeed: 0.0157s/iter; left time: 580.9642s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0922155\n",
      "\tspeed: 0.0157s/iter; left time: 579.6446s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0709125\n",
      "\tspeed: 0.0157s/iter; left time: 577.8598s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1108456\n",
      "\tspeed: 0.0157s/iter; left time: 576.0988s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1440003\n",
      "\tspeed: 0.0157s/iter; left time: 575.0735s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0494342\n",
      "\tspeed: 0.0157s/iter; left time: 572.3910s\n",
      "Epoch: 2 cost time: 70.68137311935425\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1401214 Vali Loss: 0.0337916 Test Loss: 0.1282265\n",
      "Validation loss decreased (0.038926 --> 0.033792).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1014305\n",
      "\tspeed: 0.1331s/iter; left time: 4835.1819s\n",
      "\titers: 200, epoch: 3 | loss: 0.0379759\n",
      "\tspeed: 0.0141s/iter; left time: 512.7828s\n",
      "\titers: 300, epoch: 3 | loss: 0.0994432\n",
      "\tspeed: 0.0141s/iter; left time: 511.3407s\n",
      "\titers: 400, epoch: 3 | loss: 0.0712073\n",
      "\tspeed: 0.0141s/iter; left time: 509.6183s\n",
      "\titers: 500, epoch: 3 | loss: 0.1075111\n",
      "\tspeed: 0.0141s/iter; left time: 507.7949s\n",
      "\titers: 600, epoch: 3 | loss: 0.0984936\n",
      "\tspeed: 0.0141s/iter; left time: 506.2684s\n",
      "\titers: 700, epoch: 3 | loss: 0.3183333\n",
      "\tspeed: 0.0142s/iter; left time: 506.8878s\n",
      "\titers: 800, epoch: 3 | loss: 0.1916516\n",
      "\tspeed: 0.0142s/iter; left time: 506.0113s\n",
      "\titers: 900, epoch: 3 | loss: 0.0649095\n",
      "\tspeed: 0.0142s/iter; left time: 504.5459s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1092015\n",
      "\tspeed: 0.0142s/iter; left time: 503.3117s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1190361\n",
      "\tspeed: 0.0142s/iter; left time: 501.8719s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1465427\n",
      "\tspeed: 0.0142s/iter; left time: 500.1471s\n",
      "\titers: 1300, epoch: 3 | loss: 0.1738489\n",
      "\tspeed: 0.0142s/iter; left time: 498.1616s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1382258\n",
      "\tspeed: 0.0142s/iter; left time: 496.6406s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1691803\n",
      "\tspeed: 0.0142s/iter; left time: 495.8978s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1222695\n",
      "\tspeed: 0.0142s/iter; left time: 494.2535s\n",
      "\titers: 1700, epoch: 3 | loss: 0.4585516\n",
      "\tspeed: 0.0142s/iter; left time: 492.4906s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0850166\n",
      "\tspeed: 0.0142s/iter; left time: 490.7424s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1850934\n",
      "\tspeed: 0.0142s/iter; left time: 489.1336s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2674827\n",
      "\tspeed: 0.0142s/iter; left time: 487.5946s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0625037\n",
      "\tspeed: 0.0142s/iter; left time: 486.1709s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0618431\n",
      "\tspeed: 0.0142s/iter; left time: 484.8910s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0901959\n",
      "\tspeed: 0.0142s/iter; left time: 483.3270s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0714715\n",
      "\tspeed: 0.0142s/iter; left time: 482.1134s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1330293\n",
      "\tspeed: 0.0142s/iter; left time: 480.5134s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0481850\n",
      "\tspeed: 0.0141s/iter; left time: 478.8187s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0388061\n",
      "\tspeed: 0.0142s/iter; left time: 478.1090s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1304970\n",
      "\tspeed: 0.0141s/iter; left time: 475.9012s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0604070\n",
      "\tspeed: 0.0142s/iter; left time: 475.1121s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0403207\n",
      "\tspeed: 0.0142s/iter; left time: 473.2278s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1687747\n",
      "\tspeed: 0.0142s/iter; left time: 471.9517s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2587967\n",
      "\tspeed: 0.0141s/iter; left time: 470.2405s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0760658\n",
      "\tspeed: 0.0142s/iter; left time: 469.0094s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1392425\n",
      "\tspeed: 0.0141s/iter; left time: 467.2420s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0316949\n",
      "\tspeed: 0.0141s/iter; left time: 465.8918s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1177419\n",
      "\tspeed: 0.0141s/iter; left time: 464.0654s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1785725\n",
      "\tspeed: 0.0141s/iter; left time: 462.7674s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0567839\n",
      "\tspeed: 0.0141s/iter; left time: 461.4755s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1598053\n",
      "\tspeed: 0.0141s/iter; left time: 460.0485s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1621916\n",
      "\tspeed: 0.0141s/iter; left time: 458.4240s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2470136\n",
      "\tspeed: 0.0141s/iter; left time: 457.3910s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0622313\n",
      "\tspeed: 0.0141s/iter; left time: 456.1943s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0299166\n",
      "\tspeed: 0.0141s/iter; left time: 454.3441s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0782480\n",
      "\tspeed: 0.0141s/iter; left time: 452.8858s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0333567\n",
      "\tspeed: 0.0141s/iter; left time: 451.6288s\n",
      "Epoch: 3 cost time: 64.75314426422119\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1116382 Vali Loss: 0.0348650 Test Loss: 0.1166047\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0891957\n",
      "\tspeed: 0.1325s/iter; left time: 4210.2496s\n",
      "\titers: 200, epoch: 4 | loss: 0.2752194\n",
      "\tspeed: 0.0142s/iter; left time: 450.0119s\n",
      "\titers: 300, epoch: 4 | loss: 0.0280604\n",
      "\tspeed: 0.0142s/iter; left time: 448.0570s\n",
      "\titers: 400, epoch: 4 | loss: 0.1175448\n",
      "\tspeed: 0.0142s/iter; left time: 446.6895s\n",
      "\titers: 500, epoch: 4 | loss: 0.0559379\n",
      "\tspeed: 0.0142s/iter; left time: 445.2358s\n",
      "\titers: 600, epoch: 4 | loss: 0.1366048\n",
      "\tspeed: 0.0142s/iter; left time: 444.0091s\n",
      "\titers: 700, epoch: 4 | loss: 0.0486839\n",
      "\tspeed: 0.0142s/iter; left time: 442.5865s\n",
      "\titers: 800, epoch: 4 | loss: 0.1432574\n",
      "\tspeed: 0.0142s/iter; left time: 441.0200s\n",
      "\titers: 900, epoch: 4 | loss: 0.0796577\n",
      "\tspeed: 0.0142s/iter; left time: 439.5633s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0789346\n",
      "\tspeed: 0.0142s/iter; left time: 438.1230s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0967562\n",
      "\tspeed: 0.0142s/iter; left time: 436.6174s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0686079\n",
      "\tspeed: 0.0142s/iter; left time: 435.3526s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0913985\n",
      "\tspeed: 0.0142s/iter; left time: 433.9178s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0491507\n",
      "\tspeed: 0.0142s/iter; left time: 432.6641s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2367777\n",
      "\tspeed: 0.0142s/iter; left time: 431.0334s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0701688\n",
      "\tspeed: 0.0142s/iter; left time: 429.9367s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1394525\n",
      "\tspeed: 0.0142s/iter; left time: 428.5157s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0314306\n",
      "\tspeed: 0.0142s/iter; left time: 426.9116s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0586014\n",
      "\tspeed: 0.0142s/iter; left time: 425.5643s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1722982\n",
      "\tspeed: 0.0142s/iter; left time: 424.0961s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0177163\n",
      "\tspeed: 0.0142s/iter; left time: 422.7368s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1703758\n",
      "\tspeed: 0.0142s/iter; left time: 421.2517s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1312023\n",
      "\tspeed: 0.0142s/iter; left time: 419.9235s\n",
      "\titers: 2400, epoch: 4 | loss: 0.0970191\n",
      "\tspeed: 0.0142s/iter; left time: 418.5118s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0246884\n",
      "\tspeed: 0.0142s/iter; left time: 417.7199s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0614004\n",
      "\tspeed: 0.0142s/iter; left time: 415.9905s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5548754\n",
      "\tspeed: 0.0142s/iter; left time: 414.6631s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0214910\n",
      "\tspeed: 0.0142s/iter; left time: 413.2411s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0402354\n",
      "\tspeed: 0.0142s/iter; left time: 411.6663s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0973548\n",
      "\tspeed: 0.0142s/iter; left time: 410.3116s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0639368\n",
      "\tspeed: 0.0142s/iter; left time: 409.0276s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1116217\n",
      "\tspeed: 0.0142s/iter; left time: 407.4711s\n",
      "\titers: 3300, epoch: 4 | loss: 0.1176992\n",
      "\tspeed: 0.0142s/iter; left time: 406.0928s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0528606\n",
      "\tspeed: 0.0142s/iter; left time: 404.5174s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0419632\n",
      "\tspeed: 0.0142s/iter; left time: 403.2567s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0343274\n",
      "\tspeed: 0.0142s/iter; left time: 401.7849s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1142780\n",
      "\tspeed: 0.0142s/iter; left time: 399.9518s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0596911\n",
      "\tspeed: 0.0142s/iter; left time: 398.9531s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0624433\n",
      "\tspeed: 0.0143s/iter; left time: 398.8936s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0822573\n",
      "\tspeed: 0.0143s/iter; left time: 397.5484s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0719063\n",
      "\tspeed: 0.0143s/iter; left time: 396.0220s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0739861\n",
      "\tspeed: 0.0143s/iter; left time: 394.6818s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2479020\n",
      "\tspeed: 0.0143s/iter; left time: 393.1897s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0876503\n",
      "\tspeed: 0.0143s/iter; left time: 391.8426s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0821015\n",
      "\tspeed: 0.0143s/iter; left time: 390.4167s\n",
      "Epoch: 4 cost time: 64.9618649482727\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0945617 Vali Loss: 0.0333292 Test Loss: 0.1163836\n",
      "Validation loss decreased (0.033792 --> 0.033329).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0400493\n",
      "\tspeed: 0.1281s/iter; left time: 3488.4277s\n",
      "\titers: 200, epoch: 5 | loss: 0.0681935\n",
      "\tspeed: 0.0142s/iter; left time: 384.0644s\n",
      "\titers: 300, epoch: 5 | loss: 0.0451216\n",
      "\tspeed: 0.0142s/iter; left time: 382.5603s\n",
      "\titers: 400, epoch: 5 | loss: 0.0194824\n",
      "\tspeed: 0.0141s/iter; left time: 380.6397s\n",
      "\titers: 500, epoch: 5 | loss: 0.1351655\n",
      "\tspeed: 0.0142s/iter; left time: 379.8393s\n",
      "\titers: 600, epoch: 5 | loss: 0.0279804\n",
      "\tspeed: 0.0141s/iter; left time: 378.1008s\n",
      "\titers: 700, epoch: 5 | loss: 0.0576581\n",
      "\tspeed: 0.0141s/iter; left time: 376.7642s\n",
      "\titers: 800, epoch: 5 | loss: 0.0936772\n",
      "\tspeed: 0.0141s/iter; left time: 375.3794s\n",
      "\titers: 900, epoch: 5 | loss: 0.0440781\n",
      "\tspeed: 0.0141s/iter; left time: 373.8354s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1400850\n",
      "\tspeed: 0.0142s/iter; left time: 373.4541s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0234785\n",
      "\tspeed: 0.0142s/iter; left time: 373.7271s\n",
      "\titers: 1200, epoch: 5 | loss: 0.0978122\n",
      "\tspeed: 0.0142s/iter; left time: 372.1504s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0783427\n",
      "\tspeed: 0.0142s/iter; left time: 370.5901s\n",
      "\titers: 1400, epoch: 5 | loss: 0.0873576\n",
      "\tspeed: 0.0142s/iter; left time: 369.0204s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0843340\n",
      "\tspeed: 0.0142s/iter; left time: 367.6653s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0438082\n",
      "\tspeed: 0.0142s/iter; left time: 366.1539s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1388310\n",
      "\tspeed: 0.0142s/iter; left time: 364.7941s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0687139\n",
      "\tspeed: 0.0142s/iter; left time: 363.2379s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0238717\n",
      "\tspeed: 0.0142s/iter; left time: 361.7982s\n",
      "\titers: 2000, epoch: 5 | loss: 0.4642172\n",
      "\tspeed: 0.0142s/iter; left time: 360.3964s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0620774\n",
      "\tspeed: 0.0142s/iter; left time: 359.0354s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0613001\n",
      "\tspeed: 0.0142s/iter; left time: 357.6113s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0406231\n",
      "\tspeed: 0.0142s/iter; left time: 356.3104s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0745538\n",
      "\tspeed: 0.0142s/iter; left time: 354.6015s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0535102\n",
      "\tspeed: 0.0142s/iter; left time: 353.2005s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0490046\n",
      "\tspeed: 0.0142s/iter; left time: 351.7600s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0731522\n",
      "\tspeed: 0.0142s/iter; left time: 350.4891s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0901509\n",
      "\tspeed: 0.0142s/iter; left time: 349.0466s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0372250\n",
      "\tspeed: 0.0142s/iter; left time: 347.6638s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0411828\n",
      "\tspeed: 0.0142s/iter; left time: 346.3371s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0537545\n",
      "\tspeed: 0.0142s/iter; left time: 344.7096s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0134790\n",
      "\tspeed: 0.0142s/iter; left time: 343.2328s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0607193\n",
      "\tspeed: 0.0142s/iter; left time: 341.9299s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0689287\n",
      "\tspeed: 0.0142s/iter; left time: 340.3694s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0537367\n",
      "\tspeed: 0.0142s/iter; left time: 337.9049s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0909291\n",
      "\tspeed: 0.0141s/iter; left time: 335.5715s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0605423\n",
      "\tspeed: 0.0141s/iter; left time: 334.0623s\n",
      "\titers: 3800, epoch: 5 | loss: 0.0978352\n",
      "\tspeed: 0.0141s/iter; left time: 332.5035s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0496412\n",
      "\tspeed: 0.0141s/iter; left time: 331.3945s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0312776\n",
      "\tspeed: 0.0141s/iter; left time: 330.0150s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0342154\n",
      "\tspeed: 0.0141s/iter; left time: 328.4418s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0525359\n",
      "\tspeed: 0.0141s/iter; left time: 326.9895s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0890360\n",
      "\tspeed: 0.0141s/iter; left time: 325.4879s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0270278\n",
      "\tspeed: 0.0141s/iter; left time: 324.0585s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0679017\n",
      "\tspeed: 0.0141s/iter; left time: 322.5935s\n",
      "Epoch: 5 cost time: 64.89885807037354\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0864267 Vali Loss: 0.0333582 Test Loss: 0.1174518\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0342219\n",
      "\tspeed: 0.1333s/iter; left time: 3022.1519s\n",
      "\titers: 200, epoch: 6 | loss: 0.1411407\n",
      "\tspeed: 0.0143s/iter; left time: 322.0825s\n",
      "\titers: 300, epoch: 6 | loss: 0.0938385\n",
      "\tspeed: 0.0143s/iter; left time: 320.7931s\n",
      "\titers: 400, epoch: 6 | loss: 0.0707962\n",
      "\tspeed: 0.0143s/iter; left time: 319.1223s\n",
      "\titers: 500, epoch: 6 | loss: 0.0984237\n",
      "\tspeed: 0.0143s/iter; left time: 317.7566s\n",
      "\titers: 600, epoch: 6 | loss: 0.1501533\n",
      "\tspeed: 0.0143s/iter; left time: 316.3181s\n",
      "\titers: 700, epoch: 6 | loss: 0.0280334\n",
      "\tspeed: 0.0143s/iter; left time: 314.7795s\n",
      "\titers: 800, epoch: 6 | loss: 0.0586634\n",
      "\tspeed: 0.0143s/iter; left time: 313.3749s\n",
      "\titers: 900, epoch: 6 | loss: 0.0414131\n",
      "\tspeed: 0.0143s/iter; left time: 311.9380s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1366644\n",
      "\tspeed: 0.0143s/iter; left time: 310.6488s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0339299\n",
      "\tspeed: 0.0143s/iter; left time: 309.2074s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0600153\n",
      "\tspeed: 0.0143s/iter; left time: 307.7376s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0946191\n",
      "\tspeed: 0.0143s/iter; left time: 306.2839s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0518693\n",
      "\tspeed: 0.0143s/iter; left time: 304.9019s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0603662\n",
      "\tspeed: 0.0143s/iter; left time: 303.4028s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1281831\n",
      "\tspeed: 0.0143s/iter; left time: 302.2348s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0461796\n",
      "\tspeed: 0.0143s/iter; left time: 300.5377s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1662974\n",
      "\tspeed: 0.0143s/iter; left time: 299.1400s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0958672\n",
      "\tspeed: 0.0143s/iter; left time: 297.7108s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1300649\n",
      "\tspeed: 0.0143s/iter; left time: 296.1851s\n",
      "\titers: 2100, epoch: 6 | loss: 0.0799185\n",
      "\tspeed: 0.0143s/iter; left time: 294.7739s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0961677\n",
      "\tspeed: 0.0143s/iter; left time: 293.3083s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0815814\n",
      "\tspeed: 0.0143s/iter; left time: 292.1948s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0548385\n",
      "\tspeed: 0.0143s/iter; left time: 290.6245s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0442235\n",
      "\tspeed: 0.0143s/iter; left time: 288.9510s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0601333\n",
      "\tspeed: 0.0143s/iter; left time: 287.7132s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0466705\n",
      "\tspeed: 0.0143s/iter; left time: 286.1121s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0634873\n",
      "\tspeed: 0.0143s/iter; left time: 284.8900s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1651948\n",
      "\tspeed: 0.0143s/iter; left time: 283.2679s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0241587\n",
      "\tspeed: 0.0143s/iter; left time: 281.9795s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0904785\n",
      "\tspeed: 0.0142s/iter; left time: 280.3770s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0611269\n",
      "\tspeed: 0.0143s/iter; left time: 279.0666s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0745819\n",
      "\tspeed: 0.0143s/iter; left time: 277.6422s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0252705\n",
      "\tspeed: 0.0142s/iter; left time: 276.0642s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0122746\n",
      "\tspeed: 0.0143s/iter; left time: 274.8277s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0964582\n",
      "\tspeed: 0.0143s/iter; left time: 273.3662s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0732298\n",
      "\tspeed: 0.0143s/iter; left time: 272.0091s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1273451\n",
      "\tspeed: 0.0143s/iter; left time: 270.5634s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0332479\n",
      "\tspeed: 0.0143s/iter; left time: 269.2646s\n",
      "\titers: 4000, epoch: 6 | loss: 0.0557492\n",
      "\tspeed: 0.0143s/iter; left time: 267.7054s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0145154\n",
      "\tspeed: 0.0143s/iter; left time: 266.3915s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0344219\n",
      "\tspeed: 0.0143s/iter; left time: 264.8644s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1002893\n",
      "\tspeed: 0.0143s/iter; left time: 263.3716s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0893138\n",
      "\tspeed: 0.0143s/iter; left time: 261.9986s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0754274\n",
      "\tspeed: 0.0142s/iter; left time: 260.2448s\n",
      "Epoch: 6 cost time: 65.20961356163025\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0829497 Vali Loss: 0.0338235 Test Loss: 0.1176973\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0736170\n",
      "\tspeed: 0.1373s/iter; left time: 2488.8782s\n",
      "\titers: 200, epoch: 7 | loss: 0.2018163\n",
      "\tspeed: 0.0158s/iter; left time: 284.2192s\n",
      "\titers: 300, epoch: 7 | loss: 0.1492638\n",
      "\tspeed: 0.0157s/iter; left time: 281.7665s\n",
      "\titers: 400, epoch: 7 | loss: 0.0812914\n",
      "\tspeed: 0.0157s/iter; left time: 279.9471s\n",
      "\titers: 500, epoch: 7 | loss: 0.0284752\n",
      "\tspeed: 0.0157s/iter; left time: 278.3614s\n",
      "\titers: 600, epoch: 7 | loss: 0.0439948\n",
      "\tspeed: 0.0157s/iter; left time: 276.8871s\n",
      "\titers: 700, epoch: 7 | loss: 0.0213510\n",
      "\tspeed: 0.0157s/iter; left time: 275.8307s\n",
      "\titers: 800, epoch: 7 | loss: 0.0531441\n",
      "\tspeed: 0.0157s/iter; left time: 274.1371s\n",
      "\titers: 900, epoch: 7 | loss: 0.1286761\n",
      "\tspeed: 0.0151s/iter; left time: 261.3452s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0403847\n",
      "\tspeed: 0.0142s/iter; left time: 244.1838s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1912214\n",
      "\tspeed: 0.0142s/iter; left time: 242.7113s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0472413\n",
      "\tspeed: 0.0142s/iter; left time: 241.6031s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0809141\n",
      "\tspeed: 0.0142s/iter; left time: 239.8053s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0912420\n",
      "\tspeed: 0.0142s/iter; left time: 238.2875s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0736552\n",
      "\tspeed: 0.0142s/iter; left time: 236.9338s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0676188\n",
      "\tspeed: 0.0142s/iter; left time: 235.6416s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0381103\n",
      "\tspeed: 0.0142s/iter; left time: 234.1908s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0779944\n",
      "\tspeed: 0.0142s/iter; left time: 232.8161s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0483080\n",
      "\tspeed: 0.0142s/iter; left time: 231.4473s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0446773\n",
      "\tspeed: 0.0142s/iter; left time: 230.1907s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0440165\n",
      "\tspeed: 0.0142s/iter; left time: 228.9238s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0400949\n",
      "\tspeed: 0.0142s/iter; left time: 227.2500s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0520158\n",
      "\tspeed: 0.0142s/iter; left time: 225.7059s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0762442\n",
      "\tspeed: 0.0142s/iter; left time: 224.4781s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1744032\n",
      "\tspeed: 0.0142s/iter; left time: 222.9688s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0436354\n",
      "\tspeed: 0.0142s/iter; left time: 221.4791s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0329586\n",
      "\tspeed: 0.0142s/iter; left time: 220.0906s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0370196\n",
      "\tspeed: 0.0142s/iter; left time: 218.6069s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0648700\n",
      "\tspeed: 0.0142s/iter; left time: 217.1357s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0725334\n",
      "\tspeed: 0.0142s/iter; left time: 215.7830s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0286740\n",
      "\tspeed: 0.0142s/iter; left time: 214.1777s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1409982\n",
      "\tspeed: 0.0141s/iter; left time: 212.3783s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0611257\n",
      "\tspeed: 0.0141s/iter; left time: 210.9177s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0403626\n",
      "\tspeed: 0.0141s/iter; left time: 209.5556s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0669024\n",
      "\tspeed: 0.0141s/iter; left time: 208.1254s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0517621\n",
      "\tspeed: 0.0141s/iter; left time: 206.7664s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1704694\n",
      "\tspeed: 0.0141s/iter; left time: 205.4210s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0573083\n",
      "\tspeed: 0.0141s/iter; left time: 203.9209s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0776855\n",
      "\tspeed: 0.0141s/iter; left time: 202.5620s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0598062\n",
      "\tspeed: 0.0141s/iter; left time: 201.0275s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0585723\n",
      "\tspeed: 0.0141s/iter; left time: 199.7265s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0317516\n",
      "\tspeed: 0.0141s/iter; left time: 198.2930s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0178534\n",
      "\tspeed: 0.0141s/iter; left time: 196.8386s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0648578\n",
      "\tspeed: 0.0142s/iter; left time: 195.9415s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0754427\n",
      "\tspeed: 0.0142s/iter; left time: 194.3371s\n",
      "Epoch: 7 cost time: 66.16794943809509\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0796564 Vali Loss: 0.0339147 Test Loss: 0.1189310\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0730880\n",
      "\tspeed: 0.1442s/iter; left time: 1956.5981s\n",
      "\titers: 200, epoch: 8 | loss: 0.0384478\n",
      "\tspeed: 0.0142s/iter; left time: 191.4703s\n",
      "\titers: 300, epoch: 8 | loss: 0.1326494\n",
      "\tspeed: 0.0142s/iter; left time: 190.0785s\n",
      "\titers: 400, epoch: 8 | loss: 0.0303876\n",
      "\tspeed: 0.0142s/iter; left time: 188.7852s\n",
      "\titers: 500, epoch: 8 | loss: 0.1716713\n",
      "\tspeed: 0.0142s/iter; left time: 186.7868s\n",
      "\titers: 600, epoch: 8 | loss: 0.0623359\n",
      "\tspeed: 0.0142s/iter; left time: 185.2416s\n",
      "\titers: 700, epoch: 8 | loss: 0.1839023\n",
      "\tspeed: 0.0142s/iter; left time: 183.7430s\n",
      "\titers: 800, epoch: 8 | loss: 0.1618641\n",
      "\tspeed: 0.0142s/iter; left time: 182.4041s\n",
      "\titers: 900, epoch: 8 | loss: 0.0728110\n",
      "\tspeed: 0.0142s/iter; left time: 180.8839s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0289440\n",
      "\tspeed: 0.0142s/iter; left time: 179.6126s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0810280\n",
      "\tspeed: 0.0142s/iter; left time: 178.6728s\n",
      "\titers: 1200, epoch: 8 | loss: 0.0967875\n",
      "\tspeed: 0.0142s/iter; left time: 176.5643s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3904139\n",
      "\tspeed: 0.0142s/iter; left time: 175.1517s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0475704\n",
      "\tspeed: 0.0142s/iter; left time: 173.8143s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0707508\n",
      "\tspeed: 0.0142s/iter; left time: 172.2950s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0958202\n",
      "\tspeed: 0.0142s/iter; left time: 170.8993s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0246943\n",
      "\tspeed: 0.0142s/iter; left time: 170.0308s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1057747\n",
      "\tspeed: 0.0142s/iter; left time: 168.8760s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0688427\n",
      "\tspeed: 0.0142s/iter; left time: 167.4623s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1078569\n",
      "\tspeed: 0.0142s/iter; left time: 165.9945s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0392755\n",
      "\tspeed: 0.0143s/iter; left time: 164.9920s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0579787\n",
      "\tspeed: 0.0143s/iter; left time: 163.4130s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0819157\n",
      "\tspeed: 0.0143s/iter; left time: 162.0638s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0439308\n",
      "\tspeed: 0.0142s/iter; left time: 160.4743s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0702498\n",
      "\tspeed: 0.0142s/iter; left time: 158.8940s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0383454\n",
      "\tspeed: 0.0142s/iter; left time: 157.5150s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0350445\n",
      "\tspeed: 0.0142s/iter; left time: 156.0649s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0568048\n",
      "\tspeed: 0.0142s/iter; left time: 154.6066s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0725213\n",
      "\tspeed: 0.0142s/iter; left time: 153.2168s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2807688\n",
      "\tspeed: 0.0142s/iter; left time: 151.7822s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0762518\n",
      "\tspeed: 0.0142s/iter; left time: 150.4001s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1258451\n",
      "\tspeed: 0.0143s/iter; left time: 149.1662s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0304235\n",
      "\tspeed: 0.0142s/iter; left time: 147.6657s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0589859\n",
      "\tspeed: 0.0142s/iter; left time: 146.0072s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1341798\n",
      "\tspeed: 0.0142s/iter; left time: 144.6864s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0320761\n",
      "\tspeed: 0.0142s/iter; left time: 143.3287s\n",
      "\titers: 3700, epoch: 8 | loss: 0.1434723\n",
      "\tspeed: 0.0142s/iter; left time: 141.8462s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0611595\n",
      "\tspeed: 0.0142s/iter; left time: 140.3958s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0540943\n",
      "\tspeed: 0.0142s/iter; left time: 138.9570s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0393123\n",
      "\tspeed: 0.0142s/iter; left time: 137.5424s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0255133\n",
      "\tspeed: 0.0142s/iter; left time: 136.0479s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0826906\n",
      "\tspeed: 0.0142s/iter; left time: 134.6437s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0799311\n",
      "\tspeed: 0.0142s/iter; left time: 133.1961s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1203264\n",
      "\tspeed: 0.0142s/iter; left time: 131.7584s\n",
      "\titers: 4500, epoch: 8 | loss: 0.0904823\n",
      "\tspeed: 0.0142s/iter; left time: 130.3472s\n",
      "Epoch: 8 cost time: 65.010568857193\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0798508 Vali Loss: 0.0339961 Test Loss: 0.1184595\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0340370\n",
      "\tspeed: 0.1291s/iter; left time: 1163.4967s\n",
      "\titers: 200, epoch: 9 | loss: 0.0430530\n",
      "\tspeed: 0.0143s/iter; left time: 126.9900s\n",
      "\titers: 300, epoch: 9 | loss: 0.0502656\n",
      "\tspeed: 0.0142s/iter; left time: 125.4620s\n",
      "\titers: 400, epoch: 9 | loss: 0.1158929\n",
      "\tspeed: 0.0142s/iter; left time: 124.0872s\n",
      "\titers: 500, epoch: 9 | loss: 0.1205033\n",
      "\tspeed: 0.0142s/iter; left time: 122.6012s\n",
      "\titers: 600, epoch: 9 | loss: 0.0348370\n",
      "\tspeed: 0.0142s/iter; left time: 121.0742s\n",
      "\titers: 700, epoch: 9 | loss: 0.1146590\n",
      "\tspeed: 0.0155s/iter; left time: 130.3440s\n",
      "\titers: 800, epoch: 9 | loss: 0.0372943\n",
      "\tspeed: 0.0163s/iter; left time: 135.6499s\n",
      "\titers: 900, epoch: 9 | loss: 0.0419805\n",
      "\tspeed: 0.0163s/iter; left time: 134.0470s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0698890\n",
      "\tspeed: 0.0163s/iter; left time: 132.3149s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0514168\n",
      "\tspeed: 0.0163s/iter; left time: 130.7715s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1958191\n",
      "\tspeed: 0.0163s/iter; left time: 129.0635s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0340994\n",
      "\tspeed: 0.0163s/iter; left time: 127.4365s\n",
      "\titers: 1400, epoch: 9 | loss: 0.1578627\n",
      "\tspeed: 0.0163s/iter; left time: 125.7007s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0625489\n",
      "\tspeed: 0.0163s/iter; left time: 124.0986s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0481562\n",
      "\tspeed: 0.0163s/iter; left time: 122.4858s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0604794\n",
      "\tspeed: 0.0163s/iter; left time: 120.8920s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0225921\n",
      "\tspeed: 0.0163s/iter; left time: 119.5026s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0330354\n",
      "\tspeed: 0.0163s/iter; left time: 117.6531s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0508354\n",
      "\tspeed: 0.0163s/iter; left time: 115.9452s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0256363\n",
      "\tspeed: 0.0163s/iter; left time: 114.2607s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1061517\n",
      "\tspeed: 0.0163s/iter; left time: 112.7664s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1264016\n",
      "\tspeed: 0.0163s/iter; left time: 111.1022s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0204299\n",
      "\tspeed: 0.0163s/iter; left time: 109.4817s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0267028\n",
      "\tspeed: 0.0163s/iter; left time: 107.8486s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0317736\n",
      "\tspeed: 0.0163s/iter; left time: 106.1423s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0375879\n",
      "\tspeed: 0.0163s/iter; left time: 104.5566s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1611579\n",
      "\tspeed: 0.0163s/iter; left time: 103.0709s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0491359\n",
      "\tspeed: 0.0163s/iter; left time: 101.4821s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1008165\n",
      "\tspeed: 0.0163s/iter; left time: 99.8589s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0292425\n",
      "\tspeed: 0.0163s/iter; left time: 98.1961s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0831423\n",
      "\tspeed: 0.0163s/iter; left time: 96.5758s\n",
      "\titers: 3300, epoch: 9 | loss: 0.1871342\n",
      "\tspeed: 0.0163s/iter; left time: 94.8418s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1126077\n",
      "\tspeed: 0.0163s/iter; left time: 93.1854s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0568682\n",
      "\tspeed: 0.0163s/iter; left time: 91.5789s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0328016\n",
      "\tspeed: 0.0163s/iter; left time: 89.9435s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0753960\n",
      "\tspeed: 0.0163s/iter; left time: 88.2995s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1561042\n",
      "\tspeed: 0.0163s/iter; left time: 86.6712s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0283330\n",
      "\tspeed: 0.0163s/iter; left time: 85.1047s\n",
      "\titers: 4000, epoch: 9 | loss: 0.1113818\n",
      "\tspeed: 0.0163s/iter; left time: 83.5189s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0450250\n",
      "\tspeed: 0.0163s/iter; left time: 81.8488s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0777896\n",
      "\tspeed: 0.0163s/iter; left time: 80.2425s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0420210\n",
      "\tspeed: 0.0151s/iter; left time: 72.5927s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0368697\n",
      "\tspeed: 0.0142s/iter; left time: 66.8100s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0365919\n",
      "\tspeed: 0.0142s/iter; left time: 65.3806s\n",
      "Epoch: 9 cost time: 72.75392460823059\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0785689 Vali Loss: 0.0339733 Test Loss: 0.1184993\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11638184636831284, mae:0.20750662684440613\n"
     ]
    }
   ],
   "source": [
    "# gas_price\n",
    "!bash ./scripts/TimeXer.sh --features MS --predictor gas_price --enc_in 2 --dec_in 2 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f362e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       total_load          \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             2                   Dec In:             2                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1759927\n",
      "\tspeed: 0.0258s/iter; left time: 1172.5597s\n",
      "\titers: 200, epoch: 1 | loss: 0.1005098\n",
      "\tspeed: 0.0141s/iter; left time: 638.7861s\n",
      "\titers: 300, epoch: 1 | loss: 0.0638654\n",
      "\tspeed: 0.0140s/iter; left time: 635.5138s\n",
      "\titers: 400, epoch: 1 | loss: 0.1360727\n",
      "\tspeed: 0.0141s/iter; left time: 634.9801s\n",
      "\titers: 500, epoch: 1 | loss: 0.0321013\n",
      "\tspeed: 0.0141s/iter; left time: 633.9389s\n",
      "\titers: 600, epoch: 1 | loss: 0.0772481\n",
      "\tspeed: 0.0141s/iter; left time: 632.6128s\n",
      "\titers: 700, epoch: 1 | loss: 0.1345826\n",
      "\tspeed: 0.0141s/iter; left time: 630.8819s\n",
      "\titers: 800, epoch: 1 | loss: 0.5376329\n",
      "\tspeed: 0.0141s/iter; left time: 629.0518s\n",
      "\titers: 900, epoch: 1 | loss: 0.2582480\n",
      "\tspeed: 0.0141s/iter; left time: 628.5008s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0790523\n",
      "\tspeed: 0.0141s/iter; left time: 626.5804s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0739570\n",
      "\tspeed: 0.0141s/iter; left time: 625.2250s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0714946\n",
      "\tspeed: 0.0140s/iter; left time: 623.0155s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2813226\n",
      "\tspeed: 0.0141s/iter; left time: 621.8416s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0259122\n",
      "\tspeed: 0.0141s/iter; left time: 620.6236s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1274829\n",
      "\tspeed: 0.0141s/iter; left time: 619.1422s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1681881\n",
      "\tspeed: 0.0140s/iter; left time: 617.2588s\n",
      "\titers: 1700, epoch: 1 | loss: 0.4031706\n",
      "\tspeed: 0.0141s/iter; left time: 617.2476s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0563735\n",
      "\tspeed: 0.0141s/iter; left time: 616.3865s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1549343\n",
      "\tspeed: 0.0141s/iter; left time: 614.3993s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4181454\n",
      "\tspeed: 0.0141s/iter; left time: 612.7856s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1926515\n",
      "\tspeed: 0.0141s/iter; left time: 611.6549s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2900308\n",
      "\tspeed: 0.0141s/iter; left time: 611.7272s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1530581\n",
      "\tspeed: 0.0141s/iter; left time: 610.9226s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1707360\n",
      "\tspeed: 0.0141s/iter; left time: 608.7198s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2219595\n",
      "\tspeed: 0.0141s/iter; left time: 605.1899s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1108732\n",
      "\tspeed: 0.0141s/iter; left time: 604.9389s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1635533\n",
      "\tspeed: 0.0141s/iter; left time: 603.0909s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2325931\n",
      "\tspeed: 0.0141s/iter; left time: 600.8710s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1855148\n",
      "\tspeed: 0.0141s/iter; left time: 599.5737s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0875452\n",
      "\tspeed: 0.0141s/iter; left time: 597.9571s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0723489\n",
      "\tspeed: 0.0141s/iter; left time: 596.8409s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1847000\n",
      "\tspeed: 0.0141s/iter; left time: 595.7501s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0331125\n",
      "\tspeed: 0.0141s/iter; left time: 594.2691s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2570869\n",
      "\tspeed: 0.0141s/iter; left time: 592.5155s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0895515\n",
      "\tspeed: 0.0141s/iter; left time: 590.9136s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1073619\n",
      "\tspeed: 0.0141s/iter; left time: 589.6295s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0734465\n",
      "\tspeed: 0.0141s/iter; left time: 588.2711s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1570207\n",
      "\tspeed: 0.0141s/iter; left time: 587.2800s\n",
      "\titers: 3900, epoch: 1 | loss: 0.2763611\n",
      "\tspeed: 0.0141s/iter; left time: 585.4660s\n",
      "\titers: 4000, epoch: 1 | loss: 0.5206591\n",
      "\tspeed: 0.0141s/iter; left time: 583.9015s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0891215\n",
      "\tspeed: 0.0140s/iter; left time: 582.2815s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0833094\n",
      "\tspeed: 0.0140s/iter; left time: 580.8260s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1689244\n",
      "\tspeed: 0.0141s/iter; left time: 579.6249s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0301080\n",
      "\tspeed: 0.0141s/iter; left time: 578.3085s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0529098\n",
      "\tspeed: 0.0141s/iter; left time: 577.3565s\n",
      "Epoch: 1 cost time: 65.29488968849182\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1769363 Vali Loss: 0.0391474 Test Loss: 0.1294462\n",
      "Validation loss decreased (inf --> 0.039147).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0709434\n",
      "\tspeed: 0.1255s/iter; left time: 5133.3112s\n",
      "\titers: 200, epoch: 2 | loss: 0.0488707\n",
      "\tspeed: 0.0141s/iter; left time: 573.5425s\n",
      "\titers: 300, epoch: 2 | loss: 0.0839833\n",
      "\tspeed: 0.0140s/iter; left time: 571.7570s\n",
      "\titers: 400, epoch: 2 | loss: 0.0990587\n",
      "\tspeed: 0.0141s/iter; left time: 571.1689s\n",
      "\titers: 500, epoch: 2 | loss: 0.3297422\n",
      "\tspeed: 0.0140s/iter; left time: 568.7138s\n",
      "\titers: 600, epoch: 2 | loss: 0.0925497\n",
      "\tspeed: 0.0140s/iter; left time: 567.2628s\n",
      "\titers: 700, epoch: 2 | loss: 0.0814778\n",
      "\tspeed: 0.0141s/iter; left time: 567.2126s\n",
      "\titers: 800, epoch: 2 | loss: 0.0509366\n",
      "\tspeed: 0.0140s/iter; left time: 564.4460s\n",
      "\titers: 900, epoch: 2 | loss: 0.0423700\n",
      "\tspeed: 0.0141s/iter; left time: 563.5012s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1782139\n",
      "\tspeed: 0.0140s/iter; left time: 561.0928s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1907831\n",
      "\tspeed: 0.0140s/iter; left time: 559.4213s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1204457\n",
      "\tspeed: 0.0140s/iter; left time: 558.1257s\n",
      "\titers: 1300, epoch: 2 | loss: 0.1226730\n",
      "\tspeed: 0.0140s/iter; left time: 556.6741s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1283185\n",
      "\tspeed: 0.0140s/iter; left time: 555.1076s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0706527\n",
      "\tspeed: 0.0140s/iter; left time: 553.7332s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1035237\n",
      "\tspeed: 0.0140s/iter; left time: 552.2516s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2888501\n",
      "\tspeed: 0.0140s/iter; left time: 551.0292s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1360618\n",
      "\tspeed: 0.0140s/iter; left time: 549.4024s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1179228\n",
      "\tspeed: 0.0140s/iter; left time: 547.9284s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0605080\n",
      "\tspeed: 0.0140s/iter; left time: 547.1903s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0684468\n",
      "\tspeed: 0.0140s/iter; left time: 545.8724s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1475488\n",
      "\tspeed: 0.0140s/iter; left time: 544.3575s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1355673\n",
      "\tspeed: 0.0140s/iter; left time: 543.1127s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1321094\n",
      "\tspeed: 0.0140s/iter; left time: 541.5592s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0747641\n",
      "\tspeed: 0.0140s/iter; left time: 540.3265s\n",
      "\titers: 2600, epoch: 2 | loss: 0.3410626\n",
      "\tspeed: 0.0140s/iter; left time: 538.8731s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1080215\n",
      "\tspeed: 0.0140s/iter; left time: 537.6543s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0430763\n",
      "\tspeed: 0.0140s/iter; left time: 536.1728s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0482149\n",
      "\tspeed: 0.0140s/iter; left time: 534.6822s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1212059\n",
      "\tspeed: 0.0140s/iter; left time: 533.4098s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0924855\n",
      "\tspeed: 0.0140s/iter; left time: 531.9909s\n",
      "\titers: 3200, epoch: 2 | loss: 0.1888116\n",
      "\tspeed: 0.0140s/iter; left time: 530.3368s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0510624\n",
      "\tspeed: 0.0140s/iter; left time: 529.0942s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0708582\n",
      "\tspeed: 0.0140s/iter; left time: 527.6452s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1610978\n",
      "\tspeed: 0.0141s/iter; left time: 527.7424s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1541513\n",
      "\tspeed: 0.0140s/iter; left time: 524.8236s\n",
      "\titers: 3700, epoch: 2 | loss: 0.1887961\n",
      "\tspeed: 0.0141s/iter; left time: 524.5046s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1578278\n",
      "\tspeed: 0.0141s/iter; left time: 523.2516s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2275678\n",
      "\tspeed: 0.0141s/iter; left time: 521.6522s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2216853\n",
      "\tspeed: 0.0140s/iter; left time: 519.5940s\n",
      "\titers: 4100, epoch: 2 | loss: 0.0922666\n",
      "\tspeed: 0.0141s/iter; left time: 518.4829s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0640367\n",
      "\tspeed: 0.0141s/iter; left time: 517.1050s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1141457\n",
      "\tspeed: 0.0141s/iter; left time: 516.0540s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1244170\n",
      "\tspeed: 0.0140s/iter; left time: 514.0557s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0403202\n",
      "\tspeed: 0.0140s/iter; left time: 512.4907s\n",
      "Epoch: 2 cost time: 64.19521951675415\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1516635 Vali Loss: 0.0330107 Test Loss: 0.1132908\n",
      "Validation loss decreased (0.039147 --> 0.033011).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1018267\n",
      "\tspeed: 0.1314s/iter; left time: 4774.2421s\n",
      "\titers: 200, epoch: 3 | loss: 0.0484010\n",
      "\tspeed: 0.0141s/iter; left time: 512.0925s\n",
      "\titers: 300, epoch: 3 | loss: 0.1089981\n",
      "\tspeed: 0.0141s/iter; left time: 510.2189s\n",
      "\titers: 400, epoch: 3 | loss: 0.0869171\n",
      "\tspeed: 0.0141s/iter; left time: 508.7072s\n",
      "\titers: 500, epoch: 3 | loss: 0.0957361\n",
      "\tspeed: 0.0141s/iter; left time: 507.6281s\n",
      "\titers: 600, epoch: 3 | loss: 0.1074140\n",
      "\tspeed: 0.0141s/iter; left time: 505.8141s\n",
      "\titers: 700, epoch: 3 | loss: 0.3365041\n",
      "\tspeed: 0.0141s/iter; left time: 503.8841s\n",
      "\titers: 800, epoch: 3 | loss: 0.1526949\n",
      "\tspeed: 0.0141s/iter; left time: 502.5103s\n",
      "\titers: 900, epoch: 3 | loss: 0.0599587\n",
      "\tspeed: 0.0141s/iter; left time: 500.9653s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1092401\n",
      "\tspeed: 0.0141s/iter; left time: 499.7802s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1904721\n",
      "\tspeed: 0.0141s/iter; left time: 498.0998s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1729239\n",
      "\tspeed: 0.0141s/iter; left time: 497.1867s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2153694\n",
      "\tspeed: 0.0141s/iter; left time: 495.4221s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1360826\n",
      "\tspeed: 0.0141s/iter; left time: 493.9938s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2275563\n",
      "\tspeed: 0.0141s/iter; left time: 492.3454s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1521941\n",
      "\tspeed: 0.0141s/iter; left time: 491.6482s\n",
      "\titers: 1700, epoch: 3 | loss: 0.4756070\n",
      "\tspeed: 0.0141s/iter; left time: 490.8623s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0934537\n",
      "\tspeed: 0.0141s/iter; left time: 489.4160s\n",
      "\titers: 1900, epoch: 3 | loss: 0.2139462\n",
      "\tspeed: 0.0141s/iter; left time: 487.8461s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2198817\n",
      "\tspeed: 0.0141s/iter; left time: 486.0814s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0862591\n",
      "\tspeed: 0.0141s/iter; left time: 485.0155s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0581241\n",
      "\tspeed: 0.0141s/iter; left time: 483.5843s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1154659\n",
      "\tspeed: 0.0141s/iter; left time: 482.1631s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0916204\n",
      "\tspeed: 0.0141s/iter; left time: 480.5858s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1502590\n",
      "\tspeed: 0.0141s/iter; left time: 479.0430s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0821638\n",
      "\tspeed: 0.0141s/iter; left time: 477.7935s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0532584\n",
      "\tspeed: 0.0141s/iter; left time: 476.4058s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1231633\n",
      "\tspeed: 0.0141s/iter; left time: 474.8738s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0771413\n",
      "\tspeed: 0.0141s/iter; left time: 473.4528s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0379044\n",
      "\tspeed: 0.0141s/iter; left time: 472.1042s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1473286\n",
      "\tspeed: 0.0141s/iter; left time: 470.5405s\n",
      "\titers: 3200, epoch: 3 | loss: 0.6245164\n",
      "\tspeed: 0.0141s/iter; left time: 469.5967s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0808302\n",
      "\tspeed: 0.0141s/iter; left time: 467.8198s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2345002\n",
      "\tspeed: 0.0141s/iter; left time: 466.4426s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0410836\n",
      "\tspeed: 0.0141s/iter; left time: 465.0402s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1252303\n",
      "\tspeed: 0.0141s/iter; left time: 463.7304s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1074857\n",
      "\tspeed: 0.0141s/iter; left time: 462.1519s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0587929\n",
      "\tspeed: 0.0141s/iter; left time: 460.9638s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1475587\n",
      "\tspeed: 0.0141s/iter; left time: 459.1557s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2225686\n",
      "\tspeed: 0.0141s/iter; left time: 457.8495s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2237215\n",
      "\tspeed: 0.0141s/iter; left time: 456.6373s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0605643\n",
      "\tspeed: 0.0141s/iter; left time: 455.0979s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0303679\n",
      "\tspeed: 0.0141s/iter; left time: 453.6573s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0757557\n",
      "\tspeed: 0.0141s/iter; left time: 451.5073s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0266420\n",
      "\tspeed: 0.0141s/iter; left time: 450.2391s\n",
      "Epoch: 3 cost time: 64.6257803440094\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1268762 Vali Loss: 0.0333214 Test Loss: 0.1176284\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0677748\n",
      "\tspeed: 0.1264s/iter; left time: 4018.4082s\n",
      "\titers: 200, epoch: 4 | loss: 0.3070554\n",
      "\tspeed: 0.0141s/iter; left time: 445.9932s\n",
      "\titers: 300, epoch: 4 | loss: 0.0280400\n",
      "\tspeed: 0.0141s/iter; left time: 444.6137s\n",
      "\titers: 400, epoch: 4 | loss: 0.1560239\n",
      "\tspeed: 0.0141s/iter; left time: 444.0393s\n",
      "\titers: 500, epoch: 4 | loss: 0.1099010\n",
      "\tspeed: 0.0140s/iter; left time: 440.7602s\n",
      "\titers: 600, epoch: 4 | loss: 0.0953508\n",
      "\tspeed: 0.0140s/iter; left time: 438.8389s\n",
      "\titers: 700, epoch: 4 | loss: 0.0841895\n",
      "\tspeed: 0.0140s/iter; left time: 437.9471s\n",
      "\titers: 800, epoch: 4 | loss: 0.1544153\n",
      "\tspeed: 0.0141s/iter; left time: 437.5207s\n",
      "\titers: 900, epoch: 4 | loss: 0.1274071\n",
      "\tspeed: 0.0141s/iter; left time: 436.2836s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1229334\n",
      "\tspeed: 0.0141s/iter; left time: 434.8551s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1262303\n",
      "\tspeed: 0.0141s/iter; left time: 433.7925s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0552251\n",
      "\tspeed: 0.0142s/iter; left time: 434.5695s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1007653\n",
      "\tspeed: 0.0141s/iter; left time: 432.7697s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0506458\n",
      "\tspeed: 0.0142s/iter; left time: 431.7254s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2370257\n",
      "\tspeed: 0.0142s/iter; left time: 430.1920s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0753491\n",
      "\tspeed: 0.0140s/iter; left time: 424.7220s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1660052\n",
      "\tspeed: 0.0140s/iter; left time: 422.5670s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0490977\n",
      "\tspeed: 0.0140s/iter; left time: 421.0145s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0947329\n",
      "\tspeed: 0.0140s/iter; left time: 419.4531s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1272814\n",
      "\tspeed: 0.0140s/iter; left time: 418.2681s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0179249\n",
      "\tspeed: 0.0140s/iter; left time: 417.1360s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1878305\n",
      "\tspeed: 0.0140s/iter; left time: 416.1728s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1629618\n",
      "\tspeed: 0.0140s/iter; left time: 414.7738s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1148672\n",
      "\tspeed: 0.0140s/iter; left time: 412.8587s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0289091\n",
      "\tspeed: 0.0140s/iter; left time: 411.3387s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0827030\n",
      "\tspeed: 0.0140s/iter; left time: 410.0135s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5467109\n",
      "\tspeed: 0.0140s/iter; left time: 408.3248s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0275374\n",
      "\tspeed: 0.0140s/iter; left time: 406.9886s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0314616\n",
      "\tspeed: 0.0140s/iter; left time: 405.7896s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1049759\n",
      "\tspeed: 0.0140s/iter; left time: 404.0515s\n",
      "\titers: 3100, epoch: 4 | loss: 0.1131950\n",
      "\tspeed: 0.0140s/iter; left time: 402.5035s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1471792\n",
      "\tspeed: 0.0140s/iter; left time: 400.8557s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0893551\n",
      "\tspeed: 0.0140s/iter; left time: 400.0275s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0585081\n",
      "\tspeed: 0.0140s/iter; left time: 398.6447s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0485755\n",
      "\tspeed: 0.0140s/iter; left time: 398.7616s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0397043\n",
      "\tspeed: 0.0141s/iter; left time: 397.6367s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1933319\n",
      "\tspeed: 0.0141s/iter; left time: 396.0548s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0393248\n",
      "\tspeed: 0.0141s/iter; left time: 394.7745s\n",
      "\titers: 3900, epoch: 4 | loss: 0.1037640\n",
      "\tspeed: 0.0141s/iter; left time: 393.7512s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0824659\n",
      "\tspeed: 0.0141s/iter; left time: 392.2286s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0382093\n",
      "\tspeed: 0.0141s/iter; left time: 390.5985s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0830566\n",
      "\tspeed: 0.0141s/iter; left time: 391.1618s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3238772\n",
      "\tspeed: 0.0140s/iter; left time: 387.5285s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1110998\n",
      "\tspeed: 0.0141s/iter; left time: 386.2835s\n",
      "\titers: 4500, epoch: 4 | loss: 0.1073991\n",
      "\tspeed: 0.0141s/iter; left time: 384.9440s\n",
      "Epoch: 4 cost time: 64.20874190330505\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1100078 Vali Loss: 0.0326034 Test Loss: 0.1216023\n",
      "Validation loss decreased (0.033011 --> 0.032603).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0626363\n",
      "\tspeed: 0.1272s/iter; left time: 3463.3948s\n",
      "\titers: 200, epoch: 5 | loss: 0.0651858\n",
      "\tspeed: 0.0141s/iter; left time: 382.2683s\n",
      "\titers: 300, epoch: 5 | loss: 0.0690542\n",
      "\tspeed: 0.0141s/iter; left time: 380.6548s\n",
      "\titers: 400, epoch: 5 | loss: 0.0530531\n",
      "\tspeed: 0.0141s/iter; left time: 379.0652s\n",
      "\titers: 500, epoch: 5 | loss: 0.1194133\n",
      "\tspeed: 0.0141s/iter; left time: 377.6676s\n",
      "\titers: 600, epoch: 5 | loss: 0.0193160\n",
      "\tspeed: 0.0141s/iter; left time: 376.2879s\n",
      "\titers: 700, epoch: 5 | loss: 0.0367207\n",
      "\tspeed: 0.0141s/iter; left time: 375.5078s\n",
      "\titers: 800, epoch: 5 | loss: 0.0935420\n",
      "\tspeed: 0.0141s/iter; left time: 373.5899s\n",
      "\titers: 900, epoch: 5 | loss: 0.0621724\n",
      "\tspeed: 0.0141s/iter; left time: 372.5886s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1430875\n",
      "\tspeed: 0.0141s/iter; left time: 370.8738s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0329404\n",
      "\tspeed: 0.0141s/iter; left time: 369.9721s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1003864\n",
      "\tspeed: 0.0141s/iter; left time: 368.8445s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0802891\n",
      "\tspeed: 0.0141s/iter; left time: 367.2463s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1367226\n",
      "\tspeed: 0.0141s/iter; left time: 365.8878s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1316963\n",
      "\tspeed: 0.0141s/iter; left time: 364.3930s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0467243\n",
      "\tspeed: 0.0141s/iter; left time: 362.8869s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1635501\n",
      "\tspeed: 0.0141s/iter; left time: 361.6444s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1020788\n",
      "\tspeed: 0.0141s/iter; left time: 359.7779s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0319691\n",
      "\tspeed: 0.0140s/iter; left time: 357.2691s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5800079\n",
      "\tspeed: 0.0141s/iter; left time: 356.1128s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0890467\n",
      "\tspeed: 0.0141s/iter; left time: 355.3375s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0647310\n",
      "\tspeed: 0.0141s/iter; left time: 353.8856s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0395606\n",
      "\tspeed: 0.0140s/iter; left time: 351.4351s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0787048\n",
      "\tspeed: 0.0140s/iter; left time: 350.2575s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0761334\n",
      "\tspeed: 0.0141s/iter; left time: 349.2874s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0758988\n",
      "\tspeed: 0.0141s/iter; left time: 347.9170s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1048270\n",
      "\tspeed: 0.0141s/iter; left time: 346.4269s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1204636\n",
      "\tspeed: 0.0141s/iter; left time: 345.1171s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0572192\n",
      "\tspeed: 0.0141s/iter; left time: 343.7850s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0559349\n",
      "\tspeed: 0.0141s/iter; left time: 342.3561s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0404658\n",
      "\tspeed: 0.0141s/iter; left time: 340.9355s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0190077\n",
      "\tspeed: 0.0141s/iter; left time: 339.6301s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0523258\n",
      "\tspeed: 0.0141s/iter; left time: 338.0666s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0526128\n",
      "\tspeed: 0.0141s/iter; left time: 336.6081s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0472551\n",
      "\tspeed: 0.0141s/iter; left time: 335.2479s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0672086\n",
      "\tspeed: 0.0141s/iter; left time: 333.6994s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0566152\n",
      "\tspeed: 0.0141s/iter; left time: 332.4310s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1113559\n",
      "\tspeed: 0.0141s/iter; left time: 330.9207s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0576941\n",
      "\tspeed: 0.0141s/iter; left time: 329.5083s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0448340\n",
      "\tspeed: 0.0141s/iter; left time: 328.1839s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0229164\n",
      "\tspeed: 0.0141s/iter; left time: 326.5792s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0602215\n",
      "\tspeed: 0.0141s/iter; left time: 325.0069s\n",
      "\titers: 4300, epoch: 5 | loss: 0.0985747\n",
      "\tspeed: 0.0141s/iter; left time: 323.9191s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0227245\n",
      "\tspeed: 0.0141s/iter; left time: 322.2632s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0608477\n",
      "\tspeed: 0.0141s/iter; left time: 320.8669s\n",
      "Epoch: 5 cost time: 64.38011074066162\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0998084 Vali Loss: 0.0317413 Test Loss: 0.1186500\n",
      "Validation loss decreased (0.032603 --> 0.031741).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0277388\n",
      "\tspeed: 0.1264s/iter; left time: 2866.0045s\n",
      "\titers: 200, epoch: 6 | loss: 0.1703286\n",
      "\tspeed: 0.0142s/iter; left time: 320.0167s\n",
      "\titers: 300, epoch: 6 | loss: 0.1249021\n",
      "\tspeed: 0.0142s/iter; left time: 318.4535s\n",
      "\titers: 400, epoch: 6 | loss: 0.0752089\n",
      "\tspeed: 0.0142s/iter; left time: 316.9870s\n",
      "\titers: 500, epoch: 6 | loss: 0.1561552\n",
      "\tspeed: 0.0142s/iter; left time: 315.5528s\n",
      "\titers: 600, epoch: 6 | loss: 0.1479534\n",
      "\tspeed: 0.0142s/iter; left time: 314.4950s\n",
      "\titers: 700, epoch: 6 | loss: 0.0331219\n",
      "\tspeed: 0.0142s/iter; left time: 312.6856s\n",
      "\titers: 800, epoch: 6 | loss: 0.0567712\n",
      "\tspeed: 0.0142s/iter; left time: 311.2077s\n",
      "\titers: 900, epoch: 6 | loss: 0.0486447\n",
      "\tspeed: 0.0142s/iter; left time: 310.0483s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1846031\n",
      "\tspeed: 0.0142s/iter; left time: 308.3108s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0510034\n",
      "\tspeed: 0.0142s/iter; left time: 306.8957s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0916864\n",
      "\tspeed: 0.0142s/iter; left time: 305.4631s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1022680\n",
      "\tspeed: 0.0142s/iter; left time: 304.5116s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0707264\n",
      "\tspeed: 0.0141s/iter; left time: 301.5420s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0911812\n",
      "\tspeed: 0.0141s/iter; left time: 300.0741s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1554714\n",
      "\tspeed: 0.0141s/iter; left time: 298.8213s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0485573\n",
      "\tspeed: 0.0141s/iter; left time: 297.6037s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2463143\n",
      "\tspeed: 0.0141s/iter; left time: 296.1593s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1036867\n",
      "\tspeed: 0.0141s/iter; left time: 294.7746s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1787771\n",
      "\tspeed: 0.0141s/iter; left time: 293.4840s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1134794\n",
      "\tspeed: 0.0141s/iter; left time: 292.1801s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1263179\n",
      "\tspeed: 0.0141s/iter; left time: 290.5326s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1050347\n",
      "\tspeed: 0.0141s/iter; left time: 288.9023s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0861309\n",
      "\tspeed: 0.0141s/iter; left time: 287.5061s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0649742\n",
      "\tspeed: 0.0141s/iter; left time: 286.1456s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0782598\n",
      "\tspeed: 0.0141s/iter; left time: 284.9370s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0622398\n",
      "\tspeed: 0.0141s/iter; left time: 283.5621s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1098996\n",
      "\tspeed: 0.0141s/iter; left time: 282.0574s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2320417\n",
      "\tspeed: 0.0141s/iter; left time: 280.8655s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0310082\n",
      "\tspeed: 0.0141s/iter; left time: 279.2015s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0774086\n",
      "\tspeed: 0.0141s/iter; left time: 277.8163s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0803729\n",
      "\tspeed: 0.0141s/iter; left time: 276.2571s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1023466\n",
      "\tspeed: 0.0141s/iter; left time: 274.8747s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0215578\n",
      "\tspeed: 0.0141s/iter; left time: 272.8095s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0298279\n",
      "\tspeed: 0.0141s/iter; left time: 271.5784s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0747626\n",
      "\tspeed: 0.0141s/iter; left time: 270.0343s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1326391\n",
      "\tspeed: 0.0141s/iter; left time: 268.6676s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1249703\n",
      "\tspeed: 0.0141s/iter; left time: 267.1592s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0434858\n",
      "\tspeed: 0.0141s/iter; left time: 265.8383s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1853186\n",
      "\tspeed: 0.0141s/iter; left time: 264.3501s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0165570\n",
      "\tspeed: 0.0141s/iter; left time: 262.9929s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0281204\n",
      "\tspeed: 0.0141s/iter; left time: 262.5334s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1069887\n",
      "\tspeed: 0.0142s/iter; left time: 261.7487s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0844429\n",
      "\tspeed: 0.0142s/iter; left time: 260.3195s\n",
      "\titers: 4500, epoch: 6 | loss: 0.1098494\n",
      "\tspeed: 0.0142s/iter; left time: 258.9598s\n",
      "Epoch: 6 cost time: 64.61733293533325\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0954476 Vali Loss: 0.0316978 Test Loss: 0.1208863\n",
      "Validation loss decreased (0.031741 --> 0.031698).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1025192\n",
      "\tspeed: 0.1330s/iter; left time: 2409.7466s\n",
      "\titers: 200, epoch: 7 | loss: 0.2299474\n",
      "\tspeed: 0.0141s/iter; left time: 254.6930s\n",
      "\titers: 300, epoch: 7 | loss: 0.1932043\n",
      "\tspeed: 0.0142s/iter; left time: 253.6128s\n",
      "\titers: 400, epoch: 7 | loss: 0.0472199\n",
      "\tspeed: 0.0142s/iter; left time: 252.1808s\n",
      "\titers: 500, epoch: 7 | loss: 0.0327715\n",
      "\tspeed: 0.0141s/iter; left time: 250.6837s\n",
      "\titers: 600, epoch: 7 | loss: 0.0661617\n",
      "\tspeed: 0.0141s/iter; left time: 249.1828s\n",
      "\titers: 700, epoch: 7 | loss: 0.0148620\n",
      "\tspeed: 0.0141s/iter; left time: 247.6978s\n",
      "\titers: 800, epoch: 7 | loss: 0.0486824\n",
      "\tspeed: 0.0141s/iter; left time: 245.8223s\n",
      "\titers: 900, epoch: 7 | loss: 0.2101677\n",
      "\tspeed: 0.0141s/iter; left time: 244.6219s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0482192\n",
      "\tspeed: 0.0142s/iter; left time: 243.7611s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1973685\n",
      "\tspeed: 0.0141s/iter; left time: 242.2407s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0723647\n",
      "\tspeed: 0.0141s/iter; left time: 240.5225s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1097718\n",
      "\tspeed: 0.0141s/iter; left time: 239.0978s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0843925\n",
      "\tspeed: 0.0141s/iter; left time: 237.6266s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0898031\n",
      "\tspeed: 0.0141s/iter; left time: 236.3998s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0564848\n",
      "\tspeed: 0.0141s/iter; left time: 234.9821s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0320187\n",
      "\tspeed: 0.0141s/iter; left time: 233.4649s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0875859\n",
      "\tspeed: 0.0141s/iter; left time: 231.9547s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0746750\n",
      "\tspeed: 0.0141s/iter; left time: 230.5794s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0440867\n",
      "\tspeed: 0.0141s/iter; left time: 229.0710s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0390446\n",
      "\tspeed: 0.0141s/iter; left time: 227.6932s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0599097\n",
      "\tspeed: 0.0141s/iter; left time: 226.3485s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0415539\n",
      "\tspeed: 0.0141s/iter; left time: 224.8528s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0631474\n",
      "\tspeed: 0.0141s/iter; left time: 223.3637s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1793022\n",
      "\tspeed: 0.0141s/iter; left time: 222.0389s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0988206\n",
      "\tspeed: 0.0141s/iter; left time: 220.5301s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0328353\n",
      "\tspeed: 0.0141s/iter; left time: 219.2801s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0411637\n",
      "\tspeed: 0.0141s/iter; left time: 217.9629s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0697710\n",
      "\tspeed: 0.0141s/iter; left time: 216.6850s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0701139\n",
      "\tspeed: 0.0141s/iter; left time: 215.0746s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0191403\n",
      "\tspeed: 0.0141s/iter; left time: 213.7088s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1972307\n",
      "\tspeed: 0.0141s/iter; left time: 212.2509s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0636904\n",
      "\tspeed: 0.0141s/iter; left time: 210.8835s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0350361\n",
      "\tspeed: 0.0141s/iter; left time: 209.4142s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0439327\n",
      "\tspeed: 0.0141s/iter; left time: 208.0068s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1054622\n",
      "\tspeed: 0.0141s/iter; left time: 206.5411s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1744172\n",
      "\tspeed: 0.0141s/iter; left time: 205.1984s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0419758\n",
      "\tspeed: 0.0141s/iter; left time: 203.6729s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0580527\n",
      "\tspeed: 0.0141s/iter; left time: 202.2255s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0944986\n",
      "\tspeed: 0.0141s/iter; left time: 200.8641s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0736681\n",
      "\tspeed: 0.0141s/iter; left time: 199.4555s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0239612\n",
      "\tspeed: 0.0141s/iter; left time: 198.0475s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0172359\n",
      "\tspeed: 0.0141s/iter; left time: 196.7368s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0285303\n",
      "\tspeed: 0.0141s/iter; left time: 195.1738s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0823893\n",
      "\tspeed: 0.0141s/iter; left time: 193.8602s\n",
      "Epoch: 7 cost time: 64.61558389663696\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0922200 Vali Loss: 0.0318865 Test Loss: 0.1225609\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0701767\n",
      "\tspeed: 0.1317s/iter; left time: 1786.6026s\n",
      "\titers: 200, epoch: 8 | loss: 0.0378679\n",
      "\tspeed: 0.0141s/iter; left time: 190.0396s\n",
      "\titers: 300, epoch: 8 | loss: 0.1124579\n",
      "\tspeed: 0.0141s/iter; left time: 188.9452s\n",
      "\titers: 400, epoch: 8 | loss: 0.0779987\n",
      "\tspeed: 0.0141s/iter; left time: 187.4821s\n",
      "\titers: 500, epoch: 8 | loss: 0.1887622\n",
      "\tspeed: 0.0142s/iter; left time: 186.3818s\n",
      "\titers: 600, epoch: 8 | loss: 0.0902516\n",
      "\tspeed: 0.0142s/iter; left time: 185.1599s\n",
      "\titers: 700, epoch: 8 | loss: 0.2040251\n",
      "\tspeed: 0.0141s/iter; left time: 183.4048s\n",
      "\titers: 800, epoch: 8 | loss: 0.1398674\n",
      "\tspeed: 0.0141s/iter; left time: 181.8879s\n",
      "\titers: 900, epoch: 8 | loss: 0.0605964\n",
      "\tspeed: 0.0141s/iter; left time: 180.4851s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0825206\n",
      "\tspeed: 0.0141s/iter; left time: 178.9819s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0832172\n",
      "\tspeed: 0.0141s/iter; left time: 177.4896s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1660474\n",
      "\tspeed: 0.0141s/iter; left time: 176.3742s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3562001\n",
      "\tspeed: 0.0141s/iter; left time: 174.9060s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0369190\n",
      "\tspeed: 0.0141s/iter; left time: 172.8955s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0901217\n",
      "\tspeed: 0.0141s/iter; left time: 171.3637s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1177536\n",
      "\tspeed: 0.0141s/iter; left time: 170.1087s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0307235\n",
      "\tspeed: 0.0141s/iter; left time: 168.5240s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1507467\n",
      "\tspeed: 0.0141s/iter; left time: 167.0469s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0849749\n",
      "\tspeed: 0.0141s/iter; left time: 165.5300s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1214302\n",
      "\tspeed: 0.0141s/iter; left time: 164.1404s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0446351\n",
      "\tspeed: 0.0141s/iter; left time: 162.6912s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0803655\n",
      "\tspeed: 0.0141s/iter; left time: 161.3878s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1589385\n",
      "\tspeed: 0.0141s/iter; left time: 159.9095s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0460822\n",
      "\tspeed: 0.0141s/iter; left time: 158.7012s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0784630\n",
      "\tspeed: 0.0141s/iter; left time: 157.2113s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0655714\n",
      "\tspeed: 0.0141s/iter; left time: 155.7489s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0340518\n",
      "\tspeed: 0.0141s/iter; left time: 154.3624s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0548059\n",
      "\tspeed: 0.0141s/iter; left time: 153.0005s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0825656\n",
      "\tspeed: 0.0141s/iter; left time: 151.6692s\n",
      "\titers: 3000, epoch: 8 | loss: 0.3075091\n",
      "\tspeed: 0.0141s/iter; left time: 150.3682s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0925470\n",
      "\tspeed: 0.0141s/iter; left time: 148.7352s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1359998\n",
      "\tspeed: 0.0141s/iter; left time: 147.3582s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0587429\n",
      "\tspeed: 0.0141s/iter; left time: 146.0401s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0585539\n",
      "\tspeed: 0.0140s/iter; left time: 144.2130s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1413106\n",
      "\tspeed: 0.0140s/iter; left time: 142.8098s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0347230\n",
      "\tspeed: 0.0140s/iter; left time: 141.3764s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2237663\n",
      "\tspeed: 0.0141s/iter; left time: 140.0317s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0759090\n",
      "\tspeed: 0.0140s/iter; left time: 138.6157s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0628641\n",
      "\tspeed: 0.0140s/iter; left time: 137.1656s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0421785\n",
      "\tspeed: 0.0140s/iter; left time: 135.7208s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0302483\n",
      "\tspeed: 0.0140s/iter; left time: 134.3653s\n",
      "\titers: 4200, epoch: 8 | loss: 0.1038510\n",
      "\tspeed: 0.0140s/iter; left time: 132.9684s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0814080\n",
      "\tspeed: 0.0140s/iter; left time: 131.5201s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1293320\n",
      "\tspeed: 0.0140s/iter; left time: 130.1241s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1119517\n",
      "\tspeed: 0.0140s/iter; left time: 128.7190s\n",
      "Epoch: 8 cost time: 64.40041971206665\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0923995 Vali Loss: 0.0320617 Test Loss: 0.1230773\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0487307\n",
      "\tspeed: 0.1254s/iter; left time: 1130.1156s\n",
      "\titers: 200, epoch: 9 | loss: 0.0926895\n",
      "\tspeed: 0.0141s/iter; left time: 125.7816s\n",
      "\titers: 300, epoch: 9 | loss: 0.0423588\n",
      "\tspeed: 0.0141s/iter; left time: 124.4745s\n",
      "\titers: 400, epoch: 9 | loss: 0.1462673\n",
      "\tspeed: 0.0141s/iter; left time: 122.8645s\n",
      "\titers: 500, epoch: 9 | loss: 0.0739181\n",
      "\tspeed: 0.0141s/iter; left time: 121.6032s\n",
      "\titers: 600, epoch: 9 | loss: 0.0341225\n",
      "\tspeed: 0.0141s/iter; left time: 120.2079s\n",
      "\titers: 700, epoch: 9 | loss: 0.1056968\n",
      "\tspeed: 0.0141s/iter; left time: 118.6279s\n",
      "\titers: 800, epoch: 9 | loss: 0.0368858\n",
      "\tspeed: 0.0141s/iter; left time: 117.1656s\n",
      "\titers: 900, epoch: 9 | loss: 0.0399258\n",
      "\tspeed: 0.0141s/iter; left time: 115.7500s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0637278\n",
      "\tspeed: 0.0141s/iter; left time: 114.5873s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0641843\n",
      "\tspeed: 0.0141s/iter; left time: 113.2695s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1971078\n",
      "\tspeed: 0.0141s/iter; left time: 111.7680s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0329389\n",
      "\tspeed: 0.0141s/iter; left time: 110.1722s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2216890\n",
      "\tspeed: 0.0141s/iter; left time: 108.7256s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0481686\n",
      "\tspeed: 0.0141s/iter; left time: 107.2953s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0473219\n",
      "\tspeed: 0.0141s/iter; left time: 105.8549s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0945074\n",
      "\tspeed: 0.0141s/iter; left time: 104.5142s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0288875\n",
      "\tspeed: 0.0141s/iter; left time: 103.2320s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0194001\n",
      "\tspeed: 0.0141s/iter; left time: 101.6450s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0590996\n",
      "\tspeed: 0.0141s/iter; left time: 100.2085s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0471136\n",
      "\tspeed: 0.0141s/iter; left time: 98.7940s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1239936\n",
      "\tspeed: 0.0141s/iter; left time: 97.4684s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1041419\n",
      "\tspeed: 0.0141s/iter; left time: 96.0750s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0321019\n",
      "\tspeed: 0.0141s/iter; left time: 94.6430s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0243156\n",
      "\tspeed: 0.0141s/iter; left time: 93.2344s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0629507\n",
      "\tspeed: 0.0141s/iter; left time: 91.8514s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0497457\n",
      "\tspeed: 0.0141s/iter; left time: 90.4070s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1519527\n",
      "\tspeed: 0.0141s/iter; left time: 88.9594s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0573020\n",
      "\tspeed: 0.0141s/iter; left time: 87.6191s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1340358\n",
      "\tspeed: 0.0141s/iter; left time: 86.2038s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0285150\n",
      "\tspeed: 0.0141s/iter; left time: 84.7829s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0630654\n",
      "\tspeed: 0.0141s/iter; left time: 83.3741s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2300242\n",
      "\tspeed: 0.0141s/iter; left time: 82.0256s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1096455\n",
      "\tspeed: 0.0141s/iter; left time: 80.5455s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0853339\n",
      "\tspeed: 0.0141s/iter; left time: 79.1024s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0426191\n",
      "\tspeed: 0.0141s/iter; left time: 77.7410s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0697190\n",
      "\tspeed: 0.0141s/iter; left time: 76.3135s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1190345\n",
      "\tspeed: 0.0141s/iter; left time: 74.8949s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0921732\n",
      "\tspeed: 0.0141s/iter; left time: 73.5252s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0762893\n",
      "\tspeed: 0.0141s/iter; left time: 72.1046s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0668493\n",
      "\tspeed: 0.0141s/iter; left time: 70.6717s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0891116\n",
      "\tspeed: 0.0141s/iter; left time: 69.3882s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0638719\n",
      "\tspeed: 0.0141s/iter; left time: 67.9384s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0320236\n",
      "\tspeed: 0.0141s/iter; left time: 66.5525s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0251591\n",
      "\tspeed: 0.0141s/iter; left time: 65.0635s\n",
      "Epoch: 9 cost time: 64.54189467430115\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0904113 Vali Loss: 0.0319028 Test Loss: 0.1226591\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0626794\n",
      "\tspeed: 0.1355s/iter; left time: 603.8568s\n",
      "\titers: 200, epoch: 10 | loss: 0.0836334\n",
      "\tspeed: 0.0162s/iter; left time: 70.5987s\n",
      "\titers: 300, epoch: 10 | loss: 0.0381573\n",
      "\tspeed: 0.0156s/iter; left time: 66.3312s\n",
      "\titers: 400, epoch: 10 | loss: 0.0420326\n",
      "\tspeed: 0.0142s/iter; left time: 58.8111s\n",
      "\titers: 500, epoch: 10 | loss: 0.0429586\n",
      "\tspeed: 0.0141s/iter; left time: 57.3657s\n",
      "\titers: 600, epoch: 10 | loss: 0.0924656\n",
      "\tspeed: 0.0142s/iter; left time: 55.9923s\n",
      "\titers: 700, epoch: 10 | loss: 0.0645054\n",
      "\tspeed: 0.0141s/iter; left time: 54.5213s\n",
      "\titers: 800, epoch: 10 | loss: 0.1832157\n",
      "\tspeed: 0.0141s/iter; left time: 53.0982s\n",
      "\titers: 900, epoch: 10 | loss: 0.0973747\n",
      "\tspeed: 0.0141s/iter; left time: 51.6929s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0485435\n",
      "\tspeed: 0.0141s/iter; left time: 50.2536s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0583680\n",
      "\tspeed: 0.0142s/iter; left time: 48.9243s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0900433\n",
      "\tspeed: 0.0142s/iter; left time: 47.4907s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0391196\n",
      "\tspeed: 0.0141s/iter; left time: 46.0120s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0779811\n",
      "\tspeed: 0.0141s/iter; left time: 44.6220s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2405260\n",
      "\tspeed: 0.0141s/iter; left time: 43.2051s\n",
      "\titers: 1600, epoch: 10 | loss: 0.1432933\n",
      "\tspeed: 0.0141s/iter; left time: 41.7891s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0257242\n",
      "\tspeed: 0.0141s/iter; left time: 40.3267s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0938244\n",
      "\tspeed: 0.0141s/iter; left time: 38.8833s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1106036\n",
      "\tspeed: 0.0141s/iter; left time: 37.4533s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0758235\n",
      "\tspeed: 0.0141s/iter; left time: 36.0591s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0438861\n",
      "\tspeed: 0.0141s/iter; left time: 34.6372s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0639797\n",
      "\tspeed: 0.0141s/iter; left time: 33.2451s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1234691\n",
      "\tspeed: 0.0141s/iter; left time: 31.8162s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0876351\n",
      "\tspeed: 0.0141s/iter; left time: 30.4137s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2222401\n",
      "\tspeed: 0.0141s/iter; left time: 29.0302s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1523860\n",
      "\tspeed: 0.0141s/iter; left time: 27.6400s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0838694\n",
      "\tspeed: 0.0141s/iter; left time: 26.2080s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0548758\n",
      "\tspeed: 0.0141s/iter; left time: 24.7944s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0732583\n",
      "\tspeed: 0.0141s/iter; left time: 23.3889s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0450948\n",
      "\tspeed: 0.0141s/iter; left time: 21.9699s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0604523\n",
      "\tspeed: 0.0141s/iter; left time: 20.5535s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0880652\n",
      "\tspeed: 0.0141s/iter; left time: 19.1457s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0839868\n",
      "\tspeed: 0.0141s/iter; left time: 17.7443s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1347636\n",
      "\tspeed: 0.0141s/iter; left time: 16.3369s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0267478\n",
      "\tspeed: 0.0150s/iter; left time: 15.8031s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1878310\n",
      "\tspeed: 0.0162s/iter; left time: 15.4961s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0832595\n",
      "\tspeed: 0.0162s/iter; left time: 13.8718s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0642754\n",
      "\tspeed: 0.0162s/iter; left time: 12.2505s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0230879\n",
      "\tspeed: 0.0162s/iter; left time: 10.6264s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0511064\n",
      "\tspeed: 0.0162s/iter; left time: 9.0048s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1427160\n",
      "\tspeed: 0.0162s/iter; left time: 7.3875s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0587856\n",
      "\tspeed: 0.0150s/iter; left time: 5.3418s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0342961\n",
      "\tspeed: 0.0141s/iter; left time: 3.6180s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0534264\n",
      "\tspeed: 0.0141s/iter; left time: 2.2039s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1059394\n",
      "\tspeed: 0.0141s/iter; left time: 0.7914s\n",
      "Epoch: 10 cost time: 66.51515436172485\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0903920 Vali Loss: 0.0318728 Test Loss: 0.1227035\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1209254041314125, mae:0.2057265192270279\n"
     ]
    }
   ],
   "source": [
    "# total_load\n",
    "!bash ./scripts/TimeXer.sh --features MS --predictor total_load --enc_in 2 --dec_in 2 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f1afa",
   "metadata": {},
   "source": [
    "## penetration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbe88dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_penetration   \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             2                   Dec In:             2                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1759035\n",
      "\tspeed: 0.0257s/iter; left time: 1169.7716s\n",
      "\titers: 200, epoch: 1 | loss: 0.1019014\n",
      "\tspeed: 0.0142s/iter; left time: 641.7443s\n",
      "\titers: 300, epoch: 1 | loss: 0.0675166\n",
      "\tspeed: 0.0141s/iter; left time: 639.3945s\n",
      "\titers: 400, epoch: 1 | loss: 0.1458422\n",
      "\tspeed: 0.0141s/iter; left time: 635.4661s\n",
      "\titers: 500, epoch: 1 | loss: 0.0337462\n",
      "\tspeed: 0.0141s/iter; left time: 636.1749s\n",
      "\titers: 600, epoch: 1 | loss: 0.0724578\n",
      "\tspeed: 0.0141s/iter; left time: 634.5028s\n",
      "\titers: 700, epoch: 1 | loss: 0.1377362\n",
      "\tspeed: 0.0141s/iter; left time: 633.7870s\n",
      "\titers: 800, epoch: 1 | loss: 0.4123822\n",
      "\tspeed: 0.0141s/iter; left time: 632.8213s\n",
      "\titers: 900, epoch: 1 | loss: 0.2609372\n",
      "\tspeed: 0.0141s/iter; left time: 628.6686s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0854746\n",
      "\tspeed: 0.0140s/iter; left time: 624.6512s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0478465\n",
      "\tspeed: 0.0140s/iter; left time: 623.7847s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0728862\n",
      "\tspeed: 0.0140s/iter; left time: 620.5508s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2556087\n",
      "\tspeed: 0.0140s/iter; left time: 620.2690s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0252558\n",
      "\tspeed: 0.0140s/iter; left time: 618.0549s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1776806\n",
      "\tspeed: 0.0140s/iter; left time: 616.2003s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2286750\n",
      "\tspeed: 0.0140s/iter; left time: 615.0948s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3005548\n",
      "\tspeed: 0.0140s/iter; left time: 613.5054s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0794195\n",
      "\tspeed: 0.0140s/iter; left time: 611.6110s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1905660\n",
      "\tspeed: 0.0140s/iter; left time: 610.1426s\n",
      "\titers: 2000, epoch: 1 | loss: 0.3692873\n",
      "\tspeed: 0.0140s/iter; left time: 608.7084s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2181242\n",
      "\tspeed: 0.0140s/iter; left time: 607.4588s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2663762\n",
      "\tspeed: 0.0140s/iter; left time: 606.8396s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1458775\n",
      "\tspeed: 0.0140s/iter; left time: 605.4421s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1706197\n",
      "\tspeed: 0.0140s/iter; left time: 603.9262s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2614470\n",
      "\tspeed: 0.0140s/iter; left time: 602.9868s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1126476\n",
      "\tspeed: 0.0140s/iter; left time: 601.1836s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2038525\n",
      "\tspeed: 0.0140s/iter; left time: 599.7658s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1997835\n",
      "\tspeed: 0.0140s/iter; left time: 598.7079s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1906416\n",
      "\tspeed: 0.0140s/iter; left time: 596.8236s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0958476\n",
      "\tspeed: 0.0140s/iter; left time: 595.2997s\n",
      "\titers: 3100, epoch: 1 | loss: 0.0740026\n",
      "\tspeed: 0.0140s/iter; left time: 594.2152s\n",
      "\titers: 3200, epoch: 1 | loss: 0.1310407\n",
      "\tspeed: 0.0140s/iter; left time: 592.8809s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0310477\n",
      "\tspeed: 0.0140s/iter; left time: 590.0986s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2182012\n",
      "\tspeed: 0.0140s/iter; left time: 588.4732s\n",
      "\titers: 3500, epoch: 1 | loss: 0.0859247\n",
      "\tspeed: 0.0139s/iter; left time: 586.5605s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0920254\n",
      "\tspeed: 0.0139s/iter; left time: 585.0986s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0656263\n",
      "\tspeed: 0.0140s/iter; left time: 583.8249s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1647875\n",
      "\tspeed: 0.0139s/iter; left time: 582.1700s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3426941\n",
      "\tspeed: 0.0139s/iter; left time: 580.5232s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3365933\n",
      "\tspeed: 0.0139s/iter; left time: 579.3326s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0745194\n",
      "\tspeed: 0.0139s/iter; left time: 577.8984s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0751309\n",
      "\tspeed: 0.0139s/iter; left time: 576.2838s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1757441\n",
      "\tspeed: 0.0139s/iter; left time: 575.0279s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0274968\n",
      "\tspeed: 0.0139s/iter; left time: 573.7813s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0589768\n",
      "\tspeed: 0.0139s/iter; left time: 572.4384s\n",
      "Epoch: 1 cost time: 65.0080156326294\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1777441 Vali Loss: 0.0366179 Test Loss: 0.1254984\n",
      "Validation loss decreased (inf --> 0.036618).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0676986\n",
      "\tspeed: 0.1284s/iter; left time: 5249.2757s\n",
      "\titers: 200, epoch: 2 | loss: 0.0402418\n",
      "\tspeed: 0.0144s/iter; left time: 588.8381s\n",
      "\titers: 300, epoch: 2 | loss: 0.0980492\n",
      "\tspeed: 0.0147s/iter; left time: 599.9351s\n",
      "\titers: 400, epoch: 2 | loss: 0.0935131\n",
      "\tspeed: 0.0156s/iter; left time: 634.6079s\n",
      "\titers: 500, epoch: 2 | loss: 0.3019584\n",
      "\tspeed: 0.0157s/iter; left time: 634.7157s\n",
      "\titers: 600, epoch: 2 | loss: 0.0738457\n",
      "\tspeed: 0.0157s/iter; left time: 632.4339s\n",
      "\titers: 700, epoch: 2 | loss: 0.0887291\n",
      "\tspeed: 0.0157s/iter; left time: 632.1947s\n",
      "\titers: 800, epoch: 2 | loss: 0.0421068\n",
      "\tspeed: 0.0147s/iter; left time: 589.1648s\n",
      "\titers: 900, epoch: 2 | loss: 0.0453451\n",
      "\tspeed: 0.0141s/iter; left time: 567.0475s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1751459\n",
      "\tspeed: 0.0141s/iter; left time: 565.7867s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1945211\n",
      "\tspeed: 0.0155s/iter; left time: 619.6291s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1162341\n",
      "\tspeed: 0.0157s/iter; left time: 623.0632s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0818833\n",
      "\tspeed: 0.0157s/iter; left time: 622.4071s\n",
      "\titers: 1400, epoch: 2 | loss: 0.1237866\n",
      "\tspeed: 0.0157s/iter; left time: 620.7135s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0393769\n",
      "\tspeed: 0.0157s/iter; left time: 618.5892s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1252360\n",
      "\tspeed: 0.0157s/iter; left time: 617.0734s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3748271\n",
      "\tspeed: 0.0157s/iter; left time: 615.6339s\n",
      "\titers: 1800, epoch: 2 | loss: 0.0963401\n",
      "\tspeed: 0.0157s/iter; left time: 613.5343s\n",
      "\titers: 1900, epoch: 2 | loss: 0.1303917\n",
      "\tspeed: 0.0157s/iter; left time: 612.3805s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0515234\n",
      "\tspeed: 0.0157s/iter; left time: 610.9606s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0838734\n",
      "\tspeed: 0.0157s/iter; left time: 609.2388s\n",
      "\titers: 2200, epoch: 2 | loss: 0.1556151\n",
      "\tspeed: 0.0157s/iter; left time: 607.3875s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1304311\n",
      "\tspeed: 0.0157s/iter; left time: 605.9266s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1067671\n",
      "\tspeed: 0.0157s/iter; left time: 604.4120s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0811204\n",
      "\tspeed: 0.0157s/iter; left time: 603.2757s\n",
      "\titers: 2600, epoch: 2 | loss: 0.2936197\n",
      "\tspeed: 0.0157s/iter; left time: 601.0743s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1145263\n",
      "\tspeed: 0.0156s/iter; left time: 599.1452s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0734180\n",
      "\tspeed: 0.0149s/iter; left time: 570.6776s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0606804\n",
      "\tspeed: 0.0141s/iter; left time: 538.0332s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1280475\n",
      "\tspeed: 0.0141s/iter; left time: 536.3506s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0798042\n",
      "\tspeed: 0.0141s/iter; left time: 535.1780s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2210374\n",
      "\tspeed: 0.0141s/iter; left time: 533.5398s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0457604\n",
      "\tspeed: 0.0141s/iter; left time: 532.1562s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0737865\n",
      "\tspeed: 0.0141s/iter; left time: 531.8295s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1539739\n",
      "\tspeed: 0.0156s/iter; left time: 586.5998s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1369576\n",
      "\tspeed: 0.0148s/iter; left time: 554.7856s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2429535\n",
      "\tspeed: 0.0141s/iter; left time: 526.7165s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1345761\n",
      "\tspeed: 0.0141s/iter; left time: 525.1287s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1847756\n",
      "\tspeed: 0.0141s/iter; left time: 523.8278s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2080964\n",
      "\tspeed: 0.0141s/iter; left time: 522.2515s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1179601\n",
      "\tspeed: 0.0141s/iter; left time: 520.9231s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0708349\n",
      "\tspeed: 0.0141s/iter; left time: 519.4051s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1131931\n",
      "\tspeed: 0.0141s/iter; left time: 517.9471s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1167719\n",
      "\tspeed: 0.0141s/iter; left time: 516.6749s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0416055\n",
      "\tspeed: 0.0141s/iter; left time: 514.8447s\n",
      "Epoch: 2 cost time: 68.40383195877075\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1498640 Vali Loss: 0.0332389 Test Loss: 0.1158751\n",
      "Validation loss decreased (0.036618 --> 0.033239).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0921043\n",
      "\tspeed: 0.1259s/iter; left time: 4575.9129s\n",
      "\titers: 200, epoch: 3 | loss: 0.0495223\n",
      "\tspeed: 0.0141s/iter; left time: 512.6253s\n",
      "\titers: 300, epoch: 3 | loss: 0.1037580\n",
      "\tspeed: 0.0141s/iter; left time: 510.3547s\n",
      "\titers: 400, epoch: 3 | loss: 0.1069294\n",
      "\tspeed: 0.0141s/iter; left time: 508.6519s\n",
      "\titers: 500, epoch: 3 | loss: 0.0908319\n",
      "\tspeed: 0.0141s/iter; left time: 506.9812s\n",
      "\titers: 600, epoch: 3 | loss: 0.1015741\n",
      "\tspeed: 0.0141s/iter; left time: 506.8114s\n",
      "\titers: 700, epoch: 3 | loss: 0.2745125\n",
      "\tspeed: 0.0141s/iter; left time: 504.4194s\n",
      "\titers: 800, epoch: 3 | loss: 0.1912892\n",
      "\tspeed: 0.0141s/iter; left time: 503.2255s\n",
      "\titers: 900, epoch: 3 | loss: 0.0592797\n",
      "\tspeed: 0.0141s/iter; left time: 501.7450s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1293277\n",
      "\tspeed: 0.0141s/iter; left time: 500.1079s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1663854\n",
      "\tspeed: 0.0141s/iter; left time: 497.7186s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1829112\n",
      "\tspeed: 0.0140s/iter; left time: 494.5828s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2084670\n",
      "\tspeed: 0.0140s/iter; left time: 493.1661s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1837889\n",
      "\tspeed: 0.0140s/iter; left time: 491.6898s\n",
      "\titers: 1500, epoch: 3 | loss: 0.1685538\n",
      "\tspeed: 0.0140s/iter; left time: 489.6979s\n",
      "\titers: 1600, epoch: 3 | loss: 0.1170664\n",
      "\tspeed: 0.0140s/iter; left time: 488.5827s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5605399\n",
      "\tspeed: 0.0141s/iter; left time: 488.3607s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0983172\n",
      "\tspeed: 0.0141s/iter; left time: 488.7674s\n",
      "\titers: 1900, epoch: 3 | loss: 0.1804187\n",
      "\tspeed: 0.0141s/iter; left time: 487.2180s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2196393\n",
      "\tspeed: 0.0141s/iter; left time: 485.8903s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0649662\n",
      "\tspeed: 0.0141s/iter; left time: 484.3086s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0618446\n",
      "\tspeed: 0.0141s/iter; left time: 482.8864s\n",
      "\titers: 2300, epoch: 3 | loss: 0.0881606\n",
      "\tspeed: 0.0141s/iter; left time: 481.8380s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1313312\n",
      "\tspeed: 0.0141s/iter; left time: 479.3860s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1434908\n",
      "\tspeed: 0.0140s/iter; left time: 475.5891s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0808980\n",
      "\tspeed: 0.0141s/iter; left time: 477.1764s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0771104\n",
      "\tspeed: 0.0141s/iter; left time: 475.9274s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1068674\n",
      "\tspeed: 0.0141s/iter; left time: 474.2509s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0686024\n",
      "\tspeed: 0.0141s/iter; left time: 473.0073s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0422940\n",
      "\tspeed: 0.0141s/iter; left time: 473.1453s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1321422\n",
      "\tspeed: 0.0142s/iter; left time: 471.8169s\n",
      "\titers: 3200, epoch: 3 | loss: 0.5330641\n",
      "\tspeed: 0.0141s/iter; left time: 470.1924s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0818232\n",
      "\tspeed: 0.0142s/iter; left time: 469.1092s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2312705\n",
      "\tspeed: 0.0142s/iter; left time: 467.5479s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0268960\n",
      "\tspeed: 0.0142s/iter; left time: 466.2254s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1508718\n",
      "\tspeed: 0.0141s/iter; left time: 464.6272s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1124697\n",
      "\tspeed: 0.0141s/iter; left time: 460.4672s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0642455\n",
      "\tspeed: 0.0140s/iter; left time: 457.7217s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1552861\n",
      "\tspeed: 0.0140s/iter; left time: 456.2233s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1838108\n",
      "\tspeed: 0.0140s/iter; left time: 454.7496s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2475916\n",
      "\tspeed: 0.0140s/iter; left time: 453.4409s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0642882\n",
      "\tspeed: 0.0140s/iter; left time: 452.0822s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0272382\n",
      "\tspeed: 0.0140s/iter; left time: 450.6278s\n",
      "\titers: 4400, epoch: 3 | loss: 0.0972582\n",
      "\tspeed: 0.0140s/iter; left time: 449.1768s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0271024\n",
      "\tspeed: 0.0140s/iter; left time: 447.5265s\n",
      "Epoch: 3 cost time: 64.41920208930969\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1260118 Vali Loss: 0.0335974 Test Loss: 0.1174524\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0620232\n",
      "\tspeed: 0.1264s/iter; left time: 4016.4393s\n",
      "\titers: 200, epoch: 4 | loss: 0.3000562\n",
      "\tspeed: 0.0141s/iter; left time: 446.2337s\n",
      "\titers: 300, epoch: 4 | loss: 0.0247883\n",
      "\tspeed: 0.0141s/iter; left time: 444.7132s\n",
      "\titers: 400, epoch: 4 | loss: 0.1424976\n",
      "\tspeed: 0.0141s/iter; left time: 443.6386s\n",
      "\titers: 500, epoch: 4 | loss: 0.1041294\n",
      "\tspeed: 0.0141s/iter; left time: 441.7476s\n",
      "\titers: 600, epoch: 4 | loss: 0.0927237\n",
      "\tspeed: 0.0141s/iter; left time: 440.3097s\n",
      "\titers: 700, epoch: 4 | loss: 0.1643550\n",
      "\tspeed: 0.0141s/iter; left time: 438.8456s\n",
      "\titers: 800, epoch: 4 | loss: 0.1704982\n",
      "\tspeed: 0.0141s/iter; left time: 437.3548s\n",
      "\titers: 900, epoch: 4 | loss: 0.1017573\n",
      "\tspeed: 0.0141s/iter; left time: 435.9851s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1325587\n",
      "\tspeed: 0.0141s/iter; left time: 435.6261s\n",
      "\titers: 1100, epoch: 4 | loss: 0.1194164\n",
      "\tspeed: 0.0141s/iter; left time: 433.4766s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0363478\n",
      "\tspeed: 0.0141s/iter; left time: 432.3662s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1089492\n",
      "\tspeed: 0.0141s/iter; left time: 431.0275s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0611618\n",
      "\tspeed: 0.0141s/iter; left time: 429.6197s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1956565\n",
      "\tspeed: 0.0141s/iter; left time: 429.0720s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0775029\n",
      "\tspeed: 0.0141s/iter; left time: 427.1726s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1481359\n",
      "\tspeed: 0.0141s/iter; left time: 425.8122s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0268040\n",
      "\tspeed: 0.0141s/iter; left time: 424.0555s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0733469\n",
      "\tspeed: 0.0141s/iter; left time: 422.9536s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1289952\n",
      "\tspeed: 0.0141s/iter; left time: 421.9476s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0149420\n",
      "\tspeed: 0.0141s/iter; left time: 419.9750s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1817113\n",
      "\tspeed: 0.0141s/iter; left time: 418.3225s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1636705\n",
      "\tspeed: 0.0141s/iter; left time: 417.2048s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1268176\n",
      "\tspeed: 0.0141s/iter; left time: 415.8042s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0602797\n",
      "\tspeed: 0.0141s/iter; left time: 414.4641s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0789638\n",
      "\tspeed: 0.0141s/iter; left time: 412.8023s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5747844\n",
      "\tspeed: 0.0141s/iter; left time: 411.6667s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0215517\n",
      "\tspeed: 0.0141s/iter; left time: 410.0062s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0326352\n",
      "\tspeed: 0.0141s/iter; left time: 408.1166s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0938358\n",
      "\tspeed: 0.0141s/iter; left time: 406.1707s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0862608\n",
      "\tspeed: 0.0141s/iter; left time: 404.8438s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1242540\n",
      "\tspeed: 0.0141s/iter; left time: 403.6519s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0811059\n",
      "\tspeed: 0.0141s/iter; left time: 402.1252s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0627541\n",
      "\tspeed: 0.0141s/iter; left time: 400.8693s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0406029\n",
      "\tspeed: 0.0141s/iter; left time: 399.2916s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0528897\n",
      "\tspeed: 0.0141s/iter; left time: 397.9787s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1798819\n",
      "\tspeed: 0.0141s/iter; left time: 396.5258s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0438282\n",
      "\tspeed: 0.0141s/iter; left time: 395.1174s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0805513\n",
      "\tspeed: 0.0141s/iter; left time: 393.5130s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0745673\n",
      "\tspeed: 0.0141s/iter; left time: 392.0885s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0260085\n",
      "\tspeed: 0.0141s/iter; left time: 390.6254s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0792169\n",
      "\tspeed: 0.0141s/iter; left time: 389.1387s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2562513\n",
      "\tspeed: 0.0141s/iter; left time: 387.7769s\n",
      "\titers: 4400, epoch: 4 | loss: 0.1107487\n",
      "\tspeed: 0.0141s/iter; left time: 386.4722s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0939064\n",
      "\tspeed: 0.0141s/iter; left time: 385.2825s\n",
      "Epoch: 4 cost time: 64.38318300247192\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.1097237 Vali Loss: 0.0322048 Test Loss: 0.1189298\n",
      "Validation loss decreased (0.033239 --> 0.032205).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0511175\n",
      "\tspeed: 0.1266s/iter; left time: 3446.2649s\n",
      "\titers: 200, epoch: 5 | loss: 0.0824356\n",
      "\tspeed: 0.0141s/iter; left time: 383.0184s\n",
      "\titers: 300, epoch: 5 | loss: 0.0816591\n",
      "\tspeed: 0.0141s/iter; left time: 380.5330s\n",
      "\titers: 400, epoch: 5 | loss: 0.0379792\n",
      "\tspeed: 0.0141s/iter; left time: 380.6111s\n",
      "\titers: 500, epoch: 5 | loss: 0.1198637\n",
      "\tspeed: 0.0141s/iter; left time: 378.2182s\n",
      "\titers: 600, epoch: 5 | loss: 0.0216863\n",
      "\tspeed: 0.0141s/iter; left time: 376.8513s\n",
      "\titers: 700, epoch: 5 | loss: 0.0386350\n",
      "\tspeed: 0.0141s/iter; left time: 375.4396s\n",
      "\titers: 800, epoch: 5 | loss: 0.1108105\n",
      "\tspeed: 0.0141s/iter; left time: 374.0411s\n",
      "\titers: 900, epoch: 5 | loss: 0.0761029\n",
      "\tspeed: 0.0141s/iter; left time: 372.3674s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1424043\n",
      "\tspeed: 0.0141s/iter; left time: 370.7767s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0271523\n",
      "\tspeed: 0.0141s/iter; left time: 369.3218s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1031458\n",
      "\tspeed: 0.0141s/iter; left time: 367.8267s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0975296\n",
      "\tspeed: 0.0141s/iter; left time: 366.4378s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1017932\n",
      "\tspeed: 0.0141s/iter; left time: 365.2732s\n",
      "\titers: 1500, epoch: 5 | loss: 0.1035345\n",
      "\tspeed: 0.0141s/iter; left time: 364.1223s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0510209\n",
      "\tspeed: 0.0141s/iter; left time: 362.5528s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1795432\n",
      "\tspeed: 0.0141s/iter; left time: 361.3627s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1040807\n",
      "\tspeed: 0.0141s/iter; left time: 359.9614s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0265983\n",
      "\tspeed: 0.0141s/iter; left time: 358.3511s\n",
      "\titers: 2000, epoch: 5 | loss: 0.5327029\n",
      "\tspeed: 0.0141s/iter; left time: 357.0260s\n",
      "\titers: 2100, epoch: 5 | loss: 0.1212099\n",
      "\tspeed: 0.0141s/iter; left time: 355.6111s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0577577\n",
      "\tspeed: 0.0141s/iter; left time: 354.0042s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0553769\n",
      "\tspeed: 0.0143s/iter; left time: 358.4574s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0770302\n",
      "\tspeed: 0.0156s/iter; left time: 389.7979s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0728535\n",
      "\tspeed: 0.0143s/iter; left time: 355.7696s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0644325\n",
      "\tspeed: 0.0141s/iter; left time: 349.1558s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1021650\n",
      "\tspeed: 0.0141s/iter; left time: 347.7855s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0561808\n",
      "\tspeed: 0.0141s/iter; left time: 346.3896s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0417612\n",
      "\tspeed: 0.0141s/iter; left time: 345.0803s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0566125\n",
      "\tspeed: 0.0141s/iter; left time: 342.7468s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0665086\n",
      "\tspeed: 0.0141s/iter; left time: 341.6258s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0187587\n",
      "\tspeed: 0.0141s/iter; left time: 340.3337s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0994665\n",
      "\tspeed: 0.0141s/iter; left time: 339.0288s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0587251\n",
      "\tspeed: 0.0141s/iter; left time: 337.6668s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0475226\n",
      "\tspeed: 0.0141s/iter; left time: 336.2992s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0861813\n",
      "\tspeed: 0.0141s/iter; left time: 334.2023s\n",
      "\titers: 3700, epoch: 5 | loss: 0.1473933\n",
      "\tspeed: 0.0141s/iter; left time: 332.8896s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1232944\n",
      "\tspeed: 0.0141s/iter; left time: 331.9254s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0500140\n",
      "\tspeed: 0.0141s/iter; left time: 330.6640s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0497998\n",
      "\tspeed: 0.0141s/iter; left time: 329.0939s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0362230\n",
      "\tspeed: 0.0141s/iter; left time: 327.8279s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0443827\n",
      "\tspeed: 0.0141s/iter; left time: 326.2613s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1243613\n",
      "\tspeed: 0.0141s/iter; left time: 325.0778s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0138859\n",
      "\tspeed: 0.0141s/iter; left time: 323.3531s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0650274\n",
      "\tspeed: 0.0141s/iter; left time: 322.0991s\n",
      "Epoch: 5 cost time: 64.81184458732605\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0991675 Vali Loss: 0.0314292 Test Loss: 0.1178709\n",
      "Validation loss decreased (0.032205 --> 0.031429).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0407981\n",
      "\tspeed: 0.1327s/iter; left time: 3009.3118s\n",
      "\titers: 200, epoch: 6 | loss: 0.1560671\n",
      "\tspeed: 0.0141s/iter; left time: 318.8011s\n",
      "\titers: 300, epoch: 6 | loss: 0.0980745\n",
      "\tspeed: 0.0141s/iter; left time: 317.4076s\n",
      "\titers: 400, epoch: 6 | loss: 0.0869638\n",
      "\tspeed: 0.0141s/iter; left time: 315.7521s\n",
      "\titers: 500, epoch: 6 | loss: 0.1416071\n",
      "\tspeed: 0.0141s/iter; left time: 314.1592s\n",
      "\titers: 600, epoch: 6 | loss: 0.1653059\n",
      "\tspeed: 0.0141s/iter; left time: 313.5029s\n",
      "\titers: 700, epoch: 6 | loss: 0.0402924\n",
      "\tspeed: 0.0142s/iter; left time: 312.6559s\n",
      "\titers: 800, epoch: 6 | loss: 0.0650913\n",
      "\tspeed: 0.0142s/iter; left time: 311.6753s\n",
      "\titers: 900, epoch: 6 | loss: 0.0528424\n",
      "\tspeed: 0.0142s/iter; left time: 310.1773s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1621610\n",
      "\tspeed: 0.0141s/iter; left time: 307.1819s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0452283\n",
      "\tspeed: 0.0141s/iter; left time: 305.2809s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0655639\n",
      "\tspeed: 0.0142s/iter; left time: 307.3185s\n",
      "\titers: 1300, epoch: 6 | loss: 0.0973740\n",
      "\tspeed: 0.0157s/iter; left time: 336.3581s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0602396\n",
      "\tspeed: 0.0157s/iter; left time: 334.9308s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0754681\n",
      "\tspeed: 0.0153s/iter; left time: 324.8451s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1505053\n",
      "\tspeed: 0.0141s/iter; left time: 298.3687s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0523999\n",
      "\tspeed: 0.0141s/iter; left time: 297.2803s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1966765\n",
      "\tspeed: 0.0141s/iter; left time: 295.8927s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1093751\n",
      "\tspeed: 0.0141s/iter; left time: 294.2237s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1852056\n",
      "\tspeed: 0.0141s/iter; left time: 292.7904s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1162586\n",
      "\tspeed: 0.0141s/iter; left time: 291.1885s\n",
      "\titers: 2200, epoch: 6 | loss: 0.0937619\n",
      "\tspeed: 0.0141s/iter; left time: 290.1084s\n",
      "\titers: 2300, epoch: 6 | loss: 0.0962201\n",
      "\tspeed: 0.0141s/iter; left time: 289.3398s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0875419\n",
      "\tspeed: 0.0141s/iter; left time: 287.6584s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0579049\n",
      "\tspeed: 0.0141s/iter; left time: 286.5363s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0611153\n",
      "\tspeed: 0.0141s/iter; left time: 285.0500s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0546855\n",
      "\tspeed: 0.0141s/iter; left time: 283.4277s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1478503\n",
      "\tspeed: 0.0141s/iter; left time: 282.0694s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2514475\n",
      "\tspeed: 0.0141s/iter; left time: 280.6846s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0591451\n",
      "\tspeed: 0.0141s/iter; left time: 279.1327s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0910441\n",
      "\tspeed: 0.0141s/iter; left time: 277.6401s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0814859\n",
      "\tspeed: 0.0141s/iter; left time: 276.2701s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0750403\n",
      "\tspeed: 0.0141s/iter; left time: 274.4446s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0157550\n",
      "\tspeed: 0.0141s/iter; left time: 273.1151s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0146248\n",
      "\tspeed: 0.0141s/iter; left time: 271.7745s\n",
      "\titers: 3600, epoch: 6 | loss: 0.1076531\n",
      "\tspeed: 0.0141s/iter; left time: 270.3560s\n",
      "\titers: 3700, epoch: 6 | loss: 0.1103698\n",
      "\tspeed: 0.0141s/iter; left time: 268.8903s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1906330\n",
      "\tspeed: 0.0141s/iter; left time: 267.4657s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0485306\n",
      "\tspeed: 0.0149s/iter; left time: 280.7376s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1356897\n",
      "\tspeed: 0.0157s/iter; left time: 294.4359s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0205082\n",
      "\tspeed: 0.0157s/iter; left time: 292.9226s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0322725\n",
      "\tspeed: 0.0156s/iter; left time: 290.3328s\n",
      "\titers: 4300, epoch: 6 | loss: 0.2247370\n",
      "\tspeed: 0.0156s/iter; left time: 288.5179s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0607811\n",
      "\tspeed: 0.0156s/iter; left time: 287.0391s\n",
      "\titers: 4500, epoch: 6 | loss: 0.1049364\n",
      "\tspeed: 0.0156s/iter; left time: 285.8006s\n",
      "Epoch: 6 cost time: 66.05566453933716\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0951600 Vali Loss: 0.0313815 Test Loss: 0.1188576\n",
      "Validation loss decreased (0.031429 --> 0.031381).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0899974\n",
      "\tspeed: 0.1401s/iter; left time: 2538.6402s\n",
      "\titers: 200, epoch: 7 | loss: 0.1920923\n",
      "\tspeed: 0.0142s/iter; left time: 255.0647s\n",
      "\titers: 300, epoch: 7 | loss: 0.1995395\n",
      "\tspeed: 0.0141s/iter; left time: 253.5040s\n",
      "\titers: 400, epoch: 7 | loss: 0.0535899\n",
      "\tspeed: 0.0141s/iter; left time: 251.1105s\n",
      "\titers: 500, epoch: 7 | loss: 0.0392181\n",
      "\tspeed: 0.0141s/iter; left time: 249.5094s\n",
      "\titers: 600, epoch: 7 | loss: 0.0690335\n",
      "\tspeed: 0.0141s/iter; left time: 248.2034s\n",
      "\titers: 700, epoch: 7 | loss: 0.0167827\n",
      "\tspeed: 0.0141s/iter; left time: 246.7453s\n",
      "\titers: 800, epoch: 7 | loss: 0.0497221\n",
      "\tspeed: 0.0141s/iter; left time: 245.3451s\n",
      "\titers: 900, epoch: 7 | loss: 0.1747548\n",
      "\tspeed: 0.0141s/iter; left time: 243.9234s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0333760\n",
      "\tspeed: 0.0141s/iter; left time: 242.4560s\n",
      "\titers: 1100, epoch: 7 | loss: 0.1891702\n",
      "\tspeed: 0.0141s/iter; left time: 241.0596s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0678601\n",
      "\tspeed: 0.0141s/iter; left time: 239.7604s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0906590\n",
      "\tspeed: 0.0141s/iter; left time: 238.2282s\n",
      "\titers: 1400, epoch: 7 | loss: 0.1115071\n",
      "\tspeed: 0.0141s/iter; left time: 236.8197s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0987810\n",
      "\tspeed: 0.0141s/iter; left time: 235.3967s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0614092\n",
      "\tspeed: 0.0141s/iter; left time: 233.9560s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0387152\n",
      "\tspeed: 0.0141s/iter; left time: 232.5670s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0621990\n",
      "\tspeed: 0.0141s/iter; left time: 231.1403s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0314658\n",
      "\tspeed: 0.0141s/iter; left time: 229.7297s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0657985\n",
      "\tspeed: 0.0141s/iter; left time: 228.3748s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0355708\n",
      "\tspeed: 0.0141s/iter; left time: 226.8700s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0501027\n",
      "\tspeed: 0.0141s/iter; left time: 225.4774s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0398276\n",
      "\tspeed: 0.0141s/iter; left time: 224.0442s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0946381\n",
      "\tspeed: 0.0141s/iter; left time: 222.7510s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1778894\n",
      "\tspeed: 0.0141s/iter; left time: 221.2758s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0995625\n",
      "\tspeed: 0.0141s/iter; left time: 219.8227s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0328965\n",
      "\tspeed: 0.0141s/iter; left time: 218.3930s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0441500\n",
      "\tspeed: 0.0141s/iter; left time: 217.0913s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0664506\n",
      "\tspeed: 0.0141s/iter; left time: 216.1208s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0610168\n",
      "\tspeed: 0.0141s/iter; left time: 214.7998s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0203272\n",
      "\tspeed: 0.0141s/iter; left time: 213.4333s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1897657\n",
      "\tspeed: 0.0141s/iter; left time: 211.9312s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0658893\n",
      "\tspeed: 0.0141s/iter; left time: 210.4613s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0352994\n",
      "\tspeed: 0.0141s/iter; left time: 209.0927s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0587722\n",
      "\tspeed: 0.0141s/iter; left time: 207.6450s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0640702\n",
      "\tspeed: 0.0141s/iter; left time: 206.2810s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1567446\n",
      "\tspeed: 0.0141s/iter; left time: 204.7510s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0392753\n",
      "\tspeed: 0.0141s/iter; left time: 203.3880s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0518922\n",
      "\tspeed: 0.0141s/iter; left time: 201.9749s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0733735\n",
      "\tspeed: 0.0141s/iter; left time: 200.5330s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0798267\n",
      "\tspeed: 0.0141s/iter; left time: 199.1955s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0219942\n",
      "\tspeed: 0.0141s/iter; left time: 197.6512s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0154098\n",
      "\tspeed: 0.0141s/iter; left time: 196.3353s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0287793\n",
      "\tspeed: 0.0141s/iter; left time: 194.8425s\n",
      "\titers: 4500, epoch: 7 | loss: 0.0825468\n",
      "\tspeed: 0.0141s/iter; left time: 193.4764s\n",
      "Epoch: 7 cost time: 64.44589757919312\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0915994 Vali Loss: 0.0315710 Test Loss: 0.1217522\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0675834\n",
      "\tspeed: 0.1280s/iter; left time: 1736.5864s\n",
      "\titers: 200, epoch: 8 | loss: 0.0473535\n",
      "\tspeed: 0.0142s/iter; left time: 191.4636s\n",
      "\titers: 300, epoch: 8 | loss: 0.1527453\n",
      "\tspeed: 0.0142s/iter; left time: 189.5932s\n",
      "\titers: 400, epoch: 8 | loss: 0.0433375\n",
      "\tspeed: 0.0142s/iter; left time: 188.0642s\n",
      "\titers: 500, epoch: 8 | loss: 0.2083296\n",
      "\tspeed: 0.0142s/iter; left time: 186.3801s\n",
      "\titers: 600, epoch: 8 | loss: 0.0833606\n",
      "\tspeed: 0.0142s/iter; left time: 185.3253s\n",
      "\titers: 700, epoch: 8 | loss: 0.2122833\n",
      "\tspeed: 0.0142s/iter; left time: 183.6862s\n",
      "\titers: 800, epoch: 8 | loss: 0.1810623\n",
      "\tspeed: 0.0142s/iter; left time: 182.2248s\n",
      "\titers: 900, epoch: 8 | loss: 0.0652844\n",
      "\tspeed: 0.0141s/iter; left time: 180.5815s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0822306\n",
      "\tspeed: 0.0142s/iter; left time: 179.4172s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0856094\n",
      "\tspeed: 0.0142s/iter; left time: 177.8687s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1376628\n",
      "\tspeed: 0.0142s/iter; left time: 176.5323s\n",
      "\titers: 1300, epoch: 8 | loss: 0.4314265\n",
      "\tspeed: 0.0141s/iter; left time: 174.9420s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0423010\n",
      "\tspeed: 0.0142s/iter; left time: 173.6337s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0771872\n",
      "\tspeed: 0.0142s/iter; left time: 172.2065s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1299950\n",
      "\tspeed: 0.0141s/iter; left time: 170.7008s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0232008\n",
      "\tspeed: 0.0141s/iter; left time: 169.2483s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1455651\n",
      "\tspeed: 0.0141s/iter; left time: 167.8668s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0852992\n",
      "\tspeed: 0.0142s/iter; left time: 166.5312s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1369131\n",
      "\tspeed: 0.0142s/iter; left time: 165.5110s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0387096\n",
      "\tspeed: 0.0142s/iter; left time: 163.9634s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0600151\n",
      "\tspeed: 0.0142s/iter; left time: 162.5061s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0871226\n",
      "\tspeed: 0.0141s/iter; left time: 160.8206s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0632140\n",
      "\tspeed: 0.0141s/iter; left time: 159.3429s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0622291\n",
      "\tspeed: 0.0141s/iter; left time: 157.8545s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0504508\n",
      "\tspeed: 0.0141s/iter; left time: 156.4966s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0385258\n",
      "\tspeed: 0.0141s/iter; left time: 155.0243s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0579328\n",
      "\tspeed: 0.0141s/iter; left time: 153.7299s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0965812\n",
      "\tspeed: 0.0142s/iter; left time: 152.3628s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2822627\n",
      "\tspeed: 0.0142s/iter; left time: 150.9531s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1098126\n",
      "\tspeed: 0.0142s/iter; left time: 149.5210s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1279855\n",
      "\tspeed: 0.0142s/iter; left time: 148.1149s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0380477\n",
      "\tspeed: 0.0142s/iter; left time: 146.6980s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0477200\n",
      "\tspeed: 0.0141s/iter; left time: 145.2396s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1534920\n",
      "\tspeed: 0.0141s/iter; left time: 143.8207s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0332019\n",
      "\tspeed: 0.0141s/iter; left time: 142.4196s\n",
      "\titers: 3700, epoch: 8 | loss: 0.1836825\n",
      "\tspeed: 0.0142s/iter; left time: 141.0487s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0701586\n",
      "\tspeed: 0.0141s/iter; left time: 139.5841s\n",
      "\titers: 3900, epoch: 8 | loss: 0.0616738\n",
      "\tspeed: 0.0142s/iter; left time: 138.2248s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0393574\n",
      "\tspeed: 0.0141s/iter; left time: 136.7274s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0271815\n",
      "\tspeed: 0.0142s/iter; left time: 135.3755s\n",
      "\titers: 4200, epoch: 8 | loss: 0.1156763\n",
      "\tspeed: 0.0141s/iter; left time: 133.9399s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0848880\n",
      "\tspeed: 0.0142s/iter; left time: 132.5568s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1043762\n",
      "\tspeed: 0.0141s/iter; left time: 131.0583s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1535060\n",
      "\tspeed: 0.0141s/iter; left time: 129.6620s\n",
      "Epoch: 8 cost time: 64.75236964225769\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0919921 Vali Loss: 0.0316366 Test Loss: 0.1207948\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0453466\n",
      "\tspeed: 0.1268s/iter; left time: 1142.2824s\n",
      "\titers: 200, epoch: 9 | loss: 0.1043461\n",
      "\tspeed: 0.0142s/iter; left time: 126.8866s\n",
      "\titers: 300, epoch: 9 | loss: 0.0585344\n",
      "\tspeed: 0.0143s/iter; left time: 125.6337s\n",
      "\titers: 400, epoch: 9 | loss: 0.1066550\n",
      "\tspeed: 0.0142s/iter; left time: 124.0136s\n",
      "\titers: 500, epoch: 9 | loss: 0.0683589\n",
      "\tspeed: 0.0143s/iter; left time: 122.7493s\n",
      "\titers: 600, epoch: 9 | loss: 0.0377208\n",
      "\tspeed: 0.0142s/iter; left time: 121.1806s\n",
      "\titers: 700, epoch: 9 | loss: 0.1135117\n",
      "\tspeed: 0.0141s/iter; left time: 118.9333s\n",
      "\titers: 800, epoch: 9 | loss: 0.0436504\n",
      "\tspeed: 0.0141s/iter; left time: 117.0963s\n",
      "\titers: 900, epoch: 9 | loss: 0.0389743\n",
      "\tspeed: 0.0141s/iter; left time: 115.5612s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0546781\n",
      "\tspeed: 0.0141s/iter; left time: 114.1790s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0513492\n",
      "\tspeed: 0.0141s/iter; left time: 112.7211s\n",
      "\titers: 1200, epoch: 9 | loss: 0.2055720\n",
      "\tspeed: 0.0141s/iter; left time: 111.2858s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0422602\n",
      "\tspeed: 0.0141s/iter; left time: 109.8485s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2414146\n",
      "\tspeed: 0.0141s/iter; left time: 108.4401s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0982368\n",
      "\tspeed: 0.0141s/iter; left time: 107.0333s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0592840\n",
      "\tspeed: 0.0141s/iter; left time: 105.6407s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0897854\n",
      "\tspeed: 0.0141s/iter; left time: 104.3072s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0311423\n",
      "\tspeed: 0.0141s/iter; left time: 102.8643s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0158821\n",
      "\tspeed: 0.0141s/iter; left time: 101.4031s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0632520\n",
      "\tspeed: 0.0141s/iter; left time: 99.9971s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0480218\n",
      "\tspeed: 0.0141s/iter; left time: 98.5852s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1162155\n",
      "\tspeed: 0.0141s/iter; left time: 97.2357s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1031268\n",
      "\tspeed: 0.0141s/iter; left time: 95.8204s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0258792\n",
      "\tspeed: 0.0141s/iter; left time: 94.3845s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0426270\n",
      "\tspeed: 0.0141s/iter; left time: 93.0077s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0474456\n",
      "\tspeed: 0.0141s/iter; left time: 91.5602s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0415879\n",
      "\tspeed: 0.0141s/iter; left time: 90.1809s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1598186\n",
      "\tspeed: 0.0141s/iter; left time: 88.7802s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0500710\n",
      "\tspeed: 0.0141s/iter; left time: 87.3526s\n",
      "\titers: 3000, epoch: 9 | loss: 0.1134801\n",
      "\tspeed: 0.0141s/iter; left time: 85.9693s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0335500\n",
      "\tspeed: 0.0141s/iter; left time: 84.5452s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0967030\n",
      "\tspeed: 0.0141s/iter; left time: 83.1135s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2423144\n",
      "\tspeed: 0.0141s/iter; left time: 81.7057s\n",
      "\titers: 3400, epoch: 9 | loss: 0.0968392\n",
      "\tspeed: 0.0141s/iter; left time: 80.2896s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0743165\n",
      "\tspeed: 0.0141s/iter; left time: 78.9214s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0543378\n",
      "\tspeed: 0.0141s/iter; left time: 77.5070s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0615568\n",
      "\tspeed: 0.0141s/iter; left time: 76.1048s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1607510\n",
      "\tspeed: 0.0141s/iter; left time: 74.7011s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0514239\n",
      "\tspeed: 0.0141s/iter; left time: 73.2621s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0867243\n",
      "\tspeed: 0.0141s/iter; left time: 71.8719s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0424867\n",
      "\tspeed: 0.0142s/iter; left time: 71.0484s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0735557\n",
      "\tspeed: 0.0141s/iter; left time: 69.1514s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0521968\n",
      "\tspeed: 0.0141s/iter; left time: 67.6647s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0280906\n",
      "\tspeed: 0.0141s/iter; left time: 66.2513s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0378813\n",
      "\tspeed: 0.0141s/iter; left time: 64.8440s\n",
      "Epoch: 9 cost time: 64.45616412162781\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0903486 Vali Loss: 0.0315779 Test Loss: 0.1199241\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0711938\n",
      "\tspeed: 0.1323s/iter; left time: 589.7170s\n",
      "\titers: 200, epoch: 10 | loss: 0.0510797\n",
      "\tspeed: 0.0157s/iter; left time: 68.3359s\n",
      "\titers: 300, epoch: 10 | loss: 0.0313088\n",
      "\tspeed: 0.0157s/iter; left time: 66.7492s\n",
      "\titers: 400, epoch: 10 | loss: 0.0607325\n",
      "\tspeed: 0.0157s/iter; left time: 65.1517s\n",
      "\titers: 500, epoch: 10 | loss: 0.0953347\n",
      "\tspeed: 0.0157s/iter; left time: 63.5522s\n",
      "\titers: 600, epoch: 10 | loss: 0.0838850\n",
      "\tspeed: 0.0155s/iter; left time: 61.2745s\n",
      "\titers: 700, epoch: 10 | loss: 0.0498266\n",
      "\tspeed: 0.0142s/iter; left time: 54.6912s\n",
      "\titers: 800, epoch: 10 | loss: 0.1786128\n",
      "\tspeed: 0.0142s/iter; left time: 53.3998s\n",
      "\titers: 900, epoch: 10 | loss: 0.1045990\n",
      "\tspeed: 0.0142s/iter; left time: 51.9151s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0458117\n",
      "\tspeed: 0.0142s/iter; left time: 50.4327s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0710754\n",
      "\tspeed: 0.0142s/iter; left time: 49.0000s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0375458\n",
      "\tspeed: 0.0142s/iter; left time: 47.7033s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0486443\n",
      "\tspeed: 0.0142s/iter; left time: 46.2356s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0675847\n",
      "\tspeed: 0.0142s/iter; left time: 44.8040s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2344916\n",
      "\tspeed: 0.0142s/iter; left time: 43.4451s\n",
      "\titers: 1600, epoch: 10 | loss: 0.1040399\n",
      "\tspeed: 0.0142s/iter; left time: 42.0280s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0180340\n",
      "\tspeed: 0.0142s/iter; left time: 40.5824s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0941200\n",
      "\tspeed: 0.0142s/iter; left time: 39.1423s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1134355\n",
      "\tspeed: 0.0142s/iter; left time: 37.7704s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0746490\n",
      "\tspeed: 0.0142s/iter; left time: 36.2913s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0568054\n",
      "\tspeed: 0.0142s/iter; left time: 34.8824s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0712864\n",
      "\tspeed: 0.0142s/iter; left time: 33.4391s\n",
      "\titers: 2300, epoch: 10 | loss: 0.1450652\n",
      "\tspeed: 0.0142s/iter; left time: 32.0090s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0562723\n",
      "\tspeed: 0.0142s/iter; left time: 30.6105s\n",
      "\titers: 2500, epoch: 10 | loss: 0.1768278\n",
      "\tspeed: 0.0142s/iter; left time: 29.2095s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1337380\n",
      "\tspeed: 0.0142s/iter; left time: 27.7765s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0858787\n",
      "\tspeed: 0.0142s/iter; left time: 26.3492s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0780136\n",
      "\tspeed: 0.0142s/iter; left time: 24.9445s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0651919\n",
      "\tspeed: 0.0156s/iter; left time: 25.7988s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0662182\n",
      "\tspeed: 0.0157s/iter; left time: 24.4067s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0420532\n",
      "\tspeed: 0.0157s/iter; left time: 22.8262s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0765953\n",
      "\tspeed: 0.0157s/iter; left time: 21.2567s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0796681\n",
      "\tspeed: 0.0157s/iter; left time: 19.6853s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1527232\n",
      "\tspeed: 0.0157s/iter; left time: 18.1191s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0269523\n",
      "\tspeed: 0.0157s/iter; left time: 16.5587s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1338859\n",
      "\tspeed: 0.0157s/iter; left time: 14.9828s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0781323\n",
      "\tspeed: 0.0157s/iter; left time: 13.4203s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0953985\n",
      "\tspeed: 0.0157s/iter; left time: 11.8470s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0235923\n",
      "\tspeed: 0.0157s/iter; left time: 10.2774s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0528318\n",
      "\tspeed: 0.0156s/iter; left time: 8.6856s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1451953\n",
      "\tspeed: 0.0142s/iter; left time: 6.4757s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0429889\n",
      "\tspeed: 0.0142s/iter; left time: 5.0558s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0443051\n",
      "\tspeed: 0.0142s/iter; left time: 3.6346s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0558498\n",
      "\tspeed: 0.0142s/iter; left time: 2.2148s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1473211\n",
      "\tspeed: 0.0142s/iter; left time: 0.7951s\n",
      "Epoch: 10 cost time: 67.56381893157959\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0897487 Vali Loss: 0.0315946 Test Loss: 0.1203875\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-07\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11888609081506729, mae:0.20345990359783173\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor solar_penetration --enc_in 2 --dec_in 2 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96a5bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_penetration    \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             2                   Dec In:             2                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           5                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18219\n",
      "val 2608\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1563253\n",
      "\tspeed: 0.0274s/iter; left time: 1246.3370s\n",
      "\titers: 200, epoch: 1 | loss: 0.0972581\n",
      "\tspeed: 0.0150s/iter; left time: 682.3760s\n",
      "\titers: 300, epoch: 1 | loss: 0.0647980\n",
      "\tspeed: 0.0141s/iter; left time: 636.5070s\n",
      "\titers: 400, epoch: 1 | loss: 0.1431301\n",
      "\tspeed: 0.0141s/iter; left time: 635.4475s\n",
      "\titers: 500, epoch: 1 | loss: 0.0373491\n",
      "\tspeed: 0.0142s/iter; left time: 637.5756s\n",
      "\titers: 600, epoch: 1 | loss: 0.0956427\n",
      "\tspeed: 0.0150s/iter; left time: 676.4517s\n",
      "\titers: 700, epoch: 1 | loss: 0.1355441\n",
      "\tspeed: 0.0140s/iter; left time: 629.5800s\n",
      "\titers: 800, epoch: 1 | loss: 0.6021499\n",
      "\tspeed: 0.0141s/iter; left time: 628.9057s\n",
      "\titers: 900, epoch: 1 | loss: 0.2410780\n",
      "\tspeed: 0.0140s/iter; left time: 626.9219s\n",
      "\titers: 1000, epoch: 1 | loss: 0.0747703\n",
      "\tspeed: 0.0140s/iter; left time: 625.6654s\n",
      "\titers: 1100, epoch: 1 | loss: 0.0325531\n",
      "\tspeed: 0.0142s/iter; left time: 629.3402s\n",
      "\titers: 1200, epoch: 1 | loss: 0.0703290\n",
      "\tspeed: 0.0142s/iter; left time: 628.0511s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2421518\n",
      "\tspeed: 0.0142s/iter; left time: 626.6416s\n",
      "\titers: 1400, epoch: 1 | loss: 0.0311336\n",
      "\tspeed: 0.0141s/iter; left time: 624.1615s\n",
      "\titers: 1500, epoch: 1 | loss: 0.1793718\n",
      "\tspeed: 0.0141s/iter; left time: 622.5184s\n",
      "\titers: 1600, epoch: 1 | loss: 0.1580286\n",
      "\tspeed: 0.0141s/iter; left time: 619.8817s\n",
      "\titers: 1700, epoch: 1 | loss: 0.3657564\n",
      "\tspeed: 0.0141s/iter; left time: 619.3748s\n",
      "\titers: 1800, epoch: 1 | loss: 0.0488247\n",
      "\tspeed: 0.0141s/iter; left time: 617.5199s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2107662\n",
      "\tspeed: 0.0141s/iter; left time: 615.8111s\n",
      "\titers: 2000, epoch: 1 | loss: 0.4226616\n",
      "\tspeed: 0.0141s/iter; left time: 614.6405s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1708536\n",
      "\tspeed: 0.0141s/iter; left time: 613.1388s\n",
      "\titers: 2200, epoch: 1 | loss: 0.2994604\n",
      "\tspeed: 0.0141s/iter; left time: 611.2784s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1566990\n",
      "\tspeed: 0.0141s/iter; left time: 609.9833s\n",
      "\titers: 2400, epoch: 1 | loss: 0.1506222\n",
      "\tspeed: 0.0141s/iter; left time: 608.1644s\n",
      "\titers: 2500, epoch: 1 | loss: 0.3021303\n",
      "\tspeed: 0.0141s/iter; left time: 606.9231s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1027806\n",
      "\tspeed: 0.0141s/iter; left time: 605.5013s\n",
      "\titers: 2700, epoch: 1 | loss: 0.1548713\n",
      "\tspeed: 0.0141s/iter; left time: 604.1299s\n",
      "\titers: 2800, epoch: 1 | loss: 0.2552365\n",
      "\tspeed: 0.0141s/iter; left time: 602.8869s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2244025\n",
      "\tspeed: 0.0141s/iter; left time: 601.4412s\n",
      "\titers: 3000, epoch: 1 | loss: 0.1353815\n",
      "\tspeed: 0.0141s/iter; left time: 600.5245s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1347615\n",
      "\tspeed: 0.0141s/iter; left time: 599.3945s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2426359\n",
      "\tspeed: 0.0141s/iter; left time: 597.7247s\n",
      "\titers: 3300, epoch: 1 | loss: 0.0556272\n",
      "\tspeed: 0.0141s/iter; left time: 596.0940s\n",
      "\titers: 3400, epoch: 1 | loss: 0.3138039\n",
      "\tspeed: 0.0141s/iter; left time: 594.9727s\n",
      "\titers: 3500, epoch: 1 | loss: 0.1077223\n",
      "\tspeed: 0.0141s/iter; left time: 593.3165s\n",
      "\titers: 3600, epoch: 1 | loss: 0.0938954\n",
      "\tspeed: 0.0141s/iter; left time: 591.6984s\n",
      "\titers: 3700, epoch: 1 | loss: 0.0831093\n",
      "\tspeed: 0.0141s/iter; left time: 591.3046s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1749512\n",
      "\tspeed: 0.0141s/iter; left time: 589.4215s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3136895\n",
      "\tspeed: 0.0141s/iter; left time: 588.6578s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2345701\n",
      "\tspeed: 0.0141s/iter; left time: 587.7717s\n",
      "\titers: 4100, epoch: 1 | loss: 0.0518031\n",
      "\tspeed: 0.0141s/iter; left time: 586.0125s\n",
      "\titers: 4200, epoch: 1 | loss: 0.0894661\n",
      "\tspeed: 0.0141s/iter; left time: 584.2493s\n",
      "\titers: 4300, epoch: 1 | loss: 0.1978239\n",
      "\tspeed: 0.0141s/iter; left time: 582.6242s\n",
      "\titers: 4400, epoch: 1 | loss: 0.0444240\n",
      "\tspeed: 0.0141s/iter; left time: 581.3226s\n",
      "\titers: 4500, epoch: 1 | loss: 0.0818066\n",
      "\tspeed: 0.0141s/iter; left time: 579.6689s\n",
      "Epoch: 1 cost time: 65.84677958488464\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.1845709 Vali Loss: 0.0388556 Test Loss: 0.1238783\n",
      "Validation loss decreased (inf --> 0.038856).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0823992\n",
      "\tspeed: 0.1253s/iter; left time: 5124.0184s\n",
      "\titers: 200, epoch: 2 | loss: 0.0459338\n",
      "\tspeed: 0.0141s/iter; left time: 575.4057s\n",
      "\titers: 300, epoch: 2 | loss: 0.0882376\n",
      "\tspeed: 0.0141s/iter; left time: 573.7332s\n",
      "\titers: 400, epoch: 2 | loss: 0.0970598\n",
      "\tspeed: 0.0141s/iter; left time: 571.9045s\n",
      "\titers: 500, epoch: 2 | loss: 0.1970111\n",
      "\tspeed: 0.0141s/iter; left time: 570.0217s\n",
      "\titers: 600, epoch: 2 | loss: 0.0849456\n",
      "\tspeed: 0.0141s/iter; left time: 568.9419s\n",
      "\titers: 700, epoch: 2 | loss: 0.0917697\n",
      "\tspeed: 0.0141s/iter; left time: 567.0412s\n",
      "\titers: 800, epoch: 2 | loss: 0.0698548\n",
      "\tspeed: 0.0140s/iter; left time: 564.4325s\n",
      "\titers: 900, epoch: 2 | loss: 0.0470148\n",
      "\tspeed: 0.0140s/iter; left time: 562.9995s\n",
      "\titers: 1000, epoch: 2 | loss: 0.1265071\n",
      "\tspeed: 0.0140s/iter; left time: 561.1856s\n",
      "\titers: 1100, epoch: 2 | loss: 0.1931272\n",
      "\tspeed: 0.0140s/iter; left time: 559.7521s\n",
      "\titers: 1200, epoch: 2 | loss: 0.1701560\n",
      "\tspeed: 0.0140s/iter; left time: 558.2846s\n",
      "\titers: 1300, epoch: 2 | loss: 0.0895878\n",
      "\tspeed: 0.0140s/iter; left time: 557.1946s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0959935\n",
      "\tspeed: 0.0140s/iter; left time: 555.6238s\n",
      "\titers: 1500, epoch: 2 | loss: 0.0333353\n",
      "\tspeed: 0.0140s/iter; left time: 554.1289s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1069421\n",
      "\tspeed: 0.0140s/iter; left time: 553.2389s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3336352\n",
      "\tspeed: 0.0140s/iter; left time: 551.5768s\n",
      "\titers: 1800, epoch: 2 | loss: 0.1317856\n",
      "\tspeed: 0.0140s/iter; left time: 550.3345s\n",
      "\titers: 1900, epoch: 2 | loss: 0.2047766\n",
      "\tspeed: 0.0140s/iter; left time: 548.6127s\n",
      "\titers: 2000, epoch: 2 | loss: 0.0607306\n",
      "\tspeed: 0.0140s/iter; left time: 546.5234s\n",
      "\titers: 2100, epoch: 2 | loss: 0.0438349\n",
      "\tspeed: 0.0140s/iter; left time: 545.0236s\n",
      "\titers: 2200, epoch: 2 | loss: 0.0989833\n",
      "\tspeed: 0.0140s/iter; left time: 543.4740s\n",
      "\titers: 2300, epoch: 2 | loss: 0.2636790\n",
      "\tspeed: 0.0140s/iter; left time: 542.6211s\n",
      "\titers: 2400, epoch: 2 | loss: 0.1362514\n",
      "\tspeed: 0.0140s/iter; left time: 540.5803s\n",
      "\titers: 2500, epoch: 2 | loss: 0.0657715\n",
      "\tspeed: 0.0140s/iter; left time: 539.4446s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1880625\n",
      "\tspeed: 0.0140s/iter; left time: 537.9942s\n",
      "\titers: 2700, epoch: 2 | loss: 0.1141414\n",
      "\tspeed: 0.0140s/iter; left time: 536.6255s\n",
      "\titers: 2800, epoch: 2 | loss: 0.0926635\n",
      "\tspeed: 0.0140s/iter; left time: 534.8644s\n",
      "\titers: 2900, epoch: 2 | loss: 0.0585423\n",
      "\tspeed: 0.0140s/iter; left time: 533.6191s\n",
      "\titers: 3000, epoch: 2 | loss: 0.0994413\n",
      "\tspeed: 0.0140s/iter; left time: 532.2141s\n",
      "\titers: 3100, epoch: 2 | loss: 0.0423770\n",
      "\tspeed: 0.0140s/iter; left time: 530.7797s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2233988\n",
      "\tspeed: 0.0140s/iter; left time: 529.3743s\n",
      "\titers: 3300, epoch: 2 | loss: 0.0529721\n",
      "\tspeed: 0.0140s/iter; left time: 527.7803s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0876396\n",
      "\tspeed: 0.0140s/iter; left time: 526.6420s\n",
      "\titers: 3500, epoch: 2 | loss: 0.1931413\n",
      "\tspeed: 0.0140s/iter; left time: 525.3407s\n",
      "\titers: 3600, epoch: 2 | loss: 0.1586268\n",
      "\tspeed: 0.0140s/iter; left time: 523.8059s\n",
      "\titers: 3700, epoch: 2 | loss: 0.2485291\n",
      "\tspeed: 0.0140s/iter; left time: 522.3066s\n",
      "\titers: 3800, epoch: 2 | loss: 0.0957862\n",
      "\tspeed: 0.0140s/iter; left time: 520.7927s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1986080\n",
      "\tspeed: 0.0140s/iter; left time: 520.3905s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2186810\n",
      "\tspeed: 0.0140s/iter; left time: 518.3986s\n",
      "\titers: 4100, epoch: 2 | loss: 0.1189078\n",
      "\tspeed: 0.0140s/iter; left time: 516.9186s\n",
      "\titers: 4200, epoch: 2 | loss: 0.0667320\n",
      "\tspeed: 0.0140s/iter; left time: 515.4092s\n",
      "\titers: 4300, epoch: 2 | loss: 0.1627073\n",
      "\tspeed: 0.0140s/iter; left time: 513.9705s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1136310\n",
      "\tspeed: 0.0140s/iter; left time: 512.4647s\n",
      "\titers: 4500, epoch: 2 | loss: 0.0472952\n",
      "\tspeed: 0.0140s/iter; left time: 511.3369s\n",
      "Epoch: 2 cost time: 64.15683364868164\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.1462912 Vali Loss: 0.0356053 Test Loss: 0.1159925\n",
      "Validation loss decreased (0.038856 --> 0.035605).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1065639\n",
      "\tspeed: 0.1246s/iter; left time: 4528.9157s\n",
      "\titers: 200, epoch: 3 | loss: 0.0457031\n",
      "\tspeed: 0.0141s/iter; left time: 510.5681s\n",
      "\titers: 300, epoch: 3 | loss: 0.0885979\n",
      "\tspeed: 0.0141s/iter; left time: 508.3709s\n",
      "\titers: 400, epoch: 3 | loss: 0.0828183\n",
      "\tspeed: 0.0141s/iter; left time: 507.9410s\n",
      "\titers: 500, epoch: 3 | loss: 0.1061108\n",
      "\tspeed: 0.0141s/iter; left time: 505.9903s\n",
      "\titers: 600, epoch: 3 | loss: 0.1034938\n",
      "\tspeed: 0.0141s/iter; left time: 504.1850s\n",
      "\titers: 700, epoch: 3 | loss: 0.2853724\n",
      "\tspeed: 0.0141s/iter; left time: 502.8968s\n",
      "\titers: 800, epoch: 3 | loss: 0.1566754\n",
      "\tspeed: 0.0141s/iter; left time: 501.1067s\n",
      "\titers: 900, epoch: 3 | loss: 0.0511770\n",
      "\tspeed: 0.0141s/iter; left time: 501.7957s\n",
      "\titers: 1000, epoch: 3 | loss: 0.1363623\n",
      "\tspeed: 0.0141s/iter; left time: 501.2474s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1427456\n",
      "\tspeed: 0.0141s/iter; left time: 499.8939s\n",
      "\titers: 1200, epoch: 3 | loss: 0.2312427\n",
      "\tspeed: 0.0141s/iter; left time: 498.6185s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2148285\n",
      "\tspeed: 0.0141s/iter; left time: 496.8794s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1275171\n",
      "\tspeed: 0.0141s/iter; left time: 495.6446s\n",
      "\titers: 1500, epoch: 3 | loss: 0.2424717\n",
      "\tspeed: 0.0141s/iter; left time: 493.7838s\n",
      "\titers: 1600, epoch: 3 | loss: 0.0710498\n",
      "\tspeed: 0.0141s/iter; left time: 492.7435s\n",
      "\titers: 1700, epoch: 3 | loss: 0.4991743\n",
      "\tspeed: 0.0141s/iter; left time: 491.2267s\n",
      "\titers: 1800, epoch: 3 | loss: 0.0743798\n",
      "\tspeed: 0.0141s/iter; left time: 489.7832s\n",
      "\titers: 1900, epoch: 3 | loss: 0.3589885\n",
      "\tspeed: 0.0141s/iter; left time: 488.4077s\n",
      "\titers: 2000, epoch: 3 | loss: 0.2727187\n",
      "\tspeed: 0.0141s/iter; left time: 486.9287s\n",
      "\titers: 2100, epoch: 3 | loss: 0.0569313\n",
      "\tspeed: 0.0141s/iter; left time: 485.5077s\n",
      "\titers: 2200, epoch: 3 | loss: 0.0474754\n",
      "\tspeed: 0.0141s/iter; left time: 484.1967s\n",
      "\titers: 2300, epoch: 3 | loss: 0.1319304\n",
      "\tspeed: 0.0141s/iter; left time: 482.7052s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1778789\n",
      "\tspeed: 0.0141s/iter; left time: 481.2133s\n",
      "\titers: 2500, epoch: 3 | loss: 0.1636431\n",
      "\tspeed: 0.0141s/iter; left time: 479.8142s\n",
      "\titers: 2600, epoch: 3 | loss: 0.0397877\n",
      "\tspeed: 0.0141s/iter; left time: 478.4032s\n",
      "\titers: 2700, epoch: 3 | loss: 0.0501849\n",
      "\tspeed: 0.0141s/iter; left time: 476.7171s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1227726\n",
      "\tspeed: 0.0141s/iter; left time: 475.2794s\n",
      "\titers: 2900, epoch: 3 | loss: 0.0855512\n",
      "\tspeed: 0.0141s/iter; left time: 473.9660s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0530095\n",
      "\tspeed: 0.0141s/iter; left time: 472.3908s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1657689\n",
      "\tspeed: 0.0141s/iter; left time: 471.0446s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2411741\n",
      "\tspeed: 0.0141s/iter; left time: 469.6867s\n",
      "\titers: 3300, epoch: 3 | loss: 0.0725398\n",
      "\tspeed: 0.0141s/iter; left time: 466.5136s\n",
      "\titers: 3400, epoch: 3 | loss: 0.2970087\n",
      "\tspeed: 0.0141s/iter; left time: 466.5644s\n",
      "\titers: 3500, epoch: 3 | loss: 0.0349709\n",
      "\tspeed: 0.0141s/iter; left time: 465.3573s\n",
      "\titers: 3600, epoch: 3 | loss: 0.0933889\n",
      "\tspeed: 0.0141s/iter; left time: 464.1734s\n",
      "\titers: 3700, epoch: 3 | loss: 0.1471375\n",
      "\tspeed: 0.0141s/iter; left time: 462.7595s\n",
      "\titers: 3800, epoch: 3 | loss: 0.0525463\n",
      "\tspeed: 0.0141s/iter; left time: 461.1800s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1331163\n",
      "\tspeed: 0.0141s/iter; left time: 459.7184s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1140037\n",
      "\tspeed: 0.0141s/iter; left time: 458.3989s\n",
      "\titers: 4100, epoch: 3 | loss: 0.2813100\n",
      "\tspeed: 0.0141s/iter; left time: 457.0361s\n",
      "\titers: 4200, epoch: 3 | loss: 0.0965427\n",
      "\tspeed: 0.0141s/iter; left time: 455.7713s\n",
      "\titers: 4300, epoch: 3 | loss: 0.0205819\n",
      "\tspeed: 0.0141s/iter; left time: 454.2350s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1114559\n",
      "\tspeed: 0.0141s/iter; left time: 453.0044s\n",
      "\titers: 4500, epoch: 3 | loss: 0.0422447\n",
      "\tspeed: 0.0141s/iter; left time: 451.4229s\n",
      "Epoch: 3 cost time: 64.59300470352173\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.1155625 Vali Loss: 0.0344842 Test Loss: 0.1190976\n",
      "Validation loss decreased (0.035605 --> 0.034484).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0808964\n",
      "\tspeed: 0.1389s/iter; left time: 4413.9228s\n",
      "\titers: 200, epoch: 4 | loss: 0.3147544\n",
      "\tspeed: 0.0142s/iter; left time: 448.9168s\n",
      "\titers: 300, epoch: 4 | loss: 0.0332088\n",
      "\tspeed: 0.0142s/iter; left time: 447.5938s\n",
      "\titers: 400, epoch: 4 | loss: 0.0945179\n",
      "\tspeed: 0.0142s/iter; left time: 446.5455s\n",
      "\titers: 500, epoch: 4 | loss: 0.0903225\n",
      "\tspeed: 0.0142s/iter; left time: 444.2685s\n",
      "\titers: 600, epoch: 4 | loss: 0.1182445\n",
      "\tspeed: 0.0142s/iter; left time: 443.3702s\n",
      "\titers: 700, epoch: 4 | loss: 0.0745854\n",
      "\tspeed: 0.0142s/iter; left time: 441.3842s\n",
      "\titers: 800, epoch: 4 | loss: 0.1520279\n",
      "\tspeed: 0.0141s/iter; left time: 439.6184s\n",
      "\titers: 900, epoch: 4 | loss: 0.0894282\n",
      "\tspeed: 0.0141s/iter; left time: 438.3129s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1588899\n",
      "\tspeed: 0.0142s/iter; left time: 437.0768s\n",
      "\titers: 1100, epoch: 4 | loss: 0.0862478\n",
      "\tspeed: 0.0141s/iter; left time: 435.0011s\n",
      "\titers: 1200, epoch: 4 | loss: 0.0334538\n",
      "\tspeed: 0.0141s/iter; left time: 433.7374s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0696045\n",
      "\tspeed: 0.0141s/iter; left time: 432.2300s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0762828\n",
      "\tspeed: 0.0141s/iter; left time: 430.8642s\n",
      "\titers: 1500, epoch: 4 | loss: 0.1981945\n",
      "\tspeed: 0.0141s/iter; left time: 429.2968s\n",
      "\titers: 1600, epoch: 4 | loss: 0.0817161\n",
      "\tspeed: 0.0141s/iter; left time: 428.2906s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1189489\n",
      "\tspeed: 0.0142s/iter; left time: 428.1867s\n",
      "\titers: 1800, epoch: 4 | loss: 0.0256215\n",
      "\tspeed: 0.0142s/iter; left time: 426.7515s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0460590\n",
      "\tspeed: 0.0142s/iter; left time: 425.2258s\n",
      "\titers: 2000, epoch: 4 | loss: 0.1439898\n",
      "\tspeed: 0.0142s/iter; left time: 423.9800s\n",
      "\titers: 2100, epoch: 4 | loss: 0.0176133\n",
      "\tspeed: 0.0142s/iter; left time: 421.8461s\n",
      "\titers: 2200, epoch: 4 | loss: 0.1930578\n",
      "\tspeed: 0.0141s/iter; left time: 418.2579s\n",
      "\titers: 2300, epoch: 4 | loss: 0.1603837\n",
      "\tspeed: 0.0141s/iter; left time: 416.8704s\n",
      "\titers: 2400, epoch: 4 | loss: 0.1315998\n",
      "\tspeed: 0.0141s/iter; left time: 415.4811s\n",
      "\titers: 2500, epoch: 4 | loss: 0.0361627\n",
      "\tspeed: 0.0141s/iter; left time: 414.2331s\n",
      "\titers: 2600, epoch: 4 | loss: 0.0512993\n",
      "\tspeed: 0.0141s/iter; left time: 413.6454s\n",
      "\titers: 2700, epoch: 4 | loss: 0.5859755\n",
      "\tspeed: 0.0141s/iter; left time: 412.7056s\n",
      "\titers: 2800, epoch: 4 | loss: 0.0280619\n",
      "\tspeed: 0.0141s/iter; left time: 411.2090s\n",
      "\titers: 2900, epoch: 4 | loss: 0.0236751\n",
      "\tspeed: 0.0141s/iter; left time: 409.9421s\n",
      "\titers: 3000, epoch: 4 | loss: 0.1419309\n",
      "\tspeed: 0.0141s/iter; left time: 408.3883s\n",
      "\titers: 3100, epoch: 4 | loss: 0.0917471\n",
      "\tspeed: 0.0141s/iter; left time: 406.8971s\n",
      "\titers: 3200, epoch: 4 | loss: 0.1511783\n",
      "\tspeed: 0.0141s/iter; left time: 405.5348s\n",
      "\titers: 3300, epoch: 4 | loss: 0.0805234\n",
      "\tspeed: 0.0141s/iter; left time: 404.1972s\n",
      "\titers: 3400, epoch: 4 | loss: 0.0533366\n",
      "\tspeed: 0.0141s/iter; left time: 402.8633s\n",
      "\titers: 3500, epoch: 4 | loss: 0.0583888\n",
      "\tspeed: 0.0141s/iter; left time: 401.2361s\n",
      "\titers: 3600, epoch: 4 | loss: 0.0772908\n",
      "\tspeed: 0.0141s/iter; left time: 400.2101s\n",
      "\titers: 3700, epoch: 4 | loss: 0.1199456\n",
      "\tspeed: 0.0141s/iter; left time: 398.5458s\n",
      "\titers: 3800, epoch: 4 | loss: 0.0573075\n",
      "\tspeed: 0.0141s/iter; left time: 397.2904s\n",
      "\titers: 3900, epoch: 4 | loss: 0.0811195\n",
      "\tspeed: 0.0141s/iter; left time: 395.5473s\n",
      "\titers: 4000, epoch: 4 | loss: 0.0908595\n",
      "\tspeed: 0.0141s/iter; left time: 394.4757s\n",
      "\titers: 4100, epoch: 4 | loss: 0.0346804\n",
      "\tspeed: 0.0141s/iter; left time: 392.7604s\n",
      "\titers: 4200, epoch: 4 | loss: 0.0680386\n",
      "\tspeed: 0.0141s/iter; left time: 391.7133s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2946407\n",
      "\tspeed: 0.0141s/iter; left time: 390.2253s\n",
      "\titers: 4400, epoch: 4 | loss: 0.0926565\n",
      "\tspeed: 0.0141s/iter; left time: 388.6961s\n",
      "\titers: 4500, epoch: 4 | loss: 0.0887671\n",
      "\tspeed: 0.0141s/iter; left time: 387.1472s\n",
      "Epoch: 4 cost time: 64.67805933952332\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.0972140 Vali Loss: 0.0326700 Test Loss: 0.1195778\n",
      "Validation loss decreased (0.034484 --> 0.032670).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0327397\n",
      "\tspeed: 0.1277s/iter; left time: 3476.2493s\n",
      "\titers: 200, epoch: 5 | loss: 0.0592898\n",
      "\tspeed: 0.0153s/iter; left time: 416.2533s\n",
      "\titers: 300, epoch: 5 | loss: 0.0636691\n",
      "\tspeed: 0.0157s/iter; left time: 423.1904s\n",
      "\titers: 400, epoch: 5 | loss: 0.0222537\n",
      "\tspeed: 0.0157s/iter; left time: 421.9288s\n",
      "\titers: 500, epoch: 5 | loss: 0.1401427\n",
      "\tspeed: 0.0143s/iter; left time: 383.7562s\n",
      "\titers: 600, epoch: 5 | loss: 0.0321536\n",
      "\tspeed: 0.0141s/iter; left time: 378.1972s\n",
      "\titers: 700, epoch: 5 | loss: 0.0571906\n",
      "\tspeed: 0.0142s/iter; left time: 376.9227s\n",
      "\titers: 800, epoch: 5 | loss: 0.0820572\n",
      "\tspeed: 0.0142s/iter; left time: 375.4743s\n",
      "\titers: 900, epoch: 5 | loss: 0.0488193\n",
      "\tspeed: 0.0141s/iter; left time: 373.8529s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1661029\n",
      "\tspeed: 0.0148s/iter; left time: 388.6207s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0404386\n",
      "\tspeed: 0.0154s/iter; left time: 404.9143s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1063338\n",
      "\tspeed: 0.0157s/iter; left time: 409.6725s\n",
      "\titers: 1300, epoch: 5 | loss: 0.1059139\n",
      "\tspeed: 0.0157s/iter; left time: 408.6937s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1428587\n",
      "\tspeed: 0.0157s/iter; left time: 407.2239s\n",
      "\titers: 1500, epoch: 5 | loss: 0.0961618\n",
      "\tspeed: 0.0156s/iter; left time: 404.0444s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0553513\n",
      "\tspeed: 0.0142s/iter; left time: 364.2555s\n",
      "\titers: 1700, epoch: 5 | loss: 0.1877339\n",
      "\tspeed: 0.0142s/iter; left time: 362.8928s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1133093\n",
      "\tspeed: 0.0142s/iter; left time: 361.3240s\n",
      "\titers: 1900, epoch: 5 | loss: 0.0259790\n",
      "\tspeed: 0.0142s/iter; left time: 360.2600s\n",
      "\titers: 2000, epoch: 5 | loss: 0.4543585\n",
      "\tspeed: 0.0142s/iter; left time: 359.1213s\n",
      "\titers: 2100, epoch: 5 | loss: 0.0459964\n",
      "\tspeed: 0.0155s/iter; left time: 390.9218s\n",
      "\titers: 2200, epoch: 5 | loss: 0.0456040\n",
      "\tspeed: 0.0142s/iter; left time: 356.0249s\n",
      "\titers: 2300, epoch: 5 | loss: 0.0444594\n",
      "\tspeed: 0.0142s/iter; left time: 354.4568s\n",
      "\titers: 2400, epoch: 5 | loss: 0.0674500\n",
      "\tspeed: 0.0141s/iter; left time: 352.7648s\n",
      "\titers: 2500, epoch: 5 | loss: 0.0753532\n",
      "\tspeed: 0.0142s/iter; left time: 351.3997s\n",
      "\titers: 2600, epoch: 5 | loss: 0.0385275\n",
      "\tspeed: 0.0142s/iter; left time: 350.1438s\n",
      "\titers: 2700, epoch: 5 | loss: 0.0963195\n",
      "\tspeed: 0.0141s/iter; left time: 348.4037s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0729757\n",
      "\tspeed: 0.0142s/iter; left time: 347.2031s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0627494\n",
      "\tspeed: 0.0141s/iter; left time: 345.5395s\n",
      "\titers: 3000, epoch: 5 | loss: 0.0753292\n",
      "\tspeed: 0.0142s/iter; left time: 344.3926s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0419036\n",
      "\tspeed: 0.0142s/iter; left time: 343.0231s\n",
      "\titers: 3200, epoch: 5 | loss: 0.0143885\n",
      "\tspeed: 0.0142s/iter; left time: 341.7037s\n",
      "\titers: 3300, epoch: 5 | loss: 0.0557503\n",
      "\tspeed: 0.0142s/iter; left time: 340.2291s\n",
      "\titers: 3400, epoch: 5 | loss: 0.0739679\n",
      "\tspeed: 0.0142s/iter; left time: 338.7577s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0672351\n",
      "\tspeed: 0.0142s/iter; left time: 337.2150s\n",
      "\titers: 3600, epoch: 5 | loss: 0.0715858\n",
      "\tspeed: 0.0141s/iter; left time: 335.7680s\n",
      "\titers: 3700, epoch: 5 | loss: 0.0419440\n",
      "\tspeed: 0.0141s/iter; left time: 334.3156s\n",
      "\titers: 3800, epoch: 5 | loss: 0.1041074\n",
      "\tspeed: 0.0141s/iter; left time: 332.8223s\n",
      "\titers: 3900, epoch: 5 | loss: 0.0596397\n",
      "\tspeed: 0.0141s/iter; left time: 331.5155s\n",
      "\titers: 4000, epoch: 5 | loss: 0.0390565\n",
      "\tspeed: 0.0141s/iter; left time: 330.0086s\n",
      "\titers: 4100, epoch: 5 | loss: 0.0467949\n",
      "\tspeed: 0.0141s/iter; left time: 328.6755s\n",
      "\titers: 4200, epoch: 5 | loss: 0.0410247\n",
      "\tspeed: 0.0141s/iter; left time: 327.2227s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1490004\n",
      "\tspeed: 0.0142s/iter; left time: 325.9459s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0145626\n",
      "\tspeed: 0.0142s/iter; left time: 324.5676s\n",
      "\titers: 4500, epoch: 5 | loss: 0.0616662\n",
      "\tspeed: 0.0142s/iter; left time: 323.0818s\n",
      "Epoch: 5 cost time: 66.23775267601013\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.0880438 Vali Loss: 0.0320621 Test Loss: 0.1183452\n",
      "Validation loss decreased (0.032670 --> 0.032062).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0371447\n",
      "\tspeed: 0.1253s/iter; left time: 2842.3217s\n",
      "\titers: 200, epoch: 6 | loss: 0.1323322\n",
      "\tspeed: 0.0141s/iter; left time: 318.8365s\n",
      "\titers: 300, epoch: 6 | loss: 0.0797647\n",
      "\tspeed: 0.0142s/iter; left time: 319.2181s\n",
      "\titers: 400, epoch: 6 | loss: 0.0748486\n",
      "\tspeed: 0.0142s/iter; left time: 317.2448s\n",
      "\titers: 500, epoch: 6 | loss: 0.0841114\n",
      "\tspeed: 0.0141s/iter; left time: 314.4810s\n",
      "\titers: 600, epoch: 6 | loss: 0.1425022\n",
      "\tspeed: 0.0142s/iter; left time: 314.0356s\n",
      "\titers: 700, epoch: 6 | loss: 0.0303492\n",
      "\tspeed: 0.0142s/iter; left time: 313.0446s\n",
      "\titers: 800, epoch: 6 | loss: 0.0900622\n",
      "\tspeed: 0.0142s/iter; left time: 311.2874s\n",
      "\titers: 900, epoch: 6 | loss: 0.0624585\n",
      "\tspeed: 0.0142s/iter; left time: 310.5474s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1477404\n",
      "\tspeed: 0.0142s/iter; left time: 309.4671s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0580295\n",
      "\tspeed: 0.0141s/iter; left time: 305.8037s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0689246\n",
      "\tspeed: 0.0141s/iter; left time: 303.9413s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1104512\n",
      "\tspeed: 0.0142s/iter; left time: 304.4994s\n",
      "\titers: 1400, epoch: 6 | loss: 0.0557156\n",
      "\tspeed: 0.0142s/iter; left time: 303.3672s\n",
      "\titers: 1500, epoch: 6 | loss: 0.0609349\n",
      "\tspeed: 0.0142s/iter; left time: 301.9991s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1599783\n",
      "\tspeed: 0.0142s/iter; left time: 299.6706s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0506469\n",
      "\tspeed: 0.0141s/iter; left time: 297.7646s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2740918\n",
      "\tspeed: 0.0141s/iter; left time: 295.5452s\n",
      "\titers: 1900, epoch: 6 | loss: 0.0903138\n",
      "\tspeed: 0.0141s/iter; left time: 293.5515s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1627188\n",
      "\tspeed: 0.0140s/iter; left time: 291.6808s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1097333\n",
      "\tspeed: 0.0141s/iter; left time: 292.0083s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1801362\n",
      "\tspeed: 0.0140s/iter; left time: 288.8197s\n",
      "\titers: 2300, epoch: 6 | loss: 0.1097423\n",
      "\tspeed: 0.0140s/iter; left time: 287.4897s\n",
      "\titers: 2400, epoch: 6 | loss: 0.0528665\n",
      "\tspeed: 0.0141s/iter; left time: 286.4086s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0586448\n",
      "\tspeed: 0.0141s/iter; left time: 285.5170s\n",
      "\titers: 2600, epoch: 6 | loss: 0.0628223\n",
      "\tspeed: 0.0141s/iter; left time: 284.3846s\n",
      "\titers: 2700, epoch: 6 | loss: 0.0593986\n",
      "\tspeed: 0.0141s/iter; left time: 282.8885s\n",
      "\titers: 2800, epoch: 6 | loss: 0.0720062\n",
      "\tspeed: 0.0141s/iter; left time: 282.2179s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1010607\n",
      "\tspeed: 0.0142s/iter; left time: 281.3304s\n",
      "\titers: 3000, epoch: 6 | loss: 0.0290217\n",
      "\tspeed: 0.0141s/iter; left time: 279.6041s\n",
      "\titers: 3100, epoch: 6 | loss: 0.0592466\n",
      "\tspeed: 0.0141s/iter; left time: 278.0286s\n",
      "\titers: 3200, epoch: 6 | loss: 0.0754981\n",
      "\tspeed: 0.0141s/iter; left time: 276.7286s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0622220\n",
      "\tspeed: 0.0141s/iter; left time: 275.4902s\n",
      "\titers: 3400, epoch: 6 | loss: 0.0306741\n",
      "\tspeed: 0.0142s/iter; left time: 274.6291s\n",
      "\titers: 3500, epoch: 6 | loss: 0.0182121\n",
      "\tspeed: 0.0142s/iter; left time: 273.7639s\n",
      "\titers: 3600, epoch: 6 | loss: 0.0844464\n",
      "\tspeed: 0.0142s/iter; left time: 272.2561s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0976479\n",
      "\tspeed: 0.0142s/iter; left time: 270.8951s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1226637\n",
      "\tspeed: 0.0142s/iter; left time: 269.6232s\n",
      "\titers: 3900, epoch: 6 | loss: 0.0359020\n",
      "\tspeed: 0.0142s/iter; left time: 268.1406s\n",
      "\titers: 4000, epoch: 6 | loss: 0.1085023\n",
      "\tspeed: 0.0142s/iter; left time: 266.7577s\n",
      "\titers: 4100, epoch: 6 | loss: 0.0149682\n",
      "\tspeed: 0.0142s/iter; left time: 265.3028s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0364626\n",
      "\tspeed: 0.0142s/iter; left time: 263.0752s\n",
      "\titers: 4300, epoch: 6 | loss: 0.1063752\n",
      "\tspeed: 0.0142s/iter; left time: 261.8585s\n",
      "\titers: 4400, epoch: 6 | loss: 0.0495901\n",
      "\tspeed: 0.0142s/iter; left time: 260.7598s\n",
      "\titers: 4500, epoch: 6 | loss: 0.0734984\n",
      "\tspeed: 0.0141s/iter; left time: 258.5665s\n",
      "Epoch: 6 cost time: 64.69049882888794\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.0846848 Vali Loss: 0.0323295 Test Loss: 0.1182579\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0889323\n",
      "\tspeed: 0.1238s/iter; left time: 2243.2862s\n",
      "\titers: 200, epoch: 7 | loss: 0.1572415\n",
      "\tspeed: 0.0140s/iter; left time: 253.1701s\n",
      "\titers: 300, epoch: 7 | loss: 0.1716840\n",
      "\tspeed: 0.0140s/iter; left time: 251.7263s\n",
      "\titers: 400, epoch: 7 | loss: 0.0512685\n",
      "\tspeed: 0.0141s/iter; left time: 251.7890s\n",
      "\titers: 500, epoch: 7 | loss: 0.0473002\n",
      "\tspeed: 0.0141s/iter; left time: 249.3252s\n",
      "\titers: 600, epoch: 7 | loss: 0.0459835\n",
      "\tspeed: 0.0141s/iter; left time: 248.2501s\n",
      "\titers: 700, epoch: 7 | loss: 0.0182449\n",
      "\tspeed: 0.0142s/iter; left time: 248.0554s\n",
      "\titers: 800, epoch: 7 | loss: 0.0911533\n",
      "\tspeed: 0.0142s/iter; left time: 246.7644s\n",
      "\titers: 900, epoch: 7 | loss: 0.1405085\n",
      "\tspeed: 0.0141s/iter; left time: 244.7408s\n",
      "\titers: 1000, epoch: 7 | loss: 0.0288204\n",
      "\tspeed: 0.0141s/iter; left time: 243.2901s\n",
      "\titers: 1100, epoch: 7 | loss: 0.2171633\n",
      "\tspeed: 0.0141s/iter; left time: 241.7497s\n",
      "\titers: 1200, epoch: 7 | loss: 0.0595498\n",
      "\tspeed: 0.0141s/iter; left time: 240.2924s\n",
      "\titers: 1300, epoch: 7 | loss: 0.0666365\n",
      "\tspeed: 0.0141s/iter; left time: 239.2568s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0845961\n",
      "\tspeed: 0.0142s/iter; left time: 238.0631s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1125851\n",
      "\tspeed: 0.0144s/iter; left time: 240.5285s\n",
      "\titers: 1600, epoch: 7 | loss: 0.0637547\n",
      "\tspeed: 0.0156s/iter; left time: 259.2555s\n",
      "\titers: 1700, epoch: 7 | loss: 0.0356490\n",
      "\tspeed: 0.0151s/iter; left time: 249.6798s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0515363\n",
      "\tspeed: 0.0141s/iter; left time: 231.8696s\n",
      "\titers: 1900, epoch: 7 | loss: 0.0263805\n",
      "\tspeed: 0.0141s/iter; left time: 230.3944s\n",
      "\titers: 2000, epoch: 7 | loss: 0.0442252\n",
      "\tspeed: 0.0141s/iter; left time: 228.9787s\n",
      "\titers: 2100, epoch: 7 | loss: 0.0430738\n",
      "\tspeed: 0.0141s/iter; left time: 227.4330s\n",
      "\titers: 2200, epoch: 7 | loss: 0.0485763\n",
      "\tspeed: 0.0141s/iter; left time: 226.0666s\n",
      "\titers: 2300, epoch: 7 | loss: 0.0397500\n",
      "\tspeed: 0.0141s/iter; left time: 224.8192s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0554271\n",
      "\tspeed: 0.0142s/iter; left time: 223.9671s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1556064\n",
      "\tspeed: 0.0141s/iter; left time: 222.1398s\n",
      "\titers: 2600, epoch: 7 | loss: 0.0903479\n",
      "\tspeed: 0.0141s/iter; left time: 220.6437s\n",
      "\titers: 2700, epoch: 7 | loss: 0.0287709\n",
      "\tspeed: 0.0141s/iter; left time: 219.5496s\n",
      "\titers: 2800, epoch: 7 | loss: 0.0583613\n",
      "\tspeed: 0.0142s/iter; left time: 218.2601s\n",
      "\titers: 2900, epoch: 7 | loss: 0.0644807\n",
      "\tspeed: 0.0141s/iter; left time: 216.7534s\n",
      "\titers: 3000, epoch: 7 | loss: 0.0645227\n",
      "\tspeed: 0.0141s/iter; left time: 214.7022s\n",
      "\titers: 3100, epoch: 7 | loss: 0.0159016\n",
      "\tspeed: 0.0142s/iter; left time: 214.6178s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1357405\n",
      "\tspeed: 0.0142s/iter; left time: 213.0751s\n",
      "\titers: 3300, epoch: 7 | loss: 0.0836961\n",
      "\tspeed: 0.0142s/iter; left time: 211.6814s\n",
      "\titers: 3400, epoch: 7 | loss: 0.0317245\n",
      "\tspeed: 0.0142s/iter; left time: 210.1287s\n",
      "\titers: 3500, epoch: 7 | loss: 0.0339851\n",
      "\tspeed: 0.0142s/iter; left time: 208.7316s\n",
      "\titers: 3600, epoch: 7 | loss: 0.0481444\n",
      "\tspeed: 0.0142s/iter; left time: 206.9718s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1562287\n",
      "\tspeed: 0.0141s/iter; left time: 204.9113s\n",
      "\titers: 3800, epoch: 7 | loss: 0.0360895\n",
      "\tspeed: 0.0141s/iter; left time: 203.3681s\n",
      "\titers: 3900, epoch: 7 | loss: 0.0537680\n",
      "\tspeed: 0.0141s/iter; left time: 202.0439s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0523564\n",
      "\tspeed: 0.0141s/iter; left time: 200.8445s\n",
      "\titers: 4100, epoch: 7 | loss: 0.0516314\n",
      "\tspeed: 0.0141s/iter; left time: 199.3707s\n",
      "\titers: 4200, epoch: 7 | loss: 0.0277760\n",
      "\tspeed: 0.0141s/iter; left time: 197.8394s\n",
      "\titers: 4300, epoch: 7 | loss: 0.0250210\n",
      "\tspeed: 0.0141s/iter; left time: 196.5694s\n",
      "\titers: 4400, epoch: 7 | loss: 0.0224080\n",
      "\tspeed: 0.0141s/iter; left time: 195.2234s\n",
      "\titers: 4500, epoch: 7 | loss: 0.1080759\n",
      "\tspeed: 0.0141s/iter; left time: 193.9439s\n",
      "Epoch: 7 cost time: 64.87281966209412\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.0811083 Vali Loss: 0.0322779 Test Loss: 0.1185480\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0657060\n",
      "\tspeed: 0.1246s/iter; left time: 1689.8045s\n",
      "\titers: 200, epoch: 8 | loss: 0.0201314\n",
      "\tspeed: 0.0142s/iter; left time: 191.7111s\n",
      "\titers: 300, epoch: 8 | loss: 0.0966164\n",
      "\tspeed: 0.0142s/iter; left time: 189.5307s\n",
      "\titers: 400, epoch: 8 | loss: 0.0279561\n",
      "\tspeed: 0.0142s/iter; left time: 187.8909s\n",
      "\titers: 500, epoch: 8 | loss: 0.1616721\n",
      "\tspeed: 0.0142s/iter; left time: 186.5710s\n",
      "\titers: 600, epoch: 8 | loss: 0.0662517\n",
      "\tspeed: 0.0142s/iter; left time: 185.5426s\n",
      "\titers: 700, epoch: 8 | loss: 0.1377325\n",
      "\tspeed: 0.0142s/iter; left time: 184.4745s\n",
      "\titers: 800, epoch: 8 | loss: 0.1522241\n",
      "\tspeed: 0.0142s/iter; left time: 182.3767s\n",
      "\titers: 900, epoch: 8 | loss: 0.0788020\n",
      "\tspeed: 0.0142s/iter; left time: 181.1411s\n",
      "\titers: 1000, epoch: 8 | loss: 0.0438657\n",
      "\tspeed: 0.0142s/iter; left time: 179.7832s\n",
      "\titers: 1100, epoch: 8 | loss: 0.0929321\n",
      "\tspeed: 0.0142s/iter; left time: 178.0734s\n",
      "\titers: 1200, epoch: 8 | loss: 0.1001387\n",
      "\tspeed: 0.0141s/iter; left time: 175.6745s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3611340\n",
      "\tspeed: 0.0142s/iter; left time: 175.6031s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0340944\n",
      "\tspeed: 0.0142s/iter; left time: 174.2434s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0767449\n",
      "\tspeed: 0.0142s/iter; left time: 172.7005s\n",
      "\titers: 1600, epoch: 8 | loss: 0.1177462\n",
      "\tspeed: 0.0142s/iter; left time: 171.2595s\n",
      "\titers: 1700, epoch: 8 | loss: 0.0232060\n",
      "\tspeed: 0.0142s/iter; left time: 169.8232s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1104169\n",
      "\tspeed: 0.0142s/iter; left time: 168.4687s\n",
      "\titers: 1900, epoch: 8 | loss: 0.0837058\n",
      "\tspeed: 0.0142s/iter; left time: 167.6451s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0732847\n",
      "\tspeed: 0.0142s/iter; left time: 165.5904s\n",
      "\titers: 2100, epoch: 8 | loss: 0.0332393\n",
      "\tspeed: 0.0142s/iter; left time: 164.1890s\n",
      "\titers: 2200, epoch: 8 | loss: 0.0524722\n",
      "\tspeed: 0.0142s/iter; left time: 162.5641s\n",
      "\titers: 2300, epoch: 8 | loss: 0.0882096\n",
      "\tspeed: 0.0142s/iter; left time: 161.2359s\n",
      "\titers: 2400, epoch: 8 | loss: 0.0556097\n",
      "\tspeed: 0.0152s/iter; left time: 171.3726s\n",
      "\titers: 2500, epoch: 8 | loss: 0.0640761\n",
      "\tspeed: 0.0163s/iter; left time: 181.6721s\n",
      "\titers: 2600, epoch: 8 | loss: 0.0558760\n",
      "\tspeed: 0.0163s/iter; left time: 179.8753s\n",
      "\titers: 2700, epoch: 8 | loss: 0.0386700\n",
      "\tspeed: 0.0163s/iter; left time: 178.2749s\n",
      "\titers: 2800, epoch: 8 | loss: 0.0495511\n",
      "\tspeed: 0.0146s/iter; left time: 158.2605s\n",
      "\titers: 2900, epoch: 8 | loss: 0.0673175\n",
      "\tspeed: 0.0141s/iter; left time: 152.2752s\n",
      "\titers: 3000, epoch: 8 | loss: 0.2308802\n",
      "\tspeed: 0.0141s/iter; left time: 150.7943s\n",
      "\titers: 3100, epoch: 8 | loss: 0.0729538\n",
      "\tspeed: 0.0142s/iter; left time: 149.8683s\n",
      "\titers: 3200, epoch: 8 | loss: 0.1264117\n",
      "\tspeed: 0.0142s/iter; left time: 148.5401s\n",
      "\titers: 3300, epoch: 8 | loss: 0.0330453\n",
      "\tspeed: 0.0142s/iter; left time: 146.8685s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0589012\n",
      "\tspeed: 0.0142s/iter; left time: 145.4221s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1415039\n",
      "\tspeed: 0.0142s/iter; left time: 144.1994s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0246280\n",
      "\tspeed: 0.0142s/iter; left time: 142.9128s\n",
      "\titers: 3700, epoch: 8 | loss: 0.1685719\n",
      "\tspeed: 0.0142s/iter; left time: 141.2976s\n",
      "\titers: 3800, epoch: 8 | loss: 0.0821620\n",
      "\tspeed: 0.0141s/iter; left time: 139.5719s\n",
      "\titers: 3900, epoch: 8 | loss: 0.1032403\n",
      "\tspeed: 0.0141s/iter; left time: 137.9026s\n",
      "\titers: 4000, epoch: 8 | loss: 0.0444931\n",
      "\tspeed: 0.0141s/iter; left time: 136.4107s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0308703\n",
      "\tspeed: 0.0141s/iter; left time: 134.9156s\n",
      "\titers: 4200, epoch: 8 | loss: 0.0924846\n",
      "\tspeed: 0.0141s/iter; left time: 133.5911s\n",
      "\titers: 4300, epoch: 8 | loss: 0.0755704\n",
      "\tspeed: 0.0141s/iter; left time: 132.1069s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1292378\n",
      "\tspeed: 0.0141s/iter; left time: 130.7075s\n",
      "\titers: 4500, epoch: 8 | loss: 0.0997739\n",
      "\tspeed: 0.0141s/iter; left time: 129.3501s\n",
      "Epoch: 8 cost time: 65.56069803237915\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.0815460 Vali Loss: 0.0323766 Test Loss: 0.1183982\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0293664\n",
      "\tspeed: 0.1421s/iter; left time: 1280.6298s\n",
      "\titers: 200, epoch: 9 | loss: 0.0482778\n",
      "\tspeed: 0.0157s/iter; left time: 139.7882s\n",
      "\titers: 300, epoch: 9 | loss: 0.0501785\n",
      "\tspeed: 0.0157s/iter; left time: 138.1516s\n",
      "\titers: 400, epoch: 9 | loss: 0.0954156\n",
      "\tspeed: 0.0153s/iter; left time: 132.9800s\n",
      "\titers: 500, epoch: 9 | loss: 0.0585597\n",
      "\tspeed: 0.0142s/iter; left time: 122.3395s\n",
      "\titers: 600, epoch: 9 | loss: 0.0285075\n",
      "\tspeed: 0.0142s/iter; left time: 120.6564s\n",
      "\titers: 700, epoch: 9 | loss: 0.0853510\n",
      "\tspeed: 0.0143s/iter; left time: 120.0858s\n",
      "\titers: 800, epoch: 9 | loss: 0.0259688\n",
      "\tspeed: 0.0157s/iter; left time: 130.2769s\n",
      "\titers: 900, epoch: 9 | loss: 0.0522382\n",
      "\tspeed: 0.0155s/iter; left time: 126.9019s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0562925\n",
      "\tspeed: 0.0153s/iter; left time: 124.0171s\n",
      "\titers: 1100, epoch: 9 | loss: 0.0476762\n",
      "\tspeed: 0.0142s/iter; left time: 113.5456s\n",
      "\titers: 1200, epoch: 9 | loss: 0.2179950\n",
      "\tspeed: 0.0142s/iter; left time: 112.2031s\n",
      "\titers: 1300, epoch: 9 | loss: 0.0403127\n",
      "\tspeed: 0.0142s/iter; left time: 110.7620s\n",
      "\titers: 1400, epoch: 9 | loss: 0.2050841\n",
      "\tspeed: 0.0142s/iter; left time: 109.4679s\n",
      "\titers: 1500, epoch: 9 | loss: 0.0666581\n",
      "\tspeed: 0.0142s/iter; left time: 107.8525s\n",
      "\titers: 1600, epoch: 9 | loss: 0.0463549\n",
      "\tspeed: 0.0142s/iter; left time: 106.5933s\n",
      "\titers: 1700, epoch: 9 | loss: 0.0560047\n",
      "\tspeed: 0.0142s/iter; left time: 105.3467s\n",
      "\titers: 1800, epoch: 9 | loss: 0.0238055\n",
      "\tspeed: 0.0142s/iter; left time: 103.7537s\n",
      "\titers: 1900, epoch: 9 | loss: 0.0272220\n",
      "\tspeed: 0.0142s/iter; left time: 102.3597s\n",
      "\titers: 2000, epoch: 9 | loss: 0.0606964\n",
      "\tspeed: 0.0142s/iter; left time: 100.8950s\n",
      "\titers: 2100, epoch: 9 | loss: 0.0403759\n",
      "\tspeed: 0.0141s/iter; left time: 99.1104s\n",
      "\titers: 2200, epoch: 9 | loss: 0.1231108\n",
      "\tspeed: 0.0141s/iter; left time: 97.6912s\n",
      "\titers: 2300, epoch: 9 | loss: 0.0980869\n",
      "\tspeed: 0.0141s/iter; left time: 96.2922s\n",
      "\titers: 2400, epoch: 9 | loss: 0.0200456\n",
      "\tspeed: 0.0141s/iter; left time: 94.9361s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0402394\n",
      "\tspeed: 0.0141s/iter; left time: 93.5292s\n",
      "\titers: 2600, epoch: 9 | loss: 0.0541217\n",
      "\tspeed: 0.0142s/iter; left time: 92.1425s\n",
      "\titers: 2700, epoch: 9 | loss: 0.0437152\n",
      "\tspeed: 0.0141s/iter; left time: 90.6851s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1417057\n",
      "\tspeed: 0.0142s/iter; left time: 89.7340s\n",
      "\titers: 2900, epoch: 9 | loss: 0.0506555\n",
      "\tspeed: 0.0142s/iter; left time: 87.9703s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0674490\n",
      "\tspeed: 0.0142s/iter; left time: 86.5051s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0281215\n",
      "\tspeed: 0.0141s/iter; left time: 85.0199s\n",
      "\titers: 3200, epoch: 9 | loss: 0.0584420\n",
      "\tspeed: 0.0141s/iter; left time: 83.6390s\n",
      "\titers: 3300, epoch: 9 | loss: 0.1912641\n",
      "\tspeed: 0.0141s/iter; left time: 82.0677s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1001169\n",
      "\tspeed: 0.0141s/iter; left time: 80.7925s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0654745\n",
      "\tspeed: 0.0141s/iter; left time: 79.3790s\n",
      "\titers: 3600, epoch: 9 | loss: 0.0354777\n",
      "\tspeed: 0.0141s/iter; left time: 77.9600s\n",
      "\titers: 3700, epoch: 9 | loss: 0.0551891\n",
      "\tspeed: 0.0141s/iter; left time: 76.3912s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1408811\n",
      "\tspeed: 0.0141s/iter; left time: 74.9337s\n",
      "\titers: 3900, epoch: 9 | loss: 0.0372566\n",
      "\tspeed: 0.0142s/iter; left time: 73.8295s\n",
      "\titers: 4000, epoch: 9 | loss: 0.0778167\n",
      "\tspeed: 0.0142s/iter; left time: 72.4215s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0578893\n",
      "\tspeed: 0.0142s/iter; left time: 71.0481s\n",
      "\titers: 4200, epoch: 9 | loss: 0.0784755\n",
      "\tspeed: 0.0142s/iter; left time: 69.9721s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0469630\n",
      "\tspeed: 0.0142s/iter; left time: 68.2261s\n",
      "\titers: 4400, epoch: 9 | loss: 0.0373584\n",
      "\tspeed: 0.0142s/iter; left time: 66.7237s\n",
      "\titers: 4500, epoch: 9 | loss: 0.0430527\n",
      "\tspeed: 0.0142s/iter; left time: 65.3279s\n",
      "Epoch: 9 cost time: 65.74488925933838\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.0796217 Vali Loss: 0.0323268 Test Loss: 0.1184093\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0733128\n",
      "\tspeed: 0.1291s/iter; left time: 575.1001s\n",
      "\titers: 200, epoch: 10 | loss: 0.0629437\n",
      "\tspeed: 0.0157s/iter; left time: 68.4622s\n",
      "\titers: 300, epoch: 10 | loss: 0.0483739\n",
      "\tspeed: 0.0145s/iter; left time: 61.7807s\n",
      "\titers: 400, epoch: 10 | loss: 0.0603162\n",
      "\tspeed: 0.0142s/iter; left time: 58.9171s\n",
      "\titers: 500, epoch: 10 | loss: 0.0764358\n",
      "\tspeed: 0.0141s/iter; left time: 57.3778s\n",
      "\titers: 600, epoch: 10 | loss: 0.0786081\n",
      "\tspeed: 0.0142s/iter; left time: 55.9838s\n",
      "\titers: 700, epoch: 10 | loss: 0.0784134\n",
      "\tspeed: 0.0141s/iter; left time: 54.5231s\n",
      "\titers: 800, epoch: 10 | loss: 0.1773816\n",
      "\tspeed: 0.0141s/iter; left time: 53.0410s\n",
      "\titers: 900, epoch: 10 | loss: 0.0933874\n",
      "\tspeed: 0.0141s/iter; left time: 51.6166s\n",
      "\titers: 1000, epoch: 10 | loss: 0.0427456\n",
      "\tspeed: 0.0141s/iter; left time: 50.2200s\n",
      "\titers: 1100, epoch: 10 | loss: 0.0536260\n",
      "\tspeed: 0.0141s/iter; left time: 48.8102s\n",
      "\titers: 1200, epoch: 10 | loss: 0.0165244\n",
      "\tspeed: 0.0141s/iter; left time: 47.3970s\n",
      "\titers: 1300, epoch: 10 | loss: 0.0353528\n",
      "\tspeed: 0.0141s/iter; left time: 46.0064s\n",
      "\titers: 1400, epoch: 10 | loss: 0.0739544\n",
      "\tspeed: 0.0141s/iter; left time: 44.5839s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2546976\n",
      "\tspeed: 0.0142s/iter; left time: 43.3576s\n",
      "\titers: 1600, epoch: 10 | loss: 0.0906812\n",
      "\tspeed: 0.0142s/iter; left time: 41.8965s\n",
      "\titers: 1700, epoch: 10 | loss: 0.0189781\n",
      "\tspeed: 0.0142s/iter; left time: 40.5765s\n",
      "\titers: 1800, epoch: 10 | loss: 0.0682710\n",
      "\tspeed: 0.0142s/iter; left time: 39.0467s\n",
      "\titers: 1900, epoch: 10 | loss: 0.1210673\n",
      "\tspeed: 0.0142s/iter; left time: 37.6372s\n",
      "\titers: 2000, epoch: 10 | loss: 0.0348910\n",
      "\tspeed: 0.0141s/iter; left time: 36.0647s\n",
      "\titers: 2100, epoch: 10 | loss: 0.0489501\n",
      "\tspeed: 0.0141s/iter; left time: 34.6949s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0968293\n",
      "\tspeed: 0.0141s/iter; left time: 33.2735s\n",
      "\titers: 2300, epoch: 10 | loss: 0.0955870\n",
      "\tspeed: 0.0141s/iter; left time: 31.8641s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0490312\n",
      "\tspeed: 0.0141s/iter; left time: 30.4469s\n",
      "\titers: 2500, epoch: 10 | loss: 0.1872555\n",
      "\tspeed: 0.0141s/iter; left time: 29.0328s\n",
      "\titers: 2600, epoch: 10 | loss: 0.1037949\n",
      "\tspeed: 0.0141s/iter; left time: 27.6197s\n",
      "\titers: 2700, epoch: 10 | loss: 0.0563907\n",
      "\tspeed: 0.0141s/iter; left time: 26.1994s\n",
      "\titers: 2800, epoch: 10 | loss: 0.0732815\n",
      "\tspeed: 0.0141s/iter; left time: 24.8099s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0496599\n",
      "\tspeed: 0.0141s/iter; left time: 23.4289s\n",
      "\titers: 3000, epoch: 10 | loss: 0.0498861\n",
      "\tspeed: 0.0142s/iter; left time: 22.0197s\n",
      "\titers: 3100, epoch: 10 | loss: 0.0427479\n",
      "\tspeed: 0.0141s/iter; left time: 20.6006s\n",
      "\titers: 3200, epoch: 10 | loss: 0.0763449\n",
      "\tspeed: 0.0142s/iter; left time: 19.2027s\n",
      "\titers: 3300, epoch: 10 | loss: 0.0832836\n",
      "\tspeed: 0.0142s/iter; left time: 17.7746s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1477975\n",
      "\tspeed: 0.0142s/iter; left time: 16.3659s\n",
      "\titers: 3500, epoch: 10 | loss: 0.0301685\n",
      "\tspeed: 0.0142s/iter; left time: 14.9472s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1107130\n",
      "\tspeed: 0.0142s/iter; left time: 13.5319s\n",
      "\titers: 3700, epoch: 10 | loss: 0.0707099\n",
      "\tspeed: 0.0142s/iter; left time: 12.1141s\n",
      "\titers: 3800, epoch: 10 | loss: 0.0545765\n",
      "\tspeed: 0.0142s/iter; left time: 10.7022s\n",
      "\titers: 3900, epoch: 10 | loss: 0.0291679\n",
      "\tspeed: 0.0141s/iter; left time: 9.2567s\n",
      "\titers: 4000, epoch: 10 | loss: 0.0419642\n",
      "\tspeed: 0.0141s/iter; left time: 7.8389s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1203869\n",
      "\tspeed: 0.0141s/iter; left time: 6.4276s\n",
      "\titers: 4200, epoch: 10 | loss: 0.0402055\n",
      "\tspeed: 0.0141s/iter; left time: 5.0258s\n",
      "\titers: 4300, epoch: 10 | loss: 0.0329036\n",
      "\tspeed: 0.0141s/iter; left time: 3.6192s\n",
      "\titers: 4400, epoch: 10 | loss: 0.0533564\n",
      "\tspeed: 0.0142s/iter; left time: 2.2121s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1058719\n",
      "\tspeed: 0.0142s/iter; left time: 0.7937s\n",
      "Epoch: 10 cost time: 65.04561638832092\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.0799469 Vali Loss: 0.0322496 Test Loss: 0.1181910\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11835382133722305, mae:0.20844879746437073\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor wind_penetration --enc_in 2 --dec_in 2 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e847d06",
   "metadata": {},
   "source": [
    "## itransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84712a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ECL_168_24          Model:              iTransformer        \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       wind_forecast,solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ECL_168_24_iTransformer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2103636\n",
      "\tspeed: 0.0634s/iter; left time: 2881.8546s\n",
      "\titers: 200, epoch: 1 | loss: 0.1526220\n",
      "\tspeed: 0.0085s/iter; left time: 386.8605s\n",
      "\titers: 300, epoch: 1 | loss: 0.2778089\n",
      "\tspeed: 0.0084s/iter; left time: 381.1162s\n",
      "\titers: 400, epoch: 1 | loss: 0.2527145\n",
      "\tspeed: 0.0085s/iter; left time: 383.6382s\n",
      "\titers: 500, epoch: 1 | loss: 0.5637870\n",
      "\tspeed: 0.0085s/iter; left time: 385.1205s\n",
      "\titers: 600, epoch: 1 | loss: 0.3502654\n",
      "\tspeed: 0.0086s/iter; left time: 384.4626s\n",
      "\titers: 700, epoch: 1 | loss: 1.0057138\n",
      "\tspeed: 0.0085s/iter; left time: 383.0719s\n",
      "\titers: 800, epoch: 1 | loss: 0.3340968\n",
      "\tspeed: 0.0086s/iter; left time: 382.7048s\n",
      "\titers: 900, epoch: 1 | loss: 0.1276448\n",
      "\tspeed: 0.0085s/iter; left time: 381.5573s\n",
      "\titers: 1000, epoch: 1 | loss: 0.2887061\n",
      "\tspeed: 0.0086s/iter; left time: 381.3170s\n",
      "\titers: 1100, epoch: 1 | loss: 0.1314683\n",
      "\tspeed: 0.0085s/iter; left time: 379.2642s\n",
      "\titers: 1200, epoch: 1 | loss: 0.4744981\n",
      "\tspeed: 0.0085s/iter; left time: 376.9083s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2853941\n",
      "\tspeed: 0.0085s/iter; left time: 377.8901s\n",
      "\titers: 1400, epoch: 1 | loss: 0.6722029\n",
      "\tspeed: 0.0085s/iter; left time: 376.0235s\n",
      "\titers: 1500, epoch: 1 | loss: 0.6078541\n",
      "\tspeed: 0.0085s/iter; left time: 374.9655s\n",
      "\titers: 1600, epoch: 1 | loss: 0.3080738\n",
      "\tspeed: 0.0085s/iter; left time: 373.8819s\n",
      "\titers: 1700, epoch: 1 | loss: 0.7799222\n",
      "\tspeed: 0.0085s/iter; left time: 372.2811s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1313854\n",
      "\tspeed: 0.0085s/iter; left time: 372.1058s\n",
      "\titers: 1900, epoch: 1 | loss: 0.0979598\n",
      "\tspeed: 0.0085s/iter; left time: 371.4168s\n",
      "\titers: 2000, epoch: 1 | loss: 0.2441853\n",
      "\tspeed: 0.0085s/iter; left time: 371.2253s\n",
      "\titers: 2100, epoch: 1 | loss: 0.2121305\n",
      "\tspeed: 0.0085s/iter; left time: 370.3620s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1877423\n",
      "\tspeed: 0.0085s/iter; left time: 369.1409s\n",
      "\titers: 2300, epoch: 1 | loss: 0.1282475\n",
      "\tspeed: 0.0085s/iter; left time: 368.5343s\n",
      "\titers: 2400, epoch: 1 | loss: 0.5765216\n",
      "\tspeed: 0.0085s/iter; left time: 367.5761s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2859516\n",
      "\tspeed: 0.0085s/iter; left time: 367.0610s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1786971\n",
      "\tspeed: 0.0085s/iter; left time: 365.8541s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2935292\n",
      "\tspeed: 0.0085s/iter; left time: 365.2283s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1306611\n",
      "\tspeed: 0.0085s/iter; left time: 364.2947s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1606088\n",
      "\tspeed: 0.0085s/iter; left time: 363.6854s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0815352\n",
      "\tspeed: 0.0085s/iter; left time: 362.6739s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1625618\n",
      "\tspeed: 0.0085s/iter; left time: 360.9104s\n",
      "\titers: 3200, epoch: 1 | loss: 0.3012469\n",
      "\tspeed: 0.0085s/iter; left time: 358.8276s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1420634\n",
      "\tspeed: 0.0085s/iter; left time: 357.5717s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1972117\n",
      "\tspeed: 0.0085s/iter; left time: 356.6697s\n",
      "\titers: 3500, epoch: 1 | loss: 0.3285826\n",
      "\tspeed: 0.0085s/iter; left time: 355.9794s\n",
      "\titers: 3600, epoch: 1 | loss: 0.2442553\n",
      "\tspeed: 0.0085s/iter; left time: 354.8538s\n",
      "\titers: 3700, epoch: 1 | loss: 1.2099656\n",
      "\tspeed: 0.0085s/iter; left time: 354.2820s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1743254\n",
      "\tspeed: 0.0085s/iter; left time: 353.0146s\n",
      "\titers: 3900, epoch: 1 | loss: 0.4688432\n",
      "\tspeed: 0.0085s/iter; left time: 352.5407s\n",
      "\titers: 4000, epoch: 1 | loss: 0.3086050\n",
      "\tspeed: 0.0085s/iter; left time: 351.5808s\n",
      "\titers: 4100, epoch: 1 | loss: 0.4109266\n",
      "\tspeed: 0.0085s/iter; left time: 350.8095s\n",
      "\titers: 4200, epoch: 1 | loss: 0.5340919\n",
      "\tspeed: 0.0085s/iter; left time: 349.7021s\n",
      "\titers: 4300, epoch: 1 | loss: 0.5286514\n",
      "\tspeed: 0.0085s/iter; left time: 348.6860s\n",
      "\titers: 4400, epoch: 1 | loss: 0.4093086\n",
      "\tspeed: 0.0085s/iter; left time: 348.0163s\n",
      "\titers: 4500, epoch: 1 | loss: 0.2668141\n",
      "\tspeed: 0.0084s/iter; left time: 346.8339s\n",
      "Epoch: 1 cost time: 44.24169588088989\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.3581311 Vali Loss: 0.0818325 Test Loss: 0.2256551\n",
      "Validation loss decreased (inf --> 0.081832).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7141044\n",
      "\tspeed: 0.0928s/iter; left time: 3793.8749s\n",
      "\titers: 200, epoch: 2 | loss: 0.2472435\n",
      "\tspeed: 0.0086s/iter; left time: 349.6025s\n",
      "\titers: 300, epoch: 2 | loss: 0.5662681\n",
      "\tspeed: 0.0085s/iter; left time: 346.6906s\n",
      "\titers: 400, epoch: 2 | loss: 0.7846560\n",
      "\tspeed: 0.0085s/iter; left time: 344.0886s\n",
      "\titers: 500, epoch: 2 | loss: 0.1012177\n",
      "\tspeed: 0.0085s/iter; left time: 343.7286s\n",
      "\titers: 600, epoch: 2 | loss: 0.1605925\n",
      "\tspeed: 0.0085s/iter; left time: 341.8140s\n",
      "\titers: 700, epoch: 2 | loss: 0.2179363\n",
      "\tspeed: 0.0085s/iter; left time: 340.9746s\n",
      "\titers: 800, epoch: 2 | loss: 0.6472847\n",
      "\tspeed: 0.0085s/iter; left time: 341.0562s\n",
      "\titers: 900, epoch: 2 | loss: 1.3991312\n",
      "\tspeed: 0.0085s/iter; left time: 339.6563s\n",
      "\titers: 1000, epoch: 2 | loss: 0.3369337\n",
      "\tspeed: 0.0085s/iter; left time: 339.0140s\n",
      "\titers: 1100, epoch: 2 | loss: 0.8584646\n",
      "\tspeed: 0.0085s/iter; left time: 339.2711s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0457468\n",
      "\tspeed: 0.0085s/iter; left time: 338.3037s\n",
      "\titers: 1300, epoch: 2 | loss: 0.7485403\n",
      "\tspeed: 0.0085s/iter; left time: 337.2980s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0854510\n",
      "\tspeed: 0.0085s/iter; left time: 336.0501s\n",
      "\titers: 1500, epoch: 2 | loss: 0.3393508\n",
      "\tspeed: 0.0085s/iter; left time: 335.2318s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1935302\n",
      "\tspeed: 0.0085s/iter; left time: 334.1175s\n",
      "\titers: 1700, epoch: 2 | loss: 0.3101842\n",
      "\tspeed: 0.0085s/iter; left time: 333.3047s\n",
      "\titers: 1800, epoch: 2 | loss: 0.3368283\n",
      "\tspeed: 0.0085s/iter; left time: 332.3569s\n",
      "\titers: 1900, epoch: 2 | loss: 1.1919992\n",
      "\tspeed: 0.0085s/iter; left time: 332.2885s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1174635\n",
      "\tspeed: 0.0085s/iter; left time: 331.3775s\n",
      "\titers: 2100, epoch: 2 | loss: 0.3217760\n",
      "\tspeed: 0.0085s/iter; left time: 330.2731s\n",
      "\titers: 2200, epoch: 2 | loss: 0.6285651\n",
      "\tspeed: 0.0085s/iter; left time: 329.5855s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1613125\n",
      "\tspeed: 0.0085s/iter; left time: 328.4201s\n",
      "\titers: 2400, epoch: 2 | loss: 0.6370637\n",
      "\tspeed: 0.0085s/iter; left time: 327.9757s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1801037\n",
      "\tspeed: 0.0085s/iter; left time: 326.7485s\n",
      "\titers: 2600, epoch: 2 | loss: 0.0845225\n",
      "\tspeed: 0.0085s/iter; left time: 325.7908s\n",
      "\titers: 2700, epoch: 2 | loss: 0.4970979\n",
      "\tspeed: 0.0085s/iter; left time: 324.9122s\n",
      "\titers: 2800, epoch: 2 | loss: 0.3707435\n",
      "\tspeed: 0.0085s/iter; left time: 324.0959s\n",
      "\titers: 2900, epoch: 2 | loss: 0.2951661\n",
      "\tspeed: 0.0085s/iter; left time: 322.7769s\n",
      "\titers: 3000, epoch: 2 | loss: 0.2160418\n",
      "\tspeed: 0.0085s/iter; left time: 321.4943s\n",
      "\titers: 3100, epoch: 2 | loss: 0.2357634\n",
      "\tspeed: 0.0085s/iter; left time: 320.9184s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2150846\n",
      "\tspeed: 0.0085s/iter; left time: 319.6693s\n",
      "\titers: 3300, epoch: 2 | loss: 0.2096191\n",
      "\tspeed: 0.0085s/iter; left time: 318.9078s\n",
      "\titers: 3400, epoch: 2 | loss: 0.1026914\n",
      "\tspeed: 0.0085s/iter; left time: 318.1435s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0718991\n",
      "\tspeed: 0.0085s/iter; left time: 316.9661s\n",
      "\titers: 3600, epoch: 2 | loss: 0.2930056\n",
      "\tspeed: 0.0085s/iter; left time: 316.2234s\n",
      "\titers: 3700, epoch: 2 | loss: 0.4731576\n",
      "\tspeed: 0.0085s/iter; left time: 315.4986s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1726825\n",
      "\tspeed: 0.0085s/iter; left time: 314.8272s\n",
      "\titers: 3900, epoch: 2 | loss: 0.1431180\n",
      "\tspeed: 0.0085s/iter; left time: 313.9270s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2460089\n",
      "\tspeed: 0.0085s/iter; left time: 313.0147s\n",
      "\titers: 4100, epoch: 2 | loss: 0.7218358\n",
      "\tspeed: 0.0085s/iter; left time: 312.2666s\n",
      "\titers: 4200, epoch: 2 | loss: 1.0239336\n",
      "\tspeed: 0.0085s/iter; left time: 311.2934s\n",
      "\titers: 4300, epoch: 2 | loss: 0.9441840\n",
      "\tspeed: 0.0085s/iter; left time: 310.5577s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1433934\n",
      "\tspeed: 0.0085s/iter; left time: 309.5361s\n",
      "\titers: 4500, epoch: 2 | loss: 0.1280349\n",
      "\tspeed: 0.0085s/iter; left time: 308.6419s\n",
      "Epoch: 2 cost time: 38.83260917663574\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.3696259 Vali Loss: 0.0811268 Test Loss: 0.2258406\n",
      "Validation loss decreased (0.081832 --> 0.081127).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.5423855\n",
      "\tspeed: 0.0839s/iter; left time: 3048.0029s\n",
      "\titers: 200, epoch: 3 | loss: 0.2355412\n",
      "\tspeed: 0.0096s/iter; left time: 347.1974s\n",
      "\titers: 300, epoch: 3 | loss: 0.1801472\n",
      "\tspeed: 0.0087s/iter; left time: 313.7457s\n",
      "\titers: 400, epoch: 3 | loss: 0.4996371\n",
      "\tspeed: 0.0096s/iter; left time: 347.2613s\n",
      "\titers: 500, epoch: 3 | loss: 0.1311181\n",
      "\tspeed: 0.0096s/iter; left time: 345.2388s\n",
      "\titers: 600, epoch: 3 | loss: 0.5251988\n",
      "\tspeed: 0.0096s/iter; left time: 344.6149s\n",
      "\titers: 700, epoch: 3 | loss: 0.6188905\n",
      "\tspeed: 0.0091s/iter; left time: 326.3797s\n",
      "\titers: 800, epoch: 3 | loss: 0.1742778\n",
      "\tspeed: 0.0085s/iter; left time: 304.0928s\n",
      "\titers: 900, epoch: 3 | loss: 0.1947574\n",
      "\tspeed: 0.0085s/iter; left time: 302.8418s\n",
      "\titers: 1000, epoch: 3 | loss: 0.4071538\n",
      "\tspeed: 0.0085s/iter; left time: 301.5409s\n",
      "\titers: 1100, epoch: 3 | loss: 0.2743612\n",
      "\tspeed: 0.0085s/iter; left time: 300.8178s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1240702\n",
      "\tspeed: 0.0085s/iter; left time: 300.4125s\n",
      "\titers: 1300, epoch: 3 | loss: 0.5571436\n",
      "\tspeed: 0.0085s/iter; left time: 299.0498s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1324000\n",
      "\tspeed: 0.0085s/iter; left time: 298.4548s\n",
      "\titers: 1500, epoch: 3 | loss: 0.5855134\n",
      "\tspeed: 0.0085s/iter; left time: 297.2402s\n",
      "\titers: 1600, epoch: 3 | loss: 1.7183554\n",
      "\tspeed: 0.0085s/iter; left time: 296.1490s\n",
      "\titers: 1700, epoch: 3 | loss: 0.6524147\n",
      "\tspeed: 0.0085s/iter; left time: 294.8152s\n",
      "\titers: 1800, epoch: 3 | loss: 0.4751323\n",
      "\tspeed: 0.0090s/iter; left time: 310.1678s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0747035\n",
      "\tspeed: 0.0085s/iter; left time: 293.0633s\n",
      "\titers: 2000, epoch: 3 | loss: 0.7858260\n",
      "\tspeed: 0.0085s/iter; left time: 291.7533s\n",
      "\titers: 2100, epoch: 3 | loss: 0.4898502\n",
      "\tspeed: 0.0085s/iter; left time: 292.0886s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1862489\n",
      "\tspeed: 0.0085s/iter; left time: 291.2093s\n",
      "\titers: 2300, epoch: 3 | loss: 0.2776531\n",
      "\tspeed: 0.0085s/iter; left time: 290.3653s\n",
      "\titers: 2400, epoch: 3 | loss: 0.1356668\n",
      "\tspeed: 0.0085s/iter; left time: 288.7631s\n",
      "\titers: 2500, epoch: 3 | loss: 0.2625456\n",
      "\tspeed: 0.0085s/iter; left time: 287.3840s\n",
      "\titers: 2600, epoch: 3 | loss: 0.3117591\n",
      "\tspeed: 0.0085s/iter; left time: 286.5722s\n",
      "\titers: 2700, epoch: 3 | loss: 0.2088741\n",
      "\tspeed: 0.0085s/iter; left time: 285.3343s\n",
      "\titers: 2800, epoch: 3 | loss: 0.1879412\n",
      "\tspeed: 0.0085s/iter; left time: 284.5267s\n",
      "\titers: 2900, epoch: 3 | loss: 0.4854496\n",
      "\tspeed: 0.0085s/iter; left time: 284.0610s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0723606\n",
      "\tspeed: 0.0085s/iter; left time: 283.0460s\n",
      "\titers: 3100, epoch: 3 | loss: 0.2598383\n",
      "\tspeed: 0.0085s/iter; left time: 282.4591s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2181157\n",
      "\tspeed: 0.0085s/iter; left time: 281.4789s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1559997\n",
      "\tspeed: 0.0085s/iter; left time: 281.2292s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1095940\n",
      "\tspeed: 0.0085s/iter; left time: 280.1982s\n",
      "\titers: 3500, epoch: 3 | loss: 0.2090281\n",
      "\tspeed: 0.0085s/iter; left time: 278.9865s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1639639\n",
      "\tspeed: 0.0085s/iter; left time: 278.4616s\n",
      "\titers: 3700, epoch: 3 | loss: 0.8326415\n",
      "\tspeed: 0.0085s/iter; left time: 277.2122s\n",
      "\titers: 3800, epoch: 3 | loss: 0.3808741\n",
      "\tspeed: 0.0085s/iter; left time: 276.9241s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0649913\n",
      "\tspeed: 0.0085s/iter; left time: 275.6915s\n",
      "\titers: 4000, epoch: 3 | loss: 0.1983236\n",
      "\tspeed: 0.0085s/iter; left time: 275.0033s\n",
      "\titers: 4100, epoch: 3 | loss: 0.3600937\n",
      "\tspeed: 0.0085s/iter; left time: 273.9878s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1711910\n",
      "\tspeed: 0.0085s/iter; left time: 273.1701s\n",
      "\titers: 4300, epoch: 3 | loss: 0.7610893\n",
      "\tspeed: 0.0085s/iter; left time: 272.8160s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1482118\n",
      "\tspeed: 0.0085s/iter; left time: 271.5827s\n",
      "\titers: 4500, epoch: 3 | loss: 0.2940271\n",
      "\tspeed: 0.0085s/iter; left time: 270.9708s\n",
      "Epoch: 3 cost time: 39.537904024124146\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.3691812 Vali Loss: 0.0847732 Test Loss: 0.2277438\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1726302\n",
      "\tspeed: 0.0968s/iter; left time: 3075.4088s\n",
      "\titers: 200, epoch: 4 | loss: 0.1450074\n",
      "\tspeed: 0.0085s/iter; left time: 269.7340s\n",
      "\titers: 300, epoch: 4 | loss: 0.6110269\n",
      "\tspeed: 0.0085s/iter; left time: 269.3089s\n",
      "\titers: 400, epoch: 4 | loss: 0.1107795\n",
      "\tspeed: 0.0085s/iter; left time: 268.3424s\n",
      "\titers: 500, epoch: 4 | loss: 0.1894519\n",
      "\tspeed: 0.0085s/iter; left time: 266.0940s\n",
      "\titers: 600, epoch: 4 | loss: 0.2670875\n",
      "\tspeed: 0.0085s/iter; left time: 265.1444s\n",
      "\titers: 700, epoch: 4 | loss: 0.1727572\n",
      "\tspeed: 0.0085s/iter; left time: 263.5871s\n",
      "\titers: 800, epoch: 4 | loss: 0.3234941\n",
      "\tspeed: 0.0085s/iter; left time: 262.8287s\n",
      "\titers: 900, epoch: 4 | loss: 0.1636372\n",
      "\tspeed: 0.0085s/iter; left time: 263.0963s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1253236\n",
      "\tspeed: 0.0085s/iter; left time: 261.8406s\n",
      "\titers: 1100, epoch: 4 | loss: 0.5493514\n",
      "\tspeed: 0.0085s/iter; left time: 261.1090s\n",
      "\titers: 1200, epoch: 4 | loss: 0.3513231\n",
      "\tspeed: 0.0085s/iter; left time: 260.8179s\n",
      "\titers: 1300, epoch: 4 | loss: 0.1107037\n",
      "\tspeed: 0.0085s/iter; left time: 259.7121s\n",
      "\titers: 1400, epoch: 4 | loss: 0.1283333\n",
      "\tspeed: 0.0085s/iter; left time: 259.7574s\n",
      "\titers: 1500, epoch: 4 | loss: 0.3391559\n",
      "\tspeed: 0.0085s/iter; left time: 258.9170s\n",
      "\titers: 1600, epoch: 4 | loss: 0.3973590\n",
      "\tspeed: 0.0085s/iter; left time: 257.2849s\n",
      "\titers: 1700, epoch: 4 | loss: 0.2336947\n",
      "\tspeed: 0.0085s/iter; left time: 255.8969s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1389879\n",
      "\tspeed: 0.0085s/iter; left time: 255.1536s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0645918\n",
      "\tspeed: 0.0085s/iter; left time: 253.9180s\n",
      "\titers: 2000, epoch: 4 | loss: 0.3866605\n",
      "\tspeed: 0.0086s/iter; left time: 258.0001s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1372329\n",
      "\tspeed: 0.0090s/iter; left time: 267.8492s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0667068\n",
      "\tspeed: 0.0085s/iter; left time: 252.0091s\n",
      "\titers: 2300, epoch: 4 | loss: 0.6338818\n",
      "\tspeed: 0.0085s/iter; left time: 251.5600s\n",
      "\titers: 2400, epoch: 4 | loss: 0.4153701\n",
      "\tspeed: 0.0085s/iter; left time: 250.6033s\n",
      "\titers: 2500, epoch: 4 | loss: 0.3386826\n",
      "\tspeed: 0.0085s/iter; left time: 249.6839s\n",
      "\titers: 2600, epoch: 4 | loss: 0.4598467\n",
      "\tspeed: 0.0085s/iter; left time: 248.5942s\n",
      "\titers: 2700, epoch: 4 | loss: 1.0041169\n",
      "\tspeed: 0.0095s/iter; left time: 275.8285s\n",
      "\titers: 2800, epoch: 4 | loss: 0.2590079\n",
      "\tspeed: 0.0089s/iter; left time: 259.3711s\n",
      "\titers: 2900, epoch: 4 | loss: 1.2735337\n",
      "\tspeed: 0.0085s/iter; left time: 246.5678s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0561877\n",
      "\tspeed: 0.0085s/iter; left time: 246.2726s\n",
      "\titers: 3100, epoch: 4 | loss: 0.4776770\n",
      "\tspeed: 0.0085s/iter; left time: 245.0649s\n",
      "\titers: 3200, epoch: 4 | loss: 0.2442470\n",
      "\tspeed: 0.0085s/iter; left time: 244.0790s\n",
      "\titers: 3300, epoch: 4 | loss: 0.6392419\n",
      "\tspeed: 0.0085s/iter; left time: 242.8016s\n",
      "\titers: 3400, epoch: 4 | loss: 0.4887630\n",
      "\tspeed: 0.0085s/iter; left time: 242.0392s\n",
      "\titers: 3500, epoch: 4 | loss: 0.2691843\n",
      "\tspeed: 0.0085s/iter; left time: 241.1906s\n",
      "\titers: 3600, epoch: 4 | loss: 1.0079236\n",
      "\tspeed: 0.0085s/iter; left time: 240.3943s\n",
      "\titers: 3700, epoch: 4 | loss: 0.5341007\n",
      "\tspeed: 0.0085s/iter; left time: 240.2930s\n",
      "\titers: 3800, epoch: 4 | loss: 0.4707075\n",
      "\tspeed: 0.0085s/iter; left time: 240.1154s\n",
      "\titers: 3900, epoch: 4 | loss: 0.6779426\n",
      "\tspeed: 0.0086s/iter; left time: 239.2968s\n",
      "\titers: 4000, epoch: 4 | loss: 0.2258327\n",
      "\tspeed: 0.0086s/iter; left time: 238.4958s\n",
      "\titers: 4100, epoch: 4 | loss: 0.1965771\n",
      "\tspeed: 0.0086s/iter; left time: 237.7013s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1631685\n",
      "\tspeed: 0.0085s/iter; left time: 236.6708s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3382101\n",
      "\tspeed: 0.0085s/iter; left time: 235.8441s\n",
      "\titers: 4400, epoch: 4 | loss: 0.2682280\n",
      "\tspeed: 0.0086s/iter; left time: 235.0699s\n",
      "\titers: 4500, epoch: 4 | loss: 0.5583121\n",
      "\tspeed: 0.0086s/iter; left time: 234.2098s\n",
      "Epoch: 4 cost time: 39.14331245422363\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.3692504 Vali Loss: 0.0818083 Test Loss: 0.2252312\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2054538\n",
      "\tspeed: 0.0827s/iter; left time: 2251.3923s\n",
      "\titers: 200, epoch: 5 | loss: 0.5918301\n",
      "\tspeed: 0.0085s/iter; left time: 230.6258s\n",
      "\titers: 300, epoch: 5 | loss: 0.1883310\n",
      "\tspeed: 0.0085s/iter; left time: 230.3158s\n",
      "\titers: 400, epoch: 5 | loss: 0.4823936\n",
      "\tspeed: 0.0085s/iter; left time: 229.2751s\n",
      "\titers: 500, epoch: 5 | loss: 0.0817823\n",
      "\tspeed: 0.0085s/iter; left time: 228.7059s\n",
      "\titers: 600, epoch: 5 | loss: 0.1166541\n",
      "\tspeed: 0.0085s/iter; left time: 227.6663s\n",
      "\titers: 700, epoch: 5 | loss: 0.2581902\n",
      "\tspeed: 0.0085s/iter; left time: 226.6736s\n",
      "\titers: 800, epoch: 5 | loss: 0.1186440\n",
      "\tspeed: 0.0085s/iter; left time: 225.0135s\n",
      "\titers: 900, epoch: 5 | loss: 1.8594599\n",
      "\tspeed: 0.0085s/iter; left time: 224.3689s\n",
      "\titers: 1000, epoch: 5 | loss: 0.2306925\n",
      "\tspeed: 0.0085s/iter; left time: 224.3868s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0970003\n",
      "\tspeed: 0.0085s/iter; left time: 223.5027s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1546988\n",
      "\tspeed: 0.0085s/iter; left time: 222.3224s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0778553\n",
      "\tspeed: 0.0085s/iter; left time: 221.5472s\n",
      "\titers: 1400, epoch: 5 | loss: 0.2518651\n",
      "\tspeed: 0.0085s/iter; left time: 221.0425s\n",
      "\titers: 1500, epoch: 5 | loss: 0.4546174\n",
      "\tspeed: 0.0085s/iter; left time: 220.2399s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1203758\n",
      "\tspeed: 0.0085s/iter; left time: 219.2050s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0777377\n",
      "\tspeed: 0.0085s/iter; left time: 218.0280s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1681551\n",
      "\tspeed: 0.0085s/iter; left time: 216.8323s\n",
      "\titers: 1900, epoch: 5 | loss: 0.2062833\n",
      "\tspeed: 0.0085s/iter; left time: 216.1806s\n",
      "\titers: 2000, epoch: 5 | loss: 0.3984309\n",
      "\tspeed: 0.0085s/iter; left time: 215.2771s\n",
      "\titers: 2100, epoch: 5 | loss: 1.2790980\n",
      "\tspeed: 0.0085s/iter; left time: 214.5090s\n",
      "\titers: 2200, epoch: 5 | loss: 0.3377579\n",
      "\tspeed: 0.0085s/iter; left time: 213.7004s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1682958\n",
      "\tspeed: 0.0085s/iter; left time: 213.2249s\n",
      "\titers: 2400, epoch: 5 | loss: 0.5830517\n",
      "\tspeed: 0.0085s/iter; left time: 211.8791s\n",
      "\titers: 2500, epoch: 5 | loss: 0.3332847\n",
      "\tspeed: 0.0085s/iter; left time: 210.9083s\n",
      "\titers: 2600, epoch: 5 | loss: 0.2427735\n",
      "\tspeed: 0.0085s/iter; left time: 210.6639s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1968739\n",
      "\tspeed: 0.0085s/iter; left time: 209.7957s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0771185\n",
      "\tspeed: 0.0085s/iter; left time: 209.1302s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0568133\n",
      "\tspeed: 0.0085s/iter; left time: 207.8488s\n",
      "\titers: 3000, epoch: 5 | loss: 0.5402068\n",
      "\tspeed: 0.0085s/iter; left time: 207.1376s\n",
      "\titers: 3100, epoch: 5 | loss: 0.3770343\n",
      "\tspeed: 0.0085s/iter; left time: 206.2251s\n",
      "\titers: 3200, epoch: 5 | loss: 0.2596795\n",
      "\tspeed: 0.0085s/iter; left time: 205.3942s\n",
      "\titers: 3300, epoch: 5 | loss: 0.3372656\n",
      "\tspeed: 0.0085s/iter; left time: 204.4920s\n",
      "\titers: 3400, epoch: 5 | loss: 0.6327350\n",
      "\tspeed: 0.0085s/iter; left time: 203.6235s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0916862\n",
      "\tspeed: 0.0085s/iter; left time: 202.8669s\n",
      "\titers: 3600, epoch: 5 | loss: 0.3190820\n",
      "\tspeed: 0.0085s/iter; left time: 201.9844s\n",
      "\titers: 3700, epoch: 5 | loss: 0.9176798\n",
      "\tspeed: 0.0085s/iter; left time: 201.2789s\n",
      "\titers: 3800, epoch: 5 | loss: 2.1172018\n",
      "\tspeed: 0.0085s/iter; left time: 200.3218s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1604315\n",
      "\tspeed: 0.0085s/iter; left time: 199.2124s\n",
      "\titers: 4000, epoch: 5 | loss: 0.2382350\n",
      "\tspeed: 0.0085s/iter; left time: 198.4395s\n",
      "\titers: 4100, epoch: 5 | loss: 1.6425098\n",
      "\tspeed: 0.0085s/iter; left time: 197.6815s\n",
      "\titers: 4200, epoch: 5 | loss: 0.3289046\n",
      "\tspeed: 0.0085s/iter; left time: 196.8483s\n",
      "\titers: 4300, epoch: 5 | loss: 0.3854468\n",
      "\tspeed: 0.0085s/iter; left time: 196.0490s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0544247\n",
      "\tspeed: 0.0085s/iter; left time: 195.3008s\n",
      "\titers: 4500, epoch: 5 | loss: 0.2541011\n",
      "\tspeed: 0.0085s/iter; left time: 194.4253s\n",
      "Epoch: 5 cost time: 38.94926929473877\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.3643772 Vali Loss: 0.0803798 Test Loss: 0.2217115\n",
      "Validation loss decreased (0.081127 --> 0.080380).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1744394\n",
      "\tspeed: 0.0920s/iter; left time: 2086.5276s\n",
      "\titers: 200, epoch: 6 | loss: 0.1778533\n",
      "\tspeed: 0.0084s/iter; left time: 190.6490s\n",
      "\titers: 300, epoch: 6 | loss: 0.1063188\n",
      "\tspeed: 0.0085s/iter; left time: 190.3312s\n",
      "\titers: 400, epoch: 6 | loss: 0.5233405\n",
      "\tspeed: 0.0085s/iter; left time: 190.2236s\n",
      "\titers: 500, epoch: 6 | loss: 0.7409381\n",
      "\tspeed: 0.0085s/iter; left time: 189.3029s\n",
      "\titers: 600, epoch: 6 | loss: 0.6507596\n",
      "\tspeed: 0.0085s/iter; left time: 188.8027s\n",
      "\titers: 700, epoch: 6 | loss: 0.5230991\n",
      "\tspeed: 0.0085s/iter; left time: 187.3557s\n",
      "\titers: 800, epoch: 6 | loss: 0.3351856\n",
      "\tspeed: 0.0085s/iter; left time: 185.8222s\n",
      "\titers: 900, epoch: 6 | loss: 0.5751994\n",
      "\tspeed: 0.0085s/iter; left time: 186.0666s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1331269\n",
      "\tspeed: 0.0085s/iter; left time: 185.2882s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0716786\n",
      "\tspeed: 0.0085s/iter; left time: 184.6199s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0973399\n",
      "\tspeed: 0.0085s/iter; left time: 184.0153s\n",
      "\titers: 1300, epoch: 6 | loss: 0.1962102\n",
      "\tspeed: 0.0085s/iter; left time: 182.8500s\n",
      "\titers: 1400, epoch: 6 | loss: 0.9275684\n",
      "\tspeed: 0.0085s/iter; left time: 181.8168s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1199563\n",
      "\tspeed: 0.0085s/iter; left time: 181.4211s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1108541\n",
      "\tspeed: 0.0085s/iter; left time: 180.1830s\n",
      "\titers: 1700, epoch: 6 | loss: 0.1270185\n",
      "\tspeed: 0.0085s/iter; left time: 179.0966s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1581182\n",
      "\tspeed: 0.0085s/iter; left time: 178.1671s\n",
      "\titers: 1900, epoch: 6 | loss: 0.2400771\n",
      "\tspeed: 0.0085s/iter; left time: 177.1588s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0876718\n",
      "\tspeed: 0.0085s/iter; left time: 176.3624s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1621995\n",
      "\tspeed: 0.0085s/iter; left time: 175.4309s\n",
      "\titers: 2200, epoch: 6 | loss: 0.3156948\n",
      "\tspeed: 0.0085s/iter; left time: 174.3868s\n",
      "\titers: 2300, epoch: 6 | loss: 0.2743927\n",
      "\tspeed: 0.0085s/iter; left time: 173.2947s\n",
      "\titers: 2400, epoch: 6 | loss: 0.5358555\n",
      "\tspeed: 0.0085s/iter; left time: 172.8809s\n",
      "\titers: 2500, epoch: 6 | loss: 0.0997823\n",
      "\tspeed: 0.0085s/iter; left time: 172.2802s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1902538\n",
      "\tspeed: 0.0085s/iter; left time: 171.2075s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1097042\n",
      "\tspeed: 0.0085s/iter; left time: 170.9928s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1845888\n",
      "\tspeed: 0.0085s/iter; left time: 169.9701s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1897320\n",
      "\tspeed: 0.0085s/iter; left time: 169.0593s\n",
      "\titers: 3000, epoch: 6 | loss: 0.3470436\n",
      "\tspeed: 0.0085s/iter; left time: 167.7425s\n",
      "\titers: 3100, epoch: 6 | loss: 0.2628945\n",
      "\tspeed: 0.0085s/iter; left time: 166.7595s\n",
      "\titers: 3200, epoch: 6 | loss: 0.2011458\n",
      "\tspeed: 0.0085s/iter; left time: 166.0316s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1130733\n",
      "\tspeed: 0.0085s/iter; left time: 165.1550s\n",
      "\titers: 3400, epoch: 6 | loss: 0.2498953\n",
      "\tspeed: 0.0085s/iter; left time: 164.3990s\n",
      "\titers: 3500, epoch: 6 | loss: 0.2306883\n",
      "\tspeed: 0.0085s/iter; left time: 163.6407s\n",
      "\titers: 3600, epoch: 6 | loss: 0.3585397\n",
      "\tspeed: 0.0085s/iter; left time: 162.9314s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0640259\n",
      "\tspeed: 0.0085s/iter; left time: 161.8233s\n",
      "\titers: 3800, epoch: 6 | loss: 0.2253530\n",
      "\tspeed: 0.0085s/iter; left time: 160.8290s\n",
      "\titers: 3900, epoch: 6 | loss: 0.1660247\n",
      "\tspeed: 0.0085s/iter; left time: 159.9759s\n",
      "\titers: 4000, epoch: 6 | loss: 0.5811645\n",
      "\tspeed: 0.0085s/iter; left time: 159.3292s\n",
      "\titers: 4100, epoch: 6 | loss: 0.8561869\n",
      "\tspeed: 0.0085s/iter; left time: 158.6189s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0629766\n",
      "\tspeed: 0.0085s/iter; left time: 157.6897s\n",
      "\titers: 4300, epoch: 6 | loss: 0.6145167\n",
      "\tspeed: 0.0085s/iter; left time: 156.7897s\n",
      "\titers: 4400, epoch: 6 | loss: 0.3977197\n",
      "\tspeed: 0.0085s/iter; left time: 156.0011s\n",
      "\titers: 4500, epoch: 6 | loss: 0.1151444\n",
      "\tspeed: 0.0085s/iter; left time: 155.1214s\n",
      "Epoch: 6 cost time: 38.88314199447632\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.3567400 Vali Loss: 0.0796837 Test Loss: 0.2150302\n",
      "Validation loss decreased (0.080380 --> 0.079684).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1260723\n",
      "\tspeed: 0.0844s/iter; left time: 1528.5945s\n",
      "\titers: 200, epoch: 7 | loss: 0.3422048\n",
      "\tspeed: 0.0095s/iter; left time: 171.1441s\n",
      "\titers: 300, epoch: 7 | loss: 0.2423344\n",
      "\tspeed: 0.0096s/iter; left time: 171.4969s\n",
      "\titers: 400, epoch: 7 | loss: 0.2698630\n",
      "\tspeed: 0.0091s/iter; left time: 161.9373s\n",
      "\titers: 500, epoch: 7 | loss: 0.4063784\n",
      "\tspeed: 0.0096s/iter; left time: 169.5814s\n",
      "\titers: 600, epoch: 7 | loss: 0.1038354\n",
      "\tspeed: 0.0096s/iter; left time: 168.5813s\n",
      "\titers: 700, epoch: 7 | loss: 0.0381593\n",
      "\tspeed: 0.0096s/iter; left time: 167.7124s\n",
      "\titers: 800, epoch: 7 | loss: 0.6872820\n",
      "\tspeed: 0.0096s/iter; left time: 166.4729s\n",
      "\titers: 900, epoch: 7 | loss: 0.3331341\n",
      "\tspeed: 0.0096s/iter; left time: 165.4352s\n",
      "\titers: 1000, epoch: 7 | loss: 0.2604868\n",
      "\tspeed: 0.0096s/iter; left time: 164.7634s\n",
      "\titers: 1100, epoch: 7 | loss: 0.6499478\n",
      "\tspeed: 0.0096s/iter; left time: 163.6844s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1676849\n",
      "\tspeed: 0.0096s/iter; left time: 162.7554s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1481747\n",
      "\tspeed: 0.0096s/iter; left time: 161.6998s\n",
      "\titers: 1400, epoch: 7 | loss: 0.1086910\n",
      "\tspeed: 0.0092s/iter; left time: 155.1429s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0521184\n",
      "\tspeed: 0.0085s/iter; left time: 142.4982s\n",
      "\titers: 1600, epoch: 7 | loss: 0.2151195\n",
      "\tspeed: 0.0085s/iter; left time: 141.9984s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1232320\n",
      "\tspeed: 0.0085s/iter; left time: 140.9529s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0879264\n",
      "\tspeed: 0.0085s/iter; left time: 139.8898s\n",
      "\titers: 1900, epoch: 7 | loss: 0.3222919\n",
      "\tspeed: 0.0085s/iter; left time: 138.3099s\n",
      "\titers: 2000, epoch: 7 | loss: 0.1676738\n",
      "\tspeed: 0.0085s/iter; left time: 137.3833s\n",
      "\titers: 2100, epoch: 7 | loss: 0.3332797\n",
      "\tspeed: 0.0085s/iter; left time: 136.6199s\n",
      "\titers: 2200, epoch: 7 | loss: 0.4431286\n",
      "\tspeed: 0.0085s/iter; left time: 135.6470s\n",
      "\titers: 2300, epoch: 7 | loss: 0.3479152\n",
      "\tspeed: 0.0085s/iter; left time: 134.8627s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0786910\n",
      "\tspeed: 0.0085s/iter; left time: 134.3950s\n",
      "\titers: 2500, epoch: 7 | loss: 0.7114350\n",
      "\tspeed: 0.0085s/iter; left time: 133.8753s\n",
      "\titers: 2600, epoch: 7 | loss: 0.2770947\n",
      "\tspeed: 0.0085s/iter; left time: 133.2304s\n",
      "\titers: 2700, epoch: 7 | loss: 1.3899950\n",
      "\tspeed: 0.0085s/iter; left time: 132.1625s\n",
      "\titers: 2800, epoch: 7 | loss: 0.3093673\n",
      "\tspeed: 0.0085s/iter; left time: 131.1390s\n",
      "\titers: 2900, epoch: 7 | loss: 0.3573375\n",
      "\tspeed: 0.0085s/iter; left time: 130.0467s\n",
      "\titers: 3000, epoch: 7 | loss: 1.3553827\n",
      "\tspeed: 0.0085s/iter; left time: 129.2511s\n",
      "\titers: 3100, epoch: 7 | loss: 0.9637730\n",
      "\tspeed: 0.0085s/iter; left time: 128.5891s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1639081\n",
      "\tspeed: 0.0085s/iter; left time: 127.5333s\n",
      "\titers: 3300, epoch: 7 | loss: 0.2767189\n",
      "\tspeed: 0.0085s/iter; left time: 126.9143s\n",
      "\titers: 3400, epoch: 7 | loss: 0.5554568\n",
      "\tspeed: 0.0085s/iter; left time: 125.9237s\n",
      "\titers: 3500, epoch: 7 | loss: 0.3522708\n",
      "\tspeed: 0.0085s/iter; left time: 125.1218s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1604578\n",
      "\tspeed: 0.0085s/iter; left time: 124.2255s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1454408\n",
      "\tspeed: 0.0085s/iter; left time: 123.4204s\n",
      "\titers: 3800, epoch: 7 | loss: 0.4921008\n",
      "\tspeed: 0.0085s/iter; left time: 122.5857s\n",
      "\titers: 3900, epoch: 7 | loss: 0.2321703\n",
      "\tspeed: 0.0085s/iter; left time: 121.7565s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0632307\n",
      "\tspeed: 0.0085s/iter; left time: 121.1175s\n",
      "\titers: 4100, epoch: 7 | loss: 0.2107163\n",
      "\tspeed: 0.0085s/iter; left time: 120.1096s\n",
      "\titers: 4200, epoch: 7 | loss: 0.3400186\n",
      "\tspeed: 0.0085s/iter; left time: 119.2945s\n",
      "\titers: 4300, epoch: 7 | loss: 0.2137961\n",
      "\tspeed: 0.0085s/iter; left time: 118.2997s\n",
      "\titers: 4400, epoch: 7 | loss: 0.2478939\n",
      "\tspeed: 0.0085s/iter; left time: 117.4289s\n",
      "\titers: 4500, epoch: 7 | loss: 0.6820049\n",
      "\tspeed: 0.0085s/iter; left time: 116.6939s\n",
      "Epoch: 7 cost time: 40.31760215759277\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.3499839 Vali Loss: 0.0783355 Test Loss: 0.2132642\n",
      "Validation loss decreased (0.079684 --> 0.078335).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3468518\n",
      "\tspeed: 0.0848s/iter; left time: 1150.5359s\n",
      "\titers: 200, epoch: 8 | loss: 0.0824095\n",
      "\tspeed: 0.0096s/iter; left time: 129.3764s\n",
      "\titers: 300, epoch: 8 | loss: 0.8576295\n",
      "\tspeed: 0.0097s/iter; left time: 128.9893s\n",
      "\titers: 400, epoch: 8 | loss: 0.1147616\n",
      "\tspeed: 0.0096s/iter; left time: 127.5441s\n",
      "\titers: 500, epoch: 8 | loss: 0.1049002\n",
      "\tspeed: 0.0096s/iter; left time: 126.5528s\n",
      "\titers: 600, epoch: 8 | loss: 0.1920644\n",
      "\tspeed: 0.0093s/iter; left time: 120.8737s\n",
      "\titers: 700, epoch: 8 | loss: 0.5502290\n",
      "\tspeed: 0.0095s/iter; left time: 123.0045s\n",
      "\titers: 800, epoch: 8 | loss: 0.2783914\n",
      "\tspeed: 0.0096s/iter; left time: 123.4559s\n",
      "\titers: 900, epoch: 8 | loss: 0.8084475\n",
      "\tspeed: 0.0096s/iter; left time: 122.5107s\n",
      "\titers: 1000, epoch: 8 | loss: 0.4749475\n",
      "\tspeed: 0.0096s/iter; left time: 121.4354s\n",
      "\titers: 1100, epoch: 8 | loss: 0.1868846\n",
      "\tspeed: 0.0096s/iter; left time: 120.5335s\n",
      "\titers: 1200, epoch: 8 | loss: 0.3624789\n",
      "\tspeed: 0.0096s/iter; left time: 119.4502s\n",
      "\titers: 1300, epoch: 8 | loss: 0.3275598\n",
      "\tspeed: 0.0096s/iter; left time: 118.5135s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0827592\n",
      "\tspeed: 0.0096s/iter; left time: 117.5824s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0927368\n",
      "\tspeed: 0.0096s/iter; left time: 116.7236s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0778024\n",
      "\tspeed: 0.0096s/iter; left time: 115.7242s\n",
      "\titers: 1700, epoch: 8 | loss: 0.1219416\n",
      "\tspeed: 0.0088s/iter; left time: 105.2347s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1100543\n",
      "\tspeed: 0.0086s/iter; left time: 101.5733s\n",
      "\titers: 1900, epoch: 8 | loss: 0.5072039\n",
      "\tspeed: 0.0086s/iter; left time: 100.8459s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0742809\n",
      "\tspeed: 0.0086s/iter; left time: 100.1000s\n",
      "\titers: 2100, epoch: 8 | loss: 0.1992416\n",
      "\tspeed: 0.0086s/iter; left time: 99.0449s\n",
      "\titers: 2200, epoch: 8 | loss: 0.5305715\n",
      "\tspeed: 0.0086s/iter; left time: 98.1785s\n",
      "\titers: 2300, epoch: 8 | loss: 0.2887413\n",
      "\tspeed: 0.0086s/iter; left time: 97.3701s\n",
      "\titers: 2400, epoch: 8 | loss: 0.2313470\n",
      "\tspeed: 0.0086s/iter; left time: 96.6114s\n",
      "\titers: 2500, epoch: 8 | loss: 0.3581892\n",
      "\tspeed: 0.0086s/iter; left time: 95.6793s\n",
      "\titers: 2600, epoch: 8 | loss: 0.5293508\n",
      "\tspeed: 0.0086s/iter; left time: 94.8643s\n",
      "\titers: 2700, epoch: 8 | loss: 0.4615031\n",
      "\tspeed: 0.0086s/iter; left time: 94.0524s\n",
      "\titers: 2800, epoch: 8 | loss: 0.2954328\n",
      "\tspeed: 0.0086s/iter; left time: 93.2285s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1376414\n",
      "\tspeed: 0.0086s/iter; left time: 92.3870s\n",
      "\titers: 3000, epoch: 8 | loss: 0.4681146\n",
      "\tspeed: 0.0086s/iter; left time: 91.4334s\n",
      "\titers: 3100, epoch: 8 | loss: 0.3963293\n",
      "\tspeed: 0.0086s/iter; left time: 90.5008s\n",
      "\titers: 3200, epoch: 8 | loss: 0.3330022\n",
      "\tspeed: 0.0085s/iter; left time: 89.3871s\n",
      "\titers: 3300, epoch: 8 | loss: 0.6739374\n",
      "\tspeed: 0.0085s/iter; left time: 88.5001s\n",
      "\titers: 3400, epoch: 8 | loss: 0.1765505\n",
      "\tspeed: 0.0085s/iter; left time: 87.7061s\n",
      "\titers: 3500, epoch: 8 | loss: 0.1454501\n",
      "\tspeed: 0.0085s/iter; left time: 86.6536s\n",
      "\titers: 3600, epoch: 8 | loss: 0.1841447\n",
      "\tspeed: 0.0085s/iter; left time: 85.8943s\n",
      "\titers: 3700, epoch: 8 | loss: 0.3918714\n",
      "\tspeed: 0.0085s/iter; left time: 84.9919s\n",
      "\titers: 3800, epoch: 8 | loss: 0.3667171\n",
      "\tspeed: 0.0085s/iter; left time: 84.1149s\n",
      "\titers: 3900, epoch: 8 | loss: 0.6150095\n",
      "\tspeed: 0.0086s/iter; left time: 83.9741s\n",
      "\titers: 4000, epoch: 8 | loss: 0.3038320\n",
      "\tspeed: 0.0086s/iter; left time: 83.1652s\n",
      "\titers: 4100, epoch: 8 | loss: 0.3567804\n",
      "\tspeed: 0.0086s/iter; left time: 82.3867s\n",
      "\titers: 4200, epoch: 8 | loss: 0.2689722\n",
      "\tspeed: 0.0086s/iter; left time: 81.0241s\n",
      "\titers: 4300, epoch: 8 | loss: 0.3965724\n",
      "\tspeed: 0.0086s/iter; left time: 80.5867s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1137659\n",
      "\tspeed: 0.0086s/iter; left time: 79.7798s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1857314\n",
      "\tspeed: 0.0086s/iter; left time: 78.8778s\n",
      "Epoch: 8 cost time: 40.86046051979065\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.3450886 Vali Loss: 0.0781559 Test Loss: 0.2127231\n",
      "Validation loss decreased (0.078335 --> 0.078156).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2478625\n",
      "\tspeed: 0.0876s/iter; left time: 789.0704s\n",
      "\titers: 200, epoch: 9 | loss: 0.1852779\n",
      "\tspeed: 0.0085s/iter; left time: 75.5099s\n",
      "\titers: 300, epoch: 9 | loss: 0.2439672\n",
      "\tspeed: 0.0085s/iter; left time: 74.6598s\n",
      "\titers: 400, epoch: 9 | loss: 0.1480718\n",
      "\tspeed: 0.0085s/iter; left time: 73.8221s\n",
      "\titers: 500, epoch: 9 | loss: 0.1906754\n",
      "\tspeed: 0.0085s/iter; left time: 72.9459s\n",
      "\titers: 600, epoch: 9 | loss: 1.2851256\n",
      "\tspeed: 0.0085s/iter; left time: 72.0394s\n",
      "\titers: 700, epoch: 9 | loss: 0.2951940\n",
      "\tspeed: 0.0085s/iter; left time: 71.1997s\n",
      "\titers: 800, epoch: 9 | loss: 0.3814849\n",
      "\tspeed: 0.0085s/iter; left time: 70.3902s\n",
      "\titers: 900, epoch: 9 | loss: 0.1095856\n",
      "\tspeed: 0.0085s/iter; left time: 69.5479s\n",
      "\titers: 1000, epoch: 9 | loss: 0.0387023\n",
      "\tspeed: 0.0085s/iter; left time: 68.7525s\n",
      "\titers: 1100, epoch: 9 | loss: 0.2834535\n",
      "\tspeed: 0.0085s/iter; left time: 67.8919s\n",
      "\titers: 1200, epoch: 9 | loss: 0.1935112\n",
      "\tspeed: 0.0085s/iter; left time: 67.0214s\n",
      "\titers: 1300, epoch: 9 | loss: 0.1592519\n",
      "\tspeed: 0.0085s/iter; left time: 66.2105s\n",
      "\titers: 1400, epoch: 9 | loss: 0.8336234\n",
      "\tspeed: 0.0085s/iter; left time: 65.3140s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1033291\n",
      "\tspeed: 0.0085s/iter; left time: 64.5264s\n",
      "\titers: 1600, epoch: 9 | loss: 0.2130067\n",
      "\tspeed: 0.0085s/iter; left time: 63.7115s\n",
      "\titers: 1700, epoch: 9 | loss: 0.5369913\n",
      "\tspeed: 0.0085s/iter; left time: 63.0326s\n",
      "\titers: 1800, epoch: 9 | loss: 0.4273871\n",
      "\tspeed: 0.0085s/iter; left time: 62.1472s\n",
      "\titers: 1900, epoch: 9 | loss: 0.6841606\n",
      "\tspeed: 0.0085s/iter; left time: 61.3648s\n",
      "\titers: 2000, epoch: 9 | loss: 0.1474643\n",
      "\tspeed: 0.0085s/iter; left time: 60.5055s\n",
      "\titers: 2100, epoch: 9 | loss: 0.1444540\n",
      "\tspeed: 0.0085s/iter; left time: 59.5545s\n",
      "\titers: 2200, epoch: 9 | loss: 0.2883842\n",
      "\tspeed: 0.0085s/iter; left time: 58.7491s\n",
      "\titers: 2300, epoch: 9 | loss: 0.5188645\n",
      "\tspeed: 0.0085s/iter; left time: 57.7792s\n",
      "\titers: 2400, epoch: 9 | loss: 0.3257151\n",
      "\tspeed: 0.0085s/iter; left time: 56.9822s\n",
      "\titers: 2500, epoch: 9 | loss: 0.4844060\n",
      "\tspeed: 0.0085s/iter; left time: 56.0005s\n",
      "\titers: 2600, epoch: 9 | loss: 0.3746232\n",
      "\tspeed: 0.0085s/iter; left time: 55.1673s\n",
      "\titers: 2700, epoch: 9 | loss: 0.1811689\n",
      "\tspeed: 0.0085s/iter; left time: 54.3399s\n",
      "\titers: 2800, epoch: 9 | loss: 0.3841013\n",
      "\tspeed: 0.0085s/iter; left time: 53.4609s\n",
      "\titers: 2900, epoch: 9 | loss: 0.3901861\n",
      "\tspeed: 0.0085s/iter; left time: 52.6767s\n",
      "\titers: 3000, epoch: 9 | loss: 0.0597960\n",
      "\tspeed: 0.0085s/iter; left time: 51.8312s\n",
      "\titers: 3100, epoch: 9 | loss: 0.0912568\n",
      "\tspeed: 0.0085s/iter; left time: 50.9552s\n",
      "\titers: 3200, epoch: 9 | loss: 1.2539756\n",
      "\tspeed: 0.0085s/iter; left time: 50.0833s\n",
      "\titers: 3300, epoch: 9 | loss: 0.2274053\n",
      "\tspeed: 0.0084s/iter; left time: 48.9538s\n",
      "\titers: 3400, epoch: 9 | loss: 0.1437091\n",
      "\tspeed: 0.0084s/iter; left time: 48.1539s\n",
      "\titers: 3500, epoch: 9 | loss: 0.2313495\n",
      "\tspeed: 0.0084s/iter; left time: 47.2881s\n",
      "\titers: 3600, epoch: 9 | loss: 0.1805789\n",
      "\tspeed: 0.0084s/iter; left time: 46.4223s\n",
      "\titers: 3700, epoch: 9 | loss: 0.3143694\n",
      "\tspeed: 0.0084s/iter; left time: 45.6502s\n",
      "\titers: 3800, epoch: 9 | loss: 0.1234201\n",
      "\tspeed: 0.0084s/iter; left time: 44.7957s\n",
      "\titers: 3900, epoch: 9 | loss: 0.1760708\n",
      "\tspeed: 0.0084s/iter; left time: 43.9283s\n",
      "\titers: 4000, epoch: 9 | loss: 0.6956560\n",
      "\tspeed: 0.0084s/iter; left time: 43.1347s\n",
      "\titers: 4100, epoch: 9 | loss: 0.0695774\n",
      "\tspeed: 0.0084s/iter; left time: 42.2564s\n",
      "\titers: 4200, epoch: 9 | loss: 0.4371313\n",
      "\tspeed: 0.0084s/iter; left time: 41.4080s\n",
      "\titers: 4300, epoch: 9 | loss: 0.0926625\n",
      "\tspeed: 0.0084s/iter; left time: 40.5747s\n",
      "\titers: 4400, epoch: 9 | loss: 0.2554281\n",
      "\tspeed: 0.0084s/iter; left time: 39.7639s\n",
      "\titers: 4500, epoch: 9 | loss: 0.2971209\n",
      "\tspeed: 0.0085s/iter; left time: 38.9939s\n",
      "Epoch: 9 cost time: 38.7683801651001\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.3434750 Vali Loss: 0.0779607 Test Loss: 0.2122361\n",
      "Validation loss decreased (0.078156 --> 0.077961).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1038536\n",
      "\tspeed: 0.0838s/iter; left time: 373.3879s\n",
      "\titers: 200, epoch: 10 | loss: 0.6163888\n",
      "\tspeed: 0.0085s/iter; left time: 37.1192s\n",
      "\titers: 300, epoch: 10 | loss: 0.3705229\n",
      "\tspeed: 0.0096s/iter; left time: 40.7484s\n",
      "\titers: 400, epoch: 10 | loss: 0.4647194\n",
      "\tspeed: 0.0096s/iter; left time: 39.8314s\n",
      "\titers: 500, epoch: 10 | loss: 0.3538288\n",
      "\tspeed: 0.0096s/iter; left time: 38.9028s\n",
      "\titers: 600, epoch: 10 | loss: 0.2103175\n",
      "\tspeed: 0.0096s/iter; left time: 37.9190s\n",
      "\titers: 700, epoch: 10 | loss: 0.3154380\n",
      "\tspeed: 0.0096s/iter; left time: 36.9602s\n",
      "\titers: 800, epoch: 10 | loss: 0.2661445\n",
      "\tspeed: 0.0096s/iter; left time: 36.0379s\n",
      "\titers: 900, epoch: 10 | loss: 0.6157222\n",
      "\tspeed: 0.0096s/iter; left time: 35.0541s\n",
      "\titers: 1000, epoch: 10 | loss: 0.2732908\n",
      "\tspeed: 0.0096s/iter; left time: 34.1181s\n",
      "\titers: 1100, epoch: 10 | loss: 1.2737303\n",
      "\tspeed: 0.0096s/iter; left time: 33.2065s\n",
      "\titers: 1200, epoch: 10 | loss: 0.3241143\n",
      "\tspeed: 0.0096s/iter; left time: 32.1632s\n",
      "\titers: 1300, epoch: 10 | loss: 0.6051875\n",
      "\tspeed: 0.0096s/iter; left time: 31.1966s\n",
      "\titers: 1400, epoch: 10 | loss: 0.2598170\n",
      "\tspeed: 0.0094s/iter; left time: 29.6552s\n",
      "\titers: 1500, epoch: 10 | loss: 0.1277136\n",
      "\tspeed: 0.0096s/iter; left time: 29.2573s\n",
      "\titers: 1600, epoch: 10 | loss: 0.9735979\n",
      "\tspeed: 0.0088s/iter; left time: 25.9481s\n",
      "\titers: 1700, epoch: 10 | loss: 0.5617949\n",
      "\tspeed: 0.0085s/iter; left time: 24.3297s\n",
      "\titers: 1800, epoch: 10 | loss: 0.2649891\n",
      "\tspeed: 0.0085s/iter; left time: 23.4910s\n",
      "\titers: 1900, epoch: 10 | loss: 0.2409108\n",
      "\tspeed: 0.0085s/iter; left time: 22.6190s\n",
      "\titers: 2000, epoch: 10 | loss: 0.1823963\n",
      "\tspeed: 0.0085s/iter; left time: 21.7596s\n",
      "\titers: 2100, epoch: 10 | loss: 0.4008706\n",
      "\tspeed: 0.0085s/iter; left time: 20.9520s\n",
      "\titers: 2200, epoch: 10 | loss: 0.0906084\n",
      "\tspeed: 0.0085s/iter; left time: 20.0797s\n",
      "\titers: 2300, epoch: 10 | loss: 0.5032734\n",
      "\tspeed: 0.0085s/iter; left time: 19.2269s\n",
      "\titers: 2400, epoch: 10 | loss: 0.0851501\n",
      "\tspeed: 0.0085s/iter; left time: 18.3617s\n",
      "\titers: 2500, epoch: 10 | loss: 0.3408281\n",
      "\tspeed: 0.0085s/iter; left time: 17.5231s\n",
      "\titers: 2600, epoch: 10 | loss: 0.4343184\n",
      "\tspeed: 0.0085s/iter; left time: 16.6693s\n",
      "\titers: 2700, epoch: 10 | loss: 0.7878484\n",
      "\tspeed: 0.0085s/iter; left time: 15.8012s\n",
      "\titers: 2800, epoch: 10 | loss: 0.3845364\n",
      "\tspeed: 0.0085s/iter; left time: 14.9607s\n",
      "\titers: 2900, epoch: 10 | loss: 0.0985750\n",
      "\tspeed: 0.0085s/iter; left time: 14.1161s\n",
      "\titers: 3000, epoch: 10 | loss: 0.4477319\n",
      "\tspeed: 0.0085s/iter; left time: 13.2703s\n",
      "\titers: 3100, epoch: 10 | loss: 0.1462037\n",
      "\tspeed: 0.0085s/iter; left time: 12.4097s\n",
      "\titers: 3200, epoch: 10 | loss: 0.8068828\n",
      "\tspeed: 0.0085s/iter; left time: 11.5612s\n",
      "\titers: 3300, epoch: 10 | loss: 0.1173328\n",
      "\tspeed: 0.0085s/iter; left time: 10.7031s\n",
      "\titers: 3400, epoch: 10 | loss: 0.3716115\n",
      "\tspeed: 0.0085s/iter; left time: 9.8431s\n",
      "\titers: 3500, epoch: 10 | loss: 0.5468601\n",
      "\tspeed: 0.0085s/iter; left time: 8.9990s\n",
      "\titers: 3600, epoch: 10 | loss: 0.1782919\n",
      "\tspeed: 0.0085s/iter; left time: 8.1428s\n",
      "\titers: 3700, epoch: 10 | loss: 0.9409708\n",
      "\tspeed: 0.0085s/iter; left time: 7.2909s\n",
      "\titers: 3800, epoch: 10 | loss: 0.5819483\n",
      "\tspeed: 0.0085s/iter; left time: 6.4424s\n",
      "\titers: 3900, epoch: 10 | loss: 0.2686372\n",
      "\tspeed: 0.0085s/iter; left time: 5.5940s\n",
      "\titers: 4000, epoch: 10 | loss: 0.3376606\n",
      "\tspeed: 0.0085s/iter; left time: 4.7390s\n",
      "\titers: 4100, epoch: 10 | loss: 0.2328157\n",
      "\tspeed: 0.0085s/iter; left time: 3.8858s\n",
      "\titers: 4200, epoch: 10 | loss: 0.5652817\n",
      "\tspeed: 0.0085s/iter; left time: 3.0360s\n",
      "\titers: 4300, epoch: 10 | loss: 0.2658421\n",
      "\tspeed: 0.0085s/iter; left time: 2.1807s\n",
      "\titers: 4400, epoch: 10 | loss: 0.3950286\n",
      "\tspeed: 0.0085s/iter; left time: 1.3303s\n",
      "\titers: 4500, epoch: 10 | loss: 0.1956220\n",
      "\tspeed: 0.0085s/iter; left time: 0.4777s\n",
      "Epoch: 10 cost time: 40.51251673698425\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.3424026 Vali Loss: 0.0777742 Test Loss: 0.2116806\n",
      "Validation loss decreased (0.077961 --> 0.077774).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      ">>>>>>>testing : long_term_forecast_ECL_168_24_iTransformer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.21172630786895752, mae:0.3019779622554779\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/iTransformer.sh --features MS --predictor wind_forecast,solar_forecast,total_load --enc_in 4 --dec_in 4 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a638c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ECL_168_24          Model:              iTransformer        \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           M                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ECL_168_24_iTransformer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2241614\n",
      "\tspeed: 0.0212s/iter; left time: 961.3869s\n",
      "\titers: 200, epoch: 1 | loss: 0.1792063\n",
      "\tspeed: 0.0096s/iter; left time: 437.0346s\n",
      "\titers: 300, epoch: 1 | loss: 0.1798367\n",
      "\tspeed: 0.0096s/iter; left time: 434.9782s\n",
      "\titers: 400, epoch: 1 | loss: 0.2911470\n",
      "\tspeed: 0.0097s/iter; left time: 436.8776s\n",
      "\titers: 500, epoch: 1 | loss: 0.3014578\n",
      "\tspeed: 0.0098s/iter; left time: 440.1412s\n",
      "\titers: 600, epoch: 1 | loss: 0.1471915\n",
      "\tspeed: 0.0097s/iter; left time: 438.1248s\n",
      "\titers: 700, epoch: 1 | loss: 0.2378498\n",
      "\tspeed: 0.0098s/iter; left time: 438.9267s\n",
      "\titers: 800, epoch: 1 | loss: 0.3594911\n",
      "\tspeed: 0.0096s/iter; left time: 429.5502s\n",
      "\titers: 900, epoch: 1 | loss: 0.3193202\n",
      "\tspeed: 0.0085s/iter; left time: 381.6519s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1653630\n",
      "\tspeed: 0.0085s/iter; left time: 380.2539s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2851420\n",
      "\tspeed: 0.0085s/iter; left time: 376.5671s\n",
      "\titers: 1200, epoch: 1 | loss: 0.1605131\n",
      "\tspeed: 0.0085s/iter; left time: 375.7860s\n",
      "\titers: 1300, epoch: 1 | loss: 0.2293263\n",
      "\tspeed: 0.0085s/iter; left time: 375.2911s\n",
      "\titers: 1400, epoch: 1 | loss: 0.1720209\n",
      "\tspeed: 0.0086s/iter; left time: 377.5038s\n",
      "\titers: 1500, epoch: 1 | loss: 0.2566654\n",
      "\tspeed: 0.0085s/iter; left time: 373.7661s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2138451\n",
      "\tspeed: 0.0085s/iter; left time: 372.2899s\n",
      "\titers: 1700, epoch: 1 | loss: 0.6374821\n",
      "\tspeed: 0.0085s/iter; left time: 372.0722s\n",
      "\titers: 1800, epoch: 1 | loss: 0.3609071\n",
      "\tspeed: 0.0085s/iter; left time: 371.3495s\n",
      "\titers: 1900, epoch: 1 | loss: 0.4036647\n",
      "\tspeed: 0.0085s/iter; left time: 370.6705s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1730296\n",
      "\tspeed: 0.0085s/iter; left time: 369.8208s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1566689\n",
      "\tspeed: 0.0085s/iter; left time: 367.6859s\n",
      "\titers: 2200, epoch: 1 | loss: 0.3125052\n",
      "\tspeed: 0.0084s/iter; left time: 365.9864s\n",
      "\titers: 2300, epoch: 1 | loss: 0.2061826\n",
      "\tspeed: 0.0085s/iter; left time: 365.4751s\n",
      "\titers: 2400, epoch: 1 | loss: 0.3049240\n",
      "\tspeed: 0.0084s/iter; left time: 364.3892s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2751483\n",
      "\tspeed: 0.0085s/iter; left time: 364.4496s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1798863\n",
      "\tspeed: 0.0085s/iter; left time: 363.4388s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2436396\n",
      "\tspeed: 0.0085s/iter; left time: 362.2874s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1914671\n",
      "\tspeed: 0.0085s/iter; left time: 362.2836s\n",
      "\titers: 2900, epoch: 1 | loss: 0.2136963\n",
      "\tspeed: 0.0085s/iter; left time: 361.6013s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2107868\n",
      "\tspeed: 0.0085s/iter; left time: 360.9788s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1793881\n",
      "\tspeed: 0.0085s/iter; left time: 359.4324s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2956303\n",
      "\tspeed: 0.0085s/iter; left time: 358.6981s\n",
      "\titers: 3300, epoch: 1 | loss: 0.2615293\n",
      "\tspeed: 0.0085s/iter; left time: 358.4978s\n",
      "\titers: 3400, epoch: 1 | loss: 0.1944521\n",
      "\tspeed: 0.0085s/iter; left time: 357.7825s\n",
      "\titers: 3500, epoch: 1 | loss: 0.3102437\n",
      "\tspeed: 0.0085s/iter; left time: 357.2502s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1786989\n",
      "\tspeed: 0.0085s/iter; left time: 355.5710s\n",
      "\titers: 3700, epoch: 1 | loss: 0.3629580\n",
      "\tspeed: 0.0085s/iter; left time: 354.6372s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1909334\n",
      "\tspeed: 0.0085s/iter; left time: 353.1684s\n",
      "\titers: 3900, epoch: 1 | loss: 0.4171514\n",
      "\tspeed: 0.0085s/iter; left time: 352.2963s\n",
      "\titers: 4000, epoch: 1 | loss: 0.4231155\n",
      "\tspeed: 0.0085s/iter; left time: 351.9533s\n",
      "\titers: 4100, epoch: 1 | loss: 0.2664711\n",
      "\tspeed: 0.0085s/iter; left time: 351.4602s\n",
      "\titers: 4200, epoch: 1 | loss: 0.4095914\n",
      "\tspeed: 0.0085s/iter; left time: 350.5851s\n",
      "\titers: 4300, epoch: 1 | loss: 0.5434147\n",
      "\tspeed: 0.0085s/iter; left time: 349.0670s\n",
      "\titers: 4400, epoch: 1 | loss: 0.4080334\n",
      "\tspeed: 0.0085s/iter; left time: 348.3482s\n",
      "\titers: 4500, epoch: 1 | loss: 0.3819733\n",
      "\tspeed: 0.0085s/iter; left time: 347.5399s\n",
      "Epoch: 1 cost time: 40.764588594436646\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.2780278 Vali Loss: 0.3330063 Test Loss: 0.3366209\n",
      "Validation loss decreased (inf --> 0.333006).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6245738\n",
      "\tspeed: 0.0837s/iter; left time: 3424.9795s\n",
      "\titers: 200, epoch: 2 | loss: 0.4651510\n",
      "\tspeed: 0.0085s/iter; left time: 346.8017s\n",
      "\titers: 300, epoch: 2 | loss: 0.5010759\n",
      "\tspeed: 0.0085s/iter; left time: 346.8072s\n",
      "\titers: 400, epoch: 2 | loss: 0.8643945\n",
      "\tspeed: 0.0085s/iter; left time: 345.6836s\n",
      "\titers: 500, epoch: 2 | loss: 0.3864439\n",
      "\tspeed: 0.0085s/iter; left time: 343.8067s\n",
      "\titers: 600, epoch: 2 | loss: 0.3323029\n",
      "\tspeed: 0.0085s/iter; left time: 341.7290s\n",
      "\titers: 700, epoch: 2 | loss: 0.2927831\n",
      "\tspeed: 0.0085s/iter; left time: 342.2349s\n",
      "\titers: 800, epoch: 2 | loss: 0.2813072\n",
      "\tspeed: 0.0085s/iter; left time: 340.6219s\n",
      "\titers: 900, epoch: 2 | loss: 0.5668566\n",
      "\tspeed: 0.0085s/iter; left time: 340.0544s\n",
      "\titers: 1000, epoch: 2 | loss: 0.3632416\n",
      "\tspeed: 0.0085s/iter; left time: 341.8751s\n",
      "\titers: 1100, epoch: 2 | loss: 0.6874008\n",
      "\tspeed: 0.0086s/iter; left time: 341.1779s\n",
      "\titers: 1200, epoch: 2 | loss: 0.2220950\n",
      "\tspeed: 0.0085s/iter; left time: 339.8574s\n",
      "\titers: 1300, epoch: 2 | loss: 0.4157014\n",
      "\tspeed: 0.0085s/iter; left time: 339.0700s\n",
      "\titers: 1400, epoch: 2 | loss: 0.2874042\n",
      "\tspeed: 0.0085s/iter; left time: 338.5258s\n",
      "\titers: 1500, epoch: 2 | loss: 0.2067491\n",
      "\tspeed: 0.0085s/iter; left time: 337.2784s\n",
      "\titers: 1600, epoch: 2 | loss: 0.4477712\n",
      "\tspeed: 0.0086s/iter; left time: 336.9739s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2612788\n",
      "\tspeed: 0.0085s/iter; left time: 335.9047s\n",
      "\titers: 1800, epoch: 2 | loss: 0.4098328\n",
      "\tspeed: 0.0085s/iter; left time: 334.8344s\n",
      "\titers: 1900, epoch: 2 | loss: 0.4667552\n",
      "\tspeed: 0.0085s/iter; left time: 333.9890s\n",
      "\titers: 2000, epoch: 2 | loss: 0.2322747\n",
      "\tspeed: 0.0085s/iter; left time: 332.3866s\n",
      "\titers: 2100, epoch: 2 | loss: 0.3132963\n",
      "\tspeed: 0.0085s/iter; left time: 331.4604s\n",
      "\titers: 2200, epoch: 2 | loss: 0.3116567\n",
      "\tspeed: 0.0085s/iter; left time: 330.6197s\n",
      "\titers: 2300, epoch: 2 | loss: 0.2367532\n",
      "\tspeed: 0.0085s/iter; left time: 329.8642s\n",
      "\titers: 2400, epoch: 2 | loss: 0.2413727\n",
      "\tspeed: 0.0085s/iter; left time: 328.9446s\n",
      "\titers: 2500, epoch: 2 | loss: 0.3342181\n",
      "\tspeed: 0.0085s/iter; left time: 328.2330s\n",
      "\titers: 2600, epoch: 2 | loss: 0.4166484\n",
      "\tspeed: 0.0085s/iter; left time: 327.8503s\n",
      "\titers: 2700, epoch: 2 | loss: 0.3495356\n",
      "\tspeed: 0.0085s/iter; left time: 327.3488s\n",
      "\titers: 2800, epoch: 2 | loss: 0.4721654\n",
      "\tspeed: 0.0085s/iter; left time: 325.3883s\n",
      "\titers: 2900, epoch: 2 | loss: 0.3624722\n",
      "\tspeed: 0.0085s/iter; left time: 323.3258s\n",
      "\titers: 3000, epoch: 2 | loss: 0.4769716\n",
      "\tspeed: 0.0085s/iter; left time: 322.3264s\n",
      "\titers: 3100, epoch: 2 | loss: 0.3404779\n",
      "\tspeed: 0.0085s/iter; left time: 321.4264s\n",
      "\titers: 3200, epoch: 2 | loss: 0.4717264\n",
      "\tspeed: 0.0085s/iter; left time: 320.2731s\n",
      "\titers: 3300, epoch: 2 | loss: 0.3340876\n",
      "\tspeed: 0.0085s/iter; left time: 319.2487s\n",
      "\titers: 3400, epoch: 2 | loss: 0.4051788\n",
      "\tspeed: 0.0085s/iter; left time: 318.5250s\n",
      "\titers: 3500, epoch: 2 | loss: 0.2260297\n",
      "\tspeed: 0.0085s/iter; left time: 317.6990s\n",
      "\titers: 3600, epoch: 2 | loss: 0.3337849\n",
      "\tspeed: 0.0085s/iter; left time: 316.0847s\n",
      "\titers: 3700, epoch: 2 | loss: 0.3661969\n",
      "\tspeed: 0.0085s/iter; left time: 315.9278s\n",
      "\titers: 3800, epoch: 2 | loss: 0.2352746\n",
      "\tspeed: 0.0085s/iter; left time: 316.4122s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2043985\n",
      "\tspeed: 0.0085s/iter; left time: 316.0577s\n",
      "\titers: 4000, epoch: 2 | loss: 0.3892453\n",
      "\tspeed: 0.0085s/iter; left time: 315.9718s\n",
      "\titers: 4100, epoch: 2 | loss: 0.5416948\n",
      "\tspeed: 0.0085s/iter; left time: 314.8262s\n",
      "\titers: 4200, epoch: 2 | loss: 0.4707323\n",
      "\tspeed: 0.0085s/iter; left time: 313.9629s\n",
      "\titers: 4300, epoch: 2 | loss: 0.5595652\n",
      "\tspeed: 0.0085s/iter; left time: 311.6500s\n",
      "\titers: 4400, epoch: 2 | loss: 0.3524894\n",
      "\tspeed: 0.0085s/iter; left time: 310.3537s\n",
      "\titers: 4500, epoch: 2 | loss: 0.2885393\n",
      "\tspeed: 0.0085s/iter; left time: 309.2300s\n",
      "Epoch: 2 cost time: 39.07692909240723\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.3658391 Vali Loss: 0.3480150 Test Loss: 0.3338448\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3531258\n",
      "\tspeed: 0.0819s/iter; left time: 2976.4165s\n",
      "\titers: 200, epoch: 3 | loss: 0.2928278\n",
      "\tspeed: 0.0085s/iter; left time: 307.3056s\n",
      "\titers: 300, epoch: 3 | loss: 0.3613070\n",
      "\tspeed: 0.0085s/iter; left time: 306.6701s\n",
      "\titers: 400, epoch: 3 | loss: 0.4210917\n",
      "\tspeed: 0.0085s/iter; left time: 305.5312s\n",
      "\titers: 500, epoch: 3 | loss: 0.3250217\n",
      "\tspeed: 0.0085s/iter; left time: 304.6294s\n",
      "\titers: 600, epoch: 3 | loss: 0.3303069\n",
      "\tspeed: 0.0085s/iter; left time: 303.5618s\n",
      "\titers: 700, epoch: 3 | loss: 0.3545547\n",
      "\tspeed: 0.0085s/iter; left time: 302.8192s\n",
      "\titers: 800, epoch: 3 | loss: 0.3661764\n",
      "\tspeed: 0.0085s/iter; left time: 302.0199s\n",
      "\titers: 900, epoch: 3 | loss: 0.2305752\n",
      "\tspeed: 0.0085s/iter; left time: 301.2183s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2495212\n",
      "\tspeed: 0.0085s/iter; left time: 300.5291s\n",
      "\titers: 1100, epoch: 3 | loss: 0.2884787\n",
      "\tspeed: 0.0085s/iter; left time: 299.6997s\n",
      "\titers: 1200, epoch: 3 | loss: 0.3681187\n",
      "\tspeed: 0.0085s/iter; left time: 299.1558s\n",
      "\titers: 1300, epoch: 3 | loss: 0.2911685\n",
      "\tspeed: 0.0085s/iter; left time: 298.0156s\n",
      "\titers: 1400, epoch: 3 | loss: 0.1957270\n",
      "\tspeed: 0.0085s/iter; left time: 296.9261s\n",
      "\titers: 1500, epoch: 3 | loss: 0.3681212\n",
      "\tspeed: 0.0085s/iter; left time: 296.7212s\n",
      "\titers: 1600, epoch: 3 | loss: 0.5019466\n",
      "\tspeed: 0.0085s/iter; left time: 295.3922s\n",
      "\titers: 1700, epoch: 3 | loss: 0.3174651\n",
      "\tspeed: 0.0085s/iter; left time: 294.5347s\n",
      "\titers: 1800, epoch: 3 | loss: 0.3663503\n",
      "\tspeed: 0.0085s/iter; left time: 293.4831s\n",
      "\titers: 1900, epoch: 3 | loss: 0.3525609\n",
      "\tspeed: 0.0085s/iter; left time: 292.6891s\n",
      "\titers: 2000, epoch: 3 | loss: 0.5099677\n",
      "\tspeed: 0.0085s/iter; left time: 292.1739s\n",
      "\titers: 2100, epoch: 3 | loss: 0.2850403\n",
      "\tspeed: 0.0085s/iter; left time: 290.9841s\n",
      "\titers: 2200, epoch: 3 | loss: 0.2427642\n",
      "\tspeed: 0.0085s/iter; left time: 290.0273s\n",
      "\titers: 2300, epoch: 3 | loss: 0.2084839\n",
      "\tspeed: 0.0085s/iter; left time: 289.7195s\n",
      "\titers: 2400, epoch: 3 | loss: 0.2162770\n",
      "\tspeed: 0.0085s/iter; left time: 288.8571s\n",
      "\titers: 2500, epoch: 3 | loss: 0.4720088\n",
      "\tspeed: 0.0085s/iter; left time: 288.0011s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2765365\n",
      "\tspeed: 0.0085s/iter; left time: 287.2012s\n",
      "\titers: 2700, epoch: 3 | loss: 0.2210817\n",
      "\tspeed: 0.0085s/iter; left time: 286.4535s\n",
      "\titers: 2800, epoch: 3 | loss: 0.2899006\n",
      "\tspeed: 0.0085s/iter; left time: 285.4085s\n",
      "\titers: 2900, epoch: 3 | loss: 0.3352346\n",
      "\tspeed: 0.0085s/iter; left time: 284.4906s\n",
      "\titers: 3000, epoch: 3 | loss: 0.1694722\n",
      "\tspeed: 0.0085s/iter; left time: 284.3776s\n",
      "\titers: 3100, epoch: 3 | loss: 0.2834550\n",
      "\tspeed: 0.0085s/iter; left time: 283.3373s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2778382\n",
      "\tspeed: 0.0085s/iter; left time: 282.6040s\n",
      "\titers: 3300, epoch: 3 | loss: 0.1959724\n",
      "\tspeed: 0.0085s/iter; left time: 281.9978s\n",
      "\titers: 3400, epoch: 3 | loss: 0.3400921\n",
      "\tspeed: 0.0085s/iter; left time: 280.9903s\n",
      "\titers: 3500, epoch: 3 | loss: 0.3424990\n",
      "\tspeed: 0.0085s/iter; left time: 279.8884s\n",
      "\titers: 3600, epoch: 3 | loss: 0.4769270\n",
      "\tspeed: 0.0085s/iter; left time: 279.1990s\n",
      "\titers: 3700, epoch: 3 | loss: 0.3788019\n",
      "\tspeed: 0.0085s/iter; left time: 278.2503s\n",
      "\titers: 3800, epoch: 3 | loss: 0.2326287\n",
      "\tspeed: 0.0085s/iter; left time: 277.7269s\n",
      "\titers: 3900, epoch: 3 | loss: 0.1332599\n",
      "\tspeed: 0.0085s/iter; left time: 277.1664s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2264705\n",
      "\tspeed: 0.0085s/iter; left time: 277.1234s\n",
      "\titers: 4100, epoch: 3 | loss: 0.4057423\n",
      "\tspeed: 0.0085s/iter; left time: 275.7496s\n",
      "\titers: 4200, epoch: 3 | loss: 0.3274596\n",
      "\tspeed: 0.0085s/iter; left time: 274.5077s\n",
      "\titers: 4300, epoch: 3 | loss: 0.3522335\n",
      "\tspeed: 0.0085s/iter; left time: 274.3327s\n",
      "\titers: 4400, epoch: 3 | loss: 0.2726797\n",
      "\tspeed: 0.0085s/iter; left time: 273.1293s\n",
      "\titers: 4500, epoch: 3 | loss: 0.2450479\n",
      "\tspeed: 0.0085s/iter; left time: 272.4020s\n",
      "Epoch: 3 cost time: 38.87376308441162\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.3254563 Vali Loss: 0.3043223 Test Loss: 0.2965533\n",
      "Validation loss decreased (0.333006 --> 0.304322).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2672214\n",
      "\tspeed: 0.0873s/iter; left time: 2774.7155s\n",
      "\titers: 200, epoch: 4 | loss: 0.2100296\n",
      "\tspeed: 0.0086s/iter; left time: 271.1564s\n",
      "\titers: 300, epoch: 4 | loss: 0.2402493\n",
      "\tspeed: 0.0085s/iter; left time: 269.9510s\n",
      "\titers: 400, epoch: 4 | loss: 0.2243991\n",
      "\tspeed: 0.0085s/iter; left time: 268.6198s\n",
      "\titers: 500, epoch: 4 | loss: 0.2835959\n",
      "\tspeed: 0.0085s/iter; left time: 267.6836s\n",
      "\titers: 600, epoch: 4 | loss: 0.2871449\n",
      "\tspeed: 0.0085s/iter; left time: 267.2581s\n",
      "\titers: 700, epoch: 4 | loss: 0.2562026\n",
      "\tspeed: 0.0085s/iter; left time: 266.0497s\n",
      "\titers: 800, epoch: 4 | loss: 0.2062834\n",
      "\tspeed: 0.0085s/iter; left time: 265.3105s\n",
      "\titers: 900, epoch: 4 | loss: 0.2882165\n",
      "\tspeed: 0.0085s/iter; left time: 264.7506s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1881247\n",
      "\tspeed: 0.0086s/iter; left time: 264.1155s\n",
      "\titers: 1100, epoch: 4 | loss: 0.2653435\n",
      "\tspeed: 0.0085s/iter; left time: 262.9871s\n",
      "\titers: 1200, epoch: 4 | loss: 0.2397024\n",
      "\tspeed: 0.0085s/iter; left time: 261.9948s\n",
      "\titers: 1300, epoch: 4 | loss: 0.2843825\n",
      "\tspeed: 0.0085s/iter; left time: 261.2907s\n",
      "\titers: 1400, epoch: 4 | loss: 0.2631069\n",
      "\tspeed: 0.0085s/iter; left time: 260.6084s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2374099\n",
      "\tspeed: 0.0085s/iter; left time: 259.4212s\n",
      "\titers: 1600, epoch: 4 | loss: 0.3452167\n",
      "\tspeed: 0.0085s/iter; left time: 258.3209s\n",
      "\titers: 1700, epoch: 4 | loss: 0.2496905\n",
      "\tspeed: 0.0085s/iter; left time: 257.8749s\n",
      "\titers: 1800, epoch: 4 | loss: 0.2759593\n",
      "\tspeed: 0.0085s/iter; left time: 256.8139s\n",
      "\titers: 1900, epoch: 4 | loss: 0.2528344\n",
      "\tspeed: 0.0085s/iter; left time: 256.1894s\n",
      "\titers: 2000, epoch: 4 | loss: 0.2148574\n",
      "\tspeed: 0.0085s/iter; left time: 254.9601s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1299111\n",
      "\tspeed: 0.0085s/iter; left time: 253.9173s\n",
      "\titers: 2200, epoch: 4 | loss: 0.2689496\n",
      "\tspeed: 0.0085s/iter; left time: 253.1964s\n",
      "\titers: 2300, epoch: 4 | loss: 0.2641087\n",
      "\tspeed: 0.0085s/iter; left time: 252.2062s\n",
      "\titers: 2400, epoch: 4 | loss: 0.2070006\n",
      "\tspeed: 0.0085s/iter; left time: 251.3140s\n",
      "\titers: 2500, epoch: 4 | loss: 0.2277232\n",
      "\tspeed: 0.0085s/iter; left time: 250.3333s\n",
      "\titers: 2600, epoch: 4 | loss: 0.2937957\n",
      "\tspeed: 0.0085s/iter; left time: 249.6236s\n",
      "\titers: 2700, epoch: 4 | loss: 0.7050196\n",
      "\tspeed: 0.0085s/iter; left time: 248.7648s\n",
      "\titers: 2800, epoch: 4 | loss: 0.4500842\n",
      "\tspeed: 0.0085s/iter; left time: 247.9413s\n",
      "\titers: 2900, epoch: 4 | loss: 0.3341343\n",
      "\tspeed: 0.0085s/iter; left time: 247.2704s\n",
      "\titers: 3000, epoch: 4 | loss: 0.2267563\n",
      "\tspeed: 0.0085s/iter; left time: 245.8286s\n",
      "\titers: 3100, epoch: 4 | loss: 0.2187716\n",
      "\tspeed: 0.0085s/iter; left time: 244.4213s\n",
      "\titers: 3200, epoch: 4 | loss: 0.2891522\n",
      "\tspeed: 0.0085s/iter; left time: 243.5263s\n",
      "\titers: 3300, epoch: 4 | loss: 0.4233777\n",
      "\tspeed: 0.0085s/iter; left time: 242.6938s\n",
      "\titers: 3400, epoch: 4 | loss: 0.4685960\n",
      "\tspeed: 0.0085s/iter; left time: 241.7816s\n",
      "\titers: 3500, epoch: 4 | loss: 0.3163090\n",
      "\tspeed: 0.0085s/iter; left time: 240.8329s\n",
      "\titers: 3600, epoch: 4 | loss: 0.5563865\n",
      "\tspeed: 0.0085s/iter; left time: 240.4976s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2599082\n",
      "\tspeed: 0.0085s/iter; left time: 239.1163s\n",
      "\titers: 3800, epoch: 4 | loss: 0.2998003\n",
      "\tspeed: 0.0085s/iter; left time: 238.5976s\n",
      "\titers: 3900, epoch: 4 | loss: 0.3328616\n",
      "\tspeed: 0.0085s/iter; left time: 237.7866s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1915730\n",
      "\tspeed: 0.0085s/iter; left time: 236.7993s\n",
      "\titers: 4100, epoch: 4 | loss: 0.4464213\n",
      "\tspeed: 0.0085s/iter; left time: 235.7946s\n",
      "\titers: 4200, epoch: 4 | loss: 0.2499777\n",
      "\tspeed: 0.0085s/iter; left time: 234.5115s\n",
      "\titers: 4300, epoch: 4 | loss: 0.2998366\n",
      "\tspeed: 0.0085s/iter; left time: 233.8362s\n",
      "\titers: 4400, epoch: 4 | loss: 0.2887417\n",
      "\tspeed: 0.0085s/iter; left time: 232.8916s\n",
      "\titers: 4500, epoch: 4 | loss: 0.2728286\n",
      "\tspeed: 0.0085s/iter; left time: 232.1169s\n",
      "Epoch: 4 cost time: 39.00921845436096\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.2870982 Vali Loss: 0.2598488 Test Loss: 0.2591088\n",
      "Validation loss decreased (0.304322 --> 0.259849).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2525941\n",
      "\tspeed: 0.0980s/iter; left time: 2667.3557s\n",
      "\titers: 200, epoch: 5 | loss: 0.2997883\n",
      "\tspeed: 0.0085s/iter; left time: 231.4237s\n",
      "\titers: 300, epoch: 5 | loss: 0.1469513\n",
      "\tspeed: 0.0085s/iter; left time: 230.8943s\n",
      "\titers: 400, epoch: 5 | loss: 0.2408659\n",
      "\tspeed: 0.0086s/iter; left time: 230.5130s\n",
      "\titers: 500, epoch: 5 | loss: 0.2866403\n",
      "\tspeed: 0.0085s/iter; left time: 228.5851s\n",
      "\titers: 600, epoch: 5 | loss: 0.1740932\n",
      "\tspeed: 0.0085s/iter; left time: 228.0503s\n",
      "\titers: 700, epoch: 5 | loss: 0.1982282\n",
      "\tspeed: 0.0085s/iter; left time: 226.8445s\n",
      "\titers: 800, epoch: 5 | loss: 0.3178321\n",
      "\tspeed: 0.0085s/iter; left time: 226.5886s\n",
      "\titers: 900, epoch: 5 | loss: 0.4386778\n",
      "\tspeed: 0.0085s/iter; left time: 225.1244s\n",
      "\titers: 1000, epoch: 5 | loss: 0.3533579\n",
      "\tspeed: 0.0085s/iter; left time: 224.2081s\n",
      "\titers: 1100, epoch: 5 | loss: 0.1595954\n",
      "\tspeed: 0.0085s/iter; left time: 224.1256s\n",
      "\titers: 1200, epoch: 5 | loss: 0.3449305\n",
      "\tspeed: 0.0085s/iter; left time: 223.0176s\n",
      "\titers: 1300, epoch: 5 | loss: 0.2775292\n",
      "\tspeed: 0.0085s/iter; left time: 222.2492s\n",
      "\titers: 1400, epoch: 5 | loss: 0.2122595\n",
      "\tspeed: 0.0085s/iter; left time: 221.0875s\n",
      "\titers: 1500, epoch: 5 | loss: 0.2221873\n",
      "\tspeed: 0.0085s/iter; left time: 220.1698s\n",
      "\titers: 1600, epoch: 5 | loss: 0.1564467\n",
      "\tspeed: 0.0085s/iter; left time: 219.5669s\n",
      "\titers: 1700, epoch: 5 | loss: 0.2554741\n",
      "\tspeed: 0.0085s/iter; left time: 218.7176s\n",
      "\titers: 1800, epoch: 5 | loss: 0.1857991\n",
      "\tspeed: 0.0085s/iter; left time: 217.9844s\n",
      "\titers: 1900, epoch: 5 | loss: 0.2827681\n",
      "\tspeed: 0.0085s/iter; left time: 216.9702s\n",
      "\titers: 2000, epoch: 5 | loss: 0.1339092\n",
      "\tspeed: 0.0085s/iter; left time: 216.2779s\n",
      "\titers: 2100, epoch: 5 | loss: 0.3265502\n",
      "\tspeed: 0.0085s/iter; left time: 215.3784s\n",
      "\titers: 2200, epoch: 5 | loss: 0.2993271\n",
      "\tspeed: 0.0085s/iter; left time: 214.6324s\n",
      "\titers: 2300, epoch: 5 | loss: 0.2228697\n",
      "\tspeed: 0.0085s/iter; left time: 213.7830s\n",
      "\titers: 2400, epoch: 5 | loss: 0.2344109\n",
      "\tspeed: 0.0085s/iter; left time: 212.9382s\n",
      "\titers: 2500, epoch: 5 | loss: 0.2300774\n",
      "\tspeed: 0.0085s/iter; left time: 211.9693s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1436794\n",
      "\tspeed: 0.0085s/iter; left time: 211.1141s\n",
      "\titers: 2700, epoch: 5 | loss: 0.3369169\n",
      "\tspeed: 0.0085s/iter; left time: 210.3815s\n",
      "\titers: 2800, epoch: 5 | loss: 0.1997850\n",
      "\tspeed: 0.0085s/iter; left time: 209.4847s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0968876\n",
      "\tspeed: 0.0085s/iter; left time: 208.7256s\n",
      "\titers: 3000, epoch: 5 | loss: 0.3765050\n",
      "\tspeed: 0.0085s/iter; left time: 207.7760s\n",
      "\titers: 3100, epoch: 5 | loss: 0.2389798\n",
      "\tspeed: 0.0085s/iter; left time: 206.9601s\n",
      "\titers: 3200, epoch: 5 | loss: 0.3168649\n",
      "\tspeed: 0.0086s/iter; left time: 206.5988s\n",
      "\titers: 3300, epoch: 5 | loss: 0.4848840\n",
      "\tspeed: 0.0086s/iter; left time: 206.2942s\n",
      "\titers: 3400, epoch: 5 | loss: 0.2142923\n",
      "\tspeed: 0.0086s/iter; left time: 205.6286s\n",
      "\titers: 3500, epoch: 5 | loss: 0.2295204\n",
      "\tspeed: 0.0086s/iter; left time: 204.7407s\n",
      "\titers: 3600, epoch: 5 | loss: 0.2239365\n",
      "\tspeed: 0.0086s/iter; left time: 203.8851s\n",
      "\titers: 3700, epoch: 5 | loss: 0.2631375\n",
      "\tspeed: 0.0086s/iter; left time: 203.0744s\n",
      "\titers: 3800, epoch: 5 | loss: 0.3363828\n",
      "\tspeed: 0.0086s/iter; left time: 202.1369s\n",
      "\titers: 3900, epoch: 5 | loss: 0.3567095\n",
      "\tspeed: 0.0086s/iter; left time: 201.4648s\n",
      "\titers: 4000, epoch: 5 | loss: 0.2748664\n",
      "\tspeed: 0.0086s/iter; left time: 200.6804s\n",
      "\titers: 4100, epoch: 5 | loss: 0.2249340\n",
      "\tspeed: 0.0086s/iter; left time: 199.8053s\n",
      "\titers: 4200, epoch: 5 | loss: 0.4793025\n",
      "\tspeed: 0.0086s/iter; left time: 198.9284s\n",
      "\titers: 4300, epoch: 5 | loss: 0.1635342\n",
      "\tspeed: 0.0086s/iter; left time: 198.2246s\n",
      "\titers: 4400, epoch: 5 | loss: 0.2564046\n",
      "\tspeed: 0.0086s/iter; left time: 197.0086s\n",
      "\titers: 4500, epoch: 5 | loss: 0.5245385\n",
      "\tspeed: 0.0086s/iter; left time: 195.7881s\n",
      "Epoch: 5 cost time: 39.27017259597778\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.2603830 Vali Loss: 0.2366961 Test Loss: 0.2344279\n",
      "Validation loss decreased (0.259849 --> 0.236696).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2142735\n",
      "\tspeed: 0.0874s/iter; left time: 1981.1309s\n",
      "\titers: 200, epoch: 6 | loss: 0.1786033\n",
      "\tspeed: 0.0085s/iter; left time: 192.3432s\n",
      "\titers: 300, epoch: 6 | loss: 0.2205459\n",
      "\tspeed: 0.0085s/iter; left time: 191.9085s\n",
      "\titers: 400, epoch: 6 | loss: 0.3903606\n",
      "\tspeed: 0.0085s/iter; left time: 190.6711s\n",
      "\titers: 500, epoch: 6 | loss: 0.2104526\n",
      "\tspeed: 0.0085s/iter; left time: 189.5576s\n",
      "\titers: 600, epoch: 6 | loss: 0.3759050\n",
      "\tspeed: 0.0085s/iter; left time: 188.6072s\n",
      "\titers: 700, epoch: 6 | loss: 0.2689289\n",
      "\tspeed: 0.0085s/iter; left time: 187.9469s\n",
      "\titers: 800, epoch: 6 | loss: 0.2126927\n",
      "\tspeed: 0.0085s/iter; left time: 187.3546s\n",
      "\titers: 900, epoch: 6 | loss: 0.2229729\n",
      "\tspeed: 0.0085s/iter; left time: 186.0844s\n",
      "\titers: 1000, epoch: 6 | loss: 0.1385723\n",
      "\tspeed: 0.0085s/iter; left time: 184.9373s\n",
      "\titers: 1100, epoch: 6 | loss: 0.1360074\n",
      "\tspeed: 0.0085s/iter; left time: 184.6706s\n",
      "\titers: 1200, epoch: 6 | loss: 0.1607238\n",
      "\tspeed: 0.0085s/iter; left time: 183.3881s\n",
      "\titers: 1300, epoch: 6 | loss: 0.2749804\n",
      "\tspeed: 0.0085s/iter; left time: 182.7584s\n",
      "\titers: 1400, epoch: 6 | loss: 0.1490894\n",
      "\tspeed: 0.0085s/iter; left time: 182.0182s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1487194\n",
      "\tspeed: 0.0085s/iter; left time: 181.1861s\n",
      "\titers: 1600, epoch: 6 | loss: 0.3735901\n",
      "\tspeed: 0.0085s/iter; left time: 180.0088s\n",
      "\titers: 1700, epoch: 6 | loss: 0.2847252\n",
      "\tspeed: 0.0085s/iter; left time: 178.6947s\n",
      "\titers: 1800, epoch: 6 | loss: 0.1573763\n",
      "\tspeed: 0.0085s/iter; left time: 177.6925s\n",
      "\titers: 1900, epoch: 6 | loss: 0.2140993\n",
      "\tspeed: 0.0085s/iter; left time: 177.0373s\n",
      "\titers: 2000, epoch: 6 | loss: 0.1371738\n",
      "\tspeed: 0.0085s/iter; left time: 176.0871s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1689661\n",
      "\tspeed: 0.0085s/iter; left time: 175.1984s\n",
      "\titers: 2200, epoch: 6 | loss: 0.2568092\n",
      "\tspeed: 0.0085s/iter; left time: 174.3684s\n",
      "\titers: 2300, epoch: 6 | loss: 0.4903546\n",
      "\tspeed: 0.0085s/iter; left time: 173.3924s\n",
      "\titers: 2400, epoch: 6 | loss: 0.2591270\n",
      "\tspeed: 0.0085s/iter; left time: 172.6772s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1477432\n",
      "\tspeed: 0.0085s/iter; left time: 171.5683s\n",
      "\titers: 2600, epoch: 6 | loss: 0.2225637\n",
      "\tspeed: 0.0085s/iter; left time: 170.6670s\n",
      "\titers: 2700, epoch: 6 | loss: 0.2646966\n",
      "\tspeed: 0.0085s/iter; left time: 169.8391s\n",
      "\titers: 2800, epoch: 6 | loss: 0.2631033\n",
      "\tspeed: 0.0085s/iter; left time: 168.8705s\n",
      "\titers: 2900, epoch: 6 | loss: 0.1754228\n",
      "\tspeed: 0.0085s/iter; left time: 168.0710s\n",
      "\titers: 3000, epoch: 6 | loss: 0.3526819\n",
      "\tspeed: 0.0085s/iter; left time: 167.2031s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1498267\n",
      "\tspeed: 0.0085s/iter; left time: 166.4262s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1849730\n",
      "\tspeed: 0.0085s/iter; left time: 165.5407s\n",
      "\titers: 3300, epoch: 6 | loss: 0.1432507\n",
      "\tspeed: 0.0085s/iter; left time: 164.9310s\n",
      "\titers: 3400, epoch: 6 | loss: 0.3261216\n",
      "\tspeed: 0.0085s/iter; left time: 163.9325s\n",
      "\titers: 3500, epoch: 6 | loss: 0.2188630\n",
      "\tspeed: 0.0085s/iter; left time: 162.9425s\n",
      "\titers: 3600, epoch: 6 | loss: 0.2568094\n",
      "\tspeed: 0.0085s/iter; left time: 162.1123s\n",
      "\titers: 3700, epoch: 6 | loss: 0.2530288\n",
      "\tspeed: 0.0085s/iter; left time: 161.2724s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1995590\n",
      "\tspeed: 0.0084s/iter; left time: 160.3453s\n",
      "\titers: 3900, epoch: 6 | loss: 0.2511933\n",
      "\tspeed: 0.0085s/iter; left time: 159.5479s\n",
      "\titers: 4000, epoch: 6 | loss: 0.3672163\n",
      "\tspeed: 0.0084s/iter; left time: 158.6535s\n",
      "\titers: 4100, epoch: 6 | loss: 0.3525769\n",
      "\tspeed: 0.0085s/iter; left time: 157.8958s\n",
      "\titers: 4200, epoch: 6 | loss: 0.1818097\n",
      "\tspeed: 0.0085s/iter; left time: 157.1041s\n",
      "\titers: 4300, epoch: 6 | loss: 0.4881147\n",
      "\tspeed: 0.0085s/iter; left time: 156.3249s\n",
      "\titers: 4400, epoch: 6 | loss: 0.1685981\n",
      "\tspeed: 0.0085s/iter; left time: 155.6581s\n",
      "\titers: 4500, epoch: 6 | loss: 0.2294936\n",
      "\tspeed: 0.0085s/iter; left time: 154.7780s\n",
      "Epoch: 6 cost time: 38.83159160614014\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.2408822 Vali Loss: 0.2251610 Test Loss: 0.2252287\n",
      "Validation loss decreased (0.236696 --> 0.225161).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2364382\n",
      "\tspeed: 0.0925s/iter; left time: 1675.7735s\n",
      "\titers: 200, epoch: 7 | loss: 0.2136155\n",
      "\tspeed: 0.0085s/iter; left time: 153.2013s\n",
      "\titers: 300, epoch: 7 | loss: 0.1436854\n",
      "\tspeed: 0.0085s/iter; left time: 152.2784s\n",
      "\titers: 400, epoch: 7 | loss: 0.2110558\n",
      "\tspeed: 0.0085s/iter; left time: 150.9723s\n",
      "\titers: 500, epoch: 7 | loss: 0.2316799\n",
      "\tspeed: 0.0085s/iter; left time: 150.2428s\n",
      "\titers: 600, epoch: 7 | loss: 0.1906799\n",
      "\tspeed: 0.0086s/iter; left time: 151.5768s\n",
      "\titers: 700, epoch: 7 | loss: 0.1294379\n",
      "\tspeed: 0.0092s/iter; left time: 161.9299s\n",
      "\titers: 800, epoch: 7 | loss: 0.2174028\n",
      "\tspeed: 0.0086s/iter; left time: 149.1228s\n",
      "\titers: 900, epoch: 7 | loss: 0.1748831\n",
      "\tspeed: 0.0086s/iter; left time: 148.5395s\n",
      "\titers: 1000, epoch: 7 | loss: 0.2483133\n",
      "\tspeed: 0.0086s/iter; left time: 147.6349s\n",
      "\titers: 1100, epoch: 7 | loss: 0.3786675\n",
      "\tspeed: 0.0086s/iter; left time: 146.7566s\n",
      "\titers: 1200, epoch: 7 | loss: 0.2664742\n",
      "\tspeed: 0.0086s/iter; left time: 145.9572s\n",
      "\titers: 1300, epoch: 7 | loss: 0.3948551\n",
      "\tspeed: 0.0086s/iter; left time: 145.0185s\n",
      "\titers: 1400, epoch: 7 | loss: 0.0845508\n",
      "\tspeed: 0.0086s/iter; left time: 144.1833s\n",
      "\titers: 1500, epoch: 7 | loss: 0.1777844\n",
      "\tspeed: 0.0086s/iter; left time: 143.2990s\n",
      "\titers: 1600, epoch: 7 | loss: 0.2032531\n",
      "\tspeed: 0.0085s/iter; left time: 142.0871s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1864463\n",
      "\tspeed: 0.0085s/iter; left time: 141.1942s\n",
      "\titers: 1800, epoch: 7 | loss: 0.2322651\n",
      "\tspeed: 0.0085s/iter; left time: 140.3079s\n",
      "\titers: 1900, epoch: 7 | loss: 0.2069993\n",
      "\tspeed: 0.0085s/iter; left time: 139.4426s\n",
      "\titers: 2000, epoch: 7 | loss: 0.2029282\n",
      "\tspeed: 0.0085s/iter; left time: 138.6251s\n",
      "\titers: 2100, epoch: 7 | loss: 0.2281658\n",
      "\tspeed: 0.0085s/iter; left time: 137.7334s\n",
      "\titers: 2200, epoch: 7 | loss: 0.2553199\n",
      "\tspeed: 0.0085s/iter; left time: 136.7425s\n",
      "\titers: 2300, epoch: 7 | loss: 0.2140854\n",
      "\tspeed: 0.0085s/iter; left time: 135.8850s\n",
      "\titers: 2400, epoch: 7 | loss: 0.1698422\n",
      "\tspeed: 0.0085s/iter; left time: 135.1711s\n",
      "\titers: 2500, epoch: 7 | loss: 0.1801306\n",
      "\tspeed: 0.0085s/iter; left time: 134.2665s\n",
      "\titers: 2600, epoch: 7 | loss: 0.2391938\n",
      "\tspeed: 0.0085s/iter; left time: 133.5073s\n",
      "\titers: 2700, epoch: 7 | loss: 0.3368509\n",
      "\tspeed: 0.0085s/iter; left time: 132.7030s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1124539\n",
      "\tspeed: 0.0086s/iter; left time: 131.9293s\n",
      "\titers: 2900, epoch: 7 | loss: 0.3292593\n",
      "\tspeed: 0.0086s/iter; left time: 131.0574s\n",
      "\titers: 3000, epoch: 7 | loss: 0.3534920\n",
      "\tspeed: 0.0086s/iter; left time: 130.1973s\n",
      "\titers: 3100, epoch: 7 | loss: 0.1155916\n",
      "\tspeed: 0.0086s/iter; left time: 129.3786s\n",
      "\titers: 3200, epoch: 7 | loss: 0.1636773\n",
      "\tspeed: 0.0086s/iter; left time: 128.4880s\n",
      "\titers: 3300, epoch: 7 | loss: 0.1281550\n",
      "\tspeed: 0.0085s/iter; left time: 127.4191s\n",
      "\titers: 3400, epoch: 7 | loss: 0.2441648\n",
      "\tspeed: 0.0085s/iter; left time: 126.5579s\n",
      "\titers: 3500, epoch: 7 | loss: 0.1584930\n",
      "\tspeed: 0.0086s/iter; left time: 126.1086s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1988648\n",
      "\tspeed: 0.0086s/iter; left time: 125.9021s\n",
      "\titers: 3700, epoch: 7 | loss: 0.2498216\n",
      "\tspeed: 0.0086s/iter; left time: 125.1722s\n",
      "\titers: 3800, epoch: 7 | loss: 0.1752738\n",
      "\tspeed: 0.0086s/iter; left time: 124.4681s\n",
      "\titers: 3900, epoch: 7 | loss: 0.2518936\n",
      "\tspeed: 0.0086s/iter; left time: 123.5484s\n",
      "\titers: 4000, epoch: 7 | loss: 0.1420046\n",
      "\tspeed: 0.0086s/iter; left time: 122.1142s\n",
      "\titers: 4100, epoch: 7 | loss: 0.2740989\n",
      "\tspeed: 0.0085s/iter; left time: 120.7223s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1414856\n",
      "\tspeed: 0.0085s/iter; left time: 119.7864s\n",
      "\titers: 4300, epoch: 7 | loss: 0.2408642\n",
      "\tspeed: 0.0085s/iter; left time: 118.9312s\n",
      "\titers: 4400, epoch: 7 | loss: 0.4382631\n",
      "\tspeed: 0.0085s/iter; left time: 118.0470s\n",
      "\titers: 4500, epoch: 7 | loss: 0.2474151\n",
      "\tspeed: 0.0086s/iter; left time: 117.4720s\n",
      "Epoch: 7 cost time: 39.24823260307312\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.2304524 Vali Loss: 0.2202493 Test Loss: 0.2190268\n",
      "Validation loss decreased (0.225161 --> 0.220249).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1927187\n",
      "\tspeed: 0.0831s/iter; left time: 1127.4078s\n",
      "\titers: 200, epoch: 8 | loss: 0.1886632\n",
      "\tspeed: 0.0085s/iter; left time: 113.9519s\n",
      "\titers: 300, epoch: 8 | loss: 0.2342818\n",
      "\tspeed: 0.0085s/iter; left time: 113.3235s\n",
      "\titers: 400, epoch: 8 | loss: 0.1937016\n",
      "\tspeed: 0.0085s/iter; left time: 112.5260s\n",
      "\titers: 500, epoch: 8 | loss: 0.1266495\n",
      "\tspeed: 0.0085s/iter; left time: 111.6470s\n",
      "\titers: 600, epoch: 8 | loss: 0.2721857\n",
      "\tspeed: 0.0085s/iter; left time: 110.7073s\n",
      "\titers: 700, epoch: 8 | loss: 0.1986673\n",
      "\tspeed: 0.0085s/iter; left time: 109.7346s\n",
      "\titers: 800, epoch: 8 | loss: 0.3474109\n",
      "\tspeed: 0.0085s/iter; left time: 108.9040s\n",
      "\titers: 900, epoch: 8 | loss: 0.2226358\n",
      "\tspeed: 0.0085s/iter; left time: 108.1871s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1195267\n",
      "\tspeed: 0.0085s/iter; left time: 107.4918s\n",
      "\titers: 1100, epoch: 8 | loss: 0.3047546\n",
      "\tspeed: 0.0085s/iter; left time: 107.1580s\n",
      "\titers: 1200, epoch: 8 | loss: 0.2772685\n",
      "\tspeed: 0.0085s/iter; left time: 106.5427s\n",
      "\titers: 1300, epoch: 8 | loss: 0.1962439\n",
      "\tspeed: 0.0085s/iter; left time: 105.5396s\n",
      "\titers: 1400, epoch: 8 | loss: 0.1724770\n",
      "\tspeed: 0.0085s/iter; left time: 104.5726s\n",
      "\titers: 1500, epoch: 8 | loss: 0.0920472\n",
      "\tspeed: 0.0085s/iter; left time: 103.7388s\n",
      "\titers: 1600, epoch: 8 | loss: 0.3089511\n",
      "\tspeed: 0.0085s/iter; left time: 103.0168s\n",
      "\titers: 1700, epoch: 8 | loss: 0.2718320\n",
      "\tspeed: 0.0085s/iter; left time: 102.2288s\n",
      "\titers: 1800, epoch: 8 | loss: 0.1934665\n",
      "\tspeed: 0.0085s/iter; left time: 101.3585s\n",
      "\titers: 1900, epoch: 8 | loss: 0.2068698\n",
      "\tspeed: 0.0085s/iter; left time: 100.4081s\n",
      "\titers: 2000, epoch: 8 | loss: 0.1788024\n",
      "\tspeed: 0.0086s/iter; left time: 99.7804s\n",
      "\titers: 2100, epoch: 8 | loss: 0.2388266\n",
      "\tspeed: 0.0085s/iter; left time: 98.8262s\n",
      "\titers: 2200, epoch: 8 | loss: 0.2115056\n",
      "\tspeed: 0.0086s/iter; left time: 98.0545s\n",
      "\titers: 2300, epoch: 8 | loss: 0.1570966\n",
      "\tspeed: 0.0086s/iter; left time: 97.3222s\n",
      "\titers: 2400, epoch: 8 | loss: 0.2998454\n",
      "\tspeed: 0.0085s/iter; left time: 96.2189s\n",
      "\titers: 2500, epoch: 8 | loss: 0.2658179\n",
      "\tspeed: 0.0085s/iter; left time: 94.5694s\n",
      "\titers: 2600, epoch: 8 | loss: 0.1511708\n",
      "\tspeed: 0.0085s/iter; left time: 94.0201s\n",
      "\titers: 2700, epoch: 8 | loss: 0.1361580\n",
      "\tspeed: 0.0085s/iter; left time: 93.0181s\n",
      "\titers: 2800, epoch: 8 | loss: 0.1832336\n",
      "\tspeed: 0.0085s/iter; left time: 92.1639s\n",
      "\titers: 2900, epoch: 8 | loss: 0.4273134\n",
      "\tspeed: 0.0085s/iter; left time: 91.2658s\n",
      "\titers: 3000, epoch: 8 | loss: 0.3790162\n",
      "\tspeed: 0.0085s/iter; left time: 90.4787s\n",
      "\titers: 3100, epoch: 8 | loss: 0.1717083\n",
      "\tspeed: 0.0085s/iter; left time: 89.6445s\n",
      "\titers: 3200, epoch: 8 | loss: 0.2679079\n",
      "\tspeed: 0.0085s/iter; left time: 88.7133s\n",
      "\titers: 3300, epoch: 8 | loss: 0.4921545\n",
      "\tspeed: 0.0085s/iter; left time: 87.8468s\n",
      "\titers: 3400, epoch: 8 | loss: 0.2120788\n",
      "\tspeed: 0.0085s/iter; left time: 87.0768s\n",
      "\titers: 3500, epoch: 8 | loss: 0.3408238\n",
      "\tspeed: 0.0085s/iter; left time: 86.1817s\n",
      "\titers: 3600, epoch: 8 | loss: 0.0873074\n",
      "\tspeed: 0.0085s/iter; left time: 85.2922s\n",
      "\titers: 3700, epoch: 8 | loss: 0.1547755\n",
      "\tspeed: 0.0085s/iter; left time: 84.4425s\n",
      "\titers: 3800, epoch: 8 | loss: 0.2464043\n",
      "\tspeed: 0.0085s/iter; left time: 83.7004s\n",
      "\titers: 3900, epoch: 8 | loss: 0.2352358\n",
      "\tspeed: 0.0085s/iter; left time: 83.2435s\n",
      "\titers: 4000, epoch: 8 | loss: 0.2459220\n",
      "\tspeed: 0.0085s/iter; left time: 82.3362s\n",
      "\titers: 4100, epoch: 8 | loss: 0.0936253\n",
      "\tspeed: 0.0085s/iter; left time: 81.4655s\n",
      "\titers: 4200, epoch: 8 | loss: 0.3129211\n",
      "\tspeed: 0.0085s/iter; left time: 80.5625s\n",
      "\titers: 4300, epoch: 8 | loss: 0.1592271\n",
      "\tspeed: 0.0085s/iter; left time: 79.7067s\n",
      "\titers: 4400, epoch: 8 | loss: 0.1156706\n",
      "\tspeed: 0.0085s/iter; left time: 78.7745s\n",
      "\titers: 4500, epoch: 8 | loss: 0.1555890\n",
      "\tspeed: 0.0085s/iter; left time: 77.9258s\n",
      "Epoch: 8 cost time: 38.936219453811646\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.2257474 Vali Loss: 0.2170379 Test Loss: 0.2163085\n",
      "Validation loss decreased (0.220249 --> 0.217038).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3333167\n",
      "\tspeed: 0.0815s/iter; left time: 734.1112s\n",
      "\titers: 200, epoch: 9 | loss: 0.2691150\n",
      "\tspeed: 0.0085s/iter; left time: 75.7860s\n",
      "\titers: 300, epoch: 9 | loss: 0.1635327\n",
      "\tspeed: 0.0085s/iter; left time: 74.9151s\n",
      "\titers: 400, epoch: 9 | loss: 0.2446412\n",
      "\tspeed: 0.0085s/iter; left time: 74.1031s\n",
      "\titers: 500, epoch: 9 | loss: 0.1570805\n",
      "\tspeed: 0.0085s/iter; left time: 73.0622s\n",
      "\titers: 600, epoch: 9 | loss: 0.1285637\n",
      "\tspeed: 0.0085s/iter; left time: 72.1855s\n",
      "\titers: 700, epoch: 9 | loss: 0.2364789\n",
      "\tspeed: 0.0085s/iter; left time: 71.3314s\n",
      "\titers: 800, epoch: 9 | loss: 0.2493334\n",
      "\tspeed: 0.0085s/iter; left time: 70.4937s\n",
      "\titers: 900, epoch: 9 | loss: 0.2668680\n",
      "\tspeed: 0.0085s/iter; left time: 69.7907s\n",
      "\titers: 1000, epoch: 9 | loss: 0.1637239\n",
      "\tspeed: 0.0085s/iter; left time: 68.8046s\n",
      "\titers: 1100, epoch: 9 | loss: 0.2134409\n",
      "\tspeed: 0.0085s/iter; left time: 67.9829s\n",
      "\titers: 1200, epoch: 9 | loss: 0.3385510\n",
      "\tspeed: 0.0085s/iter; left time: 67.1749s\n",
      "\titers: 1300, epoch: 9 | loss: 0.1260075\n",
      "\tspeed: 0.0085s/iter; left time: 66.2883s\n",
      "\titers: 1400, epoch: 9 | loss: 0.3661079\n",
      "\tspeed: 0.0085s/iter; left time: 65.2715s\n",
      "\titers: 1500, epoch: 9 | loss: 0.1195718\n",
      "\tspeed: 0.0084s/iter; left time: 64.3073s\n",
      "\titers: 1600, epoch: 9 | loss: 0.2322047\n",
      "\tspeed: 0.0084s/iter; left time: 63.4331s\n",
      "\titers: 1700, epoch: 9 | loss: 0.1355065\n",
      "\tspeed: 0.0085s/iter; left time: 62.7356s\n",
      "\titers: 1800, epoch: 9 | loss: 0.1356087\n",
      "\tspeed: 0.0085s/iter; left time: 62.0043s\n",
      "\titers: 1900, epoch: 9 | loss: 0.1415667\n",
      "\tspeed: 0.0085s/iter; left time: 61.2270s\n",
      "\titers: 2000, epoch: 9 | loss: 0.1571112\n",
      "\tspeed: 0.0085s/iter; left time: 60.3649s\n",
      "\titers: 2100, epoch: 9 | loss: 0.1964193\n",
      "\tspeed: 0.0085s/iter; left time: 59.5563s\n",
      "\titers: 2200, epoch: 9 | loss: 0.2632481\n",
      "\tspeed: 0.0085s/iter; left time: 58.7448s\n",
      "\titers: 2300, epoch: 9 | loss: 0.1893238\n",
      "\tspeed: 0.0085s/iter; left time: 57.8776s\n",
      "\titers: 2400, epoch: 9 | loss: 0.1900716\n",
      "\tspeed: 0.0085s/iter; left time: 57.0250s\n",
      "\titers: 2500, epoch: 9 | loss: 0.0896373\n",
      "\tspeed: 0.0085s/iter; left time: 56.2975s\n",
      "\titers: 2600, epoch: 9 | loss: 0.1627011\n",
      "\tspeed: 0.0085s/iter; left time: 55.4213s\n",
      "\titers: 2700, epoch: 9 | loss: 0.2135602\n",
      "\tspeed: 0.0085s/iter; left time: 54.5922s\n",
      "\titers: 2800, epoch: 9 | loss: 0.1664678\n",
      "\tspeed: 0.0085s/iter; left time: 53.7441s\n",
      "\titers: 2900, epoch: 9 | loss: 0.1281206\n",
      "\tspeed: 0.0085s/iter; left time: 52.8926s\n",
      "\titers: 3000, epoch: 9 | loss: 0.3241016\n",
      "\tspeed: 0.0085s/iter; left time: 52.0462s\n",
      "\titers: 3100, epoch: 9 | loss: 0.1346766\n",
      "\tspeed: 0.0085s/iter; left time: 51.1196s\n",
      "\titers: 3200, epoch: 9 | loss: 0.3193671\n",
      "\tspeed: 0.0085s/iter; left time: 50.3712s\n",
      "\titers: 3300, epoch: 9 | loss: 0.0830685\n",
      "\tspeed: 0.0085s/iter; left time: 49.4990s\n",
      "\titers: 3400, epoch: 9 | loss: 0.2299056\n",
      "\tspeed: 0.0085s/iter; left time: 48.6383s\n",
      "\titers: 3500, epoch: 9 | loss: 0.0691976\n",
      "\tspeed: 0.0085s/iter; left time: 47.7783s\n",
      "\titers: 3600, epoch: 9 | loss: 0.2310430\n",
      "\tspeed: 0.0085s/iter; left time: 46.9151s\n",
      "\titers: 3700, epoch: 9 | loss: 0.1755479\n",
      "\tspeed: 0.0085s/iter; left time: 46.0697s\n",
      "\titers: 3800, epoch: 9 | loss: 0.2655972\n",
      "\tspeed: 0.0085s/iter; left time: 45.2377s\n",
      "\titers: 3900, epoch: 9 | loss: 0.2644050\n",
      "\tspeed: 0.0085s/iter; left time: 44.3657s\n",
      "\titers: 4000, epoch: 9 | loss: 0.3320472\n",
      "\tspeed: 0.0085s/iter; left time: 43.3401s\n",
      "\titers: 4100, epoch: 9 | loss: 0.2788040\n",
      "\tspeed: 0.0085s/iter; left time: 42.5889s\n",
      "\titers: 4200, epoch: 9 | loss: 0.1245410\n",
      "\tspeed: 0.0085s/iter; left time: 41.8349s\n",
      "\titers: 4300, epoch: 9 | loss: 0.2380769\n",
      "\tspeed: 0.0085s/iter; left time: 40.9283s\n",
      "\titers: 4400, epoch: 9 | loss: 0.1436207\n",
      "\tspeed: 0.0085s/iter; left time: 40.0671s\n",
      "\titers: 4500, epoch: 9 | loss: 0.1788915\n",
      "\tspeed: 0.0085s/iter; left time: 39.2199s\n",
      "Epoch: 9 cost time: 38.913575172424316\n",
      "Epoch: 9, Steps: 4555 | Train Loss: 0.2227255 Vali Loss: 0.2155600 Test Loss: 0.2136874\n",
      "Validation loss decreased (0.217038 --> 0.215560).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.4506412\n",
      "\tspeed: 0.0920s/iter; left time: 409.7816s\n",
      "\titers: 200, epoch: 10 | loss: 0.2710690\n",
      "\tspeed: 0.0085s/iter; left time: 37.1307s\n",
      "\titers: 300, epoch: 10 | loss: 0.1195764\n",
      "\tspeed: 0.0085s/iter; left time: 36.2710s\n",
      "\titers: 400, epoch: 10 | loss: 0.1466168\n",
      "\tspeed: 0.0085s/iter; left time: 35.3728s\n",
      "\titers: 500, epoch: 10 | loss: 0.1840898\n",
      "\tspeed: 0.0085s/iter; left time: 34.5292s\n",
      "\titers: 600, epoch: 10 | loss: 0.2355264\n",
      "\tspeed: 0.0085s/iter; left time: 33.6991s\n",
      "\titers: 700, epoch: 10 | loss: 0.1164058\n",
      "\tspeed: 0.0085s/iter; left time: 32.8568s\n",
      "\titers: 800, epoch: 10 | loss: 0.2780105\n",
      "\tspeed: 0.0085s/iter; left time: 31.9962s\n",
      "\titers: 900, epoch: 10 | loss: 0.2938435\n",
      "\tspeed: 0.0085s/iter; left time: 31.1905s\n",
      "\titers: 1000, epoch: 10 | loss: 0.1563220\n",
      "\tspeed: 0.0085s/iter; left time: 30.2568s\n",
      "\titers: 1100, epoch: 10 | loss: 0.4767590\n",
      "\tspeed: 0.0086s/iter; left time: 29.5550s\n",
      "\titers: 1200, epoch: 10 | loss: 0.1356563\n",
      "\tspeed: 0.0086s/iter; left time: 28.7770s\n",
      "\titers: 1300, epoch: 10 | loss: 0.2041860\n",
      "\tspeed: 0.0086s/iter; left time: 27.9520s\n",
      "\titers: 1400, epoch: 10 | loss: 0.3396183\n",
      "\tspeed: 0.0086s/iter; left time: 27.0757s\n",
      "\titers: 1500, epoch: 10 | loss: 0.2177147\n",
      "\tspeed: 0.0086s/iter; left time: 26.1963s\n",
      "\titers: 1600, epoch: 10 | loss: 0.3020119\n",
      "\tspeed: 0.0086s/iter; left time: 25.3296s\n",
      "\titers: 1700, epoch: 10 | loss: 0.2018278\n",
      "\tspeed: 0.0086s/iter; left time: 24.5049s\n",
      "\titers: 1800, epoch: 10 | loss: 0.1987160\n",
      "\tspeed: 0.0086s/iter; left time: 23.6568s\n",
      "\titers: 1900, epoch: 10 | loss: 0.2015533\n",
      "\tspeed: 0.0086s/iter; left time: 22.7677s\n",
      "\titers: 2000, epoch: 10 | loss: 0.2007878\n",
      "\tspeed: 0.0086s/iter; left time: 21.9172s\n",
      "\titers: 2100, epoch: 10 | loss: 0.1118488\n",
      "\tspeed: 0.0085s/iter; left time: 20.9715s\n",
      "\titers: 2200, epoch: 10 | loss: 0.2306893\n",
      "\tspeed: 0.0085s/iter; left time: 19.9520s\n",
      "\titers: 2300, epoch: 10 | loss: 0.2620006\n",
      "\tspeed: 0.0085s/iter; left time: 19.1011s\n",
      "\titers: 2400, epoch: 10 | loss: 0.2330466\n",
      "\tspeed: 0.0085s/iter; left time: 18.2752s\n",
      "\titers: 2500, epoch: 10 | loss: 0.2020074\n",
      "\tspeed: 0.0085s/iter; left time: 17.4095s\n",
      "\titers: 2600, epoch: 10 | loss: 0.2518964\n",
      "\tspeed: 0.0085s/iter; left time: 16.5576s\n",
      "\titers: 2700, epoch: 10 | loss: 0.2729662\n",
      "\tspeed: 0.0085s/iter; left time: 15.7151s\n",
      "\titers: 2800, epoch: 10 | loss: 0.2641411\n",
      "\tspeed: 0.0085s/iter; left time: 14.9053s\n",
      "\titers: 2900, epoch: 10 | loss: 0.1516089\n",
      "\tspeed: 0.0085s/iter; left time: 14.0592s\n",
      "\titers: 3000, epoch: 10 | loss: 0.4138572\n",
      "\tspeed: 0.0085s/iter; left time: 13.2055s\n",
      "\titers: 3100, epoch: 10 | loss: 0.2187928\n",
      "\tspeed: 0.0085s/iter; left time: 12.3820s\n",
      "\titers: 3200, epoch: 10 | loss: 0.2800927\n",
      "\tspeed: 0.0085s/iter; left time: 11.5404s\n",
      "\titers: 3300, epoch: 10 | loss: 0.1249967\n",
      "\tspeed: 0.0085s/iter; left time: 10.6757s\n",
      "\titers: 3400, epoch: 10 | loss: 0.1825637\n",
      "\tspeed: 0.0085s/iter; left time: 9.8247s\n",
      "\titers: 3500, epoch: 10 | loss: 0.3984454\n",
      "\tspeed: 0.0085s/iter; left time: 8.9725s\n",
      "\titers: 3600, epoch: 10 | loss: 0.0752732\n",
      "\tspeed: 0.0085s/iter; left time: 8.1218s\n",
      "\titers: 3700, epoch: 10 | loss: 0.3348437\n",
      "\tspeed: 0.0085s/iter; left time: 7.2704s\n",
      "\titers: 3800, epoch: 10 | loss: 0.2380596\n",
      "\tspeed: 0.0085s/iter; left time: 6.4103s\n",
      "\titers: 3900, epoch: 10 | loss: 0.2756348\n",
      "\tspeed: 0.0085s/iter; left time: 5.5639s\n",
      "\titers: 4000, epoch: 10 | loss: 0.2104467\n",
      "\tspeed: 0.0085s/iter; left time: 4.7130s\n",
      "\titers: 4100, epoch: 10 | loss: 0.1840825\n",
      "\tspeed: 0.0085s/iter; left time: 3.8690s\n",
      "\titers: 4200, epoch: 10 | loss: 0.1713258\n",
      "\tspeed: 0.0085s/iter; left time: 3.0195s\n",
      "\titers: 4300, epoch: 10 | loss: 0.1510018\n",
      "\tspeed: 0.0085s/iter; left time: 2.1719s\n",
      "\titers: 4400, epoch: 10 | loss: 0.2949476\n",
      "\tspeed: 0.0085s/iter; left time: 1.3235s\n",
      "\titers: 4500, epoch: 10 | loss: 0.2399562\n",
      "\tspeed: 0.0085s/iter; left time: 0.4748s\n",
      "Epoch: 10 cost time: 38.97136974334717\n",
      "Epoch: 10, Steps: 4555 | Train Loss: 0.2211224 Vali Loss: 0.2147574 Test Loss: 0.2132673\n",
      "Validation loss decreased (0.215560 --> 0.214757).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      ">>>>>>>testing : long_term_forecast_ECL_168_24_iTransformer_custom_ftM_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 4) (5237, 24, 4)\n",
      "test shape: (5237, 24, 4) (5237, 24, 4)\n",
      "mse:0.21329429745674133, mae:0.3093353807926178\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/iTransformer.sh --features M --predictor solar_forecast,wind_forecast,total_load --enc_in 4 --dec_in 4 --c_out 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3390ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ECL_168_24          Model:              iTransformer        \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ECL_168_24_iTransformer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3383483\n",
      "\tspeed: 0.0198s/iter; left time: 898.4754s\n",
      "\titers: 200, epoch: 1 | loss: 0.0937683\n",
      "\tspeed: 0.0084s/iter; left time: 381.1544s\n",
      "\titers: 300, epoch: 1 | loss: 0.3826066\n",
      "\tspeed: 0.0084s/iter; left time: 380.8137s\n",
      "\titers: 400, epoch: 1 | loss: 0.3594372\n",
      "\tspeed: 0.0084s/iter; left time: 378.9998s\n",
      "\titers: 500, epoch: 1 | loss: 0.4931440\n",
      "\tspeed: 0.0084s/iter; left time: 378.1565s\n",
      "\titers: 600, epoch: 1 | loss: 0.3952905\n",
      "\tspeed: 0.0084s/iter; left time: 377.7326s\n",
      "\titers: 700, epoch: 1 | loss: 0.9455379\n",
      "\tspeed: 0.0084s/iter; left time: 376.7603s\n",
      "\titers: 800, epoch: 1 | loss: 0.3322417\n",
      "\tspeed: 0.0084s/iter; left time: 376.6958s\n",
      "\titers: 900, epoch: 1 | loss: 0.1606925\n",
      "\tspeed: 0.0084s/iter; left time: 375.4273s\n",
      "\titers: 1000, epoch: 1 | loss: 0.3542550\n",
      "\tspeed: 0.0084s/iter; left time: 375.5429s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2338824\n",
      "\tspeed: 0.0084s/iter; left time: 373.3877s\n",
      "\titers: 1200, epoch: 1 | loss: 0.4351874\n",
      "\tspeed: 0.0084s/iter; left time: 372.1464s\n",
      "\titers: 1300, epoch: 1 | loss: 0.0950237\n",
      "\tspeed: 0.0084s/iter; left time: 371.9115s\n",
      "\titers: 1400, epoch: 1 | loss: 0.5456495\n",
      "\tspeed: 0.0084s/iter; left time: 370.5297s\n",
      "\titers: 1500, epoch: 1 | loss: 0.5375092\n",
      "\tspeed: 0.0084s/iter; left time: 369.7143s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2250075\n",
      "\tspeed: 0.0084s/iter; left time: 369.1589s\n",
      "\titers: 1700, epoch: 1 | loss: 0.8144298\n",
      "\tspeed: 0.0084s/iter; left time: 367.7578s\n",
      "\titers: 1800, epoch: 1 | loss: 0.1520607\n",
      "\tspeed: 0.0084s/iter; left time: 366.4814s\n",
      "\titers: 1900, epoch: 1 | loss: 0.1268749\n",
      "\tspeed: 0.0084s/iter; left time: 365.7064s\n",
      "\titers: 2000, epoch: 1 | loss: 0.1522653\n",
      "\tspeed: 0.0084s/iter; left time: 365.1234s\n",
      "\titers: 2100, epoch: 1 | loss: 0.1833712\n",
      "\tspeed: 0.0084s/iter; left time: 363.8155s\n",
      "\titers: 2200, epoch: 1 | loss: 0.1797336\n",
      "\tspeed: 0.0084s/iter; left time: 364.0512s\n",
      "\titers: 2300, epoch: 1 | loss: 0.0876963\n",
      "\tspeed: 0.0084s/iter; left time: 362.5419s\n",
      "\titers: 2400, epoch: 1 | loss: 0.4276139\n",
      "\tspeed: 0.0084s/iter; left time: 362.8949s\n",
      "\titers: 2500, epoch: 1 | loss: 0.2079894\n",
      "\tspeed: 0.0084s/iter; left time: 361.0247s\n",
      "\titers: 2600, epoch: 1 | loss: 0.1517627\n",
      "\tspeed: 0.0084s/iter; left time: 360.5310s\n",
      "\titers: 2700, epoch: 1 | loss: 0.2927731\n",
      "\tspeed: 0.0084s/iter; left time: 359.1188s\n",
      "\titers: 2800, epoch: 1 | loss: 0.1064613\n",
      "\tspeed: 0.0084s/iter; left time: 358.4036s\n",
      "\titers: 2900, epoch: 1 | loss: 0.1765593\n",
      "\tspeed: 0.0084s/iter; left time: 358.1869s\n",
      "\titers: 3000, epoch: 1 | loss: 0.0692621\n",
      "\tspeed: 0.0084s/iter; left time: 357.2785s\n",
      "\titers: 3100, epoch: 1 | loss: 0.1469796\n",
      "\tspeed: 0.0084s/iter; left time: 356.0395s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2236366\n",
      "\tspeed: 0.0084s/iter; left time: 354.7547s\n",
      "\titers: 3300, epoch: 1 | loss: 0.1429650\n",
      "\tspeed: 0.0084s/iter; left time: 353.9759s\n",
      "\titers: 3400, epoch: 1 | loss: 0.2498830\n",
      "\tspeed: 0.0084s/iter; left time: 353.5776s\n",
      "\titers: 3500, epoch: 1 | loss: 0.2711340\n",
      "\tspeed: 0.0084s/iter; left time: 352.7110s\n",
      "\titers: 3600, epoch: 1 | loss: 0.1745270\n",
      "\tspeed: 0.0084s/iter; left time: 351.6951s\n",
      "\titers: 3700, epoch: 1 | loss: 0.9249262\n",
      "\tspeed: 0.0084s/iter; left time: 350.4641s\n",
      "\titers: 3800, epoch: 1 | loss: 0.1903939\n",
      "\tspeed: 0.0084s/iter; left time: 349.9657s\n",
      "\titers: 3900, epoch: 1 | loss: 0.3609475\n",
      "\tspeed: 0.0084s/iter; left time: 349.4499s\n",
      "\titers: 4000, epoch: 1 | loss: 0.2605188\n",
      "\tspeed: 0.0084s/iter; left time: 348.2429s\n",
      "\titers: 4100, epoch: 1 | loss: 0.3125542\n",
      "\tspeed: 0.0084s/iter; left time: 347.9044s\n",
      "\titers: 4200, epoch: 1 | loss: 0.5013072\n",
      "\tspeed: 0.0084s/iter; left time: 346.6928s\n",
      "\titers: 4300, epoch: 1 | loss: 0.5702764\n",
      "\tspeed: 0.0084s/iter; left time: 345.7843s\n",
      "\titers: 4400, epoch: 1 | loss: 0.4974548\n",
      "\tspeed: 0.0084s/iter; left time: 344.1956s\n",
      "\titers: 4500, epoch: 1 | loss: 0.2468426\n",
      "\tspeed: 0.0084s/iter; left time: 343.0035s\n",
      "Epoch: 1 cost time: 39.3963623046875\n",
      "Epoch: 1, Steps: 4555 | Train Loss: 0.3227974 Vali Loss: 0.0764558 Test Loss: 0.2160366\n",
      "Validation loss decreased (inf --> 0.076456).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7142131\n",
      "\tspeed: 0.0829s/iter; left time: 3392.2281s\n",
      "\titers: 200, epoch: 2 | loss: 0.2501137\n",
      "\tspeed: 0.0084s/iter; left time: 344.1766s\n",
      "\titers: 300, epoch: 2 | loss: 0.3907592\n",
      "\tspeed: 0.0084s/iter; left time: 342.4757s\n",
      "\titers: 400, epoch: 2 | loss: 0.7332084\n",
      "\tspeed: 0.0085s/iter; left time: 343.7316s\n",
      "\titers: 500, epoch: 2 | loss: 0.0792878\n",
      "\tspeed: 0.0085s/iter; left time: 342.6379s\n",
      "\titers: 600, epoch: 2 | loss: 0.1563655\n",
      "\tspeed: 0.0085s/iter; left time: 342.3050s\n",
      "\titers: 700, epoch: 2 | loss: 0.1404086\n",
      "\tspeed: 0.0085s/iter; left time: 342.1841s\n",
      "\titers: 800, epoch: 2 | loss: 0.7488512\n",
      "\tspeed: 0.0085s/iter; left time: 340.2718s\n",
      "\titers: 900, epoch: 2 | loss: 1.3886169\n",
      "\tspeed: 0.0085s/iter; left time: 339.2437s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2274904\n",
      "\tspeed: 0.0089s/iter; left time: 353.9808s\n",
      "\titers: 1100, epoch: 2 | loss: 0.8817586\n",
      "\tspeed: 0.0085s/iter; left time: 338.1954s\n",
      "\titers: 1200, epoch: 2 | loss: 0.0456859\n",
      "\tspeed: 0.0085s/iter; left time: 336.6111s\n",
      "\titers: 1300, epoch: 2 | loss: 1.0818727\n",
      "\tspeed: 0.0084s/iter; left time: 333.8212s\n",
      "\titers: 1400, epoch: 2 | loss: 0.0818301\n",
      "\tspeed: 0.0084s/iter; left time: 331.9702s\n",
      "\titers: 1500, epoch: 2 | loss: 0.4550385\n",
      "\tspeed: 0.0084s/iter; left time: 331.7264s\n",
      "\titers: 1600, epoch: 2 | loss: 0.1914171\n",
      "\tspeed: 0.0084s/iter; left time: 331.0036s\n",
      "\titers: 1700, epoch: 2 | loss: 0.2897817\n",
      "\tspeed: 0.0085s/iter; left time: 334.0605s\n",
      "\titers: 1800, epoch: 2 | loss: 0.2367999\n",
      "\tspeed: 0.0085s/iter; left time: 332.9801s\n",
      "\titers: 1900, epoch: 2 | loss: 0.9024547\n",
      "\tspeed: 0.0085s/iter; left time: 331.4956s\n",
      "\titers: 2000, epoch: 2 | loss: 0.1371598\n",
      "\tspeed: 0.0085s/iter; left time: 329.6423s\n",
      "\titers: 2100, epoch: 2 | loss: 0.3689032\n",
      "\tspeed: 0.0084s/iter; left time: 327.9178s\n",
      "\titers: 2200, epoch: 2 | loss: 0.5458075\n",
      "\tspeed: 0.0084s/iter; left time: 327.7062s\n",
      "\titers: 2300, epoch: 2 | loss: 0.1516711\n",
      "\tspeed: 0.0084s/iter; left time: 325.5709s\n",
      "\titers: 2400, epoch: 2 | loss: 0.5824330\n",
      "\tspeed: 0.0084s/iter; left time: 325.3371s\n",
      "\titers: 2500, epoch: 2 | loss: 0.1847297\n",
      "\tspeed: 0.0084s/iter; left time: 324.3106s\n",
      "\titers: 2600, epoch: 2 | loss: 0.1043569\n",
      "\tspeed: 0.0084s/iter; left time: 323.8397s\n",
      "\titers: 2700, epoch: 2 | loss: 0.5423755\n",
      "\tspeed: 0.0084s/iter; left time: 323.2453s\n",
      "\titers: 2800, epoch: 2 | loss: 0.3281554\n",
      "\tspeed: 0.0085s/iter; left time: 323.2444s\n",
      "\titers: 2900, epoch: 2 | loss: 0.3702515\n",
      "\tspeed: 0.0084s/iter; left time: 321.8635s\n",
      "\titers: 3000, epoch: 2 | loss: 0.1747777\n",
      "\tspeed: 0.0084s/iter; left time: 320.7731s\n",
      "\titers: 3100, epoch: 2 | loss: 0.1076411\n",
      "\tspeed: 0.0085s/iter; left time: 320.4254s\n",
      "\titers: 3200, epoch: 2 | loss: 0.2381690\n",
      "\tspeed: 0.0084s/iter; left time: 319.1138s\n",
      "\titers: 3300, epoch: 2 | loss: 0.1267567\n",
      "\tspeed: 0.0084s/iter; left time: 317.9122s\n",
      "\titers: 3400, epoch: 2 | loss: 0.0822699\n",
      "\tspeed: 0.0084s/iter; left time: 317.4512s\n",
      "\titers: 3500, epoch: 2 | loss: 0.0823660\n",
      "\tspeed: 0.0084s/iter; left time: 316.4350s\n",
      "\titers: 3600, epoch: 2 | loss: 0.2062616\n",
      "\tspeed: 0.0084s/iter; left time: 314.2447s\n",
      "\titers: 3700, epoch: 2 | loss: 0.3340177\n",
      "\tspeed: 0.0084s/iter; left time: 313.3975s\n",
      "\titers: 3800, epoch: 2 | loss: 0.1131992\n",
      "\tspeed: 0.0084s/iter; left time: 311.9412s\n",
      "\titers: 3900, epoch: 2 | loss: 0.2325082\n",
      "\tspeed: 0.0084s/iter; left time: 311.2155s\n",
      "\titers: 4000, epoch: 2 | loss: 0.2105549\n",
      "\tspeed: 0.0084s/iter; left time: 310.5803s\n",
      "\titers: 4100, epoch: 2 | loss: 0.5881748\n",
      "\tspeed: 0.0084s/iter; left time: 309.4814s\n",
      "\titers: 4200, epoch: 2 | loss: 0.5722644\n",
      "\tspeed: 0.0084s/iter; left time: 309.4444s\n",
      "\titers: 4300, epoch: 2 | loss: 0.8366055\n",
      "\tspeed: 0.0084s/iter; left time: 308.4669s\n",
      "\titers: 4400, epoch: 2 | loss: 0.1062878\n",
      "\tspeed: 0.0084s/iter; left time: 307.7106s\n",
      "\titers: 4500, epoch: 2 | loss: 0.2809774\n",
      "\tspeed: 0.0084s/iter; left time: 307.1462s\n",
      "Epoch: 2 cost time: 38.68032145500183\n",
      "Epoch: 2, Steps: 4555 | Train Loss: 0.3400744 Vali Loss: 0.0777127 Test Loss: 0.2182466\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3713241\n",
      "\tspeed: 0.0870s/iter; left time: 3161.9443s\n",
      "\titers: 200, epoch: 3 | loss: 0.1849464\n",
      "\tspeed: 0.0084s/iter; left time: 305.6101s\n",
      "\titers: 300, epoch: 3 | loss: 0.1664238\n",
      "\tspeed: 0.0084s/iter; left time: 304.8801s\n",
      "\titers: 400, epoch: 3 | loss: 0.3719575\n",
      "\tspeed: 0.0084s/iter; left time: 303.9407s\n",
      "\titers: 500, epoch: 3 | loss: 0.1297059\n",
      "\tspeed: 0.0085s/iter; left time: 303.9440s\n",
      "\titers: 600, epoch: 3 | loss: 0.3258669\n",
      "\tspeed: 0.0084s/iter; left time: 301.9583s\n",
      "\titers: 700, epoch: 3 | loss: 0.4632008\n",
      "\tspeed: 0.0084s/iter; left time: 301.2629s\n",
      "\titers: 800, epoch: 3 | loss: 0.2105758\n",
      "\tspeed: 0.0085s/iter; left time: 301.2363s\n",
      "\titers: 900, epoch: 3 | loss: 0.1890866\n",
      "\tspeed: 0.0084s/iter; left time: 299.7492s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3166313\n",
      "\tspeed: 0.0084s/iter; left time: 298.8666s\n",
      "\titers: 1100, epoch: 3 | loss: 0.1653025\n",
      "\tspeed: 0.0084s/iter; left time: 297.8972s\n",
      "\titers: 1200, epoch: 3 | loss: 0.1554964\n",
      "\tspeed: 0.0084s/iter; left time: 296.9002s\n",
      "\titers: 1300, epoch: 3 | loss: 0.4456421\n",
      "\tspeed: 0.0084s/iter; left time: 296.0503s\n",
      "\titers: 1400, epoch: 3 | loss: 0.0968892\n",
      "\tspeed: 0.0084s/iter; left time: 294.8844s\n",
      "\titers: 1500, epoch: 3 | loss: 0.4768956\n",
      "\tspeed: 0.0084s/iter; left time: 295.1382s\n",
      "\titers: 1600, epoch: 3 | loss: 0.9257110\n",
      "\tspeed: 0.0084s/iter; left time: 293.6726s\n",
      "\titers: 1700, epoch: 3 | loss: 0.5924417\n",
      "\tspeed: 0.0084s/iter; left time: 293.4202s\n",
      "\titers: 1800, epoch: 3 | loss: 0.3120061\n",
      "\tspeed: 0.0085s/iter; left time: 293.9021s\n",
      "\titers: 1900, epoch: 3 | loss: 0.0758285\n",
      "\tspeed: 0.0085s/iter; left time: 294.9583s\n",
      "\titers: 2000, epoch: 3 | loss: 0.7273237\n",
      "\tspeed: 0.0085s/iter; left time: 291.4882s\n",
      "\titers: 2100, epoch: 3 | loss: 0.4885439\n",
      "\tspeed: 0.0085s/iter; left time: 290.5947s\n",
      "\titers: 2200, epoch: 3 | loss: 0.1021854\n",
      "\tspeed: 0.0084s/iter; left time: 288.9326s\n",
      "\titers: 2300, epoch: 3 | loss: 0.2561643\n",
      "\tspeed: 0.0086s/iter; left time: 292.7996s\n",
      "\titers: 2400, epoch: 3 | loss: 0.0589538\n",
      "\tspeed: 0.0095s/iter; left time: 322.2478s\n",
      "\titers: 2500, epoch: 3 | loss: 0.2063089\n",
      "\tspeed: 0.0088s/iter; left time: 298.3140s\n",
      "\titers: 2600, epoch: 3 | loss: 0.2949898\n",
      "\tspeed: 0.0084s/iter; left time: 284.2310s\n",
      "\titers: 2700, epoch: 3 | loss: 0.2094677\n",
      "\tspeed: 0.0085s/iter; left time: 285.2701s\n",
      "\titers: 2800, epoch: 3 | loss: 0.2459334\n",
      "\tspeed: 0.0085s/iter; left time: 284.9080s\n",
      "\titers: 2900, epoch: 3 | loss: 0.4769090\n",
      "\tspeed: 0.0085s/iter; left time: 284.4619s\n",
      "\titers: 3000, epoch: 3 | loss: 0.0952217\n",
      "\tspeed: 0.0089s/iter; left time: 295.9862s\n",
      "\titers: 3100, epoch: 3 | loss: 0.1717358\n",
      "\tspeed: 0.0096s/iter; left time: 319.8335s\n",
      "\titers: 3200, epoch: 3 | loss: 0.2116722\n",
      "\tspeed: 0.0096s/iter; left time: 318.7903s\n",
      "\titers: 3300, epoch: 3 | loss: 0.2261878\n",
      "\tspeed: 0.0096s/iter; left time: 317.5853s\n",
      "\titers: 3400, epoch: 3 | loss: 0.1393160\n",
      "\tspeed: 0.0096s/iter; left time: 316.3261s\n",
      "\titers: 3500, epoch: 3 | loss: 0.3725886\n",
      "\tspeed: 0.0096s/iter; left time: 317.2336s\n",
      "\titers: 3600, epoch: 3 | loss: 0.1736308\n",
      "\tspeed: 0.0097s/iter; left time: 317.2065s\n",
      "\titers: 3700, epoch: 3 | loss: 0.5086954\n",
      "\tspeed: 0.0085s/iter; left time: 277.2799s\n",
      "\titers: 3800, epoch: 3 | loss: 0.3923805\n",
      "\tspeed: 0.0084s/iter; left time: 274.1372s\n",
      "\titers: 3900, epoch: 3 | loss: 0.0630379\n",
      "\tspeed: 0.0084s/iter; left time: 273.2555s\n",
      "\titers: 4000, epoch: 3 | loss: 0.2036247\n",
      "\tspeed: 0.0084s/iter; left time: 272.8786s\n",
      "\titers: 4100, epoch: 3 | loss: 0.1898506\n",
      "\tspeed: 0.0084s/iter; left time: 271.9039s\n",
      "\titers: 4200, epoch: 3 | loss: 0.1716248\n",
      "\tspeed: 0.0084s/iter; left time: 271.0886s\n",
      "\titers: 4300, epoch: 3 | loss: 0.4897889\n",
      "\tspeed: 0.0084s/iter; left time: 270.2699s\n",
      "\titers: 4400, epoch: 3 | loss: 0.1250155\n",
      "\tspeed: 0.0084s/iter; left time: 269.5231s\n",
      "\titers: 4500, epoch: 3 | loss: 0.2365013\n",
      "\tspeed: 0.0084s/iter; left time: 268.2473s\n",
      "Epoch: 3 cost time: 39.53513240814209\n",
      "Epoch: 3, Steps: 4555 | Train Loss: 0.3157825 Vali Loss: 0.0789028 Test Loss: 0.2270081\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1879131\n",
      "\tspeed: 0.0875s/iter; left time: 2781.1220s\n",
      "\titers: 200, epoch: 4 | loss: 0.1087095\n",
      "\tspeed: 0.0095s/iter; left time: 300.5449s\n",
      "\titers: 300, epoch: 4 | loss: 0.4338154\n",
      "\tspeed: 0.0089s/iter; left time: 279.5762s\n",
      "\titers: 400, epoch: 4 | loss: 0.1893549\n",
      "\tspeed: 0.0091s/iter; left time: 285.3226s\n",
      "\titers: 500, epoch: 4 | loss: 0.1278918\n",
      "\tspeed: 0.0095s/iter; left time: 298.5485s\n",
      "\titers: 600, epoch: 4 | loss: 0.1352796\n",
      "\tspeed: 0.0095s/iter; left time: 297.3907s\n",
      "\titers: 700, epoch: 4 | loss: 0.1177830\n",
      "\tspeed: 0.0095s/iter; left time: 297.2111s\n",
      "\titers: 800, epoch: 4 | loss: 0.2874950\n",
      "\tspeed: 0.0095s/iter; left time: 294.9755s\n",
      "\titers: 900, epoch: 4 | loss: 0.0952337\n",
      "\tspeed: 0.0095s/iter; left time: 293.8501s\n",
      "\titers: 1000, epoch: 4 | loss: 0.1125529\n",
      "\tspeed: 0.0087s/iter; left time: 267.7256s\n",
      "\titers: 1100, epoch: 4 | loss: 0.4384255\n",
      "\tspeed: 0.0084s/iter; left time: 259.5588s\n",
      "\titers: 1200, epoch: 4 | loss: 0.3299170\n",
      "\tspeed: 0.0084s/iter; left time: 258.7882s\n",
      "\titers: 1300, epoch: 4 | loss: 0.0749428\n",
      "\tspeed: 0.0084s/iter; left time: 257.8076s\n",
      "\titers: 1400, epoch: 4 | loss: 0.0727793\n",
      "\tspeed: 0.0084s/iter; left time: 256.9121s\n",
      "\titers: 1500, epoch: 4 | loss: 0.2857724\n",
      "\tspeed: 0.0084s/iter; left time: 255.9533s\n",
      "\titers: 1600, epoch: 4 | loss: 0.3125023\n",
      "\tspeed: 0.0084s/iter; left time: 255.0731s\n",
      "\titers: 1700, epoch: 4 | loss: 0.1975015\n",
      "\tspeed: 0.0084s/iter; left time: 254.2892s\n",
      "\titers: 1800, epoch: 4 | loss: 0.1834474\n",
      "\tspeed: 0.0084s/iter; left time: 253.5741s\n",
      "\titers: 1900, epoch: 4 | loss: 0.0603527\n",
      "\tspeed: 0.0084s/iter; left time: 252.0929s\n",
      "\titers: 2000, epoch: 4 | loss: 0.3518045\n",
      "\tspeed: 0.0084s/iter; left time: 250.9187s\n",
      "\titers: 2100, epoch: 4 | loss: 0.1186585\n",
      "\tspeed: 0.0084s/iter; left time: 250.1281s\n",
      "\titers: 2200, epoch: 4 | loss: 0.0611819\n",
      "\tspeed: 0.0084s/iter; left time: 249.3125s\n",
      "\titers: 2300, epoch: 4 | loss: 0.5026178\n",
      "\tspeed: 0.0084s/iter; left time: 248.2342s\n",
      "\titers: 2400, epoch: 4 | loss: 0.2007188\n",
      "\tspeed: 0.0084s/iter; left time: 247.5390s\n",
      "\titers: 2500, epoch: 4 | loss: 0.2117305\n",
      "\tspeed: 0.0084s/iter; left time: 246.7123s\n",
      "\titers: 2600, epoch: 4 | loss: 0.3905846\n",
      "\tspeed: 0.0084s/iter; left time: 245.8095s\n",
      "\titers: 2700, epoch: 4 | loss: 1.0947640\n",
      "\tspeed: 0.0084s/iter; left time: 245.0965s\n",
      "\titers: 2800, epoch: 4 | loss: 0.1661665\n",
      "\tspeed: 0.0084s/iter; left time: 244.1122s\n",
      "\titers: 2900, epoch: 4 | loss: 1.4783716\n",
      "\tspeed: 0.0084s/iter; left time: 243.3151s\n",
      "\titers: 3000, epoch: 4 | loss: 0.0723623\n",
      "\tspeed: 0.0084s/iter; left time: 242.5268s\n",
      "\titers: 3100, epoch: 4 | loss: 0.2889450\n",
      "\tspeed: 0.0084s/iter; left time: 241.6047s\n",
      "\titers: 3200, epoch: 4 | loss: 0.2983990\n",
      "\tspeed: 0.0084s/iter; left time: 240.6597s\n",
      "\titers: 3300, epoch: 4 | loss: 0.4322777\n",
      "\tspeed: 0.0084s/iter; left time: 240.0135s\n",
      "\titers: 3400, epoch: 4 | loss: 0.4393774\n",
      "\tspeed: 0.0084s/iter; left time: 239.1771s\n",
      "\titers: 3500, epoch: 4 | loss: 0.2735419\n",
      "\tspeed: 0.0084s/iter; left time: 238.0758s\n",
      "\titers: 3600, epoch: 4 | loss: 0.8834643\n",
      "\tspeed: 0.0084s/iter; left time: 237.1849s\n",
      "\titers: 3700, epoch: 4 | loss: 0.2621011\n",
      "\tspeed: 0.0084s/iter; left time: 236.3861s\n",
      "\titers: 3800, epoch: 4 | loss: 0.5318828\n",
      "\tspeed: 0.0084s/iter; left time: 235.2852s\n",
      "\titers: 3900, epoch: 4 | loss: 0.5982270\n",
      "\tspeed: 0.0084s/iter; left time: 234.3637s\n",
      "\titers: 4000, epoch: 4 | loss: 0.1458519\n",
      "\tspeed: 0.0084s/iter; left time: 233.2618s\n",
      "\titers: 4100, epoch: 4 | loss: 0.2282347\n",
      "\tspeed: 0.0084s/iter; left time: 232.4600s\n",
      "\titers: 4200, epoch: 4 | loss: 0.1320763\n",
      "\tspeed: 0.0084s/iter; left time: 231.7036s\n",
      "\titers: 4300, epoch: 4 | loss: 0.3109755\n",
      "\tspeed: 0.0084s/iter; left time: 230.8273s\n",
      "\titers: 4400, epoch: 4 | loss: 0.2485734\n",
      "\tspeed: 0.0084s/iter; left time: 229.9775s\n",
      "\titers: 4500, epoch: 4 | loss: 0.2497501\n",
      "\tspeed: 0.0084s/iter; left time: 228.8734s\n",
      "Epoch: 4 cost time: 39.37575125694275\n",
      "Epoch: 4, Steps: 4555 | Train Loss: 0.2950841 Vali Loss: 0.0740255 Test Loss: 0.2079911\n",
      "Validation loss decreased (0.076456 --> 0.074025).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1890315\n",
      "\tspeed: 0.0824s/iter; left time: 2244.7183s\n",
      "\titers: 200, epoch: 5 | loss: 0.3985639\n",
      "\tspeed: 0.0085s/iter; left time: 229.5580s\n",
      "\titers: 300, epoch: 5 | loss: 0.1625131\n",
      "\tspeed: 0.0085s/iter; left time: 228.9338s\n",
      "\titers: 400, epoch: 5 | loss: 0.4280031\n",
      "\tspeed: 0.0085s/iter; left time: 228.2037s\n",
      "\titers: 500, epoch: 5 | loss: 0.0524968\n",
      "\tspeed: 0.0085s/iter; left time: 227.1600s\n",
      "\titers: 600, epoch: 5 | loss: 0.1109164\n",
      "\tspeed: 0.0084s/iter; left time: 225.8216s\n",
      "\titers: 700, epoch: 5 | loss: 0.2971227\n",
      "\tspeed: 0.0084s/iter; left time: 224.7834s\n",
      "\titers: 800, epoch: 5 | loss: 0.1000072\n",
      "\tspeed: 0.0084s/iter; left time: 224.1817s\n",
      "\titers: 900, epoch: 5 | loss: 0.9991280\n",
      "\tspeed: 0.0084s/iter; left time: 223.0979s\n",
      "\titers: 1000, epoch: 5 | loss: 0.1303746\n",
      "\tspeed: 0.0085s/iter; left time: 222.4999s\n",
      "\titers: 1100, epoch: 5 | loss: 0.0731320\n",
      "\tspeed: 0.0085s/iter; left time: 221.7441s\n",
      "\titers: 1200, epoch: 5 | loss: 0.1295675\n",
      "\tspeed: 0.0084s/iter; left time: 220.1895s\n",
      "\titers: 1300, epoch: 5 | loss: 0.0496431\n",
      "\tspeed: 0.0084s/iter; left time: 219.4901s\n",
      "\titers: 1400, epoch: 5 | loss: 0.1289354\n",
      "\tspeed: 0.0084s/iter; left time: 218.8593s\n",
      "\titers: 1500, epoch: 5 | loss: 0.3768961\n",
      "\tspeed: 0.0084s/iter; left time: 217.4452s\n",
      "\titers: 1600, epoch: 5 | loss: 0.0569272\n",
      "\tspeed: 0.0084s/iter; left time: 216.6571s\n",
      "\titers: 1700, epoch: 5 | loss: 0.0518002\n",
      "\tspeed: 0.0084s/iter; left time: 215.8985s\n",
      "\titers: 1800, epoch: 5 | loss: 0.0919653\n",
      "\tspeed: 0.0084s/iter; left time: 215.2955s\n",
      "\titers: 1900, epoch: 5 | loss: 0.1620061\n",
      "\tspeed: 0.0084s/iter; left time: 214.2294s\n",
      "\titers: 2000, epoch: 5 | loss: 0.2138371\n",
      "\tspeed: 0.0084s/iter; left time: 213.2847s\n",
      "\titers: 2100, epoch: 5 | loss: 0.5565748\n",
      "\tspeed: 0.0084s/iter; left time: 212.3427s\n",
      "\titers: 2200, epoch: 5 | loss: 0.3720240\n",
      "\tspeed: 0.0084s/iter; left time: 211.6131s\n",
      "\titers: 2300, epoch: 5 | loss: 0.1378481\n",
      "\tspeed: 0.0084s/iter; left time: 210.8233s\n",
      "\titers: 2400, epoch: 5 | loss: 0.2793694\n",
      "\tspeed: 0.0084s/iter; left time: 209.8590s\n",
      "\titers: 2500, epoch: 5 | loss: 0.1582951\n",
      "\tspeed: 0.0084s/iter; left time: 209.0066s\n",
      "\titers: 2600, epoch: 5 | loss: 0.1885826\n",
      "\tspeed: 0.0084s/iter; left time: 208.1734s\n",
      "\titers: 2700, epoch: 5 | loss: 0.1333653\n",
      "\tspeed: 0.0084s/iter; left time: 207.3643s\n",
      "\titers: 2800, epoch: 5 | loss: 0.0812571\n",
      "\tspeed: 0.0084s/iter; left time: 206.5898s\n",
      "\titers: 2900, epoch: 5 | loss: 0.0399602\n",
      "\tspeed: 0.0084s/iter; left time: 205.6317s\n",
      "\titers: 3000, epoch: 5 | loss: 0.4344367\n",
      "\tspeed: 0.0084s/iter; left time: 204.7671s\n",
      "\titers: 3100, epoch: 5 | loss: 0.0749274\n",
      "\tspeed: 0.0084s/iter; left time: 203.9326s\n",
      "\titers: 3200, epoch: 5 | loss: 0.3382207\n",
      "\tspeed: 0.0084s/iter; left time: 203.1468s\n",
      "\titers: 3300, epoch: 5 | loss: 0.2926760\n",
      "\tspeed: 0.0084s/iter; left time: 202.8568s\n",
      "\titers: 3400, epoch: 5 | loss: 0.3941672\n",
      "\tspeed: 0.0084s/iter; left time: 201.1317s\n",
      "\titers: 3500, epoch: 5 | loss: 0.0453909\n",
      "\tspeed: 0.0084s/iter; left time: 200.4146s\n",
      "\titers: 3600, epoch: 5 | loss: 0.1960377\n",
      "\tspeed: 0.0084s/iter; left time: 199.8856s\n",
      "\titers: 3700, epoch: 5 | loss: 0.5865046\n",
      "\tspeed: 0.0084s/iter; left time: 199.4324s\n",
      "\titers: 3800, epoch: 5 | loss: 0.8397959\n",
      "\tspeed: 0.0084s/iter; left time: 198.4635s\n",
      "\titers: 3900, epoch: 5 | loss: 0.1889786\n",
      "\tspeed: 0.0084s/iter; left time: 197.8873s\n",
      "\titers: 4000, epoch: 5 | loss: 0.2967922\n",
      "\tspeed: 0.0085s/iter; left time: 197.1584s\n",
      "\titers: 4100, epoch: 5 | loss: 1.3908634\n",
      "\tspeed: 0.0084s/iter; left time: 196.2698s\n",
      "\titers: 4200, epoch: 5 | loss: 0.3464990\n",
      "\tspeed: 0.0084s/iter; left time: 195.4565s\n",
      "\titers: 4300, epoch: 5 | loss: 0.2350991\n",
      "\tspeed: 0.0084s/iter; left time: 194.6076s\n",
      "\titers: 4400, epoch: 5 | loss: 0.0494515\n",
      "\tspeed: 0.0084s/iter; left time: 193.7611s\n",
      "\titers: 4500, epoch: 5 | loss: 0.4069833\n",
      "\tspeed: 0.0085s/iter; left time: 193.0295s\n",
      "Epoch: 5 cost time: 38.641353130340576\n",
      "Epoch: 5, Steps: 4555 | Train Loss: 0.2792337 Vali Loss: 0.0685871 Test Loss: 0.1965670\n",
      "Validation loss decreased (0.074025 --> 0.068587).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1322607\n",
      "\tspeed: 0.0856s/iter; left time: 1941.3951s\n",
      "\titers: 200, epoch: 6 | loss: 0.1428641\n",
      "\tspeed: 0.0084s/iter; left time: 190.0684s\n",
      "\titers: 300, epoch: 6 | loss: 0.1227680\n",
      "\tspeed: 0.0084s/iter; left time: 189.4639s\n",
      "\titers: 400, epoch: 6 | loss: 0.2791308\n",
      "\tspeed: 0.0084s/iter; left time: 188.7687s\n",
      "\titers: 500, epoch: 6 | loss: 0.2310678\n",
      "\tspeed: 0.0084s/iter; left time: 187.5951s\n",
      "\titers: 600, epoch: 6 | loss: 0.4777047\n",
      "\tspeed: 0.0084s/iter; left time: 186.5177s\n",
      "\titers: 700, epoch: 6 | loss: 0.3401070\n",
      "\tspeed: 0.0084s/iter; left time: 185.7211s\n",
      "\titers: 800, epoch: 6 | loss: 0.1731204\n",
      "\tspeed: 0.0084s/iter; left time: 184.6635s\n",
      "\titers: 900, epoch: 6 | loss: 0.3446606\n",
      "\tspeed: 0.0084s/iter; left time: 183.7912s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0648908\n",
      "\tspeed: 0.0084s/iter; left time: 182.9812s\n",
      "\titers: 1100, epoch: 6 | loss: 0.0610173\n",
      "\tspeed: 0.0084s/iter; left time: 182.0428s\n",
      "\titers: 1200, epoch: 6 | loss: 0.0897525\n",
      "\tspeed: 0.0084s/iter; left time: 181.4970s\n",
      "\titers: 1300, epoch: 6 | loss: 0.2075655\n",
      "\tspeed: 0.0084s/iter; left time: 180.5017s\n",
      "\titers: 1400, epoch: 6 | loss: 0.5208794\n",
      "\tspeed: 0.0084s/iter; left time: 179.8823s\n",
      "\titers: 1500, epoch: 6 | loss: 0.1408711\n",
      "\tspeed: 0.0084s/iter; left time: 178.7585s\n",
      "\titers: 1600, epoch: 6 | loss: 0.1014047\n",
      "\tspeed: 0.0084s/iter; left time: 178.0612s\n",
      "\titers: 1700, epoch: 6 | loss: 0.0724487\n",
      "\tspeed: 0.0084s/iter; left time: 177.5229s\n",
      "\titers: 1800, epoch: 6 | loss: 0.2387922\n",
      "\tspeed: 0.0084s/iter; left time: 176.3918s\n",
      "\titers: 1900, epoch: 6 | loss: 0.1769284\n",
      "\tspeed: 0.0084s/iter; left time: 175.6757s\n",
      "\titers: 2000, epoch: 6 | loss: 0.0882448\n",
      "\tspeed: 0.0084s/iter; left time: 174.7513s\n",
      "\titers: 2100, epoch: 6 | loss: 0.1245074\n",
      "\tspeed: 0.0084s/iter; left time: 173.7871s\n",
      "\titers: 2200, epoch: 6 | loss: 0.1524421\n",
      "\tspeed: 0.0084s/iter; left time: 173.0612s\n",
      "\titers: 2300, epoch: 6 | loss: 0.2068089\n",
      "\tspeed: 0.0084s/iter; left time: 172.0606s\n",
      "\titers: 2400, epoch: 6 | loss: 0.2324065\n",
      "\tspeed: 0.0084s/iter; left time: 171.1747s\n",
      "\titers: 2500, epoch: 6 | loss: 0.1140242\n",
      "\tspeed: 0.0084s/iter; left time: 170.5580s\n",
      "\titers: 2600, epoch: 6 | loss: 0.1914656\n",
      "\tspeed: 0.0084s/iter; left time: 169.6545s\n",
      "\titers: 2700, epoch: 6 | loss: 0.1686910\n",
      "\tspeed: 0.0084s/iter; left time: 168.6386s\n",
      "\titers: 2800, epoch: 6 | loss: 0.1871237\n",
      "\tspeed: 0.0084s/iter; left time: 167.8190s\n",
      "\titers: 2900, epoch: 6 | loss: 0.2063713\n",
      "\tspeed: 0.0084s/iter; left time: 167.0594s\n",
      "\titers: 3000, epoch: 6 | loss: 0.1794955\n",
      "\tspeed: 0.0084s/iter; left time: 166.2721s\n",
      "\titers: 3100, epoch: 6 | loss: 0.1869358\n",
      "\tspeed: 0.0084s/iter; left time: 165.8509s\n",
      "\titers: 3200, epoch: 6 | loss: 0.1861446\n",
      "\tspeed: 0.0084s/iter; left time: 165.2811s\n",
      "\titers: 3300, epoch: 6 | loss: 0.0778912\n",
      "\tspeed: 0.0085s/iter; left time: 164.5742s\n",
      "\titers: 3400, epoch: 6 | loss: 0.1413440\n",
      "\tspeed: 0.0084s/iter; left time: 163.6325s\n",
      "\titers: 3500, epoch: 6 | loss: 0.1322583\n",
      "\tspeed: 0.0084s/iter; left time: 162.7131s\n",
      "\titers: 3600, epoch: 6 | loss: 0.2181812\n",
      "\tspeed: 0.0084s/iter; left time: 161.5645s\n",
      "\titers: 3700, epoch: 6 | loss: 0.0732046\n",
      "\tspeed: 0.0084s/iter; left time: 160.4834s\n",
      "\titers: 3800, epoch: 6 | loss: 0.1002338\n",
      "\tspeed: 0.0084s/iter; left time: 159.8581s\n",
      "\titers: 3900, epoch: 6 | loss: 0.1932928\n",
      "\tspeed: 0.0084s/iter; left time: 158.8926s\n",
      "\titers: 4000, epoch: 6 | loss: 0.5687632\n",
      "\tspeed: 0.0084s/iter; left time: 158.0022s\n",
      "\titers: 4100, epoch: 6 | loss: 0.6954714\n",
      "\tspeed: 0.0084s/iter; left time: 157.1932s\n",
      "\titers: 4200, epoch: 6 | loss: 0.0781039\n",
      "\tspeed: 0.0084s/iter; left time: 156.2817s\n",
      "\titers: 4300, epoch: 6 | loss: 0.3172317\n",
      "\tspeed: 0.0084s/iter; left time: 155.4942s\n",
      "\titers: 4400, epoch: 6 | loss: 0.3301032\n",
      "\tspeed: 0.0084s/iter; left time: 154.6454s\n",
      "\titers: 4500, epoch: 6 | loss: 0.1483583\n",
      "\tspeed: 0.0084s/iter; left time: 153.8415s\n",
      "Epoch: 6 cost time: 38.54790425300598\n",
      "Epoch: 6, Steps: 4555 | Train Loss: 0.2732937 Vali Loss: 0.0700861 Test Loss: 0.2004268\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0977147\n",
      "\tspeed: 0.0936s/iter; left time: 1695.7847s\n",
      "\titers: 200, epoch: 7 | loss: 0.3738163\n",
      "\tspeed: 0.0095s/iter; left time: 170.9675s\n",
      "\titers: 300, epoch: 7 | loss: 0.2845672\n",
      "\tspeed: 0.0095s/iter; left time: 169.6766s\n",
      "\titers: 400, epoch: 7 | loss: 0.1332166\n",
      "\tspeed: 0.0095s/iter; left time: 168.7125s\n",
      "\titers: 500, epoch: 7 | loss: 0.2493016\n",
      "\tspeed: 0.0095s/iter; left time: 167.7030s\n",
      "\titers: 600, epoch: 7 | loss: 0.0415066\n",
      "\tspeed: 0.0094s/iter; left time: 166.4651s\n",
      "\titers: 700, epoch: 7 | loss: 0.0257855\n",
      "\tspeed: 0.0095s/iter; left time: 165.5846s\n",
      "\titers: 800, epoch: 7 | loss: 0.3008042\n",
      "\tspeed: 0.0095s/iter; left time: 164.8091s\n",
      "\titers: 900, epoch: 7 | loss: 0.4117896\n",
      "\tspeed: 0.0095s/iter; left time: 163.8458s\n",
      "\titers: 1000, epoch: 7 | loss: 0.2397494\n",
      "\tspeed: 0.0095s/iter; left time: 162.7541s\n",
      "\titers: 1100, epoch: 7 | loss: 0.5769445\n",
      "\tspeed: 0.0094s/iter; left time: 161.6613s\n",
      "\titers: 1200, epoch: 7 | loss: 0.1474582\n",
      "\tspeed: 0.0094s/iter; left time: 160.7176s\n",
      "\titers: 1300, epoch: 7 | loss: 0.1473433\n",
      "\tspeed: 0.0094s/iter; left time: 159.8063s\n",
      "\titers: 1400, epoch: 7 | loss: 0.1156929\n",
      "\tspeed: 0.0094s/iter; left time: 158.8804s\n",
      "\titers: 1500, epoch: 7 | loss: 0.0392870\n",
      "\tspeed: 0.0094s/iter; left time: 157.9234s\n",
      "\titers: 1600, epoch: 7 | loss: 0.1247773\n",
      "\tspeed: 0.0094s/iter; left time: 156.8924s\n",
      "\titers: 1700, epoch: 7 | loss: 0.1271690\n",
      "\tspeed: 0.0094s/iter; left time: 155.9530s\n",
      "\titers: 1800, epoch: 7 | loss: 0.0560226\n",
      "\tspeed: 0.0094s/iter; left time: 154.9990s\n",
      "\titers: 1900, epoch: 7 | loss: 0.2632294\n",
      "\tspeed: 0.0094s/iter; left time: 153.9077s\n",
      "\titers: 2000, epoch: 7 | loss: 0.1269963\n",
      "\tspeed: 0.0084s/iter; left time: 135.7713s\n",
      "\titers: 2100, epoch: 7 | loss: 0.1450520\n",
      "\tspeed: 0.0084s/iter; left time: 135.0230s\n",
      "\titers: 2200, epoch: 7 | loss: 0.4104849\n",
      "\tspeed: 0.0084s/iter; left time: 134.0786s\n",
      "\titers: 2300, epoch: 7 | loss: 0.2356005\n",
      "\tspeed: 0.0084s/iter; left time: 133.1937s\n",
      "\titers: 2400, epoch: 7 | loss: 0.0797252\n",
      "\tspeed: 0.0084s/iter; left time: 132.4023s\n",
      "\titers: 2500, epoch: 7 | loss: 1.1197362\n",
      "\tspeed: 0.0084s/iter; left time: 131.5816s\n",
      "\titers: 2600, epoch: 7 | loss: 0.2145066\n",
      "\tspeed: 0.0084s/iter; left time: 130.7564s\n",
      "\titers: 2700, epoch: 7 | loss: 0.6052713\n",
      "\tspeed: 0.0084s/iter; left time: 129.8827s\n",
      "\titers: 2800, epoch: 7 | loss: 0.1220119\n",
      "\tspeed: 0.0084s/iter; left time: 129.1214s\n",
      "\titers: 2900, epoch: 7 | loss: 0.1870904\n",
      "\tspeed: 0.0084s/iter; left time: 128.7196s\n",
      "\titers: 3000, epoch: 7 | loss: 0.8315911\n",
      "\tspeed: 0.0084s/iter; left time: 127.8765s\n",
      "\titers: 3100, epoch: 7 | loss: 0.4363217\n",
      "\tspeed: 0.0084s/iter; left time: 127.0636s\n",
      "\titers: 3200, epoch: 7 | loss: 0.0990814\n",
      "\tspeed: 0.0084s/iter; left time: 126.1317s\n",
      "\titers: 3300, epoch: 7 | loss: 0.3015465\n",
      "\tspeed: 0.0087s/iter; left time: 129.3220s\n",
      "\titers: 3400, epoch: 7 | loss: 0.4943682\n",
      "\tspeed: 0.0094s/iter; left time: 139.9266s\n",
      "\titers: 3500, epoch: 7 | loss: 0.4375244\n",
      "\tspeed: 0.0094s/iter; left time: 139.0350s\n",
      "\titers: 3600, epoch: 7 | loss: 0.1006304\n",
      "\tspeed: 0.0094s/iter; left time: 138.0842s\n",
      "\titers: 3700, epoch: 7 | loss: 0.1772823\n",
      "\tspeed: 0.0094s/iter; left time: 137.1655s\n",
      "\titers: 3800, epoch: 7 | loss: 0.2604528\n",
      "\tspeed: 0.0094s/iter; left time: 136.2720s\n",
      "\titers: 3900, epoch: 7 | loss: 0.2381212\n",
      "\tspeed: 0.0094s/iter; left time: 135.1916s\n",
      "\titers: 4000, epoch: 7 | loss: 0.0832102\n",
      "\tspeed: 0.0094s/iter; left time: 134.3321s\n",
      "\titers: 4100, epoch: 7 | loss: 0.1604863\n",
      "\tspeed: 0.0094s/iter; left time: 133.3028s\n",
      "\titers: 4200, epoch: 7 | loss: 0.1137405\n",
      "\tspeed: 0.0092s/iter; left time: 129.1754s\n",
      "\titers: 4300, epoch: 7 | loss: 0.1849868\n",
      "\tspeed: 0.0084s/iter; left time: 116.6169s\n",
      "\titers: 4400, epoch: 7 | loss: 0.3277745\n",
      "\tspeed: 0.0084s/iter; left time: 115.9954s\n",
      "\titers: 4500, epoch: 7 | loss: 0.4109617\n",
      "\tspeed: 0.0084s/iter; left time: 115.2535s\n",
      "Epoch: 7 cost time: 41.389607667922974\n",
      "Epoch: 7, Steps: 4555 | Train Loss: 0.2693818 Vali Loss: 0.0693679 Test Loss: 0.1961555\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3263405\n",
      "\tspeed: 0.0851s/iter; left time: 1154.4645s\n",
      "\titers: 200, epoch: 8 | loss: 0.0972203\n",
      "\tspeed: 0.0084s/iter; left time: 112.6058s\n",
      "\titers: 300, epoch: 8 | loss: 0.5565593\n",
      "\tspeed: 0.0083s/iter; left time: 111.4103s\n",
      "\titers: 400, epoch: 8 | loss: 0.0759855\n",
      "\tspeed: 0.0083s/iter; left time: 110.7481s\n",
      "\titers: 500, epoch: 8 | loss: 0.1575209\n",
      "\tspeed: 0.0083s/iter; left time: 109.7858s\n",
      "\titers: 600, epoch: 8 | loss: 0.1373911\n",
      "\tspeed: 0.0083s/iter; left time: 108.8832s\n",
      "\titers: 700, epoch: 8 | loss: 0.4582339\n",
      "\tspeed: 0.0083s/iter; left time: 108.0680s\n",
      "\titers: 800, epoch: 8 | loss: 0.1942320\n",
      "\tspeed: 0.0083s/iter; left time: 107.2050s\n",
      "\titers: 900, epoch: 8 | loss: 0.6490905\n",
      "\tspeed: 0.0084s/iter; left time: 106.6144s\n",
      "\titers: 1000, epoch: 8 | loss: 0.1436242\n",
      "\tspeed: 0.0084s/iter; left time: 105.9202s\n",
      "\titers: 1100, epoch: 8 | loss: 0.1268188\n",
      "\tspeed: 0.0084s/iter; left time: 105.0792s\n",
      "\titers: 1200, epoch: 8 | loss: 0.2811716\n",
      "\tspeed: 0.0084s/iter; left time: 104.2947s\n",
      "\titers: 1300, epoch: 8 | loss: 0.2658188\n",
      "\tspeed: 0.0084s/iter; left time: 103.8604s\n",
      "\titers: 1400, epoch: 8 | loss: 0.0449197\n",
      "\tspeed: 0.0084s/iter; left time: 103.4005s\n",
      "\titers: 1500, epoch: 8 | loss: 0.2459372\n",
      "\tspeed: 0.0084s/iter; left time: 102.5250s\n",
      "\titers: 1600, epoch: 8 | loss: 0.0604659\n",
      "\tspeed: 0.0084s/iter; left time: 101.7029s\n",
      "\titers: 1700, epoch: 8 | loss: 0.1094280\n",
      "\tspeed: 0.0084s/iter; left time: 100.8151s\n",
      "\titers: 1800, epoch: 8 | loss: 0.0627489\n",
      "\tspeed: 0.0084s/iter; left time: 99.9831s\n",
      "\titers: 1900, epoch: 8 | loss: 0.3374117\n",
      "\tspeed: 0.0084s/iter; left time: 99.1020s\n",
      "\titers: 2000, epoch: 8 | loss: 0.0752180\n",
      "\tspeed: 0.0084s/iter; left time: 98.3083s\n",
      "\titers: 2100, epoch: 8 | loss: 0.3596313\n",
      "\tspeed: 0.0084s/iter; left time: 97.5828s\n",
      "\titers: 2200, epoch: 8 | loss: 0.2683871\n",
      "\tspeed: 0.0085s/iter; left time: 96.9184s\n",
      "\titers: 2300, epoch: 8 | loss: 0.2380622\n",
      "\tspeed: 0.0085s/iter; left time: 96.0987s\n",
      "\titers: 2400, epoch: 8 | loss: 0.1425365\n",
      "\tspeed: 0.0085s/iter; left time: 95.2084s\n",
      "\titers: 2500, epoch: 8 | loss: 0.3328738\n",
      "\tspeed: 0.0085s/iter; left time: 94.5406s\n",
      "\titers: 2600, epoch: 8 | loss: 0.5637074\n",
      "\tspeed: 0.0085s/iter; left time: 93.5430s\n",
      "\titers: 2700, epoch: 8 | loss: 0.2892465\n",
      "\tspeed: 0.0085s/iter; left time: 92.6883s\n",
      "\titers: 2800, epoch: 8 | loss: 0.3172151\n",
      "\tspeed: 0.0085s/iter; left time: 91.9919s\n",
      "\titers: 2900, epoch: 8 | loss: 0.1093075\n",
      "\tspeed: 0.0085s/iter; left time: 91.0980s\n",
      "\titers: 3000, epoch: 8 | loss: 0.4700245\n",
      "\tspeed: 0.0085s/iter; left time: 90.2230s\n",
      "\titers: 3100, epoch: 8 | loss: 0.3866937\n",
      "\tspeed: 0.0085s/iter; left time: 89.5039s\n",
      "\titers: 3200, epoch: 8 | loss: 0.3399641\n",
      "\tspeed: 0.0084s/iter; left time: 88.3728s\n",
      "\titers: 3300, epoch: 8 | loss: 0.6236664\n",
      "\tspeed: 0.0084s/iter; left time: 87.5596s\n",
      "\titers: 3400, epoch: 8 | loss: 0.0849689\n",
      "\tspeed: 0.0085s/iter; left time: 86.9675s\n",
      "\titers: 3500, epoch: 8 | loss: 0.2038780\n",
      "\tspeed: 0.0085s/iter; left time: 86.0633s\n",
      "\titers: 3600, epoch: 8 | loss: 0.1886929\n",
      "\tspeed: 0.0085s/iter; left time: 85.1524s\n",
      "\titers: 3700, epoch: 8 | loss: 0.2665026\n",
      "\tspeed: 0.0085s/iter; left time: 84.2872s\n",
      "\titers: 3800, epoch: 8 | loss: 0.2018952\n",
      "\tspeed: 0.0085s/iter; left time: 83.4402s\n",
      "\titers: 3900, epoch: 8 | loss: 0.3467089\n",
      "\tspeed: 0.0085s/iter; left time: 82.5836s\n",
      "\titers: 4000, epoch: 8 | loss: 0.2875113\n",
      "\tspeed: 0.0085s/iter; left time: 81.7374s\n",
      "\titers: 4100, epoch: 8 | loss: 0.3116134\n",
      "\tspeed: 0.0084s/iter; left time: 80.7292s\n",
      "\titers: 4200, epoch: 8 | loss: 0.2935303\n",
      "\tspeed: 0.0085s/iter; left time: 80.1320s\n",
      "\titers: 4300, epoch: 8 | loss: 0.3327748\n",
      "\tspeed: 0.0084s/iter; left time: 79.1391s\n",
      "\titers: 4400, epoch: 8 | loss: 0.0580797\n",
      "\tspeed: 0.0084s/iter; left time: 78.2858s\n",
      "\titers: 4500, epoch: 8 | loss: 0.2271683\n",
      "\tspeed: 0.0085s/iter; left time: 77.4901s\n",
      "Epoch: 8 cost time: 38.58648467063904\n",
      "Epoch: 8, Steps: 4555 | Train Loss: 0.2686614 Vali Loss: 0.0687414 Test Loss: 0.1952893\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_ECL_168_24_iTransformer_custom_ftS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.19663351774215698, mae:0.2936999797821045\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/iTransformer.sh --features S --predictor solar_forecast,wind_forecast,total_load --enc_in 4 --dec_in 4 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9ceef",
   "metadata": {},
   "source": [
    "## short term SAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfa4c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          short_term_forecast Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             3                   Dec In:             3                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         4                   \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                Timexer-MS          Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : short_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl36_ll18_pl18_dm512_nh8_el3_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_Timexer-MS_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load']\n",
      "train 18357\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load']\n",
      "val 2614\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 200, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_short_term_forecasting.py\", line 95, in train\n",
      "    loss_value = criterion(batch_x, self.args.frequency_map, outputs, batch_y, batch_y_mark)\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: forward() takes 3 positional arguments but 6 were given\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh --features MS --predictor solar_forecast,total_load --enc_in 3 --dec_in 3 --c_out 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ae0f2",
   "metadata": {},
   "source": [
    "# hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   solar_forecast  wind_forecast  total_load  \\\n",
      "solar_forecast           1.000000      -0.244358    0.303357   \n",
      "wind_forecast           -0.244358       1.000000    0.106281   \n",
      "total_load               0.303357       0.106281    1.000000   \n",
      "electricity_price       -0.144178      -0.394381    0.224673   \n",
      "\n",
      "                   electricity_price  \n",
      "solar_forecast             -0.144178  \n",
      "wind_forecast              -0.394381  \n",
      "total_load                  0.224673  \n",
      "electricity_price           1.000000  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv('./data/causal_data.csv', parse_dates=['date'])\n",
    "\n",
    "# Select just the columns of interest\n",
    "cols = ['solar_forecast','wind_forecast','total_load','electricity_price']\n",
    "sub = df[cols].dropna()\n",
    "\n",
    "# Compute the Pearson correlation matrix\n",
    "corr = sub.corr()\n",
    "\n",
    "# Display\n",
    "print(corr)\n",
    "\n",
    "# And a heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "plt.title(\"Feature ↔ Target Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2329b408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> LR=1e-3,DO=0.0,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1668184\n",
      "\tspeed: 0.0213s/iter; left time: 241.2037s\n",
      "\titers: 200, epoch: 1 | loss: 0.1968358\n",
      "\tspeed: 0.0099s/iter; left time: 111.0739s\n",
      "\titers: 300, epoch: 1 | loss: 0.1832968\n",
      "\tspeed: 0.0099s/iter; left time: 110.2240s\n",
      "\titers: 400, epoch: 1 | loss: 0.1859085\n",
      "\tspeed: 0.0099s/iter; left time: 109.0041s\n",
      "\titers: 500, epoch: 1 | loss: 0.2158217\n",
      "\tspeed: 0.0099s/iter; left time: 108.0766s\n",
      "Epoch: 1 cost time: 6.830486297607422\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2083975 Vali Loss: 0.0515174 Test Loss: 0.1629780\n",
      "Validation loss decreased (inf --> 0.051517).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3291485\n",
      "\tspeed: 0.0336s/iter; left time: 360.5552s\n",
      "\titers: 200, epoch: 2 | loss: 0.2241254\n",
      "\tspeed: 0.0100s/iter; left time: 106.0544s\n",
      "\titers: 300, epoch: 2 | loss: 0.1987359\n",
      "\tspeed: 0.0099s/iter; left time: 104.4111s\n",
      "\titers: 400, epoch: 2 | loss: 0.2423836\n",
      "\tspeed: 0.0099s/iter; left time: 103.7201s\n",
      "\titers: 500, epoch: 2 | loss: 0.2867766\n",
      "\tspeed: 0.0099s/iter; left time: 102.7065s\n",
      "Epoch: 2 cost time: 5.86667799949646\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2576519 Vali Loss: 0.0684576 Test Loss: 0.1809007\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1952623\n",
      "\tspeed: 0.0337s/iter; left time: 342.6485s\n",
      "\titers: 200, epoch: 3 | loss: 0.2611968\n",
      "\tspeed: 0.0101s/iter; left time: 101.6956s\n",
      "\titers: 300, epoch: 3 | loss: 0.1721100\n",
      "\tspeed: 0.0099s/iter; left time: 98.8333s\n",
      "\titers: 400, epoch: 3 | loss: 0.2667773\n",
      "\tspeed: 0.0100s/iter; left time: 98.2234s\n",
      "\titers: 500, epoch: 3 | loss: 0.3224335\n",
      "\tspeed: 0.0100s/iter; left time: 97.2234s\n",
      "Epoch: 3 cost time: 5.996889352798462\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2467329 Vali Loss: 0.0648856 Test Loss: 0.1647076\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3145069\n",
      "\tspeed: 0.0337s/iter; left time: 323.6653s\n",
      "\titers: 200, epoch: 4 | loss: 0.1908078\n",
      "\tspeed: 0.0100s/iter; left time: 94.4537s\n",
      "\titers: 300, epoch: 4 | loss: 0.2720823\n",
      "\tspeed: 0.0099s/iter; left time: 93.2326s\n",
      "\titers: 400, epoch: 4 | loss: 0.2160956\n",
      "\tspeed: 0.0100s/iter; left time: 92.7789s\n",
      "\titers: 500, epoch: 4 | loss: 0.2266363\n",
      "\tspeed: 0.0099s/iter; left time: 91.3901s\n",
      "Epoch: 4 cost time: 5.864826917648315\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2179058 Vali Loss: 0.0635907 Test Loss: 0.1608318\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1631806194782257, mae:0.26068833470344543\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1879944\n",
      "\tspeed: 0.0226s/iter; left time: 255.0970s\n",
      "\titers: 200, epoch: 1 | loss: 0.1874471\n",
      "\tspeed: 0.0101s/iter; left time: 113.3831s\n",
      "\titers: 300, epoch: 1 | loss: 0.2482666\n",
      "\tspeed: 0.0099s/iter; left time: 110.1326s\n",
      "\titers: 400, epoch: 1 | loss: 0.1488428\n",
      "\tspeed: 0.0099s/iter; left time: 108.5883s\n",
      "\titers: 500, epoch: 1 | loss: 0.1444412\n",
      "\tspeed: 0.0098s/iter; left time: 107.3352s\n",
      "Epoch: 1 cost time: 6.9548985958099365\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1682830 Vali Loss: 0.0436042 Test Loss: 0.1377011\n",
      "Validation loss decreased (inf --> 0.043604).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1398871\n",
      "\tspeed: 0.0349s/iter; left time: 374.0091s\n",
      "\titers: 200, epoch: 2 | loss: 0.1850255\n",
      "\tspeed: 0.0099s/iter; left time: 104.7994s\n",
      "\titers: 300, epoch: 2 | loss: 0.1077379\n",
      "\tspeed: 0.0098s/iter; left time: 103.3861s\n",
      "\titers: 400, epoch: 2 | loss: 0.1096446\n",
      "\tspeed: 0.0098s/iter; left time: 102.5993s\n",
      "\titers: 500, epoch: 2 | loss: 0.2028266\n",
      "\tspeed: 0.0098s/iter; left time: 101.4126s\n",
      "Epoch: 2 cost time: 5.920338153839111\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1612543 Vali Loss: 0.0431032 Test Loss: 0.1407291\n",
      "Validation loss decreased (0.043604 --> 0.043103).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.0767745\n",
      "\tspeed: 0.0347s/iter; left time: 352.1841s\n",
      "\titers: 200, epoch: 3 | loss: 0.1207427\n",
      "\tspeed: 0.0112s/iter; left time: 112.7991s\n",
      "\titers: 300, epoch: 3 | loss: 0.2777740\n",
      "\tspeed: 0.0112s/iter; left time: 111.9568s\n",
      "\titers: 400, epoch: 3 | loss: 0.1337653\n",
      "\tspeed: 0.0112s/iter; left time: 110.8008s\n",
      "\titers: 500, epoch: 3 | loss: 0.1266848\n",
      "\tspeed: 0.0112s/iter; left time: 109.7876s\n",
      "Epoch: 3 cost time: 6.618976593017578\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1380364 Vali Loss: 0.0397742 Test Loss: 0.1264060\n",
      "Validation loss decreased (0.043103 --> 0.039774).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.0792852\n",
      "\tspeed: 0.0360s/iter; left time: 345.4458s\n",
      "\titers: 200, epoch: 4 | loss: 0.0799794\n",
      "\tspeed: 0.0099s/iter; left time: 93.7976s\n",
      "\titers: 300, epoch: 4 | loss: 0.0885246\n",
      "\tspeed: 0.0099s/iter; left time: 92.6969s\n",
      "\titers: 400, epoch: 4 | loss: 0.0728028\n",
      "\tspeed: 0.0099s/iter; left time: 91.7547s\n",
      "\titers: 500, epoch: 4 | loss: 0.1350426\n",
      "\tspeed: 0.0099s/iter; left time: 90.8933s\n",
      "Epoch: 4 cost time: 5.9707629680633545\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1146851 Vali Loss: 0.0357869 Test Loss: 0.1162586\n",
      "Validation loss decreased (0.039774 --> 0.035787).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1119463\n",
      "\tspeed: 0.0325s/iter; left time: 293.1011s\n",
      "\titers: 200, epoch: 5 | loss: 0.1653396\n",
      "\tspeed: 0.0098s/iter; left time: 87.1483s\n",
      "\titers: 300, epoch: 5 | loss: 0.1019746\n",
      "\tspeed: 0.0098s/iter; left time: 86.1653s\n",
      "\titers: 400, epoch: 5 | loss: 0.1152093\n",
      "\tspeed: 0.0098s/iter; left time: 85.1980s\n",
      "\titers: 500, epoch: 5 | loss: 0.0985917\n",
      "\tspeed: 0.0098s/iter; left time: 84.2455s\n",
      "Epoch: 5 cost time: 5.77421760559082\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1009639 Vali Loss: 0.0364314 Test Loss: 0.1136684\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0581600\n",
      "\tspeed: 0.0325s/iter; left time: 274.5205s\n",
      "\titers: 200, epoch: 6 | loss: 0.1095032\n",
      "\tspeed: 0.0098s/iter; left time: 81.8542s\n",
      "\titers: 300, epoch: 6 | loss: 0.1278585\n",
      "\tspeed: 0.0098s/iter; left time: 80.7805s\n",
      "\titers: 400, epoch: 6 | loss: 0.0641890\n",
      "\tspeed: 0.0098s/iter; left time: 79.9376s\n",
      "\titers: 500, epoch: 6 | loss: 0.0766375\n",
      "\tspeed: 0.0098s/iter; left time: 78.9274s\n",
      "Epoch: 6 cost time: 5.876809597015381\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0925622 Vali Loss: 0.0370014 Test Loss: 0.1155212\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0849662\n",
      "\tspeed: 0.0365s/iter; left time: 287.2691s\n",
      "\titers: 200, epoch: 7 | loss: 0.0992069\n",
      "\tspeed: 0.0114s/iter; left time: 88.4608s\n",
      "\titers: 300, epoch: 7 | loss: 0.1484343\n",
      "\tspeed: 0.0113s/iter; left time: 86.9833s\n",
      "\titers: 400, epoch: 7 | loss: 0.1500152\n",
      "\tspeed: 0.0113s/iter; left time: 85.9140s\n",
      "\titers: 500, epoch: 7 | loss: 0.1286581\n",
      "\tspeed: 0.0113s/iter; left time: 84.8957s\n",
      "Epoch: 7 cost time: 6.679283380508423\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0875537 Vali Loss: 0.0360128 Test Loss: 0.1157063\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11642131209373474, mae:0.20923534035682678\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2429955\n",
      "\tspeed: 0.0210s/iter; left time: 237.5884s\n",
      "\titers: 200, epoch: 1 | loss: 0.1262257\n",
      "\tspeed: 0.0097s/iter; left time: 109.0855s\n",
      "\titers: 300, epoch: 1 | loss: 0.1267452\n",
      "\tspeed: 0.0098s/iter; left time: 108.3424s\n",
      "\titers: 400, epoch: 1 | loss: 0.1871254\n",
      "\tspeed: 0.0097s/iter; left time: 107.2319s\n",
      "\titers: 500, epoch: 1 | loss: 0.2177178\n",
      "\tspeed: 0.0098s/iter; left time: 106.3435s\n",
      "Epoch: 1 cost time: 6.726196050643921\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2265803 Vali Loss: 0.0527771 Test Loss: 0.1677906\n",
      "Validation loss decreased (inf --> 0.052777).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1989672\n",
      "\tspeed: 0.0345s/iter; left time: 370.3691s\n",
      "\titers: 200, epoch: 2 | loss: 0.2996998\n",
      "\tspeed: 0.0097s/iter; left time: 103.5929s\n",
      "\titers: 300, epoch: 2 | loss: 0.1947185\n",
      "\tspeed: 0.0097s/iter; left time: 102.4406s\n",
      "\titers: 400, epoch: 2 | loss: 0.2522361\n",
      "\tspeed: 0.0098s/iter; left time: 101.7210s\n",
      "\titers: 500, epoch: 2 | loss: 0.2612756\n",
      "\tspeed: 0.0097s/iter; left time: 100.5756s\n",
      "Epoch: 2 cost time: 5.764389276504517\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2259816 Vali Loss: 0.0574514 Test Loss: 0.1699410\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1365477\n",
      "\tspeed: 0.0331s/iter; left time: 336.5405s\n",
      "\titers: 200, epoch: 3 | loss: 0.1770229\n",
      "\tspeed: 0.0098s/iter; left time: 98.3457s\n",
      "\titers: 300, epoch: 3 | loss: 0.2543505\n",
      "\tspeed: 0.0098s/iter; left time: 97.5193s\n",
      "\titers: 400, epoch: 3 | loss: 0.1783216\n",
      "\tspeed: 0.0098s/iter; left time: 96.3296s\n",
      "\titers: 500, epoch: 3 | loss: 0.1370625\n",
      "\tspeed: 0.0098s/iter; left time: 95.3635s\n",
      "Epoch: 3 cost time: 5.8141090869903564\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2107144 Vali Loss: 0.0469428 Test Loss: 0.1451066\n",
      "Validation loss decreased (0.052777 --> 0.046943).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1333973\n",
      "\tspeed: 0.0333s/iter; left time: 319.0624s\n",
      "\titers: 200, epoch: 4 | loss: 0.0943953\n",
      "\tspeed: 0.0097s/iter; left time: 92.0817s\n",
      "\titers: 300, epoch: 4 | loss: 0.1297451\n",
      "\tspeed: 0.0097s/iter; left time: 90.7225s\n",
      "\titers: 400, epoch: 4 | loss: 0.2211343\n",
      "\tspeed: 0.0097s/iter; left time: 89.7495s\n",
      "\titers: 500, epoch: 4 | loss: 0.1433680\n",
      "\tspeed: 0.0097s/iter; left time: 88.7410s\n",
      "Epoch: 4 cost time: 5.73934268951416\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1749125 Vali Loss: 0.0473251 Test Loss: 0.1498124\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1313210\n",
      "\tspeed: 0.0347s/iter; left time: 312.8310s\n",
      "\titers: 200, epoch: 5 | loss: 0.1570131\n",
      "\tspeed: 0.0097s/iter; left time: 86.5760s\n",
      "\titers: 300, epoch: 5 | loss: 0.2370802\n",
      "\tspeed: 0.0097s/iter; left time: 85.6548s\n",
      "\titers: 400, epoch: 5 | loss: 0.1019689\n",
      "\tspeed: 0.0097s/iter; left time: 84.5957s\n",
      "\titers: 500, epoch: 5 | loss: 0.2425003\n",
      "\tspeed: 0.0097s/iter; left time: 83.4581s\n",
      "Epoch: 5 cost time: 5.737771034240723\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1612860 Vali Loss: 0.0438437 Test Loss: 0.1477919\n",
      "Validation loss decreased (0.046943 --> 0.043844).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1151529\n",
      "\tspeed: 0.0331s/iter; left time: 279.3732s\n",
      "\titers: 200, epoch: 6 | loss: 0.1784842\n",
      "\tspeed: 0.0097s/iter; left time: 80.7293s\n",
      "\titers: 300, epoch: 6 | loss: 0.1698503\n",
      "\tspeed: 0.0097s/iter; left time: 79.6698s\n",
      "\titers: 400, epoch: 6 | loss: 0.1325180\n",
      "\tspeed: 0.0097s/iter; left time: 78.7779s\n",
      "\titers: 500, epoch: 6 | loss: 0.1904864\n",
      "\tspeed: 0.0097s/iter; left time: 78.4857s\n",
      "Epoch: 6 cost time: 5.744234561920166\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1502975 Vali Loss: 0.0432721 Test Loss: 0.1421744\n",
      "Validation loss decreased (0.043844 --> 0.043272).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0912671\n",
      "\tspeed: 0.0328s/iter; left time: 258.6680s\n",
      "\titers: 200, epoch: 7 | loss: 0.1507141\n",
      "\tspeed: 0.0097s/iter; left time: 75.4060s\n",
      "\titers: 300, epoch: 7 | loss: 0.1111932\n",
      "\tspeed: 0.0097s/iter; left time: 74.4711s\n",
      "\titers: 400, epoch: 7 | loss: 0.0750316\n",
      "\tspeed: 0.0097s/iter; left time: 73.5131s\n",
      "\titers: 500, epoch: 7 | loss: 0.1784825\n",
      "\tspeed: 0.0097s/iter; left time: 72.5437s\n",
      "Epoch: 7 cost time: 5.749779224395752\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1443310 Vali Loss: 0.0433332 Test Loss: 0.1409613\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0878188\n",
      "\tspeed: 0.0363s/iter; left time: 265.0254s\n",
      "\titers: 200, epoch: 8 | loss: 0.1318479\n",
      "\tspeed: 0.0103s/iter; left time: 74.2189s\n",
      "\titers: 300, epoch: 8 | loss: 0.1270279\n",
      "\tspeed: 0.0098s/iter; left time: 69.6035s\n",
      "\titers: 400, epoch: 8 | loss: 0.1578188\n",
      "\tspeed: 0.0097s/iter; left time: 68.2532s\n",
      "\titers: 500, epoch: 8 | loss: 0.1500511\n",
      "\tspeed: 0.0097s/iter; left time: 67.2541s\n",
      "Epoch: 8 cost time: 5.964056730270386\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1401656 Vali Loss: 0.0427601 Test Loss: 0.1404220\n",
      "Validation loss decreased (0.043272 --> 0.042760).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1381372\n",
      "\tspeed: 0.0345s/iter; left time: 232.3346s\n",
      "\titers: 200, epoch: 9 | loss: 0.1403579\n",
      "\tspeed: 0.0097s/iter; left time: 64.2357s\n",
      "\titers: 300, epoch: 9 | loss: 0.1187568\n",
      "\tspeed: 0.0097s/iter; left time: 63.1746s\n",
      "\titers: 400, epoch: 9 | loss: 0.0885384\n",
      "\tspeed: 0.0097s/iter; left time: 62.2483s\n",
      "\titers: 500, epoch: 9 | loss: 0.1417777\n",
      "\tspeed: 0.0097s/iter; left time: 61.3291s\n",
      "Epoch: 9 cost time: 5.726182699203491\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1382025 Vali Loss: 0.0433924 Test Loss: 0.1427714\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1171558\n",
      "\tspeed: 0.0345s/iter; left time: 212.9772s\n",
      "\titers: 200, epoch: 10 | loss: 0.1783067\n",
      "\tspeed: 0.0097s/iter; left time: 59.1097s\n",
      "\titers: 300, epoch: 10 | loss: 0.1544783\n",
      "\tspeed: 0.0097s/iter; left time: 58.1312s\n",
      "\titers: 400, epoch: 10 | loss: 0.1501161\n",
      "\tspeed: 0.0098s/iter; left time: 57.2947s\n",
      "\titers: 500, epoch: 10 | loss: 0.1953642\n",
      "\tspeed: 0.0098s/iter; left time: 56.3105s\n",
      "Epoch: 10 cost time: 5.791765928268433\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1372004 Vali Loss: 0.0433279 Test Loss: 0.1432870\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1529302\n",
      "\tspeed: 0.0335s/iter; left time: 187.5239s\n",
      "\titers: 200, epoch: 11 | loss: 0.1255644\n",
      "\tspeed: 0.0097s/iter; left time: 53.3651s\n",
      "\titers: 300, epoch: 11 | loss: 0.1809491\n",
      "\tspeed: 0.0097s/iter; left time: 52.3711s\n",
      "\titers: 400, epoch: 11 | loss: 0.1512925\n",
      "\tspeed: 0.0097s/iter; left time: 51.3945s\n",
      "\titers: 500, epoch: 11 | loss: 0.1629266\n",
      "\tspeed: 0.0097s/iter; left time: 50.4634s\n",
      "Epoch: 11 cost time: 5.7402119636535645\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1363042 Vali Loss: 0.0431477 Test Loss: 0.1432028\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1405969113111496, mae:0.24108478426933289\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2673554\n",
      "\tspeed: 0.0231s/iter; left time: 261.5837s\n",
      "\titers: 200, epoch: 1 | loss: 0.2358598\n",
      "\tspeed: 0.0114s/iter; left time: 128.0445s\n",
      "\titers: 300, epoch: 1 | loss: 0.1605896\n",
      "\tspeed: 0.0114s/iter; left time: 126.9595s\n",
      "\titers: 400, epoch: 1 | loss: 0.2092649\n",
      "\tspeed: 0.0114s/iter; left time: 125.7232s\n",
      "\titers: 500, epoch: 1 | loss: 0.3037598\n",
      "\tspeed: 0.0114s/iter; left time: 124.5696s\n",
      "Epoch: 1 cost time: 7.735670804977417\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2843307 Vali Loss: 0.0668196 Test Loss: 0.1870127\n",
      "Validation loss decreased (inf --> 0.066820).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2432782\n",
      "\tspeed: 0.0366s/iter; left time: 393.1873s\n",
      "\titers: 200, epoch: 2 | loss: 0.1925197\n",
      "\tspeed: 0.0114s/iter; left time: 121.3626s\n",
      "\titers: 300, epoch: 2 | loss: 0.1782432\n",
      "\tspeed: 0.0114s/iter; left time: 120.1133s\n",
      "\titers: 400, epoch: 2 | loss: 0.1232579\n",
      "\tspeed: 0.0114s/iter; left time: 119.0043s\n",
      "\titers: 500, epoch: 2 | loss: 0.2400337\n",
      "\tspeed: 0.0114s/iter; left time: 117.8198s\n",
      "Epoch: 2 cost time: 6.749845266342163\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2810881 Vali Loss: 0.0608437 Test Loss: 0.1798609\n",
      "Validation loss decreased (0.066820 --> 0.060844).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3295406\n",
      "\tspeed: 0.0379s/iter; left time: 385.2246s\n",
      "\titers: 200, epoch: 3 | loss: 0.4188828\n",
      "\tspeed: 0.0125s/iter; left time: 125.8578s\n",
      "\titers: 300, epoch: 3 | loss: 0.1578331\n",
      "\tspeed: 0.0125s/iter; left time: 124.2030s\n",
      "\titers: 400, epoch: 3 | loss: 0.2161615\n",
      "\tspeed: 0.0124s/iter; left time: 122.7628s\n",
      "\titers: 500, epoch: 3 | loss: 0.2143182\n",
      "\tspeed: 0.0125s/iter; left time: 121.5414s\n",
      "Epoch: 3 cost time: 7.360805034637451\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2520068 Vali Loss: 0.0636789 Test Loss: 0.1797697\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1728122\n",
      "\tspeed: 0.0367s/iter; left time: 352.0598s\n",
      "\titers: 200, epoch: 4 | loss: 0.2393166\n",
      "\tspeed: 0.0115s/iter; left time: 109.1585s\n",
      "\titers: 300, epoch: 4 | loss: 0.1626475\n",
      "\tspeed: 0.0115s/iter; left time: 107.9873s\n",
      "\titers: 400, epoch: 4 | loss: 0.1333545\n",
      "\tspeed: 0.0115s/iter; left time: 106.5788s\n",
      "\titers: 500, epoch: 4 | loss: 0.1654302\n",
      "\tspeed: 0.0115s/iter; left time: 105.4602s\n",
      "Epoch: 4 cost time: 6.779788017272949\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2222100 Vali Loss: 0.0608348 Test Loss: 0.1682695\n",
      "Validation loss decreased (0.060844 --> 0.060835).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1581249\n",
      "\tspeed: 0.0365s/iter; left time: 329.4255s\n",
      "\titers: 200, epoch: 5 | loss: 0.1555440\n",
      "\tspeed: 0.0114s/iter; left time: 101.6177s\n",
      "\titers: 300, epoch: 5 | loss: 0.2902546\n",
      "\tspeed: 0.0114s/iter; left time: 100.5844s\n",
      "\titers: 400, epoch: 5 | loss: 0.1749023\n",
      "\tspeed: 0.0114s/iter; left time: 99.4020s\n",
      "\titers: 500, epoch: 5 | loss: 0.2035597\n",
      "\tspeed: 0.0114s/iter; left time: 98.4659s\n",
      "Epoch: 5 cost time: 6.7447168827056885\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2089508 Vali Loss: 0.0620068 Test Loss: 0.1694303\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1826471\n",
      "\tspeed: 0.0367s/iter; left time: 310.5705s\n",
      "\titers: 200, epoch: 6 | loss: 0.1136283\n",
      "\tspeed: 0.0114s/iter; left time: 95.4310s\n",
      "\titers: 300, epoch: 6 | loss: 0.2019654\n",
      "\tspeed: 0.0114s/iter; left time: 94.1900s\n",
      "\titers: 400, epoch: 6 | loss: 0.1772547\n",
      "\tspeed: 0.0114s/iter; left time: 93.1046s\n",
      "\titers: 500, epoch: 6 | loss: 0.1489908\n",
      "\tspeed: 0.0114s/iter; left time: 91.8281s\n",
      "Epoch: 6 cost time: 6.737531661987305\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2033361 Vali Loss: 0.0616341 Test Loss: 0.1669102\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2622835\n",
      "\tspeed: 0.0361s/iter; left time: 284.3210s\n",
      "\titers: 200, epoch: 7 | loss: 0.2151243\n",
      "\tspeed: 0.0115s/iter; left time: 89.1354s\n",
      "\titers: 300, epoch: 7 | loss: 0.2167960\n",
      "\tspeed: 0.0115s/iter; left time: 88.0449s\n",
      "\titers: 400, epoch: 7 | loss: 0.1562091\n",
      "\tspeed: 0.0115s/iter; left time: 86.9290s\n",
      "\titers: 500, epoch: 7 | loss: 0.2143835\n",
      "\tspeed: 0.0114s/iter; left time: 85.6479s\n",
      "Epoch: 7 cost time: 6.75153112411499\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1996851 Vali Loss: 0.0609210 Test Loss: 0.1667071\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1685466766357422, mae:0.27037519216537476\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1208526\n",
      "\tspeed: 0.0236s/iter; left time: 267.0632s\n",
      "\titers: 200, epoch: 1 | loss: 0.2859458\n",
      "\tspeed: 0.0114s/iter; left time: 127.9504s\n",
      "\titers: 300, epoch: 1 | loss: 0.1719414\n",
      "\tspeed: 0.0110s/iter; left time: 122.3893s\n",
      "\titers: 400, epoch: 1 | loss: 0.2156864\n",
      "\tspeed: 0.0110s/iter; left time: 121.2770s\n",
      "\titers: 500, epoch: 1 | loss: 0.1559027\n",
      "\tspeed: 0.0110s/iter; left time: 120.2797s\n",
      "Epoch: 1 cost time: 7.6299285888671875\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2086973 Vali Loss: 0.0515904 Test Loss: 0.1532065\n",
      "Validation loss decreased (inf --> 0.051590).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2164857\n",
      "\tspeed: 0.0361s/iter; left time: 387.5033s\n",
      "\titers: 200, epoch: 2 | loss: 0.1621193\n",
      "\tspeed: 0.0111s/iter; left time: 117.6473s\n",
      "\titers: 300, epoch: 2 | loss: 0.2461625\n",
      "\tspeed: 0.0111s/iter; left time: 116.7843s\n",
      "\titers: 400, epoch: 2 | loss: 0.3248372\n",
      "\tspeed: 0.0111s/iter; left time: 115.5855s\n",
      "\titers: 500, epoch: 2 | loss: 0.3872446\n",
      "\tspeed: 0.0111s/iter; left time: 114.4761s\n",
      "Epoch: 2 cost time: 6.553614616394043\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2381216 Vali Loss: 0.0526147 Test Loss: 0.1589930\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2298876\n",
      "\tspeed: 0.0374s/iter; left time: 379.6254s\n",
      "\titers: 200, epoch: 3 | loss: 0.0892347\n",
      "\tspeed: 0.0120s/iter; left time: 120.6324s\n",
      "\titers: 300, epoch: 3 | loss: 0.2041319\n",
      "\tspeed: 0.0119s/iter; left time: 119.0321s\n",
      "\titers: 400, epoch: 3 | loss: 0.2050402\n",
      "\tspeed: 0.0119s/iter; left time: 117.6313s\n",
      "\titers: 500, epoch: 3 | loss: 0.2003963\n",
      "\tspeed: 0.0120s/iter; left time: 117.0341s\n",
      "Epoch: 3 cost time: 7.035787343978882\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1838980 Vali Loss: 0.0454281 Test Loss: 0.1411109\n",
      "Validation loss decreased (0.051590 --> 0.045428).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1862133\n",
      "\tspeed: 0.0388s/iter; left time: 372.2845s\n",
      "\titers: 200, epoch: 4 | loss: 0.1526703\n",
      "\tspeed: 0.0111s/iter; left time: 105.2885s\n",
      "\titers: 300, epoch: 4 | loss: 0.0859537\n",
      "\tspeed: 0.0111s/iter; left time: 104.1985s\n",
      "\titers: 400, epoch: 4 | loss: 0.1531971\n",
      "\tspeed: 0.0111s/iter; left time: 103.0892s\n",
      "\titers: 500, epoch: 4 | loss: 0.1257672\n",
      "\tspeed: 0.0111s/iter; left time: 102.0762s\n",
      "Epoch: 4 cost time: 6.581146001815796\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1588286 Vali Loss: 0.0467338 Test Loss: 0.1424807\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1165126\n",
      "\tspeed: 0.0376s/iter; left time: 339.0539s\n",
      "\titers: 200, epoch: 5 | loss: 0.1202716\n",
      "\tspeed: 0.0116s/iter; left time: 103.1735s\n",
      "\titers: 300, epoch: 5 | loss: 0.1846251\n",
      "\tspeed: 0.0111s/iter; left time: 97.9644s\n",
      "\titers: 400, epoch: 5 | loss: 0.1324604\n",
      "\tspeed: 0.0111s/iter; left time: 96.9412s\n",
      "\titers: 500, epoch: 5 | loss: 0.1090243\n",
      "\tspeed: 0.0111s/iter; left time: 95.8799s\n",
      "Epoch: 5 cost time: 6.692414045333862\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1417700 Vali Loss: 0.0407827 Test Loss: 0.1379961\n",
      "Validation loss decreased (0.045428 --> 0.040783).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1700202\n",
      "\tspeed: 0.0371s/iter; left time: 313.5736s\n",
      "\titers: 200, epoch: 6 | loss: 0.1496533\n",
      "\tspeed: 0.0110s/iter; left time: 92.1254s\n",
      "\titers: 300, epoch: 6 | loss: 0.0920784\n",
      "\tspeed: 0.0110s/iter; left time: 91.0524s\n",
      "\titers: 400, epoch: 6 | loss: 0.0841550\n",
      "\tspeed: 0.0110s/iter; left time: 90.0602s\n",
      "\titers: 500, epoch: 6 | loss: 0.1100778\n",
      "\tspeed: 0.0110s/iter; left time: 88.7976s\n",
      "Epoch: 6 cost time: 6.540194034576416\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1324151 Vali Loss: 0.0401964 Test Loss: 0.1361016\n",
      "Validation loss decreased (0.040783 --> 0.040196).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0873016\n",
      "\tspeed: 0.0383s/iter; left time: 302.0633s\n",
      "\titers: 200, epoch: 7 | loss: 0.1881062\n",
      "\tspeed: 0.0121s/iter; left time: 94.2573s\n",
      "\titers: 300, epoch: 7 | loss: 0.1087534\n",
      "\tspeed: 0.0122s/iter; left time: 93.3399s\n",
      "\titers: 400, epoch: 7 | loss: 0.1095457\n",
      "\tspeed: 0.0114s/iter; left time: 86.6650s\n",
      "\titers: 500, epoch: 7 | loss: 0.0909741\n",
      "\tspeed: 0.0111s/iter; left time: 82.9269s\n",
      "Epoch: 7 cost time: 6.922253608703613\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1267670 Vali Loss: 0.0402969 Test Loss: 0.1349403\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1074128\n",
      "\tspeed: 0.0351s/iter; left time: 256.3502s\n",
      "\titers: 200, epoch: 8 | loss: 0.1466217\n",
      "\tspeed: 0.0111s/iter; left time: 79.7917s\n",
      "\titers: 300, epoch: 8 | loss: 0.1092096\n",
      "\tspeed: 0.0111s/iter; left time: 78.6658s\n",
      "\titers: 400, epoch: 8 | loss: 0.0924052\n",
      "\tspeed: 0.0111s/iter; left time: 77.5522s\n",
      "\titers: 500, epoch: 8 | loss: 0.1204413\n",
      "\tspeed: 0.0111s/iter; left time: 76.4513s\n",
      "Epoch: 8 cost time: 6.542129993438721\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1230836 Vali Loss: 0.0399046 Test Loss: 0.1353944\n",
      "Validation loss decreased (0.040196 --> 0.039905).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1407759\n",
      "\tspeed: 0.0366s/iter; left time: 247.0140s\n",
      "\titers: 200, epoch: 9 | loss: 0.0852659\n",
      "\tspeed: 0.0116s/iter; left time: 77.3643s\n",
      "\titers: 300, epoch: 9 | loss: 0.1555552\n",
      "\tspeed: 0.0121s/iter; left time: 79.1813s\n",
      "\titers: 400, epoch: 9 | loss: 0.1433156\n",
      "\tspeed: 0.0121s/iter; left time: 77.9629s\n",
      "\titers: 500, epoch: 9 | loss: 0.1607147\n",
      "\tspeed: 0.0121s/iter; left time: 76.7952s\n",
      "Epoch: 9 cost time: 7.051724195480347\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1213381 Vali Loss: 0.0391797 Test Loss: 0.1352211\n",
      "Validation loss decreased (0.039905 --> 0.039180).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1191340\n",
      "\tspeed: 0.0368s/iter; left time: 227.1956s\n",
      "\titers: 200, epoch: 10 | loss: 0.0910964\n",
      "\tspeed: 0.0111s/iter; left time: 67.0883s\n",
      "\titers: 300, epoch: 10 | loss: 0.1056505\n",
      "\tspeed: 0.0111s/iter; left time: 65.9846s\n",
      "\titers: 400, epoch: 10 | loss: 0.1334907\n",
      "\tspeed: 0.0111s/iter; left time: 64.9160s\n",
      "\titers: 500, epoch: 10 | loss: 0.1398455\n",
      "\tspeed: 0.0111s/iter; left time: 63.7950s\n",
      "Epoch: 10 cost time: 6.5347840785980225\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1202429 Vali Loss: 0.0395060 Test Loss: 0.1355136\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.0820738\n",
      "\tspeed: 0.0357s/iter; left time: 199.8266s\n",
      "\titers: 200, epoch: 11 | loss: 0.0953467\n",
      "\tspeed: 0.0111s/iter; left time: 60.9338s\n",
      "\titers: 300, epoch: 11 | loss: 0.1034635\n",
      "\tspeed: 0.0111s/iter; left time: 59.8571s\n",
      "\titers: 400, epoch: 11 | loss: 0.1124705\n",
      "\tspeed: 0.0111s/iter; left time: 58.9121s\n",
      "\titers: 500, epoch: 11 | loss: 0.1682471\n",
      "\tspeed: 0.0111s/iter; left time: 57.7273s\n",
      "Epoch: 11 cost time: 6.554744243621826\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1195659 Vali Loss: 0.0396204 Test Loss: 0.1359874\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1432239\n",
      "\tspeed: 0.0380s/iter; left time: 191.3159s\n",
      "\titers: 200, epoch: 12 | loss: 0.0984666\n",
      "\tspeed: 0.0121s/iter; left time: 59.6910s\n",
      "\titers: 300, epoch: 12 | loss: 0.0857073\n",
      "\tspeed: 0.0121s/iter; left time: 58.5361s\n",
      "\titers: 400, epoch: 12 | loss: 0.1472204\n",
      "\tspeed: 0.0121s/iter; left time: 57.3473s\n",
      "\titers: 500, epoch: 12 | loss: 0.1346401\n",
      "\tspeed: 0.0121s/iter; left time: 56.0539s\n",
      "Epoch: 12 cost time: 7.154144763946533\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1190357 Vali Loss: 0.0397651 Test Loss: 0.1360321\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13543544709682465, mae:0.23163580894470215\n",
      ">>> LR=1e-3,DO=0.0,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2081373\n",
      "\tspeed: 0.0219s/iter; left time: 247.9061s\n",
      "\titers: 200, epoch: 1 | loss: 0.3531043\n",
      "\tspeed: 0.0103s/iter; left time: 114.9837s\n",
      "\titers: 300, epoch: 1 | loss: 0.3283095\n",
      "\tspeed: 0.0103s/iter; left time: 114.3498s\n",
      "\titers: 400, epoch: 1 | loss: 0.2961910\n",
      "\tspeed: 0.0103s/iter; left time: 113.1692s\n",
      "\titers: 500, epoch: 1 | loss: 0.2852784\n",
      "\tspeed: 0.0103s/iter; left time: 112.0970s\n",
      "Epoch: 1 cost time: 7.066309452056885\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2701385 Vali Loss: 0.0734769 Test Loss: 0.2123958\n",
      "Validation loss decreased (inf --> 0.073477).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2973227\n",
      "\tspeed: 0.0356s/iter; left time: 381.9123s\n",
      "\titers: 200, epoch: 2 | loss: 0.2906811\n",
      "\tspeed: 0.0103s/iter; left time: 109.7761s\n",
      "\titers: 300, epoch: 2 | loss: 0.1953369\n",
      "\tspeed: 0.0103s/iter; left time: 108.7747s\n",
      "\titers: 400, epoch: 2 | loss: 0.2043611\n",
      "\tspeed: 0.0104s/iter; left time: 108.0218s\n",
      "\titers: 500, epoch: 2 | loss: 0.2483530\n",
      "\tspeed: 0.0103s/iter; left time: 106.7987s\n",
      "Epoch: 2 cost time: 6.131552457809448\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2728299 Vali Loss: 0.0679701 Test Loss: 0.1997746\n",
      "Validation loss decreased (0.073477 --> 0.067970).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1554442\n",
      "\tspeed: 0.0371s/iter; left time: 377.0280s\n",
      "\titers: 200, epoch: 3 | loss: 0.1863188\n",
      "\tspeed: 0.0114s/iter; left time: 115.1063s\n",
      "\titers: 300, epoch: 3 | loss: 0.2836081\n",
      "\tspeed: 0.0115s/iter; left time: 114.1072s\n",
      "\titers: 400, epoch: 3 | loss: 0.2544675\n",
      "\tspeed: 0.0112s/iter; left time: 110.9334s\n",
      "\titers: 500, epoch: 3 | loss: 0.2589924\n",
      "\tspeed: 0.0111s/iter; left time: 108.7125s\n",
      "Epoch: 3 cost time: 6.644105911254883\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2451306 Vali Loss: 0.0637612 Test Loss: 0.1744619\n",
      "Validation loss decreased (0.067970 --> 0.063761).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2316469\n",
      "\tspeed: 0.0366s/iter; left time: 350.5918s\n",
      "\titers: 200, epoch: 4 | loss: 0.1120871\n",
      "\tspeed: 0.0104s/iter; left time: 98.4399s\n",
      "\titers: 300, epoch: 4 | loss: 0.2192818\n",
      "\tspeed: 0.0103s/iter; left time: 97.0587s\n",
      "\titers: 400, epoch: 4 | loss: 0.3106847\n",
      "\tspeed: 0.0104s/iter; left time: 96.1759s\n",
      "\titers: 500, epoch: 4 | loss: 0.2026028\n",
      "\tspeed: 0.0104s/iter; left time: 95.1642s\n",
      "Epoch: 4 cost time: 6.153026580810547\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2175378 Vali Loss: 0.0593487 Test Loss: 0.1607444\n",
      "Validation loss decreased (0.063761 --> 0.059349).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1770361\n",
      "\tspeed: 0.0351s/iter; left time: 316.7171s\n",
      "\titers: 200, epoch: 5 | loss: 0.1810164\n",
      "\tspeed: 0.0104s/iter; left time: 92.5908s\n",
      "\titers: 300, epoch: 5 | loss: 0.2119149\n",
      "\tspeed: 0.0104s/iter; left time: 91.3285s\n",
      "\titers: 400, epoch: 5 | loss: 0.1309484\n",
      "\tspeed: 0.0104s/iter; left time: 90.4534s\n",
      "\titers: 500, epoch: 5 | loss: 0.2969440\n",
      "\tspeed: 0.0104s/iter; left time: 89.2276s\n",
      "Epoch: 5 cost time: 6.152244329452515\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1965188 Vali Loss: 0.0559909 Test Loss: 0.1539086\n",
      "Validation loss decreased (0.059349 --> 0.055991).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1419153\n",
      "\tspeed: 0.0367s/iter; left time: 309.8364s\n",
      "\titers: 200, epoch: 6 | loss: 0.1659421\n",
      "\tspeed: 0.0113s/iter; left time: 94.2184s\n",
      "\titers: 300, epoch: 6 | loss: 0.2197290\n",
      "\tspeed: 0.0112s/iter; left time: 92.3898s\n",
      "\titers: 400, epoch: 6 | loss: 0.1411058\n",
      "\tspeed: 0.0104s/iter; left time: 84.7576s\n",
      "\titers: 500, epoch: 6 | loss: 0.1455500\n",
      "\tspeed: 0.0104s/iter; left time: 83.6682s\n",
      "Epoch: 6 cost time: 6.459191560745239\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1863750 Vali Loss: 0.0566236 Test Loss: 0.1557629\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1294671\n",
      "\tspeed: 0.0351s/iter; left time: 276.8662s\n",
      "\titers: 200, epoch: 7 | loss: 0.1749182\n",
      "\tspeed: 0.0104s/iter; left time: 80.7424s\n",
      "\titers: 300, epoch: 7 | loss: 0.1338294\n",
      "\tspeed: 0.0104s/iter; left time: 79.7072s\n",
      "\titers: 400, epoch: 7 | loss: 0.2332579\n",
      "\tspeed: 0.0104s/iter; left time: 78.5182s\n",
      "\titers: 500, epoch: 7 | loss: 0.1354468\n",
      "\tspeed: 0.0104s/iter; left time: 77.6445s\n",
      "Epoch: 7 cost time: 6.158773899078369\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1792891 Vali Loss: 0.0548309 Test Loss: 0.1546936\n",
      "Validation loss decreased (0.055991 --> 0.054831).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1518206\n",
      "\tspeed: 0.0357s/iter; left time: 261.3381s\n",
      "\titers: 200, epoch: 8 | loss: 0.2262946\n",
      "\tspeed: 0.0104s/iter; left time: 74.6587s\n",
      "\titers: 300, epoch: 8 | loss: 0.2203848\n",
      "\tspeed: 0.0103s/iter; left time: 73.4322s\n",
      "\titers: 400, epoch: 8 | loss: 0.1756611\n",
      "\tspeed: 0.0103s/iter; left time: 72.4085s\n",
      "\titers: 500, epoch: 8 | loss: 0.1367280\n",
      "\tspeed: 0.0103s/iter; left time: 71.3309s\n",
      "Epoch: 8 cost time: 6.1276562213897705\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1763261 Vali Loss: 0.0542998 Test Loss: 0.1541872\n",
      "Validation loss decreased (0.054831 --> 0.054300).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1023468\n",
      "\tspeed: 0.0350s/iter; left time: 235.7364s\n",
      "\titers: 200, epoch: 9 | loss: 0.1155551\n",
      "\tspeed: 0.0104s/iter; left time: 68.9528s\n",
      "\titers: 300, epoch: 9 | loss: 0.2510663\n",
      "\tspeed: 0.0104s/iter; left time: 67.9867s\n",
      "\titers: 400, epoch: 9 | loss: 0.2702790\n",
      "\tspeed: 0.0104s/iter; left time: 66.9858s\n",
      "\titers: 500, epoch: 9 | loss: 0.0944196\n",
      "\tspeed: 0.0104s/iter; left time: 65.8844s\n",
      "Epoch: 9 cost time: 6.163362503051758\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1741916 Vali Loss: 0.0551400 Test Loss: 0.1549536\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2340378\n",
      "\tspeed: 0.0351s/iter; left time: 216.4858s\n",
      "\titers: 200, epoch: 10 | loss: 0.0925931\n",
      "\tspeed: 0.0114s/iter; left time: 69.3875s\n",
      "\titers: 300, epoch: 10 | loss: 0.1878417\n",
      "\tspeed: 0.0114s/iter; left time: 68.2264s\n",
      "\titers: 400, epoch: 10 | loss: 0.0969002\n",
      "\tspeed: 0.0114s/iter; left time: 67.1188s\n",
      "\titers: 500, epoch: 10 | loss: 0.1904809\n",
      "\tspeed: 0.0114s/iter; left time: 65.9349s\n",
      "Epoch: 10 cost time: 6.741227865219116\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1730067 Vali Loss: 0.0548287 Test Loss: 0.1543280\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1151982\n",
      "\tspeed: 0.0355s/iter; left time: 198.8272s\n",
      "\titers: 200, epoch: 11 | loss: 0.1987495\n",
      "\tspeed: 0.0103s/iter; left time: 56.5897s\n",
      "\titers: 300, epoch: 11 | loss: 0.1097980\n",
      "\tspeed: 0.0103s/iter; left time: 55.7925s\n",
      "\titers: 400, epoch: 11 | loss: 0.1088090\n",
      "\tspeed: 0.0103s/iter; left time: 54.8008s\n",
      "\titers: 500, epoch: 11 | loss: 0.1258720\n",
      "\tspeed: 0.0103s/iter; left time: 53.6374s\n",
      "Epoch: 11 cost time: 6.105431318283081\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1726455 Vali Loss: 0.0548595 Test Loss: 0.1545657\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15438935160636902, mae:0.2481307089328766\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1910524\n",
      "\tspeed: 0.0254s/iter; left time: 287.1595s\n",
      "\titers: 200, epoch: 1 | loss: 0.1724668\n",
      "\tspeed: 0.0139s/iter; left time: 155.2723s\n",
      "\titers: 300, epoch: 1 | loss: 0.2917525\n",
      "\tspeed: 0.0139s/iter; left time: 153.8870s\n",
      "\titers: 400, epoch: 1 | loss: 0.1924879\n",
      "\tspeed: 0.0139s/iter; left time: 152.5495s\n",
      "\titers: 500, epoch: 1 | loss: 0.1730633\n",
      "\tspeed: 0.0139s/iter; left time: 151.2350s\n",
      "Epoch: 1 cost time: 9.104316473007202\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2224579 Vali Loss: 0.0565231 Test Loss: 0.1590333\n",
      "Validation loss decreased (inf --> 0.056523).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2762584\n",
      "\tspeed: 0.0442s/iter; left time: 474.0393s\n",
      "\titers: 200, epoch: 2 | loss: 0.2946938\n",
      "\tspeed: 0.0139s/iter; left time: 147.8919s\n",
      "\titers: 300, epoch: 2 | loss: 0.2403137\n",
      "\tspeed: 0.0139s/iter; left time: 146.7038s\n",
      "\titers: 400, epoch: 2 | loss: 0.2267851\n",
      "\tspeed: 0.0139s/iter; left time: 145.1629s\n",
      "\titers: 500, epoch: 2 | loss: 0.3126200\n",
      "\tspeed: 0.0139s/iter; left time: 143.7732s\n",
      "Epoch: 2 cost time: 8.185847520828247\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2804122 Vali Loss: 0.0803362 Test Loss: 0.2180425\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.4174822\n",
      "\tspeed: 0.0435s/iter; left time: 441.7827s\n",
      "\titers: 200, epoch: 3 | loss: 0.3376241\n",
      "\tspeed: 0.0138s/iter; left time: 139.1182s\n",
      "\titers: 300, epoch: 3 | loss: 0.1651516\n",
      "\tspeed: 0.0138s/iter; left time: 137.4213s\n",
      "\titers: 400, epoch: 3 | loss: 0.2693733\n",
      "\tspeed: 0.0138s/iter; left time: 135.9736s\n",
      "\titers: 500, epoch: 3 | loss: 0.2479194\n",
      "\tspeed: 0.0138s/iter; left time: 134.5453s\n",
      "Epoch: 3 cost time: 8.086046695709229\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2874916 Vali Loss: 0.0774770 Test Loss: 0.1967589\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2508566\n",
      "\tspeed: 0.0478s/iter; left time: 458.2956s\n",
      "\titers: 200, epoch: 4 | loss: 0.2272388\n",
      "\tspeed: 0.0155s/iter; left time: 147.3812s\n",
      "\titers: 300, epoch: 4 | loss: 0.2643112\n",
      "\tspeed: 0.0154s/iter; left time: 144.9778s\n",
      "\titers: 400, epoch: 4 | loss: 0.2609861\n",
      "\tspeed: 0.0138s/iter; left time: 127.9217s\n",
      "\titers: 500, epoch: 4 | loss: 0.2996611\n",
      "\tspeed: 0.0137s/iter; left time: 126.3400s\n",
      "Epoch: 4 cost time: 8.619013547897339\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2648740 Vali Loss: 0.0613281 Test Loss: 0.1687082\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1592792421579361, mae:0.2639329433441162\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2919293\n",
      "\tspeed: 0.0250s/iter; left time: 282.8792s\n",
      "\titers: 200, epoch: 1 | loss: 0.1859017\n",
      "\tspeed: 0.0134s/iter; left time: 150.3004s\n",
      "\titers: 300, epoch: 1 | loss: 0.1094763\n",
      "\tspeed: 0.0134s/iter; left time: 148.6813s\n",
      "\titers: 400, epoch: 1 | loss: 0.2039593\n",
      "\tspeed: 0.0134s/iter; left time: 147.2777s\n",
      "\titers: 500, epoch: 1 | loss: 0.2330858\n",
      "\tspeed: 0.0134s/iter; left time: 146.3428s\n",
      "Epoch: 1 cost time: 8.855769634246826\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1849884 Vali Loss: 0.0479409 Test Loss: 0.1547139\n",
      "Validation loss decreased (inf --> 0.047941).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1143082\n",
      "\tspeed: 0.0429s/iter; left time: 460.1821s\n",
      "\titers: 200, epoch: 2 | loss: 0.3537095\n",
      "\tspeed: 0.0134s/iter; left time: 142.3710s\n",
      "\titers: 300, epoch: 2 | loss: 0.3505127\n",
      "\tspeed: 0.0134s/iter; left time: 141.2168s\n",
      "\titers: 400, epoch: 2 | loss: 0.2553155\n",
      "\tspeed: 0.0134s/iter; left time: 139.7414s\n",
      "\titers: 500, epoch: 2 | loss: 0.2997040\n",
      "\tspeed: 0.0134s/iter; left time: 137.9657s\n",
      "Epoch: 2 cost time: 7.882343292236328\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2630000 Vali Loss: 0.0724070 Test Loss: 0.1898956\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2477942\n",
      "\tspeed: 0.0433s/iter; left time: 440.2343s\n",
      "\titers: 200, epoch: 3 | loss: 0.1261086\n",
      "\tspeed: 0.0133s/iter; left time: 134.0534s\n",
      "\titers: 300, epoch: 3 | loss: 0.2619452\n",
      "\tspeed: 0.0140s/iter; left time: 139.9117s\n",
      "\titers: 400, epoch: 3 | loss: 0.1103649\n",
      "\tspeed: 0.0139s/iter; left time: 136.8608s\n",
      "\titers: 500, epoch: 3 | loss: 0.2083330\n",
      "\tspeed: 0.0133s/iter; left time: 130.0145s\n",
      "Epoch: 3 cost time: 8.148589611053467\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2439919 Vali Loss: 0.0668979 Test Loss: 0.1739569\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1651082\n",
      "\tspeed: 0.0463s/iter; left time: 443.8706s\n",
      "\titers: 200, epoch: 4 | loss: 0.2440787\n",
      "\tspeed: 0.0133s/iter; left time: 126.6991s\n",
      "\titers: 300, epoch: 4 | loss: 0.3100405\n",
      "\tspeed: 0.0134s/iter; left time: 125.4229s\n",
      "\titers: 400, epoch: 4 | loss: 0.1410217\n",
      "\tspeed: 0.0133s/iter; left time: 123.7789s\n",
      "\titers: 500, epoch: 4 | loss: 0.1318116\n",
      "\tspeed: 0.0133s/iter; left time: 122.6159s\n",
      "Epoch: 4 cost time: 7.838792324066162\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2142600 Vali Loss: 0.0600810 Test Loss: 0.1641871\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1549413502216339, mae:0.2512667179107666\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1190500\n",
      "\tspeed: 0.0269s/iter; left time: 304.5462s\n",
      "\titers: 200, epoch: 1 | loss: 0.3118449\n",
      "\tspeed: 0.0151s/iter; left time: 169.1396s\n",
      "\titers: 300, epoch: 1 | loss: 0.3297839\n",
      "\tspeed: 0.0151s/iter; left time: 167.9098s\n",
      "\titers: 400, epoch: 1 | loss: 0.2602028\n",
      "\tspeed: 0.0151s/iter; left time: 166.1699s\n",
      "\titers: 500, epoch: 1 | loss: 0.2521578\n",
      "\tspeed: 0.0149s/iter; left time: 162.1804s\n",
      "Epoch: 1 cost time: 9.710272550582886\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2388101 Vali Loss: 0.0638805 Test Loss: 0.1876526\n",
      "Validation loss decreased (inf --> 0.063880).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2265279\n",
      "\tspeed: 0.0433s/iter; left time: 464.3182s\n",
      "\titers: 200, epoch: 2 | loss: 0.1576295\n",
      "\tspeed: 0.0135s/iter; left time: 143.6059s\n",
      "\titers: 300, epoch: 2 | loss: 0.1770525\n",
      "\tspeed: 0.0135s/iter; left time: 142.2828s\n",
      "\titers: 400, epoch: 2 | loss: 0.2324228\n",
      "\tspeed: 0.0135s/iter; left time: 140.7320s\n",
      "\titers: 500, epoch: 2 | loss: 0.3294505\n",
      "\tspeed: 0.0135s/iter; left time: 139.2526s\n",
      "Epoch: 2 cost time: 7.94345498085022\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2661254 Vali Loss: 0.0713454 Test Loss: 0.1848145\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2670662\n",
      "\tspeed: 0.0440s/iter; left time: 446.5930s\n",
      "\titers: 200, epoch: 3 | loss: 0.2732421\n",
      "\tspeed: 0.0135s/iter; left time: 135.9631s\n",
      "\titers: 300, epoch: 3 | loss: 0.2004428\n",
      "\tspeed: 0.0135s/iter; left time: 134.5322s\n",
      "\titers: 400, epoch: 3 | loss: 0.1749303\n",
      "\tspeed: 0.0135s/iter; left time: 133.2497s\n",
      "\titers: 500, epoch: 3 | loss: 0.2124099\n",
      "\tspeed: 0.0135s/iter; left time: 131.3149s\n",
      "Epoch: 3 cost time: 8.045276403427124\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2391176 Vali Loss: 0.0647277 Test Loss: 0.1720510\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2196204\n",
      "\tspeed: 0.0459s/iter; left time: 439.8856s\n",
      "\titers: 200, epoch: 4 | loss: 0.1945581\n",
      "\tspeed: 0.0156s/iter; left time: 147.9151s\n",
      "\titers: 300, epoch: 4 | loss: 0.1817180\n",
      "\tspeed: 0.0156s/iter; left time: 146.3598s\n",
      "\titers: 400, epoch: 4 | loss: 0.1947605\n",
      "\tspeed: 0.0156s/iter; left time: 144.6849s\n",
      "\titers: 500, epoch: 4 | loss: 0.1333836\n",
      "\tspeed: 0.0156s/iter; left time: 143.3818s\n",
      "Epoch: 4 cost time: 9.005319356918335\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2129358 Vali Loss: 0.0605438 Test Loss: 0.1664300\n",
      "Validation loss decreased (0.063880 --> 0.060544).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1738715\n",
      "\tspeed: 0.0464s/iter; left time: 418.1358s\n",
      "\titers: 200, epoch: 5 | loss: 0.2195346\n",
      "\tspeed: 0.0155s/iter; left time: 138.0778s\n",
      "\titers: 300, epoch: 5 | loss: 0.1826030\n",
      "\tspeed: 0.0155s/iter; left time: 136.6323s\n",
      "\titers: 400, epoch: 5 | loss: 0.2756516\n",
      "\tspeed: 0.0155s/iter; left time: 135.3275s\n",
      "\titers: 500, epoch: 5 | loss: 0.1223429\n",
      "\tspeed: 0.0155s/iter; left time: 133.8109s\n",
      "Epoch: 5 cost time: 9.087207555770874\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1918286 Vali Loss: 0.0575880 Test Loss: 0.1577104\n",
      "Validation loss decreased (0.060544 --> 0.057588).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2719165\n",
      "\tspeed: 0.0461s/iter; left time: 389.6876s\n",
      "\titers: 200, epoch: 6 | loss: 0.1927407\n",
      "\tspeed: 0.0156s/iter; left time: 129.9493s\n",
      "\titers: 300, epoch: 6 | loss: 0.0962062\n",
      "\tspeed: 0.0156s/iter; left time: 128.4204s\n",
      "\titers: 400, epoch: 6 | loss: 0.2174937\n",
      "\tspeed: 0.0156s/iter; left time: 126.8108s\n",
      "\titers: 500, epoch: 6 | loss: 0.1846272\n",
      "\tspeed: 0.0156s/iter; left time: 125.3126s\n",
      "Epoch: 6 cost time: 9.10450530052185\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1781630 Vali Loss: 0.0573460 Test Loss: 0.1543588\n",
      "Validation loss decreased (0.057588 --> 0.057346).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1491029\n",
      "\tspeed: 0.0480s/iter; left time: 378.4794s\n",
      "\titers: 200, epoch: 7 | loss: 0.0993516\n",
      "\tspeed: 0.0155s/iter; left time: 120.5697s\n",
      "\titers: 300, epoch: 7 | loss: 0.1379019\n",
      "\tspeed: 0.0155s/iter; left time: 119.0457s\n",
      "\titers: 400, epoch: 7 | loss: 0.2205217\n",
      "\tspeed: 0.0155s/iter; left time: 117.4967s\n",
      "\titers: 500, epoch: 7 | loss: 0.1336048\n",
      "\tspeed: 0.0155s/iter; left time: 115.9590s\n",
      "Epoch: 7 cost time: 9.090817928314209\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1709848 Vali Loss: 0.0576333 Test Loss: 0.1552133\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1184507\n",
      "\tspeed: 0.0463s/iter; left time: 338.7966s\n",
      "\titers: 200, epoch: 8 | loss: 0.1265137\n",
      "\tspeed: 0.0135s/iter; left time: 97.1527s\n",
      "\titers: 300, epoch: 8 | loss: 0.1915678\n",
      "\tspeed: 0.0135s/iter; left time: 95.7793s\n",
      "\titers: 400, epoch: 8 | loss: 0.2026834\n",
      "\tspeed: 0.0135s/iter; left time: 94.3853s\n",
      "\titers: 500, epoch: 8 | loss: 0.1000716\n",
      "\tspeed: 0.0135s/iter; left time: 93.0867s\n",
      "Epoch: 8 cost time: 8.069302558898926\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1666280 Vali Loss: 0.0592988 Test Loss: 0.1568683\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1650104\n",
      "\tspeed: 0.0429s/iter; left time: 289.3531s\n",
      "\titers: 200, epoch: 9 | loss: 0.1470891\n",
      "\tspeed: 0.0134s/iter; left time: 89.1130s\n",
      "\titers: 300, epoch: 9 | loss: 0.1308380\n",
      "\tspeed: 0.0134s/iter; left time: 87.7531s\n",
      "\titers: 400, epoch: 9 | loss: 0.1730627\n",
      "\tspeed: 0.0155s/iter; left time: 99.8005s\n",
      "\titers: 500, epoch: 9 | loss: 0.1253441\n",
      "\tspeed: 0.0155s/iter; left time: 98.3577s\n",
      "Epoch: 9 cost time: 8.44125509262085\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1645545 Vali Loss: 0.0580829 Test Loss: 0.1555741\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15459345281124115, mae:0.2520411014556885\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2702464\n",
      "\tspeed: 0.0284s/iter; left time: 321.3471s\n",
      "\titers: 200, epoch: 1 | loss: 0.2674955\n",
      "\tspeed: 0.0164s/iter; left time: 183.5208s\n",
      "\titers: 300, epoch: 1 | loss: 0.3488625\n",
      "\tspeed: 0.0163s/iter; left time: 181.1784s\n",
      "\titers: 400, epoch: 1 | loss: 0.2293683\n",
      "\tspeed: 0.0163s/iter; left time: 179.7220s\n",
      "\titers: 500, epoch: 1 | loss: 0.2553122\n",
      "\tspeed: 0.0163s/iter; left time: 178.1441s\n",
      "Epoch: 1 cost time: 10.576723575592041\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3194285 Vali Loss: 0.0671749 Test Loss: 0.1898966\n",
      "Validation loss decreased (inf --> 0.067175).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3389419\n",
      "\tspeed: 0.0485s/iter; left time: 520.1703s\n",
      "\titers: 200, epoch: 2 | loss: 0.3397476\n",
      "\tspeed: 0.0163s/iter; left time: 173.2533s\n",
      "\titers: 300, epoch: 2 | loss: 0.3092797\n",
      "\tspeed: 0.0163s/iter; left time: 171.4726s\n",
      "\titers: 400, epoch: 2 | loss: 0.2053859\n",
      "\tspeed: 0.0163s/iter; left time: 169.8084s\n",
      "\titers: 500, epoch: 2 | loss: 0.2841223\n",
      "\tspeed: 0.0163s/iter; left time: 168.8500s\n",
      "Epoch: 2 cost time: 9.542523860931396\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2695681 Vali Loss: 0.0684227 Test Loss: 0.1898351\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2059644\n",
      "\tspeed: 0.0478s/iter; left time: 486.1042s\n",
      "\titers: 200, epoch: 3 | loss: 0.2477117\n",
      "\tspeed: 0.0164s/iter; left time: 165.0489s\n",
      "\titers: 300, epoch: 3 | loss: 0.3639211\n",
      "\tspeed: 0.0164s/iter; left time: 162.9861s\n",
      "\titers: 400, epoch: 3 | loss: 0.2139793\n",
      "\tspeed: 0.0163s/iter; left time: 160.7393s\n",
      "\titers: 500, epoch: 3 | loss: 0.2109823\n",
      "\tspeed: 0.0164s/iter; left time: 159.8019s\n",
      "Epoch: 3 cost time: 9.583325147628784\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2380238 Vali Loss: 0.0639201 Test Loss: 0.1737082\n",
      "Validation loss decreased (0.067175 --> 0.063920).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2504411\n",
      "\tspeed: 0.0485s/iter; left time: 464.8567s\n",
      "\titers: 200, epoch: 4 | loss: 0.1834994\n",
      "\tspeed: 0.0164s/iter; left time: 155.3344s\n",
      "\titers: 300, epoch: 4 | loss: 0.2105092\n",
      "\tspeed: 0.0163s/iter; left time: 153.4297s\n",
      "\titers: 400, epoch: 4 | loss: 0.2755169\n",
      "\tspeed: 0.0163s/iter; left time: 151.6705s\n",
      "\titers: 500, epoch: 4 | loss: 0.2003397\n",
      "\tspeed: 0.0163s/iter; left time: 150.2000s\n",
      "Epoch: 4 cost time: 9.569450855255127\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2210738 Vali Loss: 0.0653202 Test Loss: 0.1676902\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1212686\n",
      "\tspeed: 0.0478s/iter; left time: 431.3858s\n",
      "\titers: 200, epoch: 5 | loss: 0.2460476\n",
      "\tspeed: 0.0163s/iter; left time: 145.4298s\n",
      "\titers: 300, epoch: 5 | loss: 0.1491101\n",
      "\tspeed: 0.0163s/iter; left time: 143.7303s\n",
      "\titers: 400, epoch: 5 | loss: 0.1874050\n",
      "\tspeed: 0.0163s/iter; left time: 142.2134s\n",
      "\titers: 500, epoch: 5 | loss: 0.1471154\n",
      "\tspeed: 0.0163s/iter; left time: 140.6062s\n",
      "Epoch: 5 cost time: 9.537915229797363\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2057814 Vali Loss: 0.0583800 Test Loss: 0.1598453\n",
      "Validation loss decreased (0.063920 --> 0.058380).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2401012\n",
      "\tspeed: 0.0479s/iter; left time: 404.6475s\n",
      "\titers: 200, epoch: 6 | loss: 0.1714445\n",
      "\tspeed: 0.0163s/iter; left time: 136.0632s\n",
      "\titers: 300, epoch: 6 | loss: 0.2459915\n",
      "\tspeed: 0.0164s/iter; left time: 135.2890s\n",
      "\titers: 400, epoch: 6 | loss: 0.1931977\n",
      "\tspeed: 0.0164s/iter; left time: 133.7709s\n",
      "\titers: 500, epoch: 6 | loss: 0.1885254\n",
      "\tspeed: 0.0163s/iter; left time: 131.4184s\n",
      "Epoch: 6 cost time: 9.556966066360474\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1941076 Vali Loss: 0.0599090 Test Loss: 0.1608659\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1485454\n",
      "\tspeed: 0.0503s/iter; left time: 396.0769s\n",
      "\titers: 200, epoch: 7 | loss: 0.2220893\n",
      "\tspeed: 0.0177s/iter; left time: 137.9213s\n",
      "\titers: 300, epoch: 7 | loss: 0.2589035\n",
      "\tspeed: 0.0177s/iter; left time: 136.1119s\n",
      "\titers: 400, epoch: 7 | loss: 0.1114334\n",
      "\tspeed: 0.0177s/iter; left time: 134.3763s\n",
      "\titers: 500, epoch: 7 | loss: 0.1242444\n",
      "\tspeed: 0.0177s/iter; left time: 132.5198s\n",
      "Epoch: 7 cost time: 10.35298490524292\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1875073 Vali Loss: 0.0603522 Test Loss: 0.1618410\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1701197\n",
      "\tspeed: 0.0486s/iter; left time: 355.4794s\n",
      "\titers: 200, epoch: 8 | loss: 0.1515106\n",
      "\tspeed: 0.0163s/iter; left time: 117.7738s\n",
      "\titers: 300, epoch: 8 | loss: 0.2233585\n",
      "\tspeed: 0.0163s/iter; left time: 115.8866s\n",
      "\titers: 400, epoch: 8 | loss: 0.0997724\n",
      "\tspeed: 0.0163s/iter; left time: 114.3073s\n",
      "\titers: 500, epoch: 8 | loss: 0.1245760\n",
      "\tspeed: 0.0163s/iter; left time: 112.7131s\n",
      "Epoch: 8 cost time: 9.535839080810547\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1838400 Vali Loss: 0.0598232 Test Loss: 0.1601604\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1600600779056549, mae:0.25847792625427246\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2592842\n",
      "\tspeed: 0.0276s/iter; left time: 312.3622s\n",
      "\titers: 200, epoch: 1 | loss: 0.1940206\n",
      "\tspeed: 0.0158s/iter; left time: 177.4149s\n",
      "\titers: 300, epoch: 1 | loss: 0.3411418\n",
      "\tspeed: 0.0158s/iter; left time: 175.8013s\n",
      "\titers: 400, epoch: 1 | loss: 0.2999641\n",
      "\tspeed: 0.0158s/iter; left time: 174.1552s\n",
      "\titers: 500, epoch: 1 | loss: 0.1544883\n",
      "\tspeed: 0.0158s/iter; left time: 172.5403s\n",
      "Epoch: 1 cost time: 10.25948166847229\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3029523 Vali Loss: 0.0649792 Test Loss: 0.1817163\n",
      "Validation loss decreased (inf --> 0.064979).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2204731\n",
      "\tspeed: 0.0501s/iter; left time: 537.6709s\n",
      "\titers: 200, epoch: 2 | loss: 0.2015020\n",
      "\tspeed: 0.0158s/iter; left time: 167.4436s\n",
      "\titers: 300, epoch: 2 | loss: 0.3120509\n",
      "\tspeed: 0.0157s/iter; left time: 165.6793s\n",
      "\titers: 400, epoch: 2 | loss: 0.4567918\n",
      "\tspeed: 0.0158s/iter; left time: 164.3152s\n",
      "\titers: 500, epoch: 2 | loss: 0.2119895\n",
      "\tspeed: 0.0158s/iter; left time: 163.4528s\n",
      "Epoch: 2 cost time: 9.25087308883667\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2555726 Vali Loss: 0.0691800 Test Loss: 0.1844113\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2225079\n",
      "\tspeed: 0.0471s/iter; left time: 478.9666s\n",
      "\titers: 200, epoch: 3 | loss: 0.2669656\n",
      "\tspeed: 0.0160s/iter; left time: 161.1711s\n",
      "\titers: 300, epoch: 3 | loss: 0.1537148\n",
      "\tspeed: 0.0169s/iter; left time: 168.3559s\n",
      "\titers: 400, epoch: 3 | loss: 0.2292681\n",
      "\tspeed: 0.0168s/iter; left time: 166.0100s\n",
      "\titers: 500, epoch: 3 | loss: 0.2172293\n",
      "\tspeed: 0.0162s/iter; left time: 158.3916s\n",
      "Epoch: 3 cost time: 9.53176760673523\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2331770 Vali Loss: 0.0565890 Test Loss: 0.1642408\n",
      "Validation loss decreased (0.064979 --> 0.056589).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2177852\n",
      "\tspeed: 0.0505s/iter; left time: 484.3780s\n",
      "\titers: 200, epoch: 4 | loss: 0.3362504\n",
      "\tspeed: 0.0169s/iter; left time: 160.4055s\n",
      "\titers: 300, epoch: 4 | loss: 0.2126502\n",
      "\tspeed: 0.0165s/iter; left time: 154.5279s\n",
      "\titers: 400, epoch: 4 | loss: 0.1504691\n",
      "\tspeed: 0.0169s/iter; left time: 157.2183s\n",
      "\titers: 500, epoch: 4 | loss: 0.3105541\n",
      "\tspeed: 0.0163s/iter; left time: 150.1666s\n",
      "Epoch: 4 cost time: 9.74505090713501\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2091449 Vali Loss: 0.0589450 Test Loss: 0.1651395\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2018463\n",
      "\tspeed: 0.0483s/iter; left time: 435.6236s\n",
      "\titers: 200, epoch: 5 | loss: 0.3313251\n",
      "\tspeed: 0.0158s/iter; left time: 141.1629s\n",
      "\titers: 300, epoch: 5 | loss: 0.2012002\n",
      "\tspeed: 0.0158s/iter; left time: 139.2284s\n",
      "\titers: 400, epoch: 5 | loss: 0.1302354\n",
      "\tspeed: 0.0158s/iter; left time: 137.8530s\n",
      "\titers: 500, epoch: 5 | loss: 0.2003354\n",
      "\tspeed: 0.0158s/iter; left time: 136.3335s\n",
      "Epoch: 5 cost time: 9.283507108688354\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1946756 Vali Loss: 0.0567082 Test Loss: 0.1582989\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2129356\n",
      "\tspeed: 0.0478s/iter; left time: 404.1332s\n",
      "\titers: 200, epoch: 6 | loss: 0.2304875\n",
      "\tspeed: 0.0159s/iter; left time: 132.4458s\n",
      "\titers: 300, epoch: 6 | loss: 0.2754611\n",
      "\tspeed: 0.0159s/iter; left time: 130.8302s\n",
      "\titers: 400, epoch: 6 | loss: 0.1710404\n",
      "\tspeed: 0.0159s/iter; left time: 129.2586s\n",
      "\titers: 500, epoch: 6 | loss: 0.1663898\n",
      "\tspeed: 0.0158s/iter; left time: 127.3010s\n",
      "Epoch: 6 cost time: 9.303524732589722\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1853511 Vali Loss: 0.0544491 Test Loss: 0.1538800\n",
      "Validation loss decreased (0.056589 --> 0.054449).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2040665\n",
      "\tspeed: 0.0483s/iter; left time: 380.2973s\n",
      "\titers: 200, epoch: 7 | loss: 0.1018395\n",
      "\tspeed: 0.0159s/iter; left time: 123.4034s\n",
      "\titers: 300, epoch: 7 | loss: 0.1823076\n",
      "\tspeed: 0.0159s/iter; left time: 121.7728s\n",
      "\titers: 400, epoch: 7 | loss: 0.1981986\n",
      "\tspeed: 0.0158s/iter; left time: 120.1544s\n",
      "\titers: 500, epoch: 7 | loss: 0.3003233\n",
      "\tspeed: 0.0158s/iter; left time: 118.4830s\n",
      "Epoch: 7 cost time: 9.326466083526611\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1801386 Vali Loss: 0.0555357 Test Loss: 0.1576312\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1524271\n",
      "\tspeed: 0.0482s/iter; left time: 352.2133s\n",
      "\titers: 200, epoch: 8 | loss: 0.2910529\n",
      "\tspeed: 0.0158s/iter; left time: 114.2205s\n",
      "\titers: 300, epoch: 8 | loss: 0.2003015\n",
      "\tspeed: 0.0158s/iter; left time: 112.5732s\n",
      "\titers: 400, epoch: 8 | loss: 0.1914104\n",
      "\tspeed: 0.0163s/iter; left time: 114.2208s\n",
      "\titers: 500, epoch: 8 | loss: 0.1669760\n",
      "\tspeed: 0.0158s/iter; left time: 109.3902s\n",
      "Epoch: 8 cost time: 9.333168268203735\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1769568 Vali Loss: 0.0551039 Test Loss: 0.1566411\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1595635\n",
      "\tspeed: 0.0477s/iter; left time: 321.8038s\n",
      "\titers: 200, epoch: 9 | loss: 0.1816410\n",
      "\tspeed: 0.0165s/iter; left time: 109.8646s\n",
      "\titers: 300, epoch: 9 | loss: 0.0986668\n",
      "\tspeed: 0.0162s/iter; left time: 105.9167s\n",
      "\titers: 400, epoch: 9 | loss: 0.1745072\n",
      "\tspeed: 0.0159s/iter; left time: 102.1921s\n",
      "\titers: 500, epoch: 9 | loss: 0.1041569\n",
      "\tspeed: 0.0159s/iter; left time: 100.5252s\n",
      "Epoch: 9 cost time: 9.471447467803955\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1748284 Vali Loss: 0.0550290 Test Loss: 0.1563157\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15406329929828644, mae:0.2528390884399414\n",
      ">>> LR=1e-3,DO=0.0,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3087568\n",
      "\tspeed: 0.0264s/iter; left time: 298.3431s\n",
      "\titers: 200, epoch: 1 | loss: 0.3462889\n",
      "\tspeed: 0.0146s/iter; left time: 163.4437s\n",
      "\titers: 300, epoch: 1 | loss: 0.2530968\n",
      "\tspeed: 0.0145s/iter; left time: 160.9240s\n",
      "\titers: 400, epoch: 1 | loss: 0.3330573\n",
      "\tspeed: 0.0145s/iter; left time: 159.7118s\n",
      "\titers: 500, epoch: 1 | loss: 0.4840830\n",
      "\tspeed: 0.0145s/iter; left time: 158.3923s\n",
      "Epoch: 1 cost time: 9.522202491760254\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2991262 Vali Loss: 0.0749524 Test Loss: 0.2021642\n",
      "Validation loss decreased (inf --> 0.074952).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4184516\n",
      "\tspeed: 0.0453s/iter; left time: 486.5083s\n",
      "\titers: 200, epoch: 2 | loss: 0.1698274\n",
      "\tspeed: 0.0145s/iter; left time: 154.5418s\n",
      "\titers: 300, epoch: 2 | loss: 0.3770357\n",
      "\tspeed: 0.0146s/iter; left time: 153.2623s\n",
      "\titers: 400, epoch: 2 | loss: 0.2762246\n",
      "\tspeed: 0.0146s/iter; left time: 151.8268s\n",
      "\titers: 500, epoch: 2 | loss: 0.2991737\n",
      "\tspeed: 0.0145s/iter; left time: 149.6499s\n",
      "Epoch: 2 cost time: 8.525974988937378\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2748205 Vali Loss: 0.0604024 Test Loss: 0.1754508\n",
      "Validation loss decreased (0.074952 --> 0.060402).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2303173\n",
      "\tspeed: 0.0450s/iter; left time: 457.6918s\n",
      "\titers: 200, epoch: 3 | loss: 0.1447184\n",
      "\tspeed: 0.0146s/iter; left time: 146.4448s\n",
      "\titers: 300, epoch: 3 | loss: 0.2395865\n",
      "\tspeed: 0.0145s/iter; left time: 144.5658s\n",
      "\titers: 400, epoch: 3 | loss: 0.3493069\n",
      "\tspeed: 0.0145s/iter; left time: 143.4560s\n",
      "\titers: 500, epoch: 3 | loss: 0.2475004\n",
      "\tspeed: 0.0145s/iter; left time: 141.6909s\n",
      "Epoch: 3 cost time: 8.52150821685791\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2622324 Vali Loss: 0.0659564 Test Loss: 0.1945945\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1808744\n",
      "\tspeed: 0.0442s/iter; left time: 423.9618s\n",
      "\titers: 200, epoch: 4 | loss: 0.3168277\n",
      "\tspeed: 0.0146s/iter; left time: 138.3629s\n",
      "\titers: 300, epoch: 4 | loss: 0.3154862\n",
      "\tspeed: 0.0146s/iter; left time: 136.9650s\n",
      "\titers: 400, epoch: 4 | loss: 0.2198539\n",
      "\tspeed: 0.0146s/iter; left time: 135.7513s\n",
      "\titers: 500, epoch: 4 | loss: 0.2445161\n",
      "\tspeed: 0.0146s/iter; left time: 133.8446s\n",
      "Epoch: 4 cost time: 8.538766860961914\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2500855 Vali Loss: 0.0656367 Test Loss: 0.1755282\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2021067\n",
      "\tspeed: 0.0444s/iter; left time: 400.2793s\n",
      "\titers: 200, epoch: 5 | loss: 0.1661894\n",
      "\tspeed: 0.0146s/iter; left time: 130.1716s\n",
      "\titers: 300, epoch: 5 | loss: 0.1782910\n",
      "\tspeed: 0.0145s/iter; left time: 128.1015s\n",
      "\titers: 400, epoch: 5 | loss: 0.1894042\n",
      "\tspeed: 0.0145s/iter; left time: 126.3191s\n",
      "\titers: 500, epoch: 5 | loss: 0.2144025\n",
      "\tspeed: 0.0145s/iter; left time: 124.6133s\n",
      "Epoch: 5 cost time: 8.490573167800903\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2293930 Vali Loss: 0.0615150 Test Loss: 0.1702067\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.175662562251091, mae:0.26966238021850586\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2576005\n",
      "\tspeed: 0.0297s/iter; left time: 335.8499s\n",
      "\titers: 200, epoch: 1 | loss: 0.3177054\n",
      "\tspeed: 0.0174s/iter; left time: 194.5502s\n",
      "\titers: 300, epoch: 1 | loss: 0.4582160\n",
      "\tspeed: 0.0174s/iter; left time: 193.4464s\n",
      "\titers: 400, epoch: 1 | loss: 0.2453062\n",
      "\tspeed: 0.0175s/iter; left time: 192.4838s\n",
      "\titers: 500, epoch: 1 | loss: 0.1802820\n",
      "\tspeed: 0.0174s/iter; left time: 189.8903s\n",
      "Epoch: 1 cost time: 11.206317663192749\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2986423 Vali Loss: 0.0624207 Test Loss: 0.1859441\n",
      "Validation loss decreased (inf --> 0.062421).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3728313\n",
      "\tspeed: 0.0549s/iter; left time: 588.9637s\n",
      "\titers: 200, epoch: 2 | loss: 0.1625194\n",
      "\tspeed: 0.0175s/iter; left time: 185.7969s\n",
      "\titers: 300, epoch: 2 | loss: 0.2098111\n",
      "\tspeed: 0.0193s/iter; left time: 202.9277s\n",
      "\titers: 400, epoch: 2 | loss: 0.2316139\n",
      "\tspeed: 0.0184s/iter; left time: 192.2134s\n",
      "\titers: 500, epoch: 2 | loss: 0.2099084\n",
      "\tspeed: 0.0190s/iter; left time: 196.2704s\n",
      "Epoch: 2 cost time: 10.658440113067627\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2463334 Vali Loss: 0.0672368 Test Loss: 0.1854389\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1567222\n",
      "\tspeed: 0.0547s/iter; left time: 555.7211s\n",
      "\titers: 200, epoch: 3 | loss: 0.2328888\n",
      "\tspeed: 0.0175s/iter; left time: 176.5596s\n",
      "\titers: 300, epoch: 3 | loss: 0.2081664\n",
      "\tspeed: 0.0190s/iter; left time: 189.4887s\n",
      "\titers: 400, epoch: 3 | loss: 0.1160073\n",
      "\tspeed: 0.0183s/iter; left time: 180.8855s\n",
      "\titers: 500, epoch: 3 | loss: 0.1852383\n",
      "\tspeed: 0.0175s/iter; left time: 170.6408s\n",
      "Epoch: 3 cost time: 10.469424486160278\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2161097 Vali Loss: 0.0575502 Test Loss: 0.1623097\n",
      "Validation loss decreased (0.062421 --> 0.057550).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2091047\n",
      "\tspeed: 0.0537s/iter; left time: 514.7785s\n",
      "\titers: 200, epoch: 4 | loss: 0.1903538\n",
      "\tspeed: 0.0175s/iter; left time: 166.1957s\n",
      "\titers: 300, epoch: 4 | loss: 0.2823515\n",
      "\tspeed: 0.0175s/iter; left time: 164.4218s\n",
      "\titers: 400, epoch: 4 | loss: 0.1222386\n",
      "\tspeed: 0.0175s/iter; left time: 162.6878s\n",
      "\titers: 500, epoch: 4 | loss: 0.2279944\n",
      "\tspeed: 0.0175s/iter; left time: 160.9021s\n",
      "Epoch: 4 cost time: 10.235992431640625\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1895393 Vali Loss: 0.0610897 Test Loss: 0.1650018\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2419203\n",
      "\tspeed: 0.0552s/iter; left time: 497.9696s\n",
      "\titers: 200, epoch: 5 | loss: 0.1397381\n",
      "\tspeed: 0.0175s/iter; left time: 155.9017s\n",
      "\titers: 300, epoch: 5 | loss: 0.1961470\n",
      "\tspeed: 0.0174s/iter; left time: 153.7449s\n",
      "\titers: 400, epoch: 5 | loss: 0.2142258\n",
      "\tspeed: 0.0175s/iter; left time: 152.3859s\n",
      "\titers: 500, epoch: 5 | loss: 0.2020703\n",
      "\tspeed: 0.0175s/iter; left time: 150.8768s\n",
      "Epoch: 5 cost time: 10.215934753417969\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1776697 Vali Loss: 0.0597592 Test Loss: 0.1574803\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1256947\n",
      "\tspeed: 0.0553s/iter; left time: 467.4402s\n",
      "\titers: 200, epoch: 6 | loss: 0.1939256\n",
      "\tspeed: 0.0175s/iter; left time: 146.2460s\n",
      "\titers: 300, epoch: 6 | loss: 0.1321706\n",
      "\tspeed: 0.0174s/iter; left time: 143.7865s\n",
      "\titers: 400, epoch: 6 | loss: 0.1912255\n",
      "\tspeed: 0.0173s/iter; left time: 141.3807s\n",
      "\titers: 500, epoch: 6 | loss: 0.1801842\n",
      "\tspeed: 0.0173s/iter; left time: 139.4803s\n",
      "Epoch: 6 cost time: 10.249757528305054\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1691510 Vali Loss: 0.0597679 Test Loss: 0.1599545\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16255781054496765, mae:0.2579495906829834\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4043203\n",
      "\tspeed: 0.0312s/iter; left time: 352.6677s\n",
      "\titers: 200, epoch: 1 | loss: 0.1992670\n",
      "\tspeed: 0.0172s/iter; left time: 193.0135s\n",
      "\titers: 300, epoch: 1 | loss: 0.1797327\n",
      "\tspeed: 0.0199s/iter; left time: 221.1423s\n",
      "\titers: 400, epoch: 1 | loss: 0.1445734\n",
      "\tspeed: 0.0182s/iter; left time: 200.3565s\n",
      "\titers: 500, epoch: 1 | loss: 0.1533082\n",
      "\tspeed: 0.0172s/iter; left time: 187.7463s\n",
      "Epoch: 1 cost time: 11.650950908660889\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2059793 Vali Loss: 0.0638378 Test Loss: 0.1858505\n",
      "Validation loss decreased (inf --> 0.063838).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3128408\n",
      "\tspeed: 0.0539s/iter; left time: 578.1439s\n",
      "\titers: 200, epoch: 2 | loss: 0.3340852\n",
      "\tspeed: 0.0174s/iter; left time: 184.7561s\n",
      "\titers: 300, epoch: 2 | loss: 0.4815163\n",
      "\tspeed: 0.0173s/iter; left time: 181.8174s\n",
      "\titers: 400, epoch: 2 | loss: 0.2596833\n",
      "\tspeed: 0.0198s/iter; left time: 206.8084s\n",
      "\titers: 500, epoch: 2 | loss: 0.1839698\n",
      "\tspeed: 0.0199s/iter; left time: 206.0810s\n",
      "Epoch: 2 cost time: 10.858459949493408\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2772075 Vali Loss: 0.0627708 Test Loss: 0.1783495\n",
      "Validation loss decreased (0.063838 --> 0.062771).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1760170\n",
      "\tspeed: 0.0582s/iter; left time: 591.2984s\n",
      "\titers: 200, epoch: 3 | loss: 0.1966718\n",
      "\tspeed: 0.0200s/iter; left time: 200.8118s\n",
      "\titers: 300, epoch: 3 | loss: 0.1913995\n",
      "\tspeed: 0.0199s/iter; left time: 198.4681s\n",
      "\titers: 400, epoch: 3 | loss: 0.1781942\n",
      "\tspeed: 0.0199s/iter; left time: 196.6143s\n",
      "\titers: 500, epoch: 3 | loss: 0.1715818\n",
      "\tspeed: 0.0199s/iter; left time: 194.5560s\n",
      "Epoch: 3 cost time: 11.646062135696411\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2209164 Vali Loss: 0.0596031 Test Loss: 0.1728491\n",
      "Validation loss decreased (0.062771 --> 0.059603).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1805340\n",
      "\tspeed: 0.0588s/iter; left time: 564.1011s\n",
      "\titers: 200, epoch: 4 | loss: 0.0905904\n",
      "\tspeed: 0.0173s/iter; left time: 164.4389s\n",
      "\titers: 300, epoch: 4 | loss: 0.2791197\n",
      "\tspeed: 0.0173s/iter; left time: 162.5148s\n",
      "\titers: 400, epoch: 4 | loss: 0.1629296\n",
      "\tspeed: 0.0173s/iter; left time: 160.4514s\n",
      "\titers: 500, epoch: 4 | loss: 0.1282458\n",
      "\tspeed: 0.0173s/iter; left time: 158.7134s\n",
      "Epoch: 4 cost time: 10.304103136062622\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1975164 Vali Loss: 0.0578438 Test Loss: 0.1621911\n",
      "Validation loss decreased (0.059603 --> 0.057844).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1482425\n",
      "\tspeed: 0.0552s/iter; left time: 497.7346s\n",
      "\titers: 200, epoch: 5 | loss: 0.1782770\n",
      "\tspeed: 0.0193s/iter; left time: 172.0097s\n",
      "\titers: 300, epoch: 5 | loss: 0.1629610\n",
      "\tspeed: 0.0188s/iter; left time: 166.0570s\n",
      "\titers: 400, epoch: 5 | loss: 0.1764713\n",
      "\tspeed: 0.0180s/iter; left time: 157.0201s\n",
      "\titers: 500, epoch: 5 | loss: 0.1842960\n",
      "\tspeed: 0.0173s/iter; left time: 148.9631s\n",
      "Epoch: 5 cost time: 10.768644094467163\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1851035 Vali Loss: 0.0581966 Test Loss: 0.1593677\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2247585\n",
      "\tspeed: 0.0531s/iter; left time: 448.4959s\n",
      "\titers: 200, epoch: 6 | loss: 0.1118122\n",
      "\tspeed: 0.0171s/iter; left time: 142.9453s\n",
      "\titers: 300, epoch: 6 | loss: 0.1594950\n",
      "\tspeed: 0.0171s/iter; left time: 141.2828s\n",
      "\titers: 400, epoch: 6 | loss: 0.1419411\n",
      "\tspeed: 0.0171s/iter; left time: 139.4413s\n",
      "\titers: 500, epoch: 6 | loss: 0.1376313\n",
      "\tspeed: 0.0171s/iter; left time: 137.6913s\n",
      "Epoch: 6 cost time: 10.045849800109863\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1773759 Vali Loss: 0.0558292 Test Loss: 0.1570787\n",
      "Validation loss decreased (0.057844 --> 0.055829).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1612963\n",
      "\tspeed: 0.0535s/iter; left time: 421.4033s\n",
      "\titers: 200, epoch: 7 | loss: 0.1753921\n",
      "\tspeed: 0.0173s/iter; left time: 134.2516s\n",
      "\titers: 300, epoch: 7 | loss: 0.1152870\n",
      "\tspeed: 0.0172s/iter; left time: 132.3725s\n",
      "\titers: 400, epoch: 7 | loss: 0.1577708\n",
      "\tspeed: 0.0172s/iter; left time: 130.7275s\n",
      "\titers: 500, epoch: 7 | loss: 0.1604751\n",
      "\tspeed: 0.0197s/iter; left time: 147.5970s\n",
      "Epoch: 7 cost time: 10.53983473777771\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1730504 Vali Loss: 0.0571969 Test Loss: 0.1591050\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1169839\n",
      "\tspeed: 0.0566s/iter; left time: 413.9164s\n",
      "\titers: 200, epoch: 8 | loss: 0.1007759\n",
      "\tspeed: 0.0194s/iter; left time: 140.2131s\n",
      "\titers: 300, epoch: 8 | loss: 0.2006591\n",
      "\tspeed: 0.0198s/iter; left time: 141.0467s\n",
      "\titers: 400, epoch: 8 | loss: 0.2258077\n",
      "\tspeed: 0.0198s/iter; left time: 139.0592s\n",
      "\titers: 500, epoch: 8 | loss: 0.2066380\n",
      "\tspeed: 0.0199s/iter; left time: 137.2429s\n",
      "Epoch: 8 cost time: 11.498283863067627\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1705565 Vali Loss: 0.0568154 Test Loss: 0.1585337\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1093133\n",
      "\tspeed: 0.0574s/iter; left time: 386.8060s\n",
      "\titers: 200, epoch: 9 | loss: 0.1208550\n",
      "\tspeed: 0.0199s/iter; left time: 132.0124s\n",
      "\titers: 300, epoch: 9 | loss: 0.1547902\n",
      "\tspeed: 0.0199s/iter; left time: 129.9036s\n",
      "\titers: 400, epoch: 9 | loss: 0.1336278\n",
      "\tspeed: 0.0199s/iter; left time: 128.0164s\n",
      "\titers: 500, epoch: 9 | loss: 0.1185609\n",
      "\tspeed: 0.0199s/iter; left time: 125.9550s\n",
      "Epoch: 9 cost time: 11.605529546737671\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1690572 Vali Loss: 0.0568122 Test Loss: 0.1580576\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15728867053985596, mae:0.2547375559806824\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2556352\n",
      "\tspeed: 0.0313s/iter; left time: 354.0244s\n",
      "\titers: 200, epoch: 1 | loss: 0.1872244\n",
      "\tspeed: 0.0200s/iter; left time: 224.3390s\n",
      "\titers: 300, epoch: 1 | loss: 0.2462130\n",
      "\tspeed: 0.0201s/iter; left time: 222.7921s\n",
      "\titers: 400, epoch: 1 | loss: 0.2578883\n",
      "\tspeed: 0.0200s/iter; left time: 220.0233s\n",
      "\titers: 500, epoch: 1 | loss: 0.3866651\n",
      "\tspeed: 0.0200s/iter; left time: 218.3598s\n",
      "Epoch: 1 cost time: 12.603454828262329\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2572709 Vali Loss: 0.0606518 Test Loss: 0.1839672\n",
      "Validation loss decreased (inf --> 0.060652).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4579217\n",
      "\tspeed: 0.0579s/iter; left time: 621.2029s\n",
      "\titers: 200, epoch: 2 | loss: 0.3687385\n",
      "\tspeed: 0.0200s/iter; left time: 213.1456s\n",
      "\titers: 300, epoch: 2 | loss: 0.3783978\n",
      "\tspeed: 0.0201s/iter; left time: 211.2701s\n",
      "\titers: 400, epoch: 2 | loss: 0.2041424\n",
      "\tspeed: 0.0197s/iter; left time: 205.5901s\n",
      "\titers: 500, epoch: 2 | loss: 0.3030787\n",
      "\tspeed: 0.0194s/iter; left time: 200.2351s\n",
      "Epoch: 2 cost time: 11.52196717262268\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2547372 Vali Loss: 0.0611325 Test Loss: 0.1727885\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1552677\n",
      "\tspeed: 0.0568s/iter; left time: 576.6585s\n",
      "\titers: 200, epoch: 3 | loss: 0.2073597\n",
      "\tspeed: 0.0172s/iter; left time: 173.2307s\n",
      "\titers: 300, epoch: 3 | loss: 0.2164321\n",
      "\tspeed: 0.0172s/iter; left time: 171.5602s\n",
      "\titers: 400, epoch: 3 | loss: 0.2554320\n",
      "\tspeed: 0.0172s/iter; left time: 169.6766s\n",
      "\titers: 500, epoch: 3 | loss: 0.2286625\n",
      "\tspeed: 0.0172s/iter; left time: 168.3042s\n",
      "Epoch: 3 cost time: 10.075936794281006\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2108641 Vali Loss: 0.0606534 Test Loss: 0.1671809\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1449989\n",
      "\tspeed: 0.0553s/iter; left time: 530.6455s\n",
      "\titers: 200, epoch: 4 | loss: 0.1873359\n",
      "\tspeed: 0.0172s/iter; left time: 163.3583s\n",
      "\titers: 300, epoch: 4 | loss: 0.1576527\n",
      "\tspeed: 0.0172s/iter; left time: 161.5102s\n",
      "\titers: 400, epoch: 4 | loss: 0.2044394\n",
      "\tspeed: 0.0172s/iter; left time: 159.7752s\n",
      "\titers: 500, epoch: 4 | loss: 0.1120493\n",
      "\tspeed: 0.0172s/iter; left time: 158.0880s\n",
      "Epoch: 4 cost time: 10.083813190460205\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1912532 Vali Loss: 0.0578707 Test Loss: 0.1583105\n",
      "Validation loss decreased (0.060652 --> 0.057871).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1316964\n",
      "\tspeed: 0.0538s/iter; left time: 485.2558s\n",
      "\titers: 200, epoch: 5 | loss: 0.1546748\n",
      "\tspeed: 0.0174s/iter; left time: 155.0252s\n",
      "\titers: 300, epoch: 5 | loss: 0.1557725\n",
      "\tspeed: 0.0174s/iter; left time: 153.2227s\n",
      "\titers: 400, epoch: 5 | loss: 0.2160063\n",
      "\tspeed: 0.0173s/iter; left time: 150.4385s\n",
      "\titers: 500, epoch: 5 | loss: 0.1194131\n",
      "\tspeed: 0.0173s/iter; left time: 148.7314s\n",
      "Epoch: 5 cost time: 10.152634382247925\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1778205 Vali Loss: 0.0576764 Test Loss: 0.1575596\n",
      "Validation loss decreased (0.057871 --> 0.057676).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2322009\n",
      "\tspeed: 0.0535s/iter; left time: 451.8531s\n",
      "\titers: 200, epoch: 6 | loss: 0.1299272\n",
      "\tspeed: 0.0172s/iter; left time: 143.9672s\n",
      "\titers: 300, epoch: 6 | loss: 0.3989783\n",
      "\tspeed: 0.0172s/iter; left time: 142.2352s\n",
      "\titers: 400, epoch: 6 | loss: 0.1361643\n",
      "\tspeed: 0.0172s/iter; left time: 140.5216s\n",
      "\titers: 500, epoch: 6 | loss: 0.2072702\n",
      "\tspeed: 0.0172s/iter; left time: 138.8267s\n",
      "Epoch: 6 cost time: 10.10404658317566\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1666638 Vali Loss: 0.0567906 Test Loss: 0.1548798\n",
      "Validation loss decreased (0.057676 --> 0.056791).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1350531\n",
      "\tspeed: 0.0535s/iter; left time: 421.6684s\n",
      "\titers: 200, epoch: 7 | loss: 0.1668256\n",
      "\tspeed: 0.0197s/iter; left time: 153.1850s\n",
      "\titers: 300, epoch: 7 | loss: 0.1984801\n",
      "\tspeed: 0.0199s/iter; left time: 152.8190s\n",
      "\titers: 400, epoch: 7 | loss: 0.1426664\n",
      "\tspeed: 0.0199s/iter; left time: 150.7576s\n",
      "\titers: 500, epoch: 7 | loss: 0.1303774\n",
      "\tspeed: 0.0200s/iter; left time: 149.8272s\n",
      "Epoch: 7 cost time: 11.178615808486938\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1604715 Vali Loss: 0.0566994 Test Loss: 0.1539143\n",
      "Validation loss decreased (0.056791 --> 0.056699).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0793495\n",
      "\tspeed: 0.0538s/iter; left time: 393.2915s\n",
      "\titers: 200, epoch: 8 | loss: 0.0816441\n",
      "\tspeed: 0.0173s/iter; left time: 124.7026s\n",
      "\titers: 300, epoch: 8 | loss: 0.1428428\n",
      "\tspeed: 0.0173s/iter; left time: 122.8246s\n",
      "\titers: 400, epoch: 8 | loss: 0.1213741\n",
      "\tspeed: 0.0172s/iter; left time: 120.8443s\n",
      "\titers: 500, epoch: 8 | loss: 0.1387340\n",
      "\tspeed: 0.0172s/iter; left time: 118.8952s\n",
      "Epoch: 8 cost time: 10.100126504898071\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1568428 Vali Loss: 0.0571787 Test Loss: 0.1558457\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2026256\n",
      "\tspeed: 0.0533s/iter; left time: 359.2948s\n",
      "\titers: 200, epoch: 9 | loss: 0.1673106\n",
      "\tspeed: 0.0171s/iter; left time: 113.5427s\n",
      "\titers: 300, epoch: 9 | loss: 0.1801844\n",
      "\tspeed: 0.0171s/iter; left time: 112.1383s\n",
      "\titers: 400, epoch: 9 | loss: 0.2959049\n",
      "\tspeed: 0.0172s/iter; left time: 110.4693s\n",
      "\titers: 500, epoch: 9 | loss: 0.1356361\n",
      "\tspeed: 0.0172s/iter; left time: 108.7952s\n",
      "Epoch: 9 cost time: 10.026670932769775\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1549962 Vali Loss: 0.0568263 Test Loss: 0.1550211\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1370055\n",
      "\tspeed: 0.0536s/iter; left time: 331.0250s\n",
      "\titers: 200, epoch: 10 | loss: 0.1289415\n",
      "\tspeed: 0.0175s/iter; left time: 106.0927s\n",
      "\titers: 300, epoch: 10 | loss: 0.1983363\n",
      "\tspeed: 0.0173s/iter; left time: 103.5936s\n",
      "\titers: 400, epoch: 10 | loss: 0.1467326\n",
      "\tspeed: 0.0173s/iter; left time: 101.7714s\n",
      "\titers: 500, epoch: 10 | loss: 0.1663210\n",
      "\tspeed: 0.0192s/iter; left time: 111.0646s\n",
      "Epoch: 10 cost time: 10.623488903045654\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1536804 Vali Loss: 0.0567571 Test Loss: 0.1549835\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15411075949668884, mae:0.2524541914463043\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2633941\n",
      "\tspeed: 0.0331s/iter; left time: 374.0978s\n",
      "\titers: 200, epoch: 1 | loss: 0.2882230\n",
      "\tspeed: 0.0214s/iter; left time: 239.3866s\n",
      "\titers: 300, epoch: 1 | loss: 0.3079577\n",
      "\tspeed: 0.0213s/iter; left time: 236.8079s\n",
      "\titers: 400, epoch: 1 | loss: 0.4719545\n",
      "\tspeed: 0.0213s/iter; left time: 234.6780s\n",
      "\titers: 500, epoch: 1 | loss: 0.3473301\n",
      "\tspeed: 0.0213s/iter; left time: 232.4125s\n",
      "Epoch: 1 cost time: 13.392061948776245\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3496201 Vali Loss: 0.0691530 Test Loss: 0.2032876\n",
      "Validation loss decreased (inf --> 0.069153).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3026515\n",
      "\tspeed: 0.0623s/iter; left time: 668.5103s\n",
      "\titers: 200, epoch: 2 | loss: 0.2586778\n",
      "\tspeed: 0.0214s/iter; left time: 227.3466s\n",
      "\titers: 300, epoch: 2 | loss: 0.2658741\n",
      "\tspeed: 0.0213s/iter; left time: 224.6427s\n",
      "\titers: 400, epoch: 2 | loss: 0.3067841\n",
      "\tspeed: 0.0214s/iter; left time: 222.7741s\n",
      "\titers: 500, epoch: 2 | loss: 0.2336292\n",
      "\tspeed: 0.0213s/iter; left time: 220.0898s\n",
      "Epoch: 2 cost time: 12.472235679626465\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2770482 Vali Loss: 0.0684681 Test Loss: 0.1803966\n",
      "Validation loss decreased (0.069153 --> 0.068468).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.5014133\n",
      "\tspeed: 0.0618s/iter; left time: 627.9293s\n",
      "\titers: 200, epoch: 3 | loss: 0.3132436\n",
      "\tspeed: 0.0214s/iter; left time: 215.4905s\n",
      "\titers: 300, epoch: 3 | loss: 0.3100383\n",
      "\tspeed: 0.0214s/iter; left time: 213.3257s\n",
      "\titers: 400, epoch: 3 | loss: 0.2355167\n",
      "\tspeed: 0.0214s/iter; left time: 211.0424s\n",
      "\titers: 500, epoch: 3 | loss: 0.2293372\n",
      "\tspeed: 0.0214s/iter; left time: 208.8323s\n",
      "Epoch: 3 cost time: 12.48382306098938\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2852790 Vali Loss: 0.0618621 Test Loss: 0.1858376\n",
      "Validation loss decreased (0.068468 --> 0.061862).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1910913\n",
      "\tspeed: 0.0627s/iter; left time: 601.0592s\n",
      "\titers: 200, epoch: 4 | loss: 0.2064937\n",
      "\tspeed: 0.0224s/iter; left time: 212.5664s\n",
      "\titers: 300, epoch: 4 | loss: 0.3060553\n",
      "\tspeed: 0.0214s/iter; left time: 200.8438s\n",
      "\titers: 400, epoch: 4 | loss: 0.2156951\n",
      "\tspeed: 0.0227s/iter; left time: 210.4433s\n",
      "\titers: 500, epoch: 4 | loss: 0.2605067\n",
      "\tspeed: 0.0231s/iter; left time: 212.4625s\n",
      "Epoch: 4 cost time: 13.135343790054321\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2557708 Vali Loss: 0.0617414 Test Loss: 0.1765215\n",
      "Validation loss decreased (0.061862 --> 0.061741).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2731111\n",
      "\tspeed: 0.0623s/iter; left time: 562.1240s\n",
      "\titers: 200, epoch: 5 | loss: 0.1924176\n",
      "\tspeed: 0.0212s/iter; left time: 189.5618s\n",
      "\titers: 300, epoch: 5 | loss: 0.1932702\n",
      "\tspeed: 0.0213s/iter; left time: 187.5757s\n",
      "\titers: 400, epoch: 5 | loss: 0.2116029\n",
      "\tspeed: 0.0213s/iter; left time: 185.3899s\n",
      "\titers: 500, epoch: 5 | loss: 0.2595659\n",
      "\tspeed: 0.0212s/iter; left time: 183.1712s\n",
      "Epoch: 5 cost time: 12.426280498504639\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2348923 Vali Loss: 0.0627406 Test Loss: 0.1726262\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2130222\n",
      "\tspeed: 0.0622s/iter; left time: 526.0522s\n",
      "\titers: 200, epoch: 6 | loss: 0.3572890\n",
      "\tspeed: 0.0213s/iter; left time: 177.8559s\n",
      "\titers: 300, epoch: 6 | loss: 0.2064936\n",
      "\tspeed: 0.0213s/iter; left time: 175.6872s\n",
      "\titers: 400, epoch: 6 | loss: 0.2617048\n",
      "\tspeed: 0.0213s/iter; left time: 173.3674s\n",
      "\titers: 500, epoch: 6 | loss: 0.3124670\n",
      "\tspeed: 0.0213s/iter; left time: 171.4600s\n",
      "Epoch: 6 cost time: 12.433152437210083\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2221600 Vali Loss: 0.0617320 Test Loss: 0.1794897\n",
      "Validation loss decreased (0.061741 --> 0.061732).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2988350\n",
      "\tspeed: 0.0616s/iter; left time: 485.1310s\n",
      "\titers: 200, epoch: 7 | loss: 0.1856506\n",
      "\tspeed: 0.0215s/iter; left time: 166.9678s\n",
      "\titers: 300, epoch: 7 | loss: 0.1715053\n",
      "\tspeed: 0.0214s/iter; left time: 164.7519s\n",
      "\titers: 400, epoch: 7 | loss: 0.2193801\n",
      "\tspeed: 0.0214s/iter; left time: 162.2608s\n",
      "\titers: 500, epoch: 7 | loss: 0.2082414\n",
      "\tspeed: 0.0214s/iter; left time: 160.3924s\n",
      "Epoch: 7 cost time: 12.50333046913147\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2201508 Vali Loss: 0.0617448 Test Loss: 0.1733547\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1307898\n",
      "\tspeed: 0.0605s/iter; left time: 442.1634s\n",
      "\titers: 200, epoch: 8 | loss: 0.3508490\n",
      "\tspeed: 0.0213s/iter; left time: 153.6159s\n",
      "\titers: 300, epoch: 8 | loss: 0.1427951\n",
      "\tspeed: 0.0213s/iter; left time: 151.3713s\n",
      "\titers: 400, epoch: 8 | loss: 0.1989607\n",
      "\tspeed: 0.0213s/iter; left time: 149.2062s\n",
      "\titers: 500, epoch: 8 | loss: 0.2051648\n",
      "\tspeed: 0.0213s/iter; left time: 147.0964s\n",
      "Epoch: 8 cost time: 12.411915302276611\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2198184 Vali Loss: 0.0605993 Test Loss: 0.1703959\n",
      "Validation loss decreased (0.061732 --> 0.060599).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2086617\n",
      "\tspeed: 0.0611s/iter; left time: 411.7840s\n",
      "\titers: 200, epoch: 9 | loss: 0.1701879\n",
      "\tspeed: 0.0214s/iter; left time: 142.2678s\n",
      "\titers: 300, epoch: 9 | loss: 0.2340400\n",
      "\tspeed: 0.0214s/iter; left time: 139.9729s\n",
      "\titers: 400, epoch: 9 | loss: 0.2145180\n",
      "\tspeed: 0.0214s/iter; left time: 137.8207s\n",
      "\titers: 500, epoch: 9 | loss: 0.1757907\n",
      "\tspeed: 0.0214s/iter; left time: 135.6428s\n",
      "Epoch: 9 cost time: 12.47000765800476\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2117511 Vali Loss: 0.0595498 Test Loss: 0.1690634\n",
      "Validation loss decreased (0.060599 --> 0.059550).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1913969\n",
      "\tspeed: 0.0625s/iter; left time: 385.5304s\n",
      "\titers: 200, epoch: 10 | loss: 0.1887120\n",
      "\tspeed: 0.0213s/iter; left time: 129.5771s\n",
      "\titers: 300, epoch: 10 | loss: 0.1581029\n",
      "\tspeed: 0.0214s/iter; left time: 127.5005s\n",
      "\titers: 400, epoch: 10 | loss: 0.3914342\n",
      "\tspeed: 0.0214s/iter; left time: 125.3487s\n",
      "\titers: 500, epoch: 10 | loss: 0.2320722\n",
      "\tspeed: 0.0214s/iter; left time: 123.4251s\n",
      "Epoch: 10 cost time: 12.461288213729858\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2091234 Vali Loss: 0.0599943 Test Loss: 0.1690989\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1834012\n",
      "\tspeed: 0.0598s/iter; left time: 334.8299s\n",
      "\titers: 200, epoch: 11 | loss: 0.1807611\n",
      "\tspeed: 0.0213s/iter; left time: 117.0462s\n",
      "\titers: 300, epoch: 11 | loss: 0.1376544\n",
      "\tspeed: 0.0213s/iter; left time: 115.0198s\n",
      "\titers: 400, epoch: 11 | loss: 0.1764182\n",
      "\tspeed: 0.0213s/iter; left time: 112.8554s\n",
      "\titers: 500, epoch: 11 | loss: 0.1822242\n",
      "\tspeed: 0.0213s/iter; left time: 110.9320s\n",
      "Epoch: 11 cost time: 12.405365943908691\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2081567 Vali Loss: 0.0601298 Test Loss: 0.1693076\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2462718\n",
      "\tspeed: 0.0605s/iter; left time: 304.2019s\n",
      "\titers: 200, epoch: 12 | loss: 0.1744411\n",
      "\tspeed: 0.0228s/iter; left time: 112.5262s\n",
      "\titers: 300, epoch: 12 | loss: 0.3393806\n",
      "\tspeed: 0.0228s/iter; left time: 110.0560s\n",
      "\titers: 400, epoch: 12 | loss: 0.1140367\n",
      "\tspeed: 0.0214s/iter; left time: 101.0166s\n",
      "\titers: 500, epoch: 12 | loss: 0.2412114\n",
      "\tspeed: 0.0214s/iter; left time: 99.0478s\n",
      "Epoch: 12 cost time: 12.719259977340698\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2077105 Vali Loss: 0.0600915 Test Loss: 0.1693314\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16930536925792694, mae:0.26500973105430603\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.5048662\n",
      "\tspeed: 0.0348s/iter; left time: 393.4610s\n",
      "\titers: 200, epoch: 1 | loss: 0.1369040\n",
      "\tspeed: 0.0224s/iter; left time: 250.8901s\n",
      "\titers: 300, epoch: 1 | loss: 0.2355949\n",
      "\tspeed: 0.0224s/iter; left time: 248.6666s\n",
      "\titers: 400, epoch: 1 | loss: 0.2787115\n",
      "\tspeed: 0.0224s/iter; left time: 246.3301s\n",
      "\titers: 500, epoch: 1 | loss: 0.2199036\n",
      "\tspeed: 0.0224s/iter; left time: 243.7099s\n",
      "Epoch: 1 cost time: 14.058667421340942\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3321772 Vali Loss: 0.0685171 Test Loss: 0.1871986\n",
      "Validation loss decreased (inf --> 0.068517).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2593084\n",
      "\tspeed: 0.0616s/iter; left time: 661.5346s\n",
      "\titers: 200, epoch: 2 | loss: 0.1782596\n",
      "\tspeed: 0.0206s/iter; left time: 219.1538s\n",
      "\titers: 300, epoch: 2 | loss: 0.3906453\n",
      "\tspeed: 0.0206s/iter; left time: 217.0573s\n",
      "\titers: 400, epoch: 2 | loss: 0.1749899\n",
      "\tspeed: 0.0206s/iter; left time: 215.2865s\n",
      "\titers: 500, epoch: 2 | loss: 0.2072232\n",
      "\tspeed: 0.0206s/iter; left time: 212.9955s\n",
      "Epoch: 2 cost time: 12.076322317123413\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2578903 Vali Loss: 0.0668498 Test Loss: 0.1912969\n",
      "Validation loss decreased (0.068517 --> 0.066850).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2249382\n",
      "\tspeed: 0.0618s/iter; left time: 628.3880s\n",
      "\titers: 200, epoch: 3 | loss: 0.1607056\n",
      "\tspeed: 0.0207s/iter; left time: 208.3861s\n",
      "\titers: 300, epoch: 3 | loss: 0.1565936\n",
      "\tspeed: 0.0206s/iter; left time: 205.2988s\n",
      "\titers: 400, epoch: 3 | loss: 0.1524085\n",
      "\tspeed: 0.0206s/iter; left time: 202.8948s\n",
      "\titers: 500, epoch: 3 | loss: 0.1550296\n",
      "\tspeed: 0.0206s/iter; left time: 200.8311s\n",
      "Epoch: 3 cost time: 12.060786962509155\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2275447 Vali Loss: 0.0636690 Test Loss: 0.1738327\n",
      "Validation loss decreased (0.066850 --> 0.063669).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1582057\n",
      "\tspeed: 0.0605s/iter; left time: 580.7175s\n",
      "\titers: 200, epoch: 4 | loss: 0.2079531\n",
      "\tspeed: 0.0206s/iter; left time: 195.5467s\n",
      "\titers: 300, epoch: 4 | loss: 0.1579667\n",
      "\tspeed: 0.0206s/iter; left time: 193.3662s\n",
      "\titers: 400, epoch: 4 | loss: 0.2145253\n",
      "\tspeed: 0.0206s/iter; left time: 191.0488s\n",
      "\titers: 500, epoch: 4 | loss: 0.1629684\n",
      "\tspeed: 0.0206s/iter; left time: 189.3363s\n",
      "Epoch: 4 cost time: 12.028275728225708\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1952149 Vali Loss: 0.0597343 Test Loss: 0.1581925\n",
      "Validation loss decreased (0.063669 --> 0.059734).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2804677\n",
      "\tspeed: 0.0597s/iter; left time: 538.8806s\n",
      "\titers: 200, epoch: 5 | loss: 0.1796376\n",
      "\tspeed: 0.0206s/iter; left time: 183.4971s\n",
      "\titers: 300, epoch: 5 | loss: 0.1583485\n",
      "\tspeed: 0.0206s/iter; left time: 181.3023s\n",
      "\titers: 400, epoch: 5 | loss: 0.1763235\n",
      "\tspeed: 0.0206s/iter; left time: 179.2840s\n",
      "\titers: 500, epoch: 5 | loss: 0.1554981\n",
      "\tspeed: 0.0205s/iter; left time: 176.9939s\n",
      "Epoch: 5 cost time: 12.021808385848999\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1784757 Vali Loss: 0.0619457 Test Loss: 0.1574815\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1472059\n",
      "\tspeed: 0.0590s/iter; left time: 498.9621s\n",
      "\titers: 200, epoch: 6 | loss: 0.1663551\n",
      "\tspeed: 0.0205s/iter; left time: 171.3895s\n",
      "\titers: 300, epoch: 6 | loss: 0.1644344\n",
      "\tspeed: 0.0205s/iter; left time: 169.3114s\n",
      "\titers: 400, epoch: 6 | loss: 0.1136185\n",
      "\tspeed: 0.0205s/iter; left time: 167.3118s\n",
      "\titers: 500, epoch: 6 | loss: 0.1272855\n",
      "\tspeed: 0.0205s/iter; left time: 165.3388s\n",
      "Epoch: 6 cost time: 12.013638973236084\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1690723 Vali Loss: 0.0634737 Test Loss: 0.1590462\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1238686\n",
      "\tspeed: 0.0613s/iter; left time: 483.0819s\n",
      "\titers: 200, epoch: 7 | loss: 0.1984115\n",
      "\tspeed: 0.0206s/iter; left time: 160.0883s\n",
      "\titers: 300, epoch: 7 | loss: 0.1817501\n",
      "\tspeed: 0.0206s/iter; left time: 158.0131s\n",
      "\titers: 400, epoch: 7 | loss: 0.1394706\n",
      "\tspeed: 0.0206s/iter; left time: 155.9077s\n",
      "\titers: 500, epoch: 7 | loss: 0.1559870\n",
      "\tspeed: 0.0206s/iter; left time: 153.8108s\n",
      "Epoch: 7 cost time: 12.005765676498413\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1623321 Vali Loss: 0.0616060 Test Loss: 0.1566058\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1584259569644928, mae:0.2586020827293396\n",
      ">>> LR=1e-3,DO=0.0,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3459812\n",
      "\tspeed: 0.0305s/iter; left time: 344.2893s\n",
      "\titers: 200, epoch: 1 | loss: 0.2204933\n",
      "\tspeed: 0.0189s/iter; left time: 211.7845s\n",
      "\titers: 300, epoch: 1 | loss: 0.3470292\n",
      "\tspeed: 0.0189s/iter; left time: 209.8168s\n",
      "\titers: 400, epoch: 1 | loss: 0.3688976\n",
      "\tspeed: 0.0189s/iter; left time: 208.2046s\n",
      "\titers: 500, epoch: 1 | loss: 0.3242333\n",
      "\tspeed: 0.0190s/iter; left time: 206.8872s\n",
      "Epoch: 1 cost time: 12.0018630027771\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3072776 Vali Loss: 0.0635447 Test Loss: 0.1786053\n",
      "Validation loss decreased (inf --> 0.063545).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1897742\n",
      "\tspeed: 0.0589s/iter; left time: 631.5924s\n",
      "\titers: 200, epoch: 2 | loss: 0.5891153\n",
      "\tspeed: 0.0190s/iter; left time: 202.3055s\n",
      "\titers: 300, epoch: 2 | loss: 0.2883355\n",
      "\tspeed: 0.0191s/iter; left time: 201.0397s\n",
      "\titers: 400, epoch: 2 | loss: 0.1507711\n",
      "\tspeed: 0.0190s/iter; left time: 198.5945s\n",
      "\titers: 500, epoch: 2 | loss: 0.1833827\n",
      "\tspeed: 0.0190s/iter; left time: 196.2544s\n",
      "Epoch: 2 cost time: 11.161938667297363\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2681090 Vali Loss: 0.0624694 Test Loss: 0.1786789\n",
      "Validation loss decreased (0.063545 --> 0.062469).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1649375\n",
      "\tspeed: 0.0606s/iter; left time: 615.6755s\n",
      "\titers: 200, epoch: 3 | loss: 0.1788609\n",
      "\tspeed: 0.0192s/iter; left time: 192.8960s\n",
      "\titers: 300, epoch: 3 | loss: 0.1947482\n",
      "\tspeed: 0.0193s/iter; left time: 191.8587s\n",
      "\titers: 400, epoch: 3 | loss: 0.2425351\n",
      "\tspeed: 0.0205s/iter; left time: 201.7787s\n",
      "\titers: 500, epoch: 3 | loss: 0.2817035\n",
      "\tspeed: 0.0209s/iter; left time: 204.0355s\n",
      "Epoch: 3 cost time: 11.716686964035034\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2410358 Vali Loss: 0.0631183 Test Loss: 0.1702424\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1540900\n",
      "\tspeed: 0.0571s/iter; left time: 547.3773s\n",
      "\titers: 200, epoch: 4 | loss: 0.2941101\n",
      "\tspeed: 0.0190s/iter; left time: 179.9596s\n",
      "\titers: 300, epoch: 4 | loss: 0.1636501\n",
      "\tspeed: 0.0190s/iter; left time: 177.9732s\n",
      "\titers: 400, epoch: 4 | loss: 0.2417484\n",
      "\tspeed: 0.0190s/iter; left time: 176.1851s\n",
      "\titers: 500, epoch: 4 | loss: 0.1856163\n",
      "\tspeed: 0.0189s/iter; left time: 174.0945s\n",
      "Epoch: 4 cost time: 11.075124740600586\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2178987 Vali Loss: 0.0632832 Test Loss: 0.1700603\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1762531\n",
      "\tspeed: 0.0558s/iter; left time: 503.4598s\n",
      "\titers: 200, epoch: 5 | loss: 0.3220015\n",
      "\tspeed: 0.0189s/iter; left time: 168.9900s\n",
      "\titers: 300, epoch: 5 | loss: 0.1489344\n",
      "\tspeed: 0.0190s/iter; left time: 167.6524s\n",
      "\titers: 400, epoch: 5 | loss: 0.1708648\n",
      "\tspeed: 0.0190s/iter; left time: 165.3145s\n",
      "\titers: 500, epoch: 5 | loss: 0.1593254\n",
      "\tspeed: 0.0190s/iter; left time: 163.3905s\n",
      "Epoch: 5 cost time: 11.076996564865112\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2111658 Vali Loss: 0.0593761 Test Loss: 0.1635990\n",
      "Validation loss decreased (0.062469 --> 0.059376).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2397203\n",
      "\tspeed: 0.0564s/iter; left time: 476.5202s\n",
      "\titers: 200, epoch: 6 | loss: 0.2457219\n",
      "\tspeed: 0.0189s/iter; left time: 158.2310s\n",
      "\titers: 300, epoch: 6 | loss: 0.1633635\n",
      "\tspeed: 0.0189s/iter; left time: 155.8967s\n",
      "\titers: 400, epoch: 6 | loss: 0.1521265\n",
      "\tspeed: 0.0189s/iter; left time: 154.2069s\n",
      "\titers: 500, epoch: 6 | loss: 0.2515264\n",
      "\tspeed: 0.0195s/iter; left time: 157.2770s\n",
      "Epoch: 6 cost time: 11.294106483459473\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2006975 Vali Loss: 0.0586759 Test Loss: 0.1642092\n",
      "Validation loss decreased (0.059376 --> 0.058676).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2224349\n",
      "\tspeed: 0.0607s/iter; left time: 478.2777s\n",
      "\titers: 200, epoch: 7 | loss: 0.2338411\n",
      "\tspeed: 0.0189s/iter; left time: 147.0338s\n",
      "\titers: 300, epoch: 7 | loss: 0.2719557\n",
      "\tspeed: 0.0189s/iter; left time: 144.9113s\n",
      "\titers: 400, epoch: 7 | loss: 0.3161416\n",
      "\tspeed: 0.0189s/iter; left time: 143.3217s\n",
      "\titers: 500, epoch: 7 | loss: 0.1665621\n",
      "\tspeed: 0.0190s/iter; left time: 141.8032s\n",
      "Epoch: 7 cost time: 11.12330150604248\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1945284 Vali Loss: 0.0596985 Test Loss: 0.1651743\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2725866\n",
      "\tspeed: 0.0559s/iter; left time: 408.9454s\n",
      "\titers: 200, epoch: 8 | loss: 0.2585613\n",
      "\tspeed: 0.0190s/iter; left time: 136.8215s\n",
      "\titers: 300, epoch: 8 | loss: 0.2563990\n",
      "\tspeed: 0.0190s/iter; left time: 135.0441s\n",
      "\titers: 400, epoch: 8 | loss: 0.1603610\n",
      "\tspeed: 0.0190s/iter; left time: 133.2683s\n",
      "\titers: 500, epoch: 8 | loss: 0.2072012\n",
      "\tspeed: 0.0190s/iter; left time: 131.4145s\n",
      "Epoch: 8 cost time: 11.07132077217102\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1912372 Vali Loss: 0.0585950 Test Loss: 0.1643800\n",
      "Validation loss decreased (0.058676 --> 0.058595).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1484015\n",
      "\tspeed: 0.0599s/iter; left time: 403.6408s\n",
      "\titers: 200, epoch: 9 | loss: 0.1572453\n",
      "\tspeed: 0.0189s/iter; left time: 125.8209s\n",
      "\titers: 300, epoch: 9 | loss: 0.2903929\n",
      "\tspeed: 0.0189s/iter; left time: 123.5534s\n",
      "\titers: 400, epoch: 9 | loss: 0.1727037\n",
      "\tspeed: 0.0189s/iter; left time: 121.9342s\n",
      "\titers: 500, epoch: 9 | loss: 0.1098155\n",
      "\tspeed: 0.0189s/iter; left time: 120.0428s\n",
      "Epoch: 9 cost time: 11.056737422943115\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1892703 Vali Loss: 0.0585207 Test Loss: 0.1643788\n",
      "Validation loss decreased (0.058595 --> 0.058521).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2391348\n",
      "\tspeed: 0.0581s/iter; left time: 358.4766s\n",
      "\titers: 200, epoch: 10 | loss: 0.1191047\n",
      "\tspeed: 0.0189s/iter; left time: 114.8620s\n",
      "\titers: 300, epoch: 10 | loss: 0.2457750\n",
      "\tspeed: 0.0189s/iter; left time: 112.8502s\n",
      "\titers: 400, epoch: 10 | loss: 0.2172185\n",
      "\tspeed: 0.0189s/iter; left time: 110.8236s\n",
      "\titers: 500, epoch: 10 | loss: 0.1814567\n",
      "\tspeed: 0.0189s/iter; left time: 108.8034s\n",
      "Epoch: 10 cost time: 11.030076265335083\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1883511 Vali Loss: 0.0584853 Test Loss: 0.1639752\n",
      "Validation loss decreased (0.058521 --> 0.058485).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1710032\n",
      "\tspeed: 0.0572s/iter; left time: 320.1464s\n",
      "\titers: 200, epoch: 11 | loss: 0.1714001\n",
      "\tspeed: 0.0189s/iter; left time: 103.9846s\n",
      "\titers: 300, epoch: 11 | loss: 0.1431176\n",
      "\tspeed: 0.0189s/iter; left time: 102.1294s\n",
      "\titers: 400, epoch: 11 | loss: 0.2365137\n",
      "\tspeed: 0.0189s/iter; left time: 100.0444s\n",
      "\titers: 500, epoch: 11 | loss: 0.1492564\n",
      "\tspeed: 0.0189s/iter; left time: 98.0806s\n",
      "Epoch: 11 cost time: 11.048706769943237\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1880311 Vali Loss: 0.0585991 Test Loss: 0.1641587\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1611707\n",
      "\tspeed: 0.0556s/iter; left time: 279.8559s\n",
      "\titers: 200, epoch: 12 | loss: 0.1814352\n",
      "\tspeed: 0.0189s/iter; left time: 93.2477s\n",
      "\titers: 300, epoch: 12 | loss: 0.3350646\n",
      "\tspeed: 0.0189s/iter; left time: 91.2981s\n",
      "\titers: 400, epoch: 12 | loss: 0.2441531\n",
      "\tspeed: 0.0189s/iter; left time: 89.3126s\n",
      "\titers: 500, epoch: 12 | loss: 0.2772302\n",
      "\tspeed: 0.0189s/iter; left time: 87.4272s\n",
      "Epoch: 12 cost time: 11.025453090667725\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1877626 Vali Loss: 0.0584666 Test Loss: 0.1642219\n",
      "Validation loss decreased (0.058485 --> 0.058467).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1725542\n",
      "\tspeed: 0.0571s/iter; left time: 254.9167s\n",
      "\titers: 200, epoch: 13 | loss: 0.1736616\n",
      "\tspeed: 0.0189s/iter; left time: 82.5101s\n",
      "\titers: 300, epoch: 13 | loss: 0.1994443\n",
      "\tspeed: 0.0189s/iter; left time: 80.6705s\n",
      "\titers: 400, epoch: 13 | loss: 0.1422995\n",
      "\tspeed: 0.0189s/iter; left time: 78.6675s\n",
      "\titers: 500, epoch: 13 | loss: 0.1748704\n",
      "\tspeed: 0.0189s/iter; left time: 76.7632s\n",
      "Epoch: 13 cost time: 11.070146083831787\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1873977 Vali Loss: 0.0586605 Test Loss: 0.1642319\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2812103\n",
      "\tspeed: 0.0560s/iter; left time: 217.9010s\n",
      "\titers: 200, epoch: 14 | loss: 0.2077650\n",
      "\tspeed: 0.0190s/iter; left time: 71.8791s\n",
      "\titers: 300, epoch: 14 | loss: 0.1620535\n",
      "\tspeed: 0.0189s/iter; left time: 69.7771s\n",
      "\titers: 400, epoch: 14 | loss: 0.1186182\n",
      "\tspeed: 0.0188s/iter; left time: 67.6851s\n",
      "\titers: 500, epoch: 14 | loss: 0.1526008\n",
      "\tspeed: 0.0189s/iter; left time: 65.8300s\n",
      "Epoch: 14 cost time: 11.048658847808838\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1874303 Vali Loss: 0.0584769 Test Loss: 0.1642397\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1220352\n",
      "\tspeed: 0.0588s/iter; left time: 195.2897s\n",
      "\titers: 200, epoch: 15 | loss: 0.2429871\n",
      "\tspeed: 0.0203s/iter; left time: 65.3831s\n",
      "\titers: 300, epoch: 15 | loss: 0.1776120\n",
      "\tspeed: 0.0190s/iter; left time: 59.2424s\n",
      "\titers: 400, epoch: 15 | loss: 0.1808693\n",
      "\tspeed: 0.0190s/iter; left time: 57.3458s\n",
      "\titers: 500, epoch: 15 | loss: 0.1404826\n",
      "\tspeed: 0.0190s/iter; left time: 55.4263s\n",
      "Epoch: 15 cost time: 11.469205141067505\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1875341 Vali Loss: 0.0586241 Test Loss: 0.1642427\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16442342102527618, mae:0.2625240385532379\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1777191\n",
      "\tspeed: 0.0238s/iter; left time: 268.9202s\n",
      "\titers: 200, epoch: 1 | loss: 0.1935108\n",
      "\tspeed: 0.0121s/iter; left time: 135.5033s\n",
      "\titers: 300, epoch: 1 | loss: 0.2110804\n",
      "\tspeed: 0.0121s/iter; left time: 134.6341s\n",
      "\titers: 400, epoch: 1 | loss: 0.2278834\n",
      "\tspeed: 0.0110s/iter; left time: 121.5409s\n",
      "\titers: 500, epoch: 1 | loss: 0.2286975\n",
      "\tspeed: 0.0106s/iter; left time: 115.3439s\n",
      "Epoch: 1 cost time: 7.752419710159302\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2393754 Vali Loss: 0.0499301 Test Loss: 0.1551164\n",
      "Validation loss decreased (inf --> 0.049930).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3729154\n",
      "\tspeed: 0.0368s/iter; left time: 394.4019s\n",
      "\titers: 200, epoch: 2 | loss: 0.3245559\n",
      "\tspeed: 0.0105s/iter; left time: 111.3671s\n",
      "\titers: 300, epoch: 2 | loss: 0.2269728\n",
      "\tspeed: 0.0104s/iter; left time: 109.8291s\n",
      "\titers: 400, epoch: 2 | loss: 0.1790629\n",
      "\tspeed: 0.0105s/iter; left time: 109.1040s\n",
      "\titers: 500, epoch: 2 | loss: 0.2005681\n",
      "\tspeed: 0.0105s/iter; left time: 108.0501s\n",
      "Epoch: 2 cost time: 6.268818378448486\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2194005 Vali Loss: 0.0467984 Test Loss: 0.1458801\n",
      "Validation loss decreased (0.049930 --> 0.046798).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1609079\n",
      "\tspeed: 0.0380s/iter; left time: 386.5270s\n",
      "\titers: 200, epoch: 3 | loss: 0.2279218\n",
      "\tspeed: 0.0120s/iter; left time: 120.7985s\n",
      "\titers: 300, epoch: 3 | loss: 0.1517598\n",
      "\tspeed: 0.0120s/iter; left time: 119.9730s\n",
      "\titers: 400, epoch: 3 | loss: 0.1926924\n",
      "\tspeed: 0.0120s/iter; left time: 118.7692s\n",
      "\titers: 500, epoch: 3 | loss: 0.2619040\n",
      "\tspeed: 0.0105s/iter; left time: 102.9504s\n",
      "Epoch: 3 cost time: 6.8958868980407715\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1928285 Vali Loss: 0.0434940 Test Loss: 0.1316906\n",
      "Validation loss decreased (0.046798 --> 0.043494).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1974011\n",
      "\tspeed: 0.0361s/iter; left time: 346.2621s\n",
      "\titers: 200, epoch: 4 | loss: 0.0997751\n",
      "\tspeed: 0.0106s/iter; left time: 100.1460s\n",
      "\titers: 300, epoch: 4 | loss: 0.1813466\n",
      "\tspeed: 0.0106s/iter; left time: 99.9863s\n",
      "\titers: 400, epoch: 4 | loss: 0.1814042\n",
      "\tspeed: 0.0107s/iter; left time: 98.9833s\n",
      "\titers: 500, epoch: 4 | loss: 0.1456451\n",
      "\tspeed: 0.0106s/iter; left time: 97.3676s\n",
      "Epoch: 4 cost time: 6.322227239608765\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1741642 Vali Loss: 0.0419364 Test Loss: 0.1319740\n",
      "Validation loss decreased (0.043494 --> 0.041936).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2005041\n",
      "\tspeed: 0.0374s/iter; left time: 337.3694s\n",
      "\titers: 200, epoch: 5 | loss: 0.1612382\n",
      "\tspeed: 0.0106s/iter; left time: 94.3749s\n",
      "\titers: 300, epoch: 5 | loss: 0.1828484\n",
      "\tspeed: 0.0106s/iter; left time: 93.2346s\n",
      "\titers: 400, epoch: 5 | loss: 0.1539630\n",
      "\tspeed: 0.0106s/iter; left time: 92.3336s\n",
      "\titers: 500, epoch: 5 | loss: 0.1531187\n",
      "\tspeed: 0.0106s/iter; left time: 91.4658s\n",
      "Epoch: 5 cost time: 6.323783874511719\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1631698 Vali Loss: 0.0412901 Test Loss: 0.1334837\n",
      "Validation loss decreased (0.041936 --> 0.041290).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1511397\n",
      "\tspeed: 0.0362s/iter; left time: 305.8715s\n",
      "\titers: 200, epoch: 6 | loss: 0.1123220\n",
      "\tspeed: 0.0105s/iter; left time: 88.0235s\n",
      "\titers: 300, epoch: 6 | loss: 0.2148315\n",
      "\tspeed: 0.0105s/iter; left time: 86.9359s\n",
      "\titers: 400, epoch: 6 | loss: 0.1687021\n",
      "\tspeed: 0.0106s/iter; left time: 86.1881s\n",
      "\titers: 500, epoch: 6 | loss: 0.1469876\n",
      "\tspeed: 0.0106s/iter; left time: 85.1792s\n",
      "Epoch: 6 cost time: 6.304595708847046\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1537006 Vali Loss: 0.0401119 Test Loss: 0.1313482\n",
      "Validation loss decreased (0.041290 --> 0.040112).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1674063\n",
      "\tspeed: 0.0362s/iter; left time: 285.6566s\n",
      "\titers: 200, epoch: 7 | loss: 0.2014802\n",
      "\tspeed: 0.0106s/iter; left time: 82.4307s\n",
      "\titers: 300, epoch: 7 | loss: 0.1516421\n",
      "\tspeed: 0.0106s/iter; left time: 81.3933s\n",
      "\titers: 400, epoch: 7 | loss: 0.1144426\n",
      "\tspeed: 0.0106s/iter; left time: 80.2846s\n",
      "\titers: 500, epoch: 7 | loss: 0.1227431\n",
      "\tspeed: 0.0106s/iter; left time: 79.2461s\n",
      "Epoch: 7 cost time: 6.340215682983398\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1511042 Vali Loss: 0.0403949 Test Loss: 0.1302827\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1535092\n",
      "\tspeed: 0.0360s/iter; left time: 263.0849s\n",
      "\titers: 200, epoch: 8 | loss: 0.1330004\n",
      "\tspeed: 0.0105s/iter; left time: 75.5065s\n",
      "\titers: 300, epoch: 8 | loss: 0.1745556\n",
      "\tspeed: 0.0105s/iter; left time: 74.3918s\n",
      "\titers: 400, epoch: 8 | loss: 0.2076908\n",
      "\tspeed: 0.0105s/iter; left time: 73.3394s\n",
      "\titers: 500, epoch: 8 | loss: 0.1386951\n",
      "\tspeed: 0.0105s/iter; left time: 72.4341s\n",
      "Epoch: 8 cost time: 6.258642673492432\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1473867 Vali Loss: 0.0400983 Test Loss: 0.1308237\n",
      "Validation loss decreased (0.040112 --> 0.040098).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2033126\n",
      "\tspeed: 0.0361s/iter; left time: 243.0848s\n",
      "\titers: 200, epoch: 9 | loss: 0.1391253\n",
      "\tspeed: 0.0105s/iter; left time: 69.8943s\n",
      "\titers: 300, epoch: 9 | loss: 0.0781884\n",
      "\tspeed: 0.0105s/iter; left time: 68.8972s\n",
      "\titers: 400, epoch: 9 | loss: 0.1423060\n",
      "\tspeed: 0.0105s/iter; left time: 67.9119s\n",
      "\titers: 500, epoch: 9 | loss: 0.1406676\n",
      "\tspeed: 0.0105s/iter; left time: 66.5778s\n",
      "Epoch: 9 cost time: 6.297163248062134\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1468509 Vali Loss: 0.0405770 Test Loss: 0.1315916\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1316060\n",
      "\tspeed: 0.0358s/iter; left time: 221.1025s\n",
      "\titers: 200, epoch: 10 | loss: 0.1492279\n",
      "\tspeed: 0.0106s/iter; left time: 64.1196s\n",
      "\titers: 300, epoch: 10 | loss: 0.1004063\n",
      "\tspeed: 0.0106s/iter; left time: 63.1361s\n",
      "\titers: 400, epoch: 10 | loss: 0.1272062\n",
      "\tspeed: 0.0106s/iter; left time: 62.0683s\n",
      "\titers: 500, epoch: 10 | loss: 0.1947585\n",
      "\tspeed: 0.0106s/iter; left time: 61.0463s\n",
      "Epoch: 10 cost time: 6.268905878067017\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1463307 Vali Loss: 0.0401553 Test Loss: 0.1311446\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1390495\n",
      "\tspeed: 0.0360s/iter; left time: 201.4882s\n",
      "\titers: 200, epoch: 11 | loss: 0.1162399\n",
      "\tspeed: 0.0105s/iter; left time: 57.8756s\n",
      "\titers: 300, epoch: 11 | loss: 0.1146599\n",
      "\tspeed: 0.0105s/iter; left time: 56.8185s\n",
      "\titers: 400, epoch: 11 | loss: 0.1900888\n",
      "\tspeed: 0.0106s/iter; left time: 55.9588s\n",
      "\titers: 500, epoch: 11 | loss: 0.1377741\n",
      "\tspeed: 0.0105s/iter; left time: 54.4807s\n",
      "Epoch: 11 cost time: 6.2658257484436035\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1467670 Vali Loss: 0.0401198 Test Loss: 0.1309380\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1310017704963684, mae:0.22413523495197296\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2011288\n",
      "\tspeed: 0.0234s/iter; left time: 264.7549s\n",
      "\titers: 200, epoch: 1 | loss: 0.2121019\n",
      "\tspeed: 0.0116s/iter; left time: 129.9359s\n",
      "\titers: 300, epoch: 1 | loss: 0.2554605\n",
      "\tspeed: 0.0116s/iter; left time: 129.1522s\n",
      "\titers: 400, epoch: 1 | loss: 0.2007774\n",
      "\tspeed: 0.0116s/iter; left time: 127.6245s\n",
      "\titers: 500, epoch: 1 | loss: 0.1596668\n",
      "\tspeed: 0.0116s/iter; left time: 126.9803s\n",
      "Epoch: 1 cost time: 7.8594982624053955\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1986092 Vali Loss: 0.0408514 Test Loss: 0.1363988\n",
      "Validation loss decreased (inf --> 0.040851).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1544909\n",
      "\tspeed: 0.0371s/iter; left time: 397.6135s\n",
      "\titers: 200, epoch: 2 | loss: 0.1977520\n",
      "\tspeed: 0.0103s/iter; left time: 109.6560s\n",
      "\titers: 300, epoch: 2 | loss: 0.1335832\n",
      "\tspeed: 0.0103s/iter; left time: 108.2704s\n",
      "\titers: 400, epoch: 2 | loss: 0.1909222\n",
      "\tspeed: 0.0103s/iter; left time: 107.4477s\n",
      "\titers: 500, epoch: 2 | loss: 0.1895629\n",
      "\tspeed: 0.0103s/iter; left time: 106.2021s\n",
      "Epoch: 2 cost time: 6.185486078262329\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1816486 Vali Loss: 0.0411289 Test Loss: 0.1282441\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.0831251\n",
      "\tspeed: 0.0361s/iter; left time: 366.9576s\n",
      "\titers: 200, epoch: 3 | loss: 0.1447144\n",
      "\tspeed: 0.0103s/iter; left time: 103.5916s\n",
      "\titers: 300, epoch: 3 | loss: 0.3137289\n",
      "\tspeed: 0.0103s/iter; left time: 102.4793s\n",
      "\titers: 400, epoch: 3 | loss: 0.1911944\n",
      "\tspeed: 0.0103s/iter; left time: 101.5514s\n",
      "\titers: 500, epoch: 3 | loss: 0.1429696\n",
      "\tspeed: 0.0103s/iter; left time: 100.7683s\n",
      "Epoch: 3 cost time: 6.148857831954956\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1624195 Vali Loss: 0.0374713 Test Loss: 0.1170176\n",
      "Validation loss decreased (0.040851 --> 0.037471).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1103782\n",
      "\tspeed: 0.0380s/iter; left time: 364.3522s\n",
      "\titers: 200, epoch: 4 | loss: 0.1187697\n",
      "\tspeed: 0.0103s/iter; left time: 97.8902s\n",
      "\titers: 300, epoch: 4 | loss: 0.0884880\n",
      "\tspeed: 0.0103s/iter; left time: 96.6539s\n",
      "\titers: 400, epoch: 4 | loss: 0.0976685\n",
      "\tspeed: 0.0103s/iter; left time: 95.2522s\n",
      "\titers: 500, epoch: 4 | loss: 0.1811225\n",
      "\tspeed: 0.0103s/iter; left time: 94.2544s\n",
      "Epoch: 4 cost time: 6.189178943634033\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1434960 Vali Loss: 0.0341161 Test Loss: 0.1104835\n",
      "Validation loss decreased (0.037471 --> 0.034116).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1103035\n",
      "\tspeed: 0.0359s/iter; left time: 323.9673s\n",
      "\titers: 200, epoch: 5 | loss: 0.1790779\n",
      "\tspeed: 0.0103s/iter; left time: 91.6705s\n",
      "\titers: 300, epoch: 5 | loss: 0.1173813\n",
      "\tspeed: 0.0103s/iter; left time: 90.6466s\n",
      "\titers: 400, epoch: 5 | loss: 0.1384535\n",
      "\tspeed: 0.0103s/iter; left time: 89.5840s\n",
      "\titers: 500, epoch: 5 | loss: 0.1380685\n",
      "\tspeed: 0.0103s/iter; left time: 88.5185s\n",
      "Epoch: 5 cost time: 6.165667533874512\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1332018 Vali Loss: 0.0354310 Test Loss: 0.1137024\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0893482\n",
      "\tspeed: 0.0369s/iter; left time: 311.9228s\n",
      "\titers: 200, epoch: 6 | loss: 0.1794420\n",
      "\tspeed: 0.0103s/iter; left time: 86.1360s\n",
      "\titers: 300, epoch: 6 | loss: 0.1483492\n",
      "\tspeed: 0.0103s/iter; left time: 84.7973s\n",
      "\titers: 400, epoch: 6 | loss: 0.0914391\n",
      "\tspeed: 0.0103s/iter; left time: 83.9036s\n",
      "\titers: 500, epoch: 6 | loss: 0.0890388\n",
      "\tspeed: 0.0103s/iter; left time: 82.7081s\n",
      "Epoch: 6 cost time: 6.1496453285217285\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1277808 Vali Loss: 0.0344818 Test Loss: 0.1114692\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1217993\n",
      "\tspeed: 0.0355s/iter; left time: 279.9894s\n",
      "\titers: 200, epoch: 7 | loss: 0.1507055\n",
      "\tspeed: 0.0103s/iter; left time: 80.1510s\n",
      "\titers: 300, epoch: 7 | loss: 0.2469890\n",
      "\tspeed: 0.0103s/iter; left time: 79.0531s\n",
      "\titers: 400, epoch: 7 | loss: 0.1859451\n",
      "\tspeed: 0.0103s/iter; left time: 77.9231s\n",
      "\titers: 500, epoch: 7 | loss: 0.1438853\n",
      "\tspeed: 0.0103s/iter; left time: 76.9853s\n",
      "Epoch: 7 cost time: 6.158804178237915\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1256190 Vali Loss: 0.0339808 Test Loss: 0.1099915\n",
      "Validation loss decreased (0.034116 --> 0.033981).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1184343\n",
      "\tspeed: 0.0358s/iter; left time: 261.7182s\n",
      "\titers: 200, epoch: 8 | loss: 0.0885658\n",
      "\tspeed: 0.0103s/iter; left time: 74.2666s\n",
      "\titers: 300, epoch: 8 | loss: 0.1112555\n",
      "\tspeed: 0.0103s/iter; left time: 73.0912s\n",
      "\titers: 400, epoch: 8 | loss: 0.1127180\n",
      "\tspeed: 0.0103s/iter; left time: 71.9605s\n",
      "\titers: 500, epoch: 8 | loss: 0.0920499\n",
      "\tspeed: 0.0103s/iter; left time: 70.9784s\n",
      "Epoch: 8 cost time: 6.137487411499023\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1232764 Vali Loss: 0.0343710 Test Loss: 0.1123049\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1152850\n",
      "\tspeed: 0.0364s/iter; left time: 245.5019s\n",
      "\titers: 200, epoch: 9 | loss: 0.1366268\n",
      "\tspeed: 0.0103s/iter; left time: 68.2267s\n",
      "\titers: 300, epoch: 9 | loss: 0.1035083\n",
      "\tspeed: 0.0103s/iter; left time: 67.0681s\n",
      "\titers: 400, epoch: 9 | loss: 0.1578759\n",
      "\tspeed: 0.0102s/iter; left time: 65.9932s\n",
      "\titers: 500, epoch: 9 | loss: 0.1022582\n",
      "\tspeed: 0.0103s/iter; left time: 65.0175s\n",
      "Epoch: 9 cost time: 6.108844041824341\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1214196 Vali Loss: 0.0341249 Test Loss: 0.1114252\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1141844\n",
      "\tspeed: 0.0368s/iter; left time: 227.3996s\n",
      "\titers: 200, epoch: 10 | loss: 0.1014069\n",
      "\tspeed: 0.0104s/iter; left time: 62.9745s\n",
      "\titers: 300, epoch: 10 | loss: 0.1240180\n",
      "\tspeed: 0.0103s/iter; left time: 61.6584s\n",
      "\titers: 400, epoch: 10 | loss: 0.0875974\n",
      "\tspeed: 0.0104s/iter; left time: 60.7895s\n",
      "\titers: 500, epoch: 10 | loss: 0.0761100\n",
      "\tspeed: 0.0103s/iter; left time: 59.7288s\n",
      "Epoch: 10 cost time: 6.301466464996338\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1217547 Vali Loss: 0.0341139 Test Loss: 0.1113909\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11014990508556366, mae:0.20213402807712555\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2760210\n",
      "\tspeed: 0.0218s/iter; left time: 246.7949s\n",
      "\titers: 200, epoch: 1 | loss: 0.1569620\n",
      "\tspeed: 0.0102s/iter; left time: 114.5410s\n",
      "\titers: 300, epoch: 1 | loss: 0.1703339\n",
      "\tspeed: 0.0102s/iter; left time: 113.7514s\n",
      "\titers: 400, epoch: 1 | loss: 0.1834075\n",
      "\tspeed: 0.0102s/iter; left time: 112.4777s\n",
      "\titers: 500, epoch: 1 | loss: 0.2316654\n",
      "\tspeed: 0.0103s/iter; left time: 112.0850s\n",
      "Epoch: 1 cost time: 7.05786395072937\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2487522 Vali Loss: 0.0535518 Test Loss: 0.1598241\n",
      "Validation loss decreased (inf --> 0.053552).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2102094\n",
      "\tspeed: 0.0360s/iter; left time: 386.6367s\n",
      "\titers: 200, epoch: 2 | loss: 0.5037255\n",
      "\tspeed: 0.0103s/iter; left time: 109.4009s\n",
      "\titers: 300, epoch: 2 | loss: 0.3371377\n",
      "\tspeed: 0.0103s/iter; left time: 108.5280s\n",
      "\titers: 400, epoch: 2 | loss: 0.4152882\n",
      "\tspeed: 0.0103s/iter; left time: 107.2727s\n",
      "\titers: 500, epoch: 2 | loss: 0.3067176\n",
      "\tspeed: 0.0103s/iter; left time: 106.6653s\n",
      "Epoch: 2 cost time: 6.172447443008423\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2717112 Vali Loss: 0.0605821 Test Loss: 0.1763985\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1737072\n",
      "\tspeed: 0.0380s/iter; left time: 385.7898s\n",
      "\titers: 200, epoch: 3 | loss: 0.2173738\n",
      "\tspeed: 0.0104s/iter; left time: 104.3124s\n",
      "\titers: 300, epoch: 3 | loss: 0.3133671\n",
      "\tspeed: 0.0103s/iter; left time: 102.8407s\n",
      "\titers: 400, epoch: 3 | loss: 0.2098190\n",
      "\tspeed: 0.0103s/iter; left time: 101.2277s\n",
      "\titers: 500, epoch: 3 | loss: 0.2057551\n",
      "\tspeed: 0.0103s/iter; left time: 100.2669s\n",
      "Epoch: 3 cost time: 6.175764560699463\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2512181 Vali Loss: 0.0534191 Test Loss: 0.1621263\n",
      "Validation loss decreased (0.053552 --> 0.053419).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1413024\n",
      "\tspeed: 0.0377s/iter; left time: 361.4350s\n",
      "\titers: 200, epoch: 4 | loss: 0.1723188\n",
      "\tspeed: 0.0102s/iter; left time: 97.0642s\n",
      "\titers: 300, epoch: 4 | loss: 0.1920038\n",
      "\tspeed: 0.0103s/iter; left time: 96.3669s\n",
      "\titers: 400, epoch: 4 | loss: 0.3015375\n",
      "\tspeed: 0.0103s/iter; left time: 95.4532s\n",
      "\titers: 500, epoch: 4 | loss: 0.1878215\n",
      "\tspeed: 0.0103s/iter; left time: 94.6603s\n",
      "Epoch: 4 cost time: 6.114684104919434\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2239809 Vali Loss: 0.0535721 Test Loss: 0.1619853\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2102711\n",
      "\tspeed: 0.0357s/iter; left time: 321.8175s\n",
      "\titers: 200, epoch: 5 | loss: 0.1812565\n",
      "\tspeed: 0.0103s/iter; left time: 91.6078s\n",
      "\titers: 300, epoch: 5 | loss: 0.2270057\n",
      "\tspeed: 0.0103s/iter; left time: 90.7893s\n",
      "\titers: 400, epoch: 5 | loss: 0.1802817\n",
      "\tspeed: 0.0103s/iter; left time: 89.6809s\n",
      "\titers: 500, epoch: 5 | loss: 0.3264125\n",
      "\tspeed: 0.0103s/iter; left time: 88.4506s\n",
      "Epoch: 5 cost time: 6.129375219345093\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2111830 Vali Loss: 0.0497295 Test Loss: 0.1540494\n",
      "Validation loss decreased (0.053419 --> 0.049729).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1856858\n",
      "\tspeed: 0.0377s/iter; left time: 318.7472s\n",
      "\titers: 200, epoch: 6 | loss: 0.2089431\n",
      "\tspeed: 0.0104s/iter; left time: 86.6383s\n",
      "\titers: 300, epoch: 6 | loss: 0.1692455\n",
      "\tspeed: 0.0104s/iter; left time: 85.6884s\n",
      "\titers: 400, epoch: 6 | loss: 0.1765668\n",
      "\tspeed: 0.0104s/iter; left time: 84.6236s\n",
      "\titers: 500, epoch: 6 | loss: 0.2485469\n",
      "\tspeed: 0.0104s/iter; left time: 83.5188s\n",
      "Epoch: 6 cost time: 6.188523054122925\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2018977 Vali Loss: 0.0494379 Test Loss: 0.1522202\n",
      "Validation loss decreased (0.049729 --> 0.049438).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1340874\n",
      "\tspeed: 0.0378s/iter; left time: 298.2077s\n",
      "\titers: 200, epoch: 7 | loss: 0.1357253\n",
      "\tspeed: 0.0103s/iter; left time: 79.9638s\n",
      "\titers: 300, epoch: 7 | loss: 0.1856140\n",
      "\tspeed: 0.0103s/iter; left time: 79.0075s\n",
      "\titers: 400, epoch: 7 | loss: 0.1292744\n",
      "\tspeed: 0.0103s/iter; left time: 77.8833s\n",
      "\titers: 500, epoch: 7 | loss: 0.2150543\n",
      "\tspeed: 0.0103s/iter; left time: 76.7577s\n",
      "Epoch: 7 cost time: 6.143115758895874\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1983729 Vali Loss: 0.0494550 Test Loss: 0.1508071\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1108174\n",
      "\tspeed: 0.0370s/iter; left time: 270.3019s\n",
      "\titers: 200, epoch: 8 | loss: 0.1593016\n",
      "\tspeed: 0.0103s/iter; left time: 73.9682s\n",
      "\titers: 300, epoch: 8 | loss: 0.1904207\n",
      "\tspeed: 0.0103s/iter; left time: 73.0146s\n",
      "\titers: 400, epoch: 8 | loss: 0.1893917\n",
      "\tspeed: 0.0103s/iter; left time: 72.1365s\n",
      "\titers: 500, epoch: 8 | loss: 0.1773364\n",
      "\tspeed: 0.0102s/iter; left time: 70.8345s\n",
      "Epoch: 8 cost time: 6.121063470840454\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1954682 Vali Loss: 0.0490171 Test Loss: 0.1501433\n",
      "Validation loss decreased (0.049438 --> 0.049017).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1811720\n",
      "\tspeed: 0.0362s/iter; left time: 244.1310s\n",
      "\titers: 200, epoch: 9 | loss: 0.2337634\n",
      "\tspeed: 0.0102s/iter; left time: 68.0644s\n",
      "\titers: 300, epoch: 9 | loss: 0.1826662\n",
      "\tspeed: 0.0102s/iter; left time: 67.0384s\n",
      "\titers: 400, epoch: 9 | loss: 0.1685027\n",
      "\tspeed: 0.0103s/iter; left time: 66.0676s\n",
      "\titers: 500, epoch: 9 | loss: 0.2114587\n",
      "\tspeed: 0.0103s/iter; left time: 65.1635s\n",
      "Epoch: 9 cost time: 6.162268400192261\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1932855 Vali Loss: 0.0489839 Test Loss: 0.1497300\n",
      "Validation loss decreased (0.049017 --> 0.048984).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1468959\n",
      "\tspeed: 0.0360s/iter; left time: 222.2793s\n",
      "\titers: 200, epoch: 10 | loss: 0.2943932\n",
      "\tspeed: 0.0104s/iter; left time: 62.8841s\n",
      "\titers: 300, epoch: 10 | loss: 0.2734065\n",
      "\tspeed: 0.0103s/iter; left time: 61.6084s\n",
      "\titers: 400, epoch: 10 | loss: 0.2248660\n",
      "\tspeed: 0.0103s/iter; left time: 60.4980s\n",
      "\titers: 500, epoch: 10 | loss: 0.2291971\n",
      "\tspeed: 0.0103s/iter; left time: 59.6292s\n",
      "Epoch: 10 cost time: 6.2003562450408936\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1929867 Vali Loss: 0.0490412 Test Loss: 0.1497344\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2016304\n",
      "\tspeed: 0.0355s/iter; left time: 198.9469s\n",
      "\titers: 200, epoch: 11 | loss: 0.1890531\n",
      "\tspeed: 0.0104s/iter; left time: 57.2362s\n",
      "\titers: 300, epoch: 11 | loss: 0.2562253\n",
      "\tspeed: 0.0104s/iter; left time: 56.0980s\n",
      "\titers: 400, epoch: 11 | loss: 0.1936106\n",
      "\tspeed: 0.0104s/iter; left time: 54.9590s\n",
      "\titers: 500, epoch: 11 | loss: 0.1959444\n",
      "\tspeed: 0.0103s/iter; left time: 53.5385s\n",
      "Epoch: 11 cost time: 6.196860074996948\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1928420 Vali Loss: 0.0488997 Test Loss: 0.1497000\n",
      "Validation loss decreased (0.048984 --> 0.048900).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1751735\n",
      "\tspeed: 0.0363s/iter; left time: 182.3906s\n",
      "\titers: 200, epoch: 12 | loss: 0.2051178\n",
      "\tspeed: 0.0103s/iter; left time: 50.6253s\n",
      "\titers: 300, epoch: 12 | loss: 0.1418368\n",
      "\tspeed: 0.0103s/iter; left time: 49.6258s\n",
      "\titers: 400, epoch: 12 | loss: 0.1730569\n",
      "\tspeed: 0.0103s/iter; left time: 48.4953s\n",
      "\titers: 500, epoch: 12 | loss: 0.0879592\n",
      "\tspeed: 0.0103s/iter; left time: 47.4730s\n",
      "Epoch: 12 cost time: 6.13531494140625\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1935263 Vali Loss: 0.0489174 Test Loss: 0.1497241\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1612099\n",
      "\tspeed: 0.0367s/iter; left time: 163.8705s\n",
      "\titers: 200, epoch: 13 | loss: 0.2018778\n",
      "\tspeed: 0.0103s/iter; left time: 44.9823s\n",
      "\titers: 300, epoch: 13 | loss: 0.1750134\n",
      "\tspeed: 0.0103s/iter; left time: 43.9811s\n",
      "\titers: 400, epoch: 13 | loss: 0.2200643\n",
      "\tspeed: 0.0103s/iter; left time: 43.0243s\n",
      "\titers: 500, epoch: 13 | loss: 0.1767964\n",
      "\tspeed: 0.0103s/iter; left time: 41.9909s\n",
      "Epoch: 13 cost time: 6.167438507080078\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1924223 Vali Loss: 0.0490513 Test Loss: 0.1496656\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1728437\n",
      "\tspeed: 0.0365s/iter; left time: 142.1364s\n",
      "\titers: 200, epoch: 14 | loss: 0.2402483\n",
      "\tspeed: 0.0102s/iter; left time: 38.8365s\n",
      "\titers: 300, epoch: 14 | loss: 0.1398079\n",
      "\tspeed: 0.0103s/iter; left time: 37.8496s\n",
      "\titers: 400, epoch: 14 | loss: 0.1153657\n",
      "\tspeed: 0.0103s/iter; left time: 36.8432s\n",
      "\titers: 500, epoch: 14 | loss: 0.2750791\n",
      "\tspeed: 0.0103s/iter; left time: 35.8245s\n",
      "Epoch: 14 cost time: 6.112223863601685\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1909967 Vali Loss: 0.0488073 Test Loss: 0.1496678\n",
      "Validation loss decreased (0.048900 --> 0.048807).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1617509\n",
      "\tspeed: 0.0367s/iter; left time: 121.7193s\n",
      "\titers: 200, epoch: 15 | loss: 0.2353400\n",
      "\tspeed: 0.0103s/iter; left time: 33.2493s\n",
      "\titers: 300, epoch: 15 | loss: 0.1058330\n",
      "\tspeed: 0.0103s/iter; left time: 32.2646s\n",
      "\titers: 400, epoch: 15 | loss: 0.0849429\n",
      "\tspeed: 0.0103s/iter; left time: 31.1705s\n",
      "\titers: 500, epoch: 15 | loss: 0.2182889\n",
      "\tspeed: 0.0103s/iter; left time: 30.1941s\n",
      "Epoch: 15 cost time: 6.192059516906738\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1929903 Vali Loss: 0.0488566 Test Loss: 0.1496770\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1995316\n",
      "\tspeed: 0.0355s/iter; left time: 97.7388s\n",
      "\titers: 200, epoch: 16 | loss: 0.1273779\n",
      "\tspeed: 0.0103s/iter; left time: 27.2695s\n",
      "\titers: 300, epoch: 16 | loss: 0.1738085\n",
      "\tspeed: 0.0103s/iter; left time: 26.1546s\n",
      "\titers: 400, epoch: 16 | loss: 0.1989763\n",
      "\tspeed: 0.0103s/iter; left time: 25.1810s\n",
      "\titers: 500, epoch: 16 | loss: 0.1810515\n",
      "\tspeed: 0.0103s/iter; left time: 24.1620s\n",
      "Epoch: 16 cost time: 6.131420850753784\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1922915 Vali Loss: 0.0491062 Test Loss: 0.1496811\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1572271\n",
      "\tspeed: 0.0375s/iter; left time: 81.7958s\n",
      "\titers: 200, epoch: 17 | loss: 0.1318000\n",
      "\tspeed: 0.0116s/iter; left time: 24.1305s\n",
      "\titers: 300, epoch: 17 | loss: 0.1215536\n",
      "\tspeed: 0.0116s/iter; left time: 22.9077s\n",
      "\titers: 400, epoch: 17 | loss: 0.1946611\n",
      "\tspeed: 0.0116s/iter; left time: 21.8021s\n",
      "\titers: 500, epoch: 17 | loss: 0.1164205\n",
      "\tspeed: 0.0116s/iter; left time: 20.6652s\n",
      "Epoch: 17 cost time: 6.922452449798584\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1921213 Vali Loss: 0.0491891 Test Loss: 0.1496846\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14987124502658844, mae:0.24645471572875977\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3265904\n",
      "\tspeed: 0.0236s/iter; left time: 266.2868s\n",
      "\titers: 200, epoch: 1 | loss: 0.2824314\n",
      "\tspeed: 0.0119s/iter; left time: 133.1106s\n",
      "\titers: 300, epoch: 1 | loss: 0.2027302\n",
      "\tspeed: 0.0119s/iter; left time: 132.0395s\n",
      "\titers: 400, epoch: 1 | loss: 0.1829416\n",
      "\tspeed: 0.0119s/iter; left time: 130.7299s\n",
      "\titers: 500, epoch: 1 | loss: 0.2720419\n",
      "\tspeed: 0.0119s/iter; left time: 129.6923s\n",
      "Epoch: 1 cost time: 7.9935197830200195\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2819023 Vali Loss: 0.0710527 Test Loss: 0.1796554\n",
      "Validation loss decreased (inf --> 0.071053).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2995609\n",
      "\tspeed: 0.0402s/iter; left time: 431.4182s\n",
      "\titers: 200, epoch: 2 | loss: 0.2093853\n",
      "\tspeed: 0.0129s/iter; left time: 137.5814s\n",
      "\titers: 300, epoch: 2 | loss: 0.2064758\n",
      "\tspeed: 0.0129s/iter; left time: 136.1142s\n",
      "\titers: 400, epoch: 2 | loss: 0.1652062\n",
      "\tspeed: 0.0128s/iter; left time: 133.6310s\n",
      "\titers: 500, epoch: 2 | loss: 0.2448857\n",
      "\tspeed: 0.0127s/iter; left time: 131.3982s\n",
      "Epoch: 2 cost time: 7.6550750732421875\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3054748 Vali Loss: 0.0641825 Test Loss: 0.1975380\n",
      "Validation loss decreased (0.071053 --> 0.064182).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3358017\n",
      "\tspeed: 0.0412s/iter; left time: 418.9136s\n",
      "\titers: 200, epoch: 3 | loss: 0.4676081\n",
      "\tspeed: 0.0128s/iter; left time: 128.6543s\n",
      "\titers: 300, epoch: 3 | loss: 0.1594378\n",
      "\tspeed: 0.0128s/iter; left time: 127.4465s\n",
      "\titers: 400, epoch: 3 | loss: 0.2910548\n",
      "\tspeed: 0.0128s/iter; left time: 126.0807s\n",
      "\titers: 500, epoch: 3 | loss: 0.2464300\n",
      "\tspeed: 0.0128s/iter; left time: 124.6235s\n",
      "Epoch: 3 cost time: 7.577352285385132\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2865843 Vali Loss: 0.0679678 Test Loss: 0.1888290\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1879116\n",
      "\tspeed: 0.0404s/iter; left time: 387.1622s\n",
      "\titers: 200, epoch: 4 | loss: 0.3424387\n",
      "\tspeed: 0.0128s/iter; left time: 121.6570s\n",
      "\titers: 300, epoch: 4 | loss: 0.1733771\n",
      "\tspeed: 0.0128s/iter; left time: 120.3757s\n",
      "\titers: 400, epoch: 4 | loss: 0.1620296\n",
      "\tspeed: 0.0128s/iter; left time: 119.0433s\n",
      "\titers: 500, epoch: 4 | loss: 0.2221527\n",
      "\tspeed: 0.0128s/iter; left time: 117.7860s\n",
      "Epoch: 4 cost time: 7.5949506759643555\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2680516 Vali Loss: 0.0610722 Test Loss: 0.1731480\n",
      "Validation loss decreased (0.064182 --> 0.061072).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2432520\n",
      "\tspeed: 0.0424s/iter; left time: 382.4869s\n",
      "\titers: 200, epoch: 5 | loss: 0.2601934\n",
      "\tspeed: 0.0129s/iter; left time: 114.8846s\n",
      "\titers: 300, epoch: 5 | loss: 0.2983795\n",
      "\tspeed: 0.0129s/iter; left time: 113.6109s\n",
      "\titers: 400, epoch: 5 | loss: 0.2202662\n",
      "\tspeed: 0.0128s/iter; left time: 112.0282s\n",
      "\titers: 500, epoch: 5 | loss: 0.2291308\n",
      "\tspeed: 0.0128s/iter; left time: 110.5043s\n",
      "Epoch: 5 cost time: 7.620111703872681\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2547676 Vali Loss: 0.0613823 Test Loss: 0.1707520\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2284081\n",
      "\tspeed: 0.0402s/iter; left time: 340.1192s\n",
      "\titers: 200, epoch: 6 | loss: 0.1313824\n",
      "\tspeed: 0.0118s/iter; left time: 98.7320s\n",
      "\titers: 300, epoch: 6 | loss: 0.2413198\n",
      "\tspeed: 0.0118s/iter; left time: 97.6341s\n",
      "\titers: 400, epoch: 6 | loss: 0.3221701\n",
      "\tspeed: 0.0119s/iter; left time: 96.7003s\n",
      "\titers: 500, epoch: 6 | loss: 0.1760595\n",
      "\tspeed: 0.0119s/iter; left time: 95.7048s\n",
      "Epoch: 6 cost time: 7.090245485305786\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2456368 Vali Loss: 0.0611322 Test Loss: 0.1657845\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2951231\n",
      "\tspeed: 0.0398s/iter; left time: 313.8312s\n",
      "\titers: 200, epoch: 7 | loss: 0.2469725\n",
      "\tspeed: 0.0129s/iter; left time: 100.7156s\n",
      "\titers: 300, epoch: 7 | loss: 0.2372079\n",
      "\tspeed: 0.0130s/iter; left time: 99.7396s\n",
      "\titers: 400, epoch: 7 | loss: 0.1685892\n",
      "\tspeed: 0.0129s/iter; left time: 98.1408s\n",
      "\titers: 500, epoch: 7 | loss: 0.2855080\n",
      "\tspeed: 0.0129s/iter; left time: 96.6784s\n",
      "Epoch: 7 cost time: 7.698976993560791\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2416486 Vali Loss: 0.0592097 Test Loss: 0.1649025\n",
      "Validation loss decreased (0.061072 --> 0.059210).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1336527\n",
      "\tspeed: 0.0401s/iter; left time: 293.4065s\n",
      "\titers: 200, epoch: 8 | loss: 0.2625571\n",
      "\tspeed: 0.0119s/iter; left time: 85.7087s\n",
      "\titers: 300, epoch: 8 | loss: 0.1553656\n",
      "\tspeed: 0.0119s/iter; left time: 84.4816s\n",
      "\titers: 400, epoch: 8 | loss: 0.1543682\n",
      "\tspeed: 0.0119s/iter; left time: 83.2852s\n",
      "\titers: 500, epoch: 8 | loss: 0.2160325\n",
      "\tspeed: 0.0119s/iter; left time: 82.0836s\n",
      "Epoch: 8 cost time: 7.0823445320129395\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2399890 Vali Loss: 0.0591067 Test Loss: 0.1641041\n",
      "Validation loss decreased (0.059210 --> 0.059107).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1801660\n",
      "\tspeed: 0.0405s/iter; left time: 272.8266s\n",
      "\titers: 200, epoch: 9 | loss: 0.1819342\n",
      "\tspeed: 0.0119s/iter; left time: 79.1104s\n",
      "\titers: 300, epoch: 9 | loss: 0.2147153\n",
      "\tspeed: 0.0119s/iter; left time: 77.9025s\n",
      "\titers: 400, epoch: 9 | loss: 0.2052983\n",
      "\tspeed: 0.0119s/iter; left time: 76.7300s\n",
      "\titers: 500, epoch: 9 | loss: 0.2915892\n",
      "\tspeed: 0.0119s/iter; left time: 75.5423s\n",
      "Epoch: 9 cost time: 7.099531173706055\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2384743 Vali Loss: 0.0582299 Test Loss: 0.1634074\n",
      "Validation loss decreased (0.059107 --> 0.058230).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1885498\n",
      "\tspeed: 0.0410s/iter; left time: 253.2495s\n",
      "\titers: 200, epoch: 10 | loss: 0.2248306\n",
      "\tspeed: 0.0129s/iter; left time: 78.3151s\n",
      "\titers: 300, epoch: 10 | loss: 0.1583959\n",
      "\tspeed: 0.0130s/iter; left time: 77.3848s\n",
      "\titers: 400, epoch: 10 | loss: 0.2134813\n",
      "\tspeed: 0.0129s/iter; left time: 75.8564s\n",
      "\titers: 500, epoch: 10 | loss: 0.4080054\n",
      "\tspeed: 0.0129s/iter; left time: 74.4594s\n",
      "Epoch: 10 cost time: 7.674607753753662\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2358646 Vali Loss: 0.0583463 Test Loss: 0.1634760\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1674199\n",
      "\tspeed: 0.0400s/iter; left time: 224.0886s\n",
      "\titers: 200, epoch: 11 | loss: 0.3668125\n",
      "\tspeed: 0.0118s/iter; left time: 65.0907s\n",
      "\titers: 300, epoch: 11 | loss: 0.2715604\n",
      "\tspeed: 0.0119s/iter; left time: 64.0274s\n",
      "\titers: 400, epoch: 11 | loss: 0.2415816\n",
      "\tspeed: 0.0118s/iter; left time: 62.8112s\n",
      "\titers: 500, epoch: 11 | loss: 0.2843077\n",
      "\tspeed: 0.0118s/iter; left time: 61.4950s\n",
      "Epoch: 11 cost time: 7.059962511062622\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2364390 Vali Loss: 0.0585822 Test Loss: 0.1633486\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2286063\n",
      "\tspeed: 0.0388s/iter; left time: 195.4520s\n",
      "\titers: 200, epoch: 12 | loss: 0.1283193\n",
      "\tspeed: 0.0118s/iter; left time: 58.2506s\n",
      "\titers: 300, epoch: 12 | loss: 0.2065744\n",
      "\tspeed: 0.0118s/iter; left time: 57.0910s\n",
      "\titers: 400, epoch: 12 | loss: 0.2635843\n",
      "\tspeed: 0.0118s/iter; left time: 55.8720s\n",
      "\titers: 500, epoch: 12 | loss: 0.2058234\n",
      "\tspeed: 0.0118s/iter; left time: 54.6911s\n",
      "Epoch: 12 cost time: 7.025933504104614\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2372949 Vali Loss: 0.0585811 Test Loss: 0.1633064\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16357727348804474, mae:0.25873905420303345\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1796685\n",
      "\tspeed: 0.0234s/iter; left time: 264.3280s\n",
      "\titers: 200, epoch: 1 | loss: 0.3203726\n",
      "\tspeed: 0.0115s/iter; left time: 129.2047s\n",
      "\titers: 300, epoch: 1 | loss: 0.2118618\n",
      "\tspeed: 0.0115s/iter; left time: 127.6743s\n",
      "\titers: 400, epoch: 1 | loss: 0.2701736\n",
      "\tspeed: 0.0115s/iter; left time: 126.8347s\n",
      "\titers: 500, epoch: 1 | loss: 0.2387635\n",
      "\tspeed: 0.0115s/iter; left time: 125.6170s\n",
      "Epoch: 1 cost time: 7.8071465492248535\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2496155 Vali Loss: 0.0789338 Test Loss: 0.2181921\n",
      "Validation loss decreased (inf --> 0.078934).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2929499\n",
      "\tspeed: 0.0393s/iter; left time: 421.3863s\n",
      "\titers: 200, epoch: 2 | loss: 0.2174159\n",
      "\tspeed: 0.0115s/iter; left time: 121.9142s\n",
      "\titers: 300, epoch: 2 | loss: 0.2474419\n",
      "\tspeed: 0.0115s/iter; left time: 120.9087s\n",
      "\titers: 400, epoch: 2 | loss: 0.3588068\n",
      "\tspeed: 0.0115s/iter; left time: 119.5871s\n",
      "\titers: 500, epoch: 2 | loss: 0.5979308\n",
      "\tspeed: 0.0115s/iter; left time: 118.5506s\n",
      "Epoch: 2 cost time: 6.874336242675781\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3421846 Vali Loss: 0.0665686 Test Loss: 0.1938915\n",
      "Validation loss decreased (0.078934 --> 0.066569).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3956637\n",
      "\tspeed: 0.0389s/iter; left time: 395.1999s\n",
      "\titers: 200, epoch: 3 | loss: 0.1582731\n",
      "\tspeed: 0.0115s/iter; left time: 115.9027s\n",
      "\titers: 300, epoch: 3 | loss: 0.3669603\n",
      "\tspeed: 0.0115s/iter; left time: 114.6235s\n",
      "\titers: 400, epoch: 3 | loss: 0.2793542\n",
      "\tspeed: 0.0115s/iter; left time: 113.5073s\n",
      "\titers: 500, epoch: 3 | loss: 0.3144830\n",
      "\tspeed: 0.0115s/iter; left time: 112.2974s\n",
      "Epoch: 3 cost time: 6.877509117126465\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2952531 Vali Loss: 0.0628071 Test Loss: 0.1831682\n",
      "Validation loss decreased (0.066569 --> 0.062807).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2594198\n",
      "\tspeed: 0.0394s/iter; left time: 377.4492s\n",
      "\titers: 200, epoch: 4 | loss: 0.1618074\n",
      "\tspeed: 0.0115s/iter; left time: 108.8489s\n",
      "\titers: 300, epoch: 4 | loss: 0.1491181\n",
      "\tspeed: 0.0115s/iter; left time: 107.6535s\n",
      "\titers: 400, epoch: 4 | loss: 0.2702274\n",
      "\tspeed: 0.0115s/iter; left time: 106.4256s\n",
      "\titers: 500, epoch: 4 | loss: 0.2366442\n",
      "\tspeed: 0.0114s/iter; left time: 105.1258s\n",
      "Epoch: 4 cost time: 6.862708330154419\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2701998 Vali Loss: 0.0637959 Test Loss: 0.1755745\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2900212\n",
      "\tspeed: 0.0381s/iter; left time: 343.9830s\n",
      "\titers: 200, epoch: 5 | loss: 0.2182794\n",
      "\tspeed: 0.0115s/iter; left time: 102.2402s\n",
      "\titers: 300, epoch: 5 | loss: 0.3421086\n",
      "\tspeed: 0.0115s/iter; left time: 101.0645s\n",
      "\titers: 400, epoch: 5 | loss: 0.1955701\n",
      "\tspeed: 0.0114s/iter; left time: 99.7393s\n",
      "\titers: 500, epoch: 5 | loss: 0.2884355\n",
      "\tspeed: 0.0114s/iter; left time: 98.5806s\n",
      "Epoch: 5 cost time: 6.837055921554565\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2586808 Vali Loss: 0.0617454 Test Loss: 0.1717319\n",
      "Validation loss decreased (0.062807 --> 0.061745).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2905227\n",
      "\tspeed: 0.0405s/iter; left time: 341.8478s\n",
      "\titers: 200, epoch: 6 | loss: 0.2730651\n",
      "\tspeed: 0.0115s/iter; left time: 95.6328s\n",
      "\titers: 300, epoch: 6 | loss: 0.1626481\n",
      "\tspeed: 0.0115s/iter; left time: 94.4938s\n",
      "\titers: 400, epoch: 6 | loss: 0.1758117\n",
      "\tspeed: 0.0115s/iter; left time: 93.3478s\n",
      "\titers: 500, epoch: 6 | loss: 0.2027135\n",
      "\tspeed: 0.0114s/iter; left time: 92.1707s\n",
      "Epoch: 6 cost time: 6.850890636444092\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2503739 Vali Loss: 0.0603163 Test Loss: 0.1696076\n",
      "Validation loss decreased (0.061745 --> 0.060316).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1629372\n",
      "\tspeed: 0.0390s/iter; left time: 307.2557s\n",
      "\titers: 200, epoch: 7 | loss: 0.4093698\n",
      "\tspeed: 0.0115s/iter; left time: 89.2057s\n",
      "\titers: 300, epoch: 7 | loss: 0.2375754\n",
      "\tspeed: 0.0115s/iter; left time: 88.0898s\n",
      "\titers: 400, epoch: 7 | loss: 0.1748221\n",
      "\tspeed: 0.0115s/iter; left time: 86.9479s\n",
      "\titers: 500, epoch: 7 | loss: 0.1773041\n",
      "\tspeed: 0.0115s/iter; left time: 85.7660s\n",
      "Epoch: 7 cost time: 6.871272325515747\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2470633 Vali Loss: 0.0601705 Test Loss: 0.1683172\n",
      "Validation loss decreased (0.060316 --> 0.060171).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1559660\n",
      "\tspeed: 0.0404s/iter; left time: 295.3649s\n",
      "\titers: 200, epoch: 8 | loss: 0.2440483\n",
      "\tspeed: 0.0115s/iter; left time: 83.2524s\n",
      "\titers: 300, epoch: 8 | loss: 0.2035100\n",
      "\tspeed: 0.0115s/iter; left time: 81.9869s\n",
      "\titers: 400, epoch: 8 | loss: 0.2517019\n",
      "\tspeed: 0.0115s/iter; left time: 80.6658s\n",
      "\titers: 500, epoch: 8 | loss: 0.1877975\n",
      "\tspeed: 0.0115s/iter; left time: 79.3569s\n",
      "Epoch: 8 cost time: 6.883461236953735\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2481832 Vali Loss: 0.0595144 Test Loss: 0.1678012\n",
      "Validation loss decreased (0.060171 --> 0.059514).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3043793\n",
      "\tspeed: 0.0399s/iter; left time: 269.1428s\n",
      "\titers: 200, epoch: 9 | loss: 0.1733602\n",
      "\tspeed: 0.0126s/iter; left time: 83.5311s\n",
      "\titers: 300, epoch: 9 | loss: 0.3501782\n",
      "\tspeed: 0.0126s/iter; left time: 82.6483s\n",
      "\titers: 400, epoch: 9 | loss: 0.3207723\n",
      "\tspeed: 0.0126s/iter; left time: 81.0793s\n",
      "\titers: 500, epoch: 9 | loss: 0.3596117\n",
      "\tspeed: 0.0126s/iter; left time: 79.8454s\n",
      "Epoch: 9 cost time: 7.482785224914551\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2453948 Vali Loss: 0.0594102 Test Loss: 0.1675649\n",
      "Validation loss decreased (0.059514 --> 0.059410).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1794396\n",
      "\tspeed: 0.0414s/iter; left time: 255.4380s\n",
      "\titers: 200, epoch: 10 | loss: 0.2279155\n",
      "\tspeed: 0.0126s/iter; left time: 76.4346s\n",
      "\titers: 300, epoch: 10 | loss: 0.1558017\n",
      "\tspeed: 0.0126s/iter; left time: 75.2337s\n",
      "\titers: 400, epoch: 10 | loss: 0.2411603\n",
      "\tspeed: 0.0122s/iter; left time: 71.7407s\n",
      "\titers: 500, epoch: 10 | loss: 0.2668256\n",
      "\tspeed: 0.0115s/iter; left time: 66.2896s\n",
      "Epoch: 10 cost time: 7.2960474491119385\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2453270 Vali Loss: 0.0596609 Test Loss: 0.1673820\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1222677\n",
      "\tspeed: 0.0383s/iter; left time: 214.6352s\n",
      "\titers: 200, epoch: 11 | loss: 0.1957473\n",
      "\tspeed: 0.0115s/iter; left time: 63.1627s\n",
      "\titers: 300, epoch: 11 | loss: 0.2241580\n",
      "\tspeed: 0.0115s/iter; left time: 61.9962s\n",
      "\titers: 400, epoch: 11 | loss: 0.2861233\n",
      "\tspeed: 0.0115s/iter; left time: 60.7988s\n",
      "\titers: 500, epoch: 11 | loss: 0.3761418\n",
      "\tspeed: 0.0115s/iter; left time: 59.7100s\n",
      "Epoch: 11 cost time: 6.849255084991455\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2456568 Vali Loss: 0.0596132 Test Loss: 0.1673657\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2111523\n",
      "\tspeed: 0.0396s/iter; left time: 199.3380s\n",
      "\titers: 200, epoch: 12 | loss: 0.1821611\n",
      "\tspeed: 0.0125s/iter; left time: 61.8137s\n",
      "\titers: 300, epoch: 12 | loss: 0.1647033\n",
      "\tspeed: 0.0116s/iter; left time: 55.8772s\n",
      "\titers: 400, epoch: 12 | loss: 0.3011793\n",
      "\tspeed: 0.0115s/iter; left time: 54.5603s\n",
      "\titers: 500, epoch: 12 | loss: 0.2388800\n",
      "\tspeed: 0.0115s/iter; left time: 53.2559s\n",
      "Epoch: 12 cost time: 7.077351093292236\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2440781 Vali Loss: 0.0597405 Test Loss: 0.1674265\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16781757771968842, mae:0.2611628472805023\n",
      ">>> LR=1e-3,DO=0.1,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3327174\n",
      "\tspeed: 0.0224s/iter; left time: 252.7297s\n",
      "\titers: 200, epoch: 1 | loss: 0.3182364\n",
      "\tspeed: 0.0107s/iter; left time: 120.2609s\n",
      "\titers: 300, epoch: 1 | loss: 0.3426073\n",
      "\tspeed: 0.0107s/iter; left time: 119.3052s\n",
      "\titers: 400, epoch: 1 | loss: 0.3059278\n",
      "\tspeed: 0.0107s/iter; left time: 117.9327s\n",
      "\titers: 500, epoch: 1 | loss: 0.4027936\n",
      "\tspeed: 0.0107s/iter; left time: 116.9564s\n",
      "Epoch: 1 cost time: 7.333016633987427\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3039419 Vali Loss: 0.0807907 Test Loss: 0.2209334\n",
      "Validation loss decreased (inf --> 0.080791).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3536313\n",
      "\tspeed: 0.0380s/iter; left time: 407.5686s\n",
      "\titers: 200, epoch: 2 | loss: 0.3966191\n",
      "\tspeed: 0.0108s/iter; left time: 114.7497s\n",
      "\titers: 300, epoch: 2 | loss: 0.2305571\n",
      "\tspeed: 0.0108s/iter; left time: 113.8881s\n",
      "\titers: 400, epoch: 2 | loss: 0.2279711\n",
      "\tspeed: 0.0108s/iter; left time: 112.7206s\n",
      "\titers: 500, epoch: 2 | loss: 0.2820280\n",
      "\tspeed: 0.0110s/iter; left time: 113.1368s\n",
      "Epoch: 2 cost time: 6.6476891040802\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3421060 Vali Loss: 0.0724195 Test Loss: 0.1958509\n",
      "Validation loss decreased (0.080791 --> 0.072419).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2353123\n",
      "\tspeed: 0.0401s/iter; left time: 407.6811s\n",
      "\titers: 200, epoch: 3 | loss: 0.2257837\n",
      "\tspeed: 0.0108s/iter; left time: 108.8855s\n",
      "\titers: 300, epoch: 3 | loss: 0.2265708\n",
      "\tspeed: 0.0108s/iter; left time: 107.8223s\n",
      "\titers: 400, epoch: 3 | loss: 0.2759785\n",
      "\tspeed: 0.0108s/iter; left time: 106.8726s\n",
      "\titers: 500, epoch: 3 | loss: 0.2378863\n",
      "\tspeed: 0.0108s/iter; left time: 105.6019s\n",
      "Epoch: 3 cost time: 6.477308750152588\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2737085 Vali Loss: 0.0630089 Test Loss: 0.1675890\n",
      "Validation loss decreased (0.072419 --> 0.063009).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2410200\n",
      "\tspeed: 0.0369s/iter; left time: 353.6827s\n",
      "\titers: 200, epoch: 4 | loss: 0.1390311\n",
      "\tspeed: 0.0108s/iter; left time: 102.7994s\n",
      "\titers: 300, epoch: 4 | loss: 0.2606155\n",
      "\tspeed: 0.0108s/iter; left time: 101.5144s\n",
      "\titers: 400, epoch: 4 | loss: 0.4015678\n",
      "\tspeed: 0.0108s/iter; left time: 100.7296s\n",
      "\titers: 500, epoch: 4 | loss: 0.2342789\n",
      "\tspeed: 0.0109s/iter; left time: 99.8402s\n",
      "Epoch: 4 cost time: 6.490089178085327\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2503709 Vali Loss: 0.0647304 Test Loss: 0.1658181\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2422166\n",
      "\tspeed: 0.0369s/iter; left time: 332.5827s\n",
      "\titers: 200, epoch: 5 | loss: 0.1894854\n",
      "\tspeed: 0.0108s/iter; left time: 96.5027s\n",
      "\titers: 300, epoch: 5 | loss: 0.2011713\n",
      "\tspeed: 0.0109s/iter; left time: 95.7158s\n",
      "\titers: 400, epoch: 5 | loss: 0.1729721\n",
      "\tspeed: 0.0109s/iter; left time: 94.6368s\n",
      "\titers: 500, epoch: 5 | loss: 0.3241611\n",
      "\tspeed: 0.0108s/iter; left time: 93.3218s\n",
      "Epoch: 5 cost time: 6.42744779586792\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2379840 Vali Loss: 0.0632330 Test Loss: 0.1627935\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1657743\n",
      "\tspeed: 0.0368s/iter; left time: 311.0912s\n",
      "\titers: 200, epoch: 6 | loss: 0.2104838\n",
      "\tspeed: 0.0120s/iter; left time: 100.6118s\n",
      "\titers: 300, epoch: 6 | loss: 0.2671312\n",
      "\tspeed: 0.0120s/iter; left time: 99.1668s\n",
      "\titers: 400, epoch: 6 | loss: 0.2211233\n",
      "\tspeed: 0.0120s/iter; left time: 97.8038s\n",
      "\titers: 500, epoch: 6 | loss: 0.1637257\n",
      "\tspeed: 0.0120s/iter; left time: 96.5522s\n",
      "Epoch: 6 cost time: 7.116463899612427\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2299352 Vali Loss: 0.0637382 Test Loss: 0.1610361\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16782337427139282, mae:0.265283465385437\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2021424\n",
      "\tspeed: 0.0261s/iter; left time: 295.4831s\n",
      "\titers: 200, epoch: 1 | loss: 0.1925161\n",
      "\tspeed: 0.0149s/iter; left time: 166.4269s\n",
      "\titers: 300, epoch: 1 | loss: 0.3646648\n",
      "\tspeed: 0.0161s/iter; left time: 178.7709s\n",
      "\titers: 400, epoch: 1 | loss: 0.3172990\n",
      "\tspeed: 0.0163s/iter; left time: 178.9028s\n",
      "\titers: 500, epoch: 1 | loss: 0.1780760\n",
      "\tspeed: 0.0162s/iter; left time: 177.1047s\n",
      "Epoch: 1 cost time: 10.13577914237976\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2514787 Vali Loss: 0.0618211 Test Loss: 0.1670834\n",
      "Validation loss decreased (inf --> 0.061821).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3758127\n",
      "\tspeed: 0.0487s/iter; left time: 522.3844s\n",
      "\titers: 200, epoch: 2 | loss: 0.2328117\n",
      "\tspeed: 0.0146s/iter; left time: 155.2524s\n",
      "\titers: 300, epoch: 2 | loss: 0.2166753\n",
      "\tspeed: 0.0146s/iter; left time: 153.7696s\n",
      "\titers: 400, epoch: 2 | loss: 0.1600483\n",
      "\tspeed: 0.0146s/iter; left time: 152.0925s\n",
      "\titers: 500, epoch: 2 | loss: 0.2440310\n",
      "\tspeed: 0.0145s/iter; left time: 149.7966s\n",
      "Epoch: 2 cost time: 8.609283447265625\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2545696 Vali Loss: 0.0566529 Test Loss: 0.1636013\n",
      "Validation loss decreased (0.061821 --> 0.056653).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3808836\n",
      "\tspeed: 0.0473s/iter; left time: 480.1604s\n",
      "\titers: 200, epoch: 3 | loss: 0.2466947\n",
      "\tspeed: 0.0146s/iter; left time: 146.3890s\n",
      "\titers: 300, epoch: 3 | loss: 0.1148460\n",
      "\tspeed: 0.0145s/iter; left time: 144.3908s\n",
      "\titers: 400, epoch: 3 | loss: 0.1709522\n",
      "\tspeed: 0.0145s/iter; left time: 142.8327s\n",
      "\titers: 500, epoch: 3 | loss: 0.1981073\n",
      "\tspeed: 0.0144s/iter; left time: 141.0258s\n",
      "Epoch: 3 cost time: 8.535670042037964\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2202817 Vali Loss: 0.0512793 Test Loss: 0.1501765\n",
      "Validation loss decreased (0.056653 --> 0.051279).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1944005\n",
      "\tspeed: 0.0468s/iter; left time: 448.7181s\n",
      "\titers: 200, epoch: 4 | loss: 0.1566377\n",
      "\tspeed: 0.0146s/iter; left time: 138.3710s\n",
      "\titers: 300, epoch: 4 | loss: 0.2081069\n",
      "\tspeed: 0.0146s/iter; left time: 136.7889s\n",
      "\titers: 400, epoch: 4 | loss: 0.1892206\n",
      "\tspeed: 0.0146s/iter; left time: 135.5481s\n",
      "\titers: 500, epoch: 4 | loss: 0.2124998\n",
      "\tspeed: 0.0146s/iter; left time: 133.9848s\n",
      "Epoch: 4 cost time: 8.614907026290894\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1995000 Vali Loss: 0.0463360 Test Loss: 0.1446078\n",
      "Validation loss decreased (0.051279 --> 0.046336).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1876682\n",
      "\tspeed: 0.0461s/iter; left time: 415.7079s\n",
      "\titers: 200, epoch: 5 | loss: 0.1668259\n",
      "\tspeed: 0.0146s/iter; left time: 130.4522s\n",
      "\titers: 300, epoch: 5 | loss: 0.1381411\n",
      "\tspeed: 0.0146s/iter; left time: 128.4487s\n",
      "\titers: 400, epoch: 5 | loss: 0.2317812\n",
      "\tspeed: 0.0145s/iter; left time: 126.8898s\n",
      "\titers: 500, epoch: 5 | loss: 0.1862955\n",
      "\tspeed: 0.0146s/iter; left time: 125.5394s\n",
      "Epoch: 5 cost time: 8.609616994857788\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1858906 Vali Loss: 0.0453545 Test Loss: 0.1382442\n",
      "Validation loss decreased (0.046336 --> 0.045354).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1635985\n",
      "\tspeed: 0.0456s/iter; left time: 385.3825s\n",
      "\titers: 200, epoch: 6 | loss: 0.1883183\n",
      "\tspeed: 0.0145s/iter; left time: 121.1226s\n",
      "\titers: 300, epoch: 6 | loss: 0.1240506\n",
      "\tspeed: 0.0145s/iter; left time: 119.5246s\n",
      "\titers: 400, epoch: 6 | loss: 0.1124600\n",
      "\tspeed: 0.0144s/iter; left time: 117.5318s\n",
      "\titers: 500, epoch: 6 | loss: 0.1650752\n",
      "\tspeed: 0.0144s/iter; left time: 116.1037s\n",
      "Epoch: 6 cost time: 8.53345251083374\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1777709 Vali Loss: 0.0447077 Test Loss: 0.1378238\n",
      "Validation loss decreased (0.045354 --> 0.044708).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1421961\n",
      "\tspeed: 0.0489s/iter; left time: 385.5535s\n",
      "\titers: 200, epoch: 7 | loss: 0.1592079\n",
      "\tspeed: 0.0163s/iter; left time: 126.6373s\n",
      "\titers: 300, epoch: 7 | loss: 0.1536149\n",
      "\tspeed: 0.0162s/iter; left time: 124.7953s\n",
      "\titers: 400, epoch: 7 | loss: 0.1435184\n",
      "\tspeed: 0.0162s/iter; left time: 123.0241s\n",
      "\titers: 500, epoch: 7 | loss: 0.1989434\n",
      "\tspeed: 0.0162s/iter; left time: 121.1031s\n",
      "Epoch: 7 cost time: 9.523902893066406\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1743078 Vali Loss: 0.0442103 Test Loss: 0.1381981\n",
      "Validation loss decreased (0.044708 --> 0.044210).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2754486\n",
      "\tspeed: 0.0501s/iter; left time: 366.0589s\n",
      "\titers: 200, epoch: 8 | loss: 0.1697963\n",
      "\tspeed: 0.0153s/iter; left time: 110.1116s\n",
      "\titers: 300, epoch: 8 | loss: 0.2003881\n",
      "\tspeed: 0.0144s/iter; left time: 102.2669s\n",
      "\titers: 400, epoch: 8 | loss: 0.1058447\n",
      "\tspeed: 0.0144s/iter; left time: 100.7546s\n",
      "\titers: 500, epoch: 8 | loss: 0.2092163\n",
      "\tspeed: 0.0144s/iter; left time: 99.3471s\n",
      "Epoch: 8 cost time: 8.75621223449707\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1720699 Vali Loss: 0.0444646 Test Loss: 0.1381192\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2775129\n",
      "\tspeed: 0.0453s/iter; left time: 305.2790s\n",
      "\titers: 200, epoch: 9 | loss: 0.1869513\n",
      "\tspeed: 0.0145s/iter; left time: 96.6039s\n",
      "\titers: 300, epoch: 9 | loss: 0.2404243\n",
      "\tspeed: 0.0145s/iter; left time: 95.0619s\n",
      "\titers: 400, epoch: 9 | loss: 0.1877683\n",
      "\tspeed: 0.0145s/iter; left time: 93.1437s\n",
      "\titers: 500, epoch: 9 | loss: 0.1491519\n",
      "\tspeed: 0.0145s/iter; left time: 91.6437s\n",
      "Epoch: 9 cost time: 8.518932342529297\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1709610 Vali Loss: 0.0442922 Test Loss: 0.1373717\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1937200\n",
      "\tspeed: 0.0457s/iter; left time: 281.9314s\n",
      "\titers: 200, epoch: 10 | loss: 0.1355558\n",
      "\tspeed: 0.0145s/iter; left time: 87.8745s\n",
      "\titers: 300, epoch: 10 | loss: 0.1136709\n",
      "\tspeed: 0.0145s/iter; left time: 86.6496s\n",
      "\titers: 400, epoch: 10 | loss: 0.1265586\n",
      "\tspeed: 0.0145s/iter; left time: 85.0258s\n",
      "\titers: 500, epoch: 10 | loss: 0.2174215\n",
      "\tspeed: 0.0145s/iter; left time: 83.7299s\n",
      "Epoch: 10 cost time: 8.561594724655151\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1704257 Vali Loss: 0.0441107 Test Loss: 0.1373031\n",
      "Validation loss decreased (0.044210 --> 0.044111).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1637001\n",
      "\tspeed: 0.0470s/iter; left time: 263.0941s\n",
      "\titers: 200, epoch: 11 | loss: 0.1189625\n",
      "\tspeed: 0.0146s/iter; left time: 80.0991s\n",
      "\titers: 300, epoch: 11 | loss: 0.1546430\n",
      "\tspeed: 0.0146s/iter; left time: 78.7612s\n",
      "\titers: 400, epoch: 11 | loss: 0.1115248\n",
      "\tspeed: 0.0146s/iter; left time: 77.3783s\n",
      "\titers: 500, epoch: 11 | loss: 0.1244606\n",
      "\tspeed: 0.0145s/iter; left time: 75.6639s\n",
      "Epoch: 11 cost time: 8.624287605285645\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1703624 Vali Loss: 0.0442272 Test Loss: 0.1372467\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1294420\n",
      "\tspeed: 0.0461s/iter; left time: 231.8664s\n",
      "\titers: 200, epoch: 12 | loss: 0.1040739\n",
      "\tspeed: 0.0145s/iter; left time: 71.4425s\n",
      "\titers: 300, epoch: 12 | loss: 0.1994811\n",
      "\tspeed: 0.0145s/iter; left time: 70.0134s\n",
      "\titers: 400, epoch: 12 | loss: 0.1641083\n",
      "\tspeed: 0.0145s/iter; left time: 68.4898s\n",
      "\titers: 500, epoch: 12 | loss: 0.1259927\n",
      "\tspeed: 0.0145s/iter; left time: 67.0517s\n",
      "Epoch: 12 cost time: 8.521199226379395\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1692939 Vali Loss: 0.0441475 Test Loss: 0.1371971\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2209660\n",
      "\tspeed: 0.0455s/iter; left time: 203.1341s\n",
      "\titers: 200, epoch: 13 | loss: 0.2390860\n",
      "\tspeed: 0.0144s/iter; left time: 62.8219s\n",
      "\titers: 300, epoch: 13 | loss: 0.2171553\n",
      "\tspeed: 0.0144s/iter; left time: 61.3206s\n",
      "\titers: 400, epoch: 13 | loss: 0.1052573\n",
      "\tspeed: 0.0144s/iter; left time: 59.9723s\n",
      "\titers: 500, epoch: 13 | loss: 0.1743914\n",
      "\tspeed: 0.0144s/iter; left time: 58.4082s\n",
      "Epoch: 13 cost time: 8.455994606018066\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1694824 Vali Loss: 0.0442130 Test Loss: 0.1371884\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13750919699668884, mae:0.2309766709804535\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3579784\n",
      "\tspeed: 0.0277s/iter; left time: 312.8905s\n",
      "\titers: 200, epoch: 1 | loss: 0.2492652\n",
      "\tspeed: 0.0160s/iter; left time: 178.9026s\n",
      "\titers: 300, epoch: 1 | loss: 0.1267075\n",
      "\tspeed: 0.0159s/iter; left time: 176.9214s\n",
      "\titers: 400, epoch: 1 | loss: 0.2885725\n",
      "\tspeed: 0.0160s/iter; left time: 175.6811s\n",
      "\titers: 500, epoch: 1 | loss: 0.1961450\n",
      "\tspeed: 0.0159s/iter; left time: 173.8484s\n",
      "Epoch: 1 cost time: 10.321907997131348\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2143978 Vali Loss: 0.0422654 Test Loss: 0.1404254\n",
      "Validation loss decreased (inf --> 0.042265).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1853428\n",
      "\tspeed: 0.0469s/iter; left time: 502.9530s\n",
      "\titers: 200, epoch: 2 | loss: 0.3092574\n",
      "\tspeed: 0.0142s/iter; left time: 151.3551s\n",
      "\titers: 300, epoch: 2 | loss: 0.2416447\n",
      "\tspeed: 0.0143s/iter; left time: 150.4347s\n",
      "\titers: 400, epoch: 2 | loss: 0.1907769\n",
      "\tspeed: 0.0142s/iter; left time: 148.4494s\n",
      "\titers: 500, epoch: 2 | loss: 0.3004417\n",
      "\tspeed: 0.0143s/iter; left time: 147.3943s\n",
      "Epoch: 2 cost time: 8.407721519470215\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2455699 Vali Loss: 0.0700985 Test Loss: 0.1969756\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3261602\n",
      "\tspeed: 0.0458s/iter; left time: 465.8118s\n",
      "\titers: 200, epoch: 3 | loss: 0.1153121\n",
      "\tspeed: 0.0142s/iter; left time: 143.1272s\n",
      "\titers: 300, epoch: 3 | loss: 0.3054507\n",
      "\tspeed: 0.0142s/iter; left time: 141.7771s\n",
      "\titers: 400, epoch: 3 | loss: 0.1318701\n",
      "\tspeed: 0.0153s/iter; left time: 151.1852s\n",
      "\titers: 500, epoch: 3 | loss: 0.2147380\n",
      "\tspeed: 0.0165s/iter; left time: 160.9711s\n",
      "Epoch: 3 cost time: 8.904938220977783\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2506641 Vali Loss: 0.0531762 Test Loss: 0.1599999\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2274706\n",
      "\tspeed: 0.0483s/iter; left time: 463.2524s\n",
      "\titers: 200, epoch: 4 | loss: 0.1663541\n",
      "\tspeed: 0.0153s/iter; left time: 145.3886s\n",
      "\titers: 300, epoch: 4 | loss: 0.3156635\n",
      "\tspeed: 0.0142s/iter; left time: 133.4678s\n",
      "\titers: 400, epoch: 4 | loss: 0.1417001\n",
      "\tspeed: 0.0142s/iter; left time: 131.6773s\n",
      "\titers: 500, epoch: 4 | loss: 0.1505014\n",
      "\tspeed: 0.0142s/iter; left time: 130.2347s\n",
      "Epoch: 4 cost time: 8.691866397857666\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2101283 Vali Loss: 0.0509230 Test Loss: 0.1516359\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1406233012676239, mae:0.23075206577777863\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1255287\n",
      "\tspeed: 0.0260s/iter; left time: 293.3913s\n",
      "\titers: 200, epoch: 1 | loss: 0.3506041\n",
      "\tspeed: 0.0143s/iter; left time: 160.2603s\n",
      "\titers: 300, epoch: 1 | loss: 0.3245904\n",
      "\tspeed: 0.0143s/iter; left time: 158.6814s\n",
      "\titers: 400, epoch: 1 | loss: 0.2325457\n",
      "\tspeed: 0.0143s/iter; left time: 157.5253s\n",
      "\titers: 500, epoch: 1 | loss: 0.2726045\n",
      "\tspeed: 0.0143s/iter; left time: 156.3285s\n",
      "Epoch: 1 cost time: 9.372231006622314\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2607614 Vali Loss: 0.0578785 Test Loss: 0.1718232\n",
      "Validation loss decreased (inf --> 0.057879).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2770151\n",
      "\tspeed: 0.0506s/iter; left time: 542.6456s\n",
      "\titers: 200, epoch: 2 | loss: 0.2334131\n",
      "\tspeed: 0.0160s/iter; left time: 170.4086s\n",
      "\titers: 300, epoch: 2 | loss: 0.1974951\n",
      "\tspeed: 0.0161s/iter; left time: 169.3040s\n",
      "\titers: 400, epoch: 2 | loss: 0.2227008\n",
      "\tspeed: 0.0161s/iter; left time: 167.4661s\n",
      "\titers: 500, epoch: 2 | loss: 0.3233249\n",
      "\tspeed: 0.0160s/iter; left time: 165.5200s\n",
      "Epoch: 2 cost time: 9.452913761138916\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2962000 Vali Loss: 0.0627234 Test Loss: 0.1698018\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2439906\n",
      "\tspeed: 0.0490s/iter; left time: 497.5363s\n",
      "\titers: 200, epoch: 3 | loss: 0.3127278\n",
      "\tspeed: 0.0161s/iter; left time: 161.7126s\n",
      "\titers: 300, epoch: 3 | loss: 0.2239275\n",
      "\tspeed: 0.0142s/iter; left time: 141.7785s\n",
      "\titers: 400, epoch: 3 | loss: 0.2060677\n",
      "\tspeed: 0.0142s/iter; left time: 139.5722s\n",
      "\titers: 500, epoch: 3 | loss: 0.2406228\n",
      "\tspeed: 0.0142s/iter; left time: 138.1937s\n",
      "Epoch: 3 cost time: 8.742047786712646\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2465655 Vali Loss: 0.0627705 Test Loss: 0.1686419\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3114236\n",
      "\tspeed: 0.0459s/iter; left time: 440.1801s\n",
      "\titers: 200, epoch: 4 | loss: 0.1990649\n",
      "\tspeed: 0.0143s/iter; left time: 135.7442s\n",
      "\titers: 300, epoch: 4 | loss: 0.2273496\n",
      "\tspeed: 0.0143s/iter; left time: 134.3094s\n",
      "\titers: 400, epoch: 4 | loss: 0.2008995\n",
      "\tspeed: 0.0143s/iter; left time: 132.9020s\n",
      "\titers: 500, epoch: 4 | loss: 0.1867630\n",
      "\tspeed: 0.0143s/iter; left time: 131.4626s\n",
      "Epoch: 4 cost time: 8.43689227104187\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2317419 Vali Loss: 0.0565305 Test Loss: 0.1541988\n",
      "Validation loss decreased (0.057879 --> 0.056530).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1827768\n",
      "\tspeed: 0.0476s/iter; left time: 429.6215s\n",
      "\titers: 200, epoch: 5 | loss: 0.2424145\n",
      "\tspeed: 0.0143s/iter; left time: 127.3910s\n",
      "\titers: 300, epoch: 5 | loss: 0.1769633\n",
      "\tspeed: 0.0143s/iter; left time: 126.0091s\n",
      "\titers: 400, epoch: 5 | loss: 0.3183462\n",
      "\tspeed: 0.0142s/iter; left time: 124.2563s\n",
      "\titers: 500, epoch: 5 | loss: 0.1517796\n",
      "\tspeed: 0.0142s/iter; left time: 122.5869s\n",
      "Epoch: 5 cost time: 8.45310926437378\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2206718 Vali Loss: 0.0559052 Test Loss: 0.1531190\n",
      "Validation loss decreased (0.056530 --> 0.055905).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2887405\n",
      "\tspeed: 0.0479s/iter; left time: 404.9665s\n",
      "\titers: 200, epoch: 6 | loss: 0.2290369\n",
      "\tspeed: 0.0161s/iter; left time: 134.1014s\n",
      "\titers: 300, epoch: 6 | loss: 0.1388823\n",
      "\tspeed: 0.0159s/iter; left time: 130.9054s\n",
      "\titers: 400, epoch: 6 | loss: 0.2632432\n",
      "\tspeed: 0.0143s/iter; left time: 116.2822s\n",
      "\titers: 500, epoch: 6 | loss: 0.2012624\n",
      "\tspeed: 0.0143s/iter; left time: 115.2269s\n",
      "Epoch: 6 cost time: 8.947103500366211\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2157504 Vali Loss: 0.0545692 Test Loss: 0.1535233\n",
      "Validation loss decreased (0.055905 --> 0.054569).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2609769\n",
      "\tspeed: 0.0475s/iter; left time: 374.1550s\n",
      "\titers: 200, epoch: 7 | loss: 0.1198593\n",
      "\tspeed: 0.0143s/iter; left time: 111.3879s\n",
      "\titers: 300, epoch: 7 | loss: 0.1548982\n",
      "\tspeed: 0.0143s/iter; left time: 110.1155s\n",
      "\titers: 400, epoch: 7 | loss: 0.2881845\n",
      "\tspeed: 0.0143s/iter; left time: 108.4778s\n",
      "\titers: 500, epoch: 7 | loss: 0.1886747\n",
      "\tspeed: 0.0143s/iter; left time: 107.1238s\n",
      "Epoch: 7 cost time: 8.47185230255127\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2104953 Vali Loss: 0.0552722 Test Loss: 0.1542632\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1520666\n",
      "\tspeed: 0.0468s/iter; left time: 341.9201s\n",
      "\titers: 200, epoch: 8 | loss: 0.1610115\n",
      "\tspeed: 0.0143s/iter; left time: 102.9019s\n",
      "\titers: 300, epoch: 8 | loss: 0.2201933\n",
      "\tspeed: 0.0142s/iter; left time: 100.7326s\n",
      "\titers: 400, epoch: 8 | loss: 0.1989685\n",
      "\tspeed: 0.0141s/iter; left time: 99.1871s\n",
      "\titers: 500, epoch: 8 | loss: 0.1495541\n",
      "\tspeed: 0.0142s/iter; left time: 97.9377s\n",
      "Epoch: 8 cost time: 8.410669326782227\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2098225 Vali Loss: 0.0553594 Test Loss: 0.1540214\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2585093\n",
      "\tspeed: 0.0455s/iter; left time: 306.7893s\n",
      "\titers: 200, epoch: 9 | loss: 0.1992339\n",
      "\tspeed: 0.0142s/iter; left time: 94.4599s\n",
      "\titers: 300, epoch: 9 | loss: 0.1346188\n",
      "\tspeed: 0.0142s/iter; left time: 92.8848s\n",
      "\titers: 400, epoch: 9 | loss: 0.1820253\n",
      "\tspeed: 0.0142s/iter; left time: 91.4743s\n",
      "\titers: 500, epoch: 9 | loss: 0.1771368\n",
      "\tspeed: 0.0142s/iter; left time: 90.0396s\n",
      "Epoch: 9 cost time: 8.393941879272461\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2070904 Vali Loss: 0.0549729 Test Loss: 0.1534572\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15372373163700104, mae:0.25145941972732544\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3330847\n",
      "\tspeed: 0.0290s/iter; left time: 327.7584s\n",
      "\titers: 200, epoch: 1 | loss: 0.4218915\n",
      "\tspeed: 0.0170s/iter; left time: 189.9956s\n",
      "\titers: 300, epoch: 1 | loss: 0.2805955\n",
      "\tspeed: 0.0170s/iter; left time: 188.5472s\n",
      "\titers: 400, epoch: 1 | loss: 0.3035471\n",
      "\tspeed: 0.0169s/iter; left time: 186.3460s\n",
      "\titers: 500, epoch: 1 | loss: 0.2725039\n",
      "\tspeed: 0.0169s/iter; left time: 184.0657s\n",
      "Epoch: 1 cost time: 10.915311574935913\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3454965 Vali Loss: 0.0697863 Test Loss: 0.1908302\n",
      "Validation loss decreased (inf --> 0.069786).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4603559\n",
      "\tspeed: 0.0516s/iter; left time: 553.2431s\n",
      "\titers: 200, epoch: 2 | loss: 0.3383348\n",
      "\tspeed: 0.0181s/iter; left time: 192.6896s\n",
      "\titers: 300, epoch: 2 | loss: 0.2927231\n",
      "\tspeed: 0.0180s/iter; left time: 190.0654s\n",
      "\titers: 400, epoch: 2 | loss: 0.3209269\n",
      "\tspeed: 0.0180s/iter; left time: 187.9980s\n",
      "\titers: 500, epoch: 2 | loss: 0.3092749\n",
      "\tspeed: 0.0180s/iter; left time: 186.2786s\n",
      "Epoch: 2 cost time: 10.530222177505493\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2911564 Vali Loss: 0.0641279 Test Loss: 0.1778177\n",
      "Validation loss decreased (0.069786 --> 0.064128).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2104033\n",
      "\tspeed: 0.0560s/iter; left time: 568.6721s\n",
      "\titers: 200, epoch: 3 | loss: 0.2310549\n",
      "\tspeed: 0.0181s/iter; left time: 182.4344s\n",
      "\titers: 300, epoch: 3 | loss: 0.4158018\n",
      "\tspeed: 0.0181s/iter; left time: 180.3146s\n",
      "\titers: 400, epoch: 3 | loss: 0.2733926\n",
      "\tspeed: 0.0179s/iter; left time: 176.4718s\n",
      "\titers: 500, epoch: 3 | loss: 0.2293164\n",
      "\tspeed: 0.0169s/iter; left time: 165.3396s\n",
      "Epoch: 3 cost time: 10.383117198944092\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2670521 Vali Loss: 0.0600581 Test Loss: 0.1713327\n",
      "Validation loss decreased (0.064128 --> 0.060058).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3341044\n",
      "\tspeed: 0.0538s/iter; left time: 516.4654s\n",
      "\titers: 200, epoch: 4 | loss: 0.2223510\n",
      "\tspeed: 0.0180s/iter; left time: 171.1491s\n",
      "\titers: 300, epoch: 4 | loss: 0.2641203\n",
      "\tspeed: 0.0177s/iter; left time: 166.3319s\n",
      "\titers: 400, epoch: 4 | loss: 0.3035638\n",
      "\tspeed: 0.0184s/iter; left time: 171.0603s\n",
      "\titers: 500, epoch: 4 | loss: 0.2579872\n",
      "\tspeed: 0.0184s/iter; left time: 169.3141s\n",
      "Epoch: 4 cost time: 10.660863637924194\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2549842 Vali Loss: 0.0583454 Test Loss: 0.1644505\n",
      "Validation loss decreased (0.060058 --> 0.058345).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1485283\n",
      "\tspeed: 0.0535s/iter; left time: 482.2782s\n",
      "\titers: 200, epoch: 5 | loss: 0.3071730\n",
      "\tspeed: 0.0184s/iter; left time: 164.2444s\n",
      "\titers: 300, epoch: 5 | loss: 0.2212565\n",
      "\tspeed: 0.0184s/iter; left time: 162.4304s\n",
      "\titers: 400, epoch: 5 | loss: 0.2692057\n",
      "\tspeed: 0.0177s/iter; left time: 154.5860s\n",
      "\titers: 500, epoch: 5 | loss: 0.1740633\n",
      "\tspeed: 0.0184s/iter; left time: 158.6820s\n",
      "Epoch: 5 cost time: 10.679895162582397\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2406700 Vali Loss: 0.0549469 Test Loss: 0.1616737\n",
      "Validation loss decreased (0.058345 --> 0.054947).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3004594\n",
      "\tspeed: 0.0522s/iter; left time: 441.5272s\n",
      "\titers: 200, epoch: 6 | loss: 0.2355074\n",
      "\tspeed: 0.0169s/iter; left time: 141.1865s\n",
      "\titers: 300, epoch: 6 | loss: 0.3626229\n",
      "\tspeed: 0.0169s/iter; left time: 139.7849s\n",
      "\titers: 400, epoch: 6 | loss: 0.2579860\n",
      "\tspeed: 0.0169s/iter; left time: 137.9362s\n",
      "\titers: 500, epoch: 6 | loss: 0.2163518\n",
      "\tspeed: 0.0169s/iter; left time: 136.2478s\n",
      "Epoch: 6 cost time: 9.936220645904541\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2339336 Vali Loss: 0.0543129 Test Loss: 0.1576901\n",
      "Validation loss decreased (0.054947 --> 0.054313).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1825576\n",
      "\tspeed: 0.0507s/iter; left time: 399.2094s\n",
      "\titers: 200, epoch: 7 | loss: 0.2221727\n",
      "\tspeed: 0.0169s/iter; left time: 131.4279s\n",
      "\titers: 300, epoch: 7 | loss: 0.3218852\n",
      "\tspeed: 0.0169s/iter; left time: 129.9132s\n",
      "\titers: 400, epoch: 7 | loss: 0.1682148\n",
      "\tspeed: 0.0170s/iter; left time: 128.5252s\n",
      "\titers: 500, epoch: 7 | loss: 0.1819262\n",
      "\tspeed: 0.0169s/iter; left time: 126.4857s\n",
      "Epoch: 7 cost time: 9.950610160827637\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2284274 Vali Loss: 0.0537960 Test Loss: 0.1573032\n",
      "Validation loss decreased (0.054313 --> 0.053796).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2675963\n",
      "\tspeed: 0.0516s/iter; left time: 376.9889s\n",
      "\titers: 200, epoch: 8 | loss: 0.1502899\n",
      "\tspeed: 0.0170s/iter; left time: 122.3493s\n",
      "\titers: 300, epoch: 8 | loss: 0.2123189\n",
      "\tspeed: 0.0170s/iter; left time: 120.7772s\n",
      "\titers: 400, epoch: 8 | loss: 0.1475389\n",
      "\tspeed: 0.0169s/iter; left time: 118.7521s\n",
      "\titers: 500, epoch: 8 | loss: 0.1974163\n",
      "\tspeed: 0.0169s/iter; left time: 117.0609s\n",
      "Epoch: 8 cost time: 9.964040040969849\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2272331 Vali Loss: 0.0537581 Test Loss: 0.1568641\n",
      "Validation loss decreased (0.053796 --> 0.053758).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2166151\n",
      "\tspeed: 0.0528s/iter; left time: 356.0533s\n",
      "\titers: 200, epoch: 9 | loss: 0.1553343\n",
      "\tspeed: 0.0169s/iter; left time: 112.5538s\n",
      "\titers: 300, epoch: 9 | loss: 0.2586129\n",
      "\tspeed: 0.0169s/iter; left time: 110.4761s\n",
      "\titers: 400, epoch: 9 | loss: 0.2952958\n",
      "\tspeed: 0.0168s/iter; left time: 108.4290s\n",
      "\titers: 500, epoch: 9 | loss: 0.3034004\n",
      "\tspeed: 0.0169s/iter; left time: 106.9183s\n",
      "Epoch: 9 cost time: 9.932249069213867\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2247944 Vali Loss: 0.0537391 Test Loss: 0.1563775\n",
      "Validation loss decreased (0.053758 --> 0.053739).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3168563\n",
      "\tspeed: 0.0525s/iter; left time: 324.0684s\n",
      "\titers: 200, epoch: 10 | loss: 0.0878856\n",
      "\tspeed: 0.0171s/iter; left time: 103.5454s\n",
      "\titers: 300, epoch: 10 | loss: 0.3373968\n",
      "\tspeed: 0.0170s/iter; left time: 101.3596s\n",
      "\titers: 400, epoch: 10 | loss: 0.2689933\n",
      "\tspeed: 0.0170s/iter; left time: 99.6348s\n",
      "\titers: 500, epoch: 10 | loss: 0.2301403\n",
      "\tspeed: 0.0170s/iter; left time: 97.8200s\n",
      "Epoch: 10 cost time: 10.013245582580566\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2245514 Vali Loss: 0.0535284 Test Loss: 0.1562473\n",
      "Validation loss decreased (0.053739 --> 0.053528).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1871051\n",
      "\tspeed: 0.0506s/iter; left time: 283.2794s\n",
      "\titers: 200, epoch: 11 | loss: 0.2793106\n",
      "\tspeed: 0.0169s/iter; left time: 92.9518s\n",
      "\titers: 300, epoch: 11 | loss: 0.1377579\n",
      "\tspeed: 0.0169s/iter; left time: 91.2840s\n",
      "\titers: 400, epoch: 11 | loss: 0.2009258\n",
      "\tspeed: 0.0169s/iter; left time: 89.5297s\n",
      "\titers: 500, epoch: 11 | loss: 0.3015958\n",
      "\tspeed: 0.0169s/iter; left time: 87.8634s\n",
      "Epoch: 11 cost time: 9.950321197509766\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2249263 Vali Loss: 0.0536926 Test Loss: 0.1561432\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2742327\n",
      "\tspeed: 0.0502s/iter; left time: 252.7344s\n",
      "\titers: 200, epoch: 12 | loss: 0.2219787\n",
      "\tspeed: 0.0169s/iter; left time: 83.4300s\n",
      "\titers: 300, epoch: 12 | loss: 0.1666441\n",
      "\tspeed: 0.0170s/iter; left time: 81.9292s\n",
      "\titers: 400, epoch: 12 | loss: 0.2601201\n",
      "\tspeed: 0.0170s/iter; left time: 80.2319s\n",
      "\titers: 500, epoch: 12 | loss: 0.2562214\n",
      "\tspeed: 0.0169s/iter; left time: 78.4406s\n",
      "Epoch: 12 cost time: 9.945672988891602\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2220818 Vali Loss: 0.0534947 Test Loss: 0.1561710\n",
      "Validation loss decreased (0.053528 --> 0.053495).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2680194\n",
      "\tspeed: 0.0513s/iter; left time: 228.9612s\n",
      "\titers: 200, epoch: 13 | loss: 0.2794768\n",
      "\tspeed: 0.0169s/iter; left time: 73.4930s\n",
      "\titers: 300, epoch: 13 | loss: 0.2121294\n",
      "\tspeed: 0.0170s/iter; left time: 72.2807s\n",
      "\titers: 400, epoch: 13 | loss: 0.1974308\n",
      "\tspeed: 0.0170s/iter; left time: 70.6165s\n",
      "\titers: 500, epoch: 13 | loss: 0.1572743\n",
      "\tspeed: 0.0170s/iter; left time: 68.9495s\n",
      "Epoch: 13 cost time: 9.943744897842407\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2236310 Vali Loss: 0.0535584 Test Loss: 0.1561416\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2315268\n",
      "\tspeed: 0.0514s/iter; left time: 199.9147s\n",
      "\titers: 200, epoch: 14 | loss: 0.1732250\n",
      "\tspeed: 0.0169s/iter; left time: 64.0149s\n",
      "\titers: 300, epoch: 14 | loss: 0.3574424\n",
      "\tspeed: 0.0169s/iter; left time: 62.3994s\n",
      "\titers: 400, epoch: 14 | loss: 0.1625539\n",
      "\tspeed: 0.0169s/iter; left time: 60.6901s\n",
      "\titers: 500, epoch: 14 | loss: 0.1681940\n",
      "\tspeed: 0.0169s/iter; left time: 59.0757s\n",
      "Epoch: 14 cost time: 10.011796951293945\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2241748 Vali Loss: 0.0537431 Test Loss: 0.1561242\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2904820\n",
      "\tspeed: 0.0510s/iter; left time: 169.5029s\n",
      "\titers: 200, epoch: 15 | loss: 0.2192653\n",
      "\tspeed: 0.0169s/iter; left time: 54.5439s\n",
      "\titers: 300, epoch: 15 | loss: 0.2122760\n",
      "\tspeed: 0.0170s/iter; left time: 52.9750s\n",
      "\titers: 400, epoch: 15 | loss: 0.2134637\n",
      "\tspeed: 0.0170s/iter; left time: 51.2103s\n",
      "\titers: 500, epoch: 15 | loss: 0.1997483\n",
      "\tspeed: 0.0170s/iter; left time: 49.6584s\n",
      "Epoch: 15 cost time: 9.967105627059937\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2233492 Vali Loss: 0.0536967 Test Loss: 0.1561356\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15627369284629822, mae:0.25418999791145325\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2144737\n",
      "\tspeed: 0.0299s/iter; left time: 337.7738s\n",
      "\titers: 200, epoch: 1 | loss: 0.3402271\n",
      "\tspeed: 0.0179s/iter; left time: 199.9915s\n",
      "\titers: 300, epoch: 1 | loss: 0.4058509\n",
      "\tspeed: 0.0178s/iter; left time: 197.9943s\n",
      "\titers: 400, epoch: 1 | loss: 0.2971203\n",
      "\tspeed: 0.0179s/iter; left time: 196.3783s\n",
      "\titers: 500, epoch: 1 | loss: 0.1734860\n",
      "\tspeed: 0.0179s/iter; left time: 194.6604s\n",
      "Epoch: 1 cost time: 11.436915636062622\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3428784 Vali Loss: 0.0651933 Test Loss: 0.1866074\n",
      "Validation loss decreased (inf --> 0.065193).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2711714\n",
      "\tspeed: 0.0512s/iter; left time: 549.5044s\n",
      "\titers: 200, epoch: 2 | loss: 0.1883248\n",
      "\tspeed: 0.0164s/iter; left time: 174.3023s\n",
      "\titers: 300, epoch: 2 | loss: 0.2948382\n",
      "\tspeed: 0.0164s/iter; left time: 172.7332s\n",
      "\titers: 400, epoch: 2 | loss: 0.3790845\n",
      "\tspeed: 0.0164s/iter; left time: 170.9711s\n",
      "\titers: 500, epoch: 2 | loss: 0.2702579\n",
      "\tspeed: 0.0164s/iter; left time: 169.3399s\n",
      "Epoch: 2 cost time: 9.677000999450684\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2971498 Vali Loss: 0.0673031 Test Loss: 0.1794859\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2691230\n",
      "\tspeed: 0.0486s/iter; left time: 493.9500s\n",
      "\titers: 200, epoch: 3 | loss: 0.2847893\n",
      "\tspeed: 0.0163s/iter; left time: 164.0302s\n",
      "\titers: 300, epoch: 3 | loss: 0.1823582\n",
      "\tspeed: 0.0163s/iter; left time: 162.5988s\n",
      "\titers: 400, epoch: 3 | loss: 0.3311642\n",
      "\tspeed: 0.0163s/iter; left time: 160.9761s\n",
      "\titers: 500, epoch: 3 | loss: 0.1644257\n",
      "\tspeed: 0.0164s/iter; left time: 159.9197s\n",
      "Epoch: 3 cost time: 9.567793607711792\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2645965 Vali Loss: 0.0609325 Test Loss: 0.1718027\n",
      "Validation loss decreased (0.065193 --> 0.060933).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2358768\n",
      "\tspeed: 0.0514s/iter; left time: 493.4164s\n",
      "\titers: 200, epoch: 4 | loss: 0.5337734\n",
      "\tspeed: 0.0163s/iter; left time: 154.9209s\n",
      "\titers: 300, epoch: 4 | loss: 0.2043590\n",
      "\tspeed: 0.0163s/iter; left time: 153.2302s\n",
      "\titers: 400, epoch: 4 | loss: 0.1743312\n",
      "\tspeed: 0.0163s/iter; left time: 151.7143s\n",
      "\titers: 500, epoch: 4 | loss: 0.3348444\n",
      "\tspeed: 0.0163s/iter; left time: 149.9871s\n",
      "Epoch: 4 cost time: 9.623189926147461\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2481150 Vali Loss: 0.0584789 Test Loss: 0.1644980\n",
      "Validation loss decreased (0.060933 --> 0.058479).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2391319\n",
      "\tspeed: 0.0504s/iter; left time: 454.7635s\n",
      "\titers: 200, epoch: 5 | loss: 0.3235899\n",
      "\tspeed: 0.0163s/iter; left time: 145.3319s\n",
      "\titers: 300, epoch: 5 | loss: 0.1979586\n",
      "\tspeed: 0.0163s/iter; left time: 143.6726s\n",
      "\titers: 400, epoch: 5 | loss: 0.1689177\n",
      "\tspeed: 0.0163s/iter; left time: 142.1817s\n",
      "\titers: 500, epoch: 5 | loss: 0.2987221\n",
      "\tspeed: 0.0163s/iter; left time: 140.5519s\n",
      "Epoch: 5 cost time: 9.715510606765747\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2362265 Vali Loss: 0.0601912 Test Loss: 0.1622988\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2104980\n",
      "\tspeed: 0.0515s/iter; left time: 435.4017s\n",
      "\titers: 200, epoch: 6 | loss: 0.3277321\n",
      "\tspeed: 0.0179s/iter; left time: 149.4187s\n",
      "\titers: 300, epoch: 6 | loss: 0.4561567\n",
      "\tspeed: 0.0179s/iter; left time: 147.6687s\n",
      "\titers: 400, epoch: 6 | loss: 0.2088858\n",
      "\tspeed: 0.0179s/iter; left time: 145.9596s\n",
      "\titers: 500, epoch: 6 | loss: 0.2088178\n",
      "\tspeed: 0.0179s/iter; left time: 143.9811s\n",
      "Epoch: 6 cost time: 10.484382629394531\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2292538 Vali Loss: 0.0588901 Test Loss: 0.1615104\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3115033\n",
      "\tspeed: 0.0507s/iter; left time: 399.5167s\n",
      "\titers: 200, epoch: 7 | loss: 0.1426938\n",
      "\tspeed: 0.0164s/iter; left time: 127.4448s\n",
      "\titers: 300, epoch: 7 | loss: 0.2557999\n",
      "\tspeed: 0.0164s/iter; left time: 125.7853s\n",
      "\titers: 400, epoch: 7 | loss: 0.3230401\n",
      "\tspeed: 0.0164s/iter; left time: 124.1696s\n",
      "\titers: 500, epoch: 7 | loss: 0.3296988\n",
      "\tspeed: 0.0164s/iter; left time: 122.5001s\n",
      "Epoch: 7 cost time: 9.61618185043335\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2278809 Vali Loss: 0.0594008 Test Loss: 0.1616243\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16471463441848755, mae:0.2595165967941284\n",
      ">>> LR=1e-3,DO=0.1,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2977015\n",
      "\tspeed: 0.0270s/iter; left time: 304.6271s\n",
      "\titers: 200, epoch: 1 | loss: 0.3025633\n",
      "\tspeed: 0.0154s/iter; left time: 171.9514s\n",
      "\titers: 300, epoch: 1 | loss: 0.2813328\n",
      "\tspeed: 0.0153s/iter; left time: 170.1427s\n",
      "\titers: 400, epoch: 1 | loss: 0.3453762\n",
      "\tspeed: 0.0154s/iter; left time: 168.8853s\n",
      "\titers: 500, epoch: 1 | loss: 0.6158192\n",
      "\tspeed: 0.0153s/iter; left time: 167.1520s\n",
      "Epoch: 1 cost time: 9.965268850326538\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3398425 Vali Loss: 0.0795060 Test Loss: 0.2115769\n",
      "Validation loss decreased (inf --> 0.079506).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4421618\n",
      "\tspeed: 0.0483s/iter; left time: 518.7490s\n",
      "\titers: 200, epoch: 2 | loss: 0.1749569\n",
      "\tspeed: 0.0155s/iter; left time: 164.8145s\n",
      "\titers: 300, epoch: 2 | loss: 0.2967185\n",
      "\tspeed: 0.0155s/iter; left time: 163.0474s\n",
      "\titers: 400, epoch: 2 | loss: 0.2667936\n",
      "\tspeed: 0.0155s/iter; left time: 161.5114s\n",
      "\titers: 500, epoch: 2 | loss: 0.3372166\n",
      "\tspeed: 0.0155s/iter; left time: 159.8994s\n",
      "Epoch: 2 cost time: 9.139402866363525\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2999388 Vali Loss: 0.0653191 Test Loss: 0.1817264\n",
      "Validation loss decreased (0.079506 --> 0.065319).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2608772\n",
      "\tspeed: 0.0483s/iter; left time: 490.5442s\n",
      "\titers: 200, epoch: 3 | loss: 0.1706884\n",
      "\tspeed: 0.0154s/iter; left time: 154.4896s\n",
      "\titers: 300, epoch: 3 | loss: 0.3815733\n",
      "\tspeed: 0.0153s/iter; left time: 152.8722s\n",
      "\titers: 400, epoch: 3 | loss: 0.2664266\n",
      "\tspeed: 0.0154s/iter; left time: 151.4610s\n",
      "\titers: 500, epoch: 3 | loss: 0.2473663\n",
      "\tspeed: 0.0153s/iter; left time: 149.7760s\n",
      "Epoch: 3 cost time: 9.065750122070312\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2685149 Vali Loss: 0.0612395 Test Loss: 0.1667740\n",
      "Validation loss decreased (0.065319 --> 0.061240).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2239064\n",
      "\tspeed: 0.0485s/iter; left time: 464.9356s\n",
      "\titers: 200, epoch: 4 | loss: 0.2824219\n",
      "\tspeed: 0.0154s/iter; left time: 145.9323s\n",
      "\titers: 300, epoch: 4 | loss: 0.3538493\n",
      "\tspeed: 0.0154s/iter; left time: 144.3565s\n",
      "\titers: 400, epoch: 4 | loss: 0.2288669\n",
      "\tspeed: 0.0154s/iter; left time: 142.9081s\n",
      "\titers: 500, epoch: 4 | loss: 0.3610906\n",
      "\tspeed: 0.0154s/iter; left time: 141.3496s\n",
      "Epoch: 4 cost time: 9.073383092880249\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2505508 Vali Loss: 0.0605412 Test Loss: 0.1656972\n",
      "Validation loss decreased (0.061240 --> 0.060541).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2240075\n",
      "\tspeed: 0.0481s/iter; left time: 433.7307s\n",
      "\titers: 200, epoch: 5 | loss: 0.2118904\n",
      "\tspeed: 0.0153s/iter; left time: 136.8497s\n",
      "\titers: 300, epoch: 5 | loss: 0.2205756\n",
      "\tspeed: 0.0153s/iter; left time: 135.0694s\n",
      "\titers: 400, epoch: 5 | loss: 0.2091028\n",
      "\tspeed: 0.0153s/iter; left time: 133.5348s\n",
      "\titers: 500, epoch: 5 | loss: 0.1949403\n",
      "\tspeed: 0.0153s/iter; left time: 132.0068s\n",
      "Epoch: 5 cost time: 9.050579309463501\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2422824 Vali Loss: 0.0583978 Test Loss: 0.1641003\n",
      "Validation loss decreased (0.060541 --> 0.058398).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2339019\n",
      "\tspeed: 0.0485s/iter; left time: 409.5797s\n",
      "\titers: 200, epoch: 6 | loss: 0.1755083\n",
      "\tspeed: 0.0154s/iter; left time: 128.7014s\n",
      "\titers: 300, epoch: 6 | loss: 0.1500492\n",
      "\tspeed: 0.0154s/iter; left time: 127.0835s\n",
      "\titers: 400, epoch: 6 | loss: 0.1486233\n",
      "\tspeed: 0.0154s/iter; left time: 125.6263s\n",
      "\titers: 500, epoch: 6 | loss: 0.2213362\n",
      "\tspeed: 0.0154s/iter; left time: 124.0902s\n",
      "Epoch: 6 cost time: 9.08841323852539\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2372961 Vali Loss: 0.0585612 Test Loss: 0.1626155\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3091866\n",
      "\tspeed: 0.0493s/iter; left time: 388.5770s\n",
      "\titers: 200, epoch: 7 | loss: 0.2188091\n",
      "\tspeed: 0.0154s/iter; left time: 119.5949s\n",
      "\titers: 300, epoch: 7 | loss: 0.1809100\n",
      "\tspeed: 0.0153s/iter; left time: 117.8848s\n",
      "\titers: 400, epoch: 7 | loss: 0.2696238\n",
      "\tspeed: 0.0153s/iter; left time: 116.3001s\n",
      "\titers: 500, epoch: 7 | loss: 0.3083701\n",
      "\tspeed: 0.0154s/iter; left time: 114.9005s\n",
      "Epoch: 7 cost time: 9.029694080352783\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2330361 Vali Loss: 0.0587148 Test Loss: 0.1625889\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2236156\n",
      "\tspeed: 0.0486s/iter; left time: 355.1244s\n",
      "\titers: 200, epoch: 8 | loss: 0.2934226\n",
      "\tspeed: 0.0154s/iter; left time: 110.9197s\n",
      "\titers: 300, epoch: 8 | loss: 0.1606408\n",
      "\tspeed: 0.0154s/iter; left time: 109.2378s\n",
      "\titers: 400, epoch: 8 | loss: 0.2432509\n",
      "\tspeed: 0.0154s/iter; left time: 107.7939s\n",
      "\titers: 500, epoch: 8 | loss: 0.1970395\n",
      "\tspeed: 0.0154s/iter; left time: 106.1521s\n",
      "Epoch: 8 cost time: 9.066486835479736\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2315208 Vali Loss: 0.0578444 Test Loss: 0.1620613\n",
      "Validation loss decreased (0.058398 --> 0.057844).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1934782\n",
      "\tspeed: 0.0480s/iter; left time: 323.8708s\n",
      "\titers: 200, epoch: 9 | loss: 0.2204364\n",
      "\tspeed: 0.0154s/iter; left time: 102.0107s\n",
      "\titers: 300, epoch: 9 | loss: 0.2244284\n",
      "\tspeed: 0.0154s/iter; left time: 100.4870s\n",
      "\titers: 400, epoch: 9 | loss: 0.1628443\n",
      "\tspeed: 0.0154s/iter; left time: 99.3362s\n",
      "\titers: 500, epoch: 9 | loss: 0.2247437\n",
      "\tspeed: 0.0154s/iter; left time: 97.3435s\n",
      "Epoch: 9 cost time: 9.065080165863037\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2303209 Vali Loss: 0.0581975 Test Loss: 0.1621438\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2562991\n",
      "\tspeed: 0.0485s/iter; left time: 299.5120s\n",
      "\titers: 200, epoch: 10 | loss: 0.2500834\n",
      "\tspeed: 0.0154s/iter; left time: 93.3983s\n",
      "\titers: 300, epoch: 10 | loss: 0.2114974\n",
      "\tspeed: 0.0154s/iter; left time: 91.6750s\n",
      "\titers: 400, epoch: 10 | loss: 0.2712291\n",
      "\tspeed: 0.0154s/iter; left time: 90.2081s\n",
      "\titers: 500, epoch: 10 | loss: 0.3080527\n",
      "\tspeed: 0.0154s/iter; left time: 88.6194s\n",
      "Epoch: 10 cost time: 9.031496286392212\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2312544 Vali Loss: 0.0581431 Test Loss: 0.1621729\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2140190\n",
      "\tspeed: 0.0466s/iter; left time: 260.8890s\n",
      "\titers: 200, epoch: 11 | loss: 0.1459406\n",
      "\tspeed: 0.0154s/iter; left time: 84.4553s\n",
      "\titers: 300, epoch: 11 | loss: 0.2325401\n",
      "\tspeed: 0.0153s/iter; left time: 82.8210s\n",
      "\titers: 400, epoch: 11 | loss: 0.2320054\n",
      "\tspeed: 0.0154s/iter; left time: 81.8945s\n",
      "\titers: 500, epoch: 11 | loss: 0.3245713\n",
      "\tspeed: 0.0154s/iter; left time: 79.9384s\n",
      "Epoch: 11 cost time: 9.04107666015625\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2307453 Vali Loss: 0.0582252 Test Loss: 0.1622383\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16231337189674377, mae:0.25703009963035583\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1897985\n",
      "\tspeed: 0.0302s/iter; left time: 341.4175s\n",
      "\titers: 200, epoch: 1 | loss: 0.2887057\n",
      "\tspeed: 0.0187s/iter; left time: 209.5214s\n",
      "\titers: 300, epoch: 1 | loss: 0.4413811\n",
      "\tspeed: 0.0187s/iter; left time: 208.0012s\n",
      "\titers: 400, epoch: 1 | loss: 0.2888007\n",
      "\tspeed: 0.0187s/iter; left time: 206.0313s\n",
      "\titers: 500, epoch: 1 | loss: 0.2090833\n",
      "\tspeed: 0.0188s/iter; left time: 204.4520s\n",
      "Epoch: 1 cost time: 11.873567581176758\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2503787 Vali Loss: 0.0560616 Test Loss: 0.1611836\n",
      "Validation loss decreased (inf --> 0.056062).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3634200\n",
      "\tspeed: 0.0577s/iter; left time: 619.6034s\n",
      "\titers: 200, epoch: 2 | loss: 0.2976031\n",
      "\tspeed: 0.0189s/iter; left time: 200.9422s\n",
      "\titers: 300, epoch: 2 | loss: 0.2022951\n",
      "\tspeed: 0.0188s/iter; left time: 198.5031s\n",
      "\titers: 400, epoch: 2 | loss: 0.2796339\n",
      "\tspeed: 0.0188s/iter; left time: 196.2489s\n",
      "\titers: 500, epoch: 2 | loss: 0.2375820\n",
      "\tspeed: 0.0188s/iter; left time: 193.9311s\n",
      "Epoch: 2 cost time: 11.07320237159729\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2878703 Vali Loss: 0.0673999 Test Loss: 0.1945417\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1964109\n",
      "\tspeed: 0.0589s/iter; left time: 598.1876s\n",
      "\titers: 200, epoch: 3 | loss: 0.2394517\n",
      "\tspeed: 0.0208s/iter; left time: 209.6902s\n",
      "\titers: 300, epoch: 3 | loss: 0.2163593\n",
      "\tspeed: 0.0209s/iter; left time: 207.7165s\n",
      "\titers: 400, epoch: 3 | loss: 0.1630104\n",
      "\tspeed: 0.0208s/iter; left time: 205.5253s\n",
      "\titers: 500, epoch: 3 | loss: 0.2052260\n",
      "\tspeed: 0.0188s/iter; left time: 183.5460s\n",
      "Epoch: 3 cost time: 11.841167688369751\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2572694 Vali Loss: 0.0582203 Test Loss: 0.1707456\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2638965\n",
      "\tspeed: 0.0572s/iter; left time: 548.4662s\n",
      "\titers: 200, epoch: 4 | loss: 0.2769852\n",
      "\tspeed: 0.0187s/iter; left time: 177.6522s\n",
      "\titers: 300, epoch: 4 | loss: 0.2741510\n",
      "\tspeed: 0.0187s/iter; left time: 175.7048s\n",
      "\titers: 400, epoch: 4 | loss: 0.1484838\n",
      "\tspeed: 0.0187s/iter; left time: 173.8169s\n",
      "\titers: 500, epoch: 4 | loss: 0.2438200\n",
      "\tspeed: 0.0187s/iter; left time: 172.0478s\n",
      "Epoch: 4 cost time: 11.008790016174316\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2353540 Vali Loss: 0.0568876 Test Loss: 0.1671759\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16143308579921722, mae:0.261800080537796\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4577753\n",
      "\tspeed: 0.0299s/iter; left time: 337.4111s\n",
      "\titers: 200, epoch: 1 | loss: 0.1889921\n",
      "\tspeed: 0.0183s/iter; left time: 204.6829s\n",
      "\titers: 300, epoch: 1 | loss: 0.1929015\n",
      "\tspeed: 0.0182s/iter; left time: 202.5273s\n",
      "\titers: 400, epoch: 1 | loss: 0.1672915\n",
      "\tspeed: 0.0183s/iter; left time: 200.7760s\n",
      "\titers: 500, epoch: 1 | loss: 0.3148077\n",
      "\tspeed: 0.0183s/iter; left time: 199.0971s\n",
      "Epoch: 1 cost time: 11.61776614189148\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2371157 Vali Loss: 0.0581155 Test Loss: 0.1931640\n",
      "Validation loss decreased (inf --> 0.058115).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1904044\n",
      "\tspeed: 0.0559s/iter; left time: 600.0165s\n",
      "\titers: 200, epoch: 2 | loss: 0.3319464\n",
      "\tspeed: 0.0183s/iter; left time: 195.0237s\n",
      "\titers: 300, epoch: 2 | loss: 0.5493437\n",
      "\tspeed: 0.0183s/iter; left time: 193.1479s\n",
      "\titers: 400, epoch: 2 | loss: 0.2594875\n",
      "\tspeed: 0.0183s/iter; left time: 191.0955s\n",
      "\titers: 500, epoch: 2 | loss: 0.1825258\n",
      "\tspeed: 0.0183s/iter; left time: 189.0684s\n",
      "Epoch: 2 cost time: 10.756470918655396\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2861869 Vali Loss: 0.0610071 Test Loss: 0.1707035\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1784080\n",
      "\tspeed: 0.0571s/iter; left time: 580.5772s\n",
      "\titers: 200, epoch: 3 | loss: 0.1907181\n",
      "\tspeed: 0.0194s/iter; left time: 194.7726s\n",
      "\titers: 300, epoch: 3 | loss: 0.2150978\n",
      "\tspeed: 0.0185s/iter; left time: 184.4459s\n",
      "\titers: 400, epoch: 3 | loss: 0.1987578\n",
      "\tspeed: 0.0183s/iter; left time: 180.8183s\n",
      "\titers: 500, epoch: 3 | loss: 0.1934327\n",
      "\tspeed: 0.0193s/iter; left time: 188.6903s\n",
      "Epoch: 3 cost time: 11.01572036743164\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2373628 Vali Loss: 0.0554212 Test Loss: 0.1654473\n",
      "Validation loss decreased (0.058115 --> 0.055421).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2528630\n",
      "\tspeed: 0.0557s/iter; left time: 534.1706s\n",
      "\titers: 200, epoch: 4 | loss: 0.1235906\n",
      "\tspeed: 0.0183s/iter; left time: 173.2503s\n",
      "\titers: 300, epoch: 4 | loss: 0.2539161\n",
      "\tspeed: 0.0182s/iter; left time: 171.3088s\n",
      "\titers: 400, epoch: 4 | loss: 0.1838807\n",
      "\tspeed: 0.0182s/iter; left time: 169.4970s\n",
      "\titers: 500, epoch: 4 | loss: 0.1295810\n",
      "\tspeed: 0.0183s/iter; left time: 168.1744s\n",
      "Epoch: 4 cost time: 10.707305192947388\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2169620 Vali Loss: 0.0554162 Test Loss: 0.1523077\n",
      "Validation loss decreased (0.055421 --> 0.055416).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1547552\n",
      "\tspeed: 0.0561s/iter; left time: 505.8499s\n",
      "\titers: 200, epoch: 5 | loss: 0.2026396\n",
      "\tspeed: 0.0183s/iter; left time: 163.5621s\n",
      "\titers: 300, epoch: 5 | loss: 0.1919032\n",
      "\tspeed: 0.0183s/iter; left time: 161.5846s\n",
      "\titers: 400, epoch: 5 | loss: 0.2043490\n",
      "\tspeed: 0.0183s/iter; left time: 159.7645s\n",
      "\titers: 500, epoch: 5 | loss: 0.1990732\n",
      "\tspeed: 0.0183s/iter; left time: 157.8234s\n",
      "Epoch: 5 cost time: 10.733757495880127\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2062961 Vali Loss: 0.0577610 Test Loss: 0.1532470\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2716459\n",
      "\tspeed: 0.0565s/iter; left time: 477.6480s\n",
      "\titers: 200, epoch: 6 | loss: 0.1615687\n",
      "\tspeed: 0.0183s/iter; left time: 153.1051s\n",
      "\titers: 300, epoch: 6 | loss: 0.2168526\n",
      "\tspeed: 0.0183s/iter; left time: 151.1631s\n",
      "\titers: 400, epoch: 6 | loss: 0.1593640\n",
      "\tspeed: 0.0183s/iter; left time: 149.3560s\n",
      "\titers: 500, epoch: 6 | loss: 0.1377405\n",
      "\tspeed: 0.0183s/iter; left time: 147.5432s\n",
      "Epoch: 6 cost time: 10.76628828048706\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2025473 Vali Loss: 0.0542774 Test Loss: 0.1501711\n",
      "Validation loss decreased (0.055416 --> 0.054277).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1793858\n",
      "\tspeed: 0.0560s/iter; left time: 441.0344s\n",
      "\titers: 200, epoch: 7 | loss: 0.3043558\n",
      "\tspeed: 0.0182s/iter; left time: 141.4773s\n",
      "\titers: 300, epoch: 7 | loss: 0.1265976\n",
      "\tspeed: 0.0182s/iter; left time: 139.6327s\n",
      "\titers: 400, epoch: 7 | loss: 0.2142242\n",
      "\tspeed: 0.0182s/iter; left time: 137.8376s\n",
      "\titers: 500, epoch: 7 | loss: 0.1831573\n",
      "\tspeed: 0.0182s/iter; left time: 135.9618s\n",
      "Epoch: 7 cost time: 10.675867319107056\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1984276 Vali Loss: 0.0547133 Test Loss: 0.1501346\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1364244\n",
      "\tspeed: 0.0548s/iter; left time: 400.5291s\n",
      "\titers: 200, epoch: 8 | loss: 0.1428776\n",
      "\tspeed: 0.0183s/iter; left time: 132.0111s\n",
      "\titers: 300, epoch: 8 | loss: 0.2285191\n",
      "\tspeed: 0.0183s/iter; left time: 130.0545s\n",
      "\titers: 400, epoch: 8 | loss: 0.2212646\n",
      "\tspeed: 0.0183s/iter; left time: 128.1680s\n",
      "\titers: 500, epoch: 8 | loss: 0.2518257\n",
      "\tspeed: 0.0183s/iter; left time: 126.6403s\n",
      "Epoch: 8 cost time: 10.687090396881104\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1964491 Vali Loss: 0.0545418 Test Loss: 0.1491979\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1176716\n",
      "\tspeed: 0.0566s/iter; left time: 381.6915s\n",
      "\titers: 200, epoch: 9 | loss: 0.2062732\n",
      "\tspeed: 0.0185s/iter; left time: 122.6133s\n",
      "\titers: 300, epoch: 9 | loss: 0.2390584\n",
      "\tspeed: 0.0184s/iter; left time: 120.5081s\n",
      "\titers: 400, epoch: 9 | loss: 0.1789776\n",
      "\tspeed: 0.0183s/iter; left time: 117.7881s\n",
      "\titers: 500, epoch: 9 | loss: 0.1672723\n",
      "\tspeed: 0.0182s/iter; left time: 115.7222s\n",
      "Epoch: 9 cost time: 10.774792671203613\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1971224 Vali Loss: 0.0542637 Test Loss: 0.1488945\n",
      "Validation loss decreased (0.054277 --> 0.054264).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1279312\n",
      "\tspeed: 0.0557s/iter; left time: 343.4339s\n",
      "\titers: 200, epoch: 10 | loss: 0.1533802\n",
      "\tspeed: 0.0183s/iter; left time: 111.1480s\n",
      "\titers: 300, epoch: 10 | loss: 0.2471815\n",
      "\tspeed: 0.0183s/iter; left time: 109.3874s\n",
      "\titers: 400, epoch: 10 | loss: 0.2247956\n",
      "\tspeed: 0.0183s/iter; left time: 107.5294s\n",
      "\titers: 500, epoch: 10 | loss: 0.2233932\n",
      "\tspeed: 0.0183s/iter; left time: 105.6984s\n",
      "Epoch: 10 cost time: 10.74770975112915\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1957112 Vali Loss: 0.0542442 Test Loss: 0.1490749\n",
      "Validation loss decreased (0.054264 --> 0.054244).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2585023\n",
      "\tspeed: 0.0584s/iter; left time: 326.9067s\n",
      "\titers: 200, epoch: 11 | loss: 0.2559385\n",
      "\tspeed: 0.0205s/iter; left time: 112.6402s\n",
      "\titers: 300, epoch: 11 | loss: 0.1487543\n",
      "\tspeed: 0.0205s/iter; left time: 110.5736s\n",
      "\titers: 400, epoch: 11 | loss: 0.2045055\n",
      "\tspeed: 0.0205s/iter; left time: 108.5089s\n",
      "\titers: 500, epoch: 11 | loss: 0.1912563\n",
      "\tspeed: 0.0205s/iter; left time: 106.3737s\n",
      "Epoch: 11 cost time: 11.973850727081299\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1963690 Vali Loss: 0.0544495 Test Loss: 0.1492017\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2479386\n",
      "\tspeed: 0.0561s/iter; left time: 282.3239s\n",
      "\titers: 200, epoch: 12 | loss: 0.2418561\n",
      "\tspeed: 0.0182s/iter; left time: 89.9427s\n",
      "\titers: 300, epoch: 12 | loss: 0.1374248\n",
      "\tspeed: 0.0182s/iter; left time: 88.1194s\n",
      "\titers: 400, epoch: 12 | loss: 0.1622343\n",
      "\tspeed: 0.0183s/iter; left time: 86.3600s\n",
      "\titers: 500, epoch: 12 | loss: 0.1916432\n",
      "\tspeed: 0.0183s/iter; left time: 84.5197s\n",
      "Epoch: 12 cost time: 10.687935829162598\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1951744 Vali Loss: 0.0542789 Test Loss: 0.1491429\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1677894\n",
      "\tspeed: 0.0556s/iter; left time: 248.0950s\n",
      "\titers: 200, epoch: 13 | loss: 0.2673208\n",
      "\tspeed: 0.0183s/iter; left time: 79.9255s\n",
      "\titers: 300, epoch: 13 | loss: 0.1751967\n",
      "\tspeed: 0.0182s/iter; left time: 77.4333s\n",
      "\titers: 400, epoch: 13 | loss: 0.2055026\n",
      "\tspeed: 0.0182s/iter; left time: 75.8584s\n",
      "\titers: 500, epoch: 13 | loss: 0.2717972\n",
      "\tspeed: 0.0182s/iter; left time: 73.9348s\n",
      "Epoch: 13 cost time: 10.71845030784607\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1961132 Vali Loss: 0.0543760 Test Loss: 0.1491305\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14925047755241394, mae:0.24429486691951752\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3445646\n",
      "\tspeed: 0.0325s/iter; left time: 367.5071s\n",
      "\titers: 200, epoch: 1 | loss: 0.1877764\n",
      "\tspeed: 0.0209s/iter; left time: 234.3439s\n",
      "\titers: 300, epoch: 1 | loss: 0.3729849\n",
      "\tspeed: 0.0209s/iter; left time: 232.4118s\n",
      "\titers: 400, epoch: 1 | loss: 0.3171844\n",
      "\tspeed: 0.0210s/iter; left time: 230.5459s\n",
      "\titers: 500, epoch: 1 | loss: 0.3730814\n",
      "\tspeed: 0.0209s/iter; left time: 227.8800s\n",
      "Epoch: 1 cost time: 13.147491216659546\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2813609 Vali Loss: 0.0633417 Test Loss: 0.1786923\n",
      "Validation loss decreased (inf --> 0.063342).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3185824\n",
      "\tspeed: 0.0601s/iter; left time: 644.9160s\n",
      "\titers: 200, epoch: 2 | loss: 0.3732510\n",
      "\tspeed: 0.0210s/iter; left time: 223.1100s\n",
      "\titers: 300, epoch: 2 | loss: 0.4587632\n",
      "\tspeed: 0.0210s/iter; left time: 220.7202s\n",
      "\titers: 400, epoch: 2 | loss: 0.2056504\n",
      "\tspeed: 0.0210s/iter; left time: 218.5979s\n",
      "\titers: 500, epoch: 2 | loss: 0.1887688\n",
      "\tspeed: 0.0210s/iter; left time: 216.5527s\n",
      "Epoch: 2 cost time: 12.259498596191406\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2697050 Vali Loss: 0.0622225 Test Loss: 0.1683851\n",
      "Validation loss decreased (0.063342 --> 0.062223).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1565359\n",
      "\tspeed: 0.0603s/iter; left time: 613.0545s\n",
      "\titers: 200, epoch: 3 | loss: 0.2706599\n",
      "\tspeed: 0.0209s/iter; left time: 210.7677s\n",
      "\titers: 300, epoch: 3 | loss: 0.2815258\n",
      "\tspeed: 0.0209s/iter; left time: 208.2688s\n",
      "\titers: 400, epoch: 3 | loss: 0.3146517\n",
      "\tspeed: 0.0209s/iter; left time: 205.9058s\n",
      "\titers: 500, epoch: 3 | loss: 0.2551386\n",
      "\tspeed: 0.0209s/iter; left time: 204.2763s\n",
      "Epoch: 3 cost time: 12.19207763671875\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2377717 Vali Loss: 0.0598904 Test Loss: 0.1628170\n",
      "Validation loss decreased (0.062223 --> 0.059890).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2135967\n",
      "\tspeed: 0.0599s/iter; left time: 574.4865s\n",
      "\titers: 200, epoch: 4 | loss: 0.2030511\n",
      "\tspeed: 0.0210s/iter; left time: 199.1187s\n",
      "\titers: 300, epoch: 4 | loss: 0.1808579\n",
      "\tspeed: 0.0209s/iter; left time: 196.6963s\n",
      "\titers: 400, epoch: 4 | loss: 0.2235925\n",
      "\tspeed: 0.0210s/iter; left time: 194.6805s\n",
      "\titers: 500, epoch: 4 | loss: 0.1414864\n",
      "\tspeed: 0.0209s/iter; left time: 192.5174s\n",
      "Epoch: 4 cost time: 12.257015943527222\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2207926 Vali Loss: 0.0544014 Test Loss: 0.1543365\n",
      "Validation loss decreased (0.059890 --> 0.054401).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1340903\n",
      "\tspeed: 0.0597s/iter; left time: 538.8949s\n",
      "\titers: 200, epoch: 5 | loss: 0.2077597\n",
      "\tspeed: 0.0209s/iter; left time: 186.7808s\n",
      "\titers: 300, epoch: 5 | loss: 0.1357080\n",
      "\tspeed: 0.0209s/iter; left time: 184.6088s\n",
      "\titers: 400, epoch: 5 | loss: 0.1882963\n",
      "\tspeed: 0.0209s/iter; left time: 182.4303s\n",
      "\titers: 500, epoch: 5 | loss: 0.1163111\n",
      "\tspeed: 0.0209s/iter; left time: 180.4169s\n",
      "Epoch: 5 cost time: 12.211029052734375\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2083121 Vali Loss: 0.0520057 Test Loss: 0.1467012\n",
      "Validation loss decreased (0.054401 --> 0.052006).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2263478\n",
      "\tspeed: 0.0632s/iter; left time: 533.9268s\n",
      "\titers: 200, epoch: 6 | loss: 0.1294562\n",
      "\tspeed: 0.0185s/iter; left time: 154.8164s\n",
      "\titers: 300, epoch: 6 | loss: 0.4127450\n",
      "\tspeed: 0.0182s/iter; left time: 150.3984s\n",
      "\titers: 400, epoch: 6 | loss: 0.1523789\n",
      "\tspeed: 0.0182s/iter; left time: 148.3754s\n",
      "\titers: 500, epoch: 6 | loss: 0.2137214\n",
      "\tspeed: 0.0182s/iter; left time: 146.5145s\n",
      "Epoch: 6 cost time: 10.937878131866455\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1999501 Vali Loss: 0.0507813 Test Loss: 0.1440697\n",
      "Validation loss decreased (0.052006 --> 0.050781).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1522244\n",
      "\tspeed: 0.0557s/iter; left time: 438.8332s\n",
      "\titers: 200, epoch: 7 | loss: 0.1638225\n",
      "\tspeed: 0.0183s/iter; left time: 142.0820s\n",
      "\titers: 300, epoch: 7 | loss: 0.2337635\n",
      "\tspeed: 0.0182s/iter; left time: 140.0604s\n",
      "\titers: 400, epoch: 7 | loss: 0.1708054\n",
      "\tspeed: 0.0182s/iter; left time: 138.1213s\n",
      "\titers: 500, epoch: 7 | loss: 0.1548209\n",
      "\tspeed: 0.0182s/iter; left time: 136.2622s\n",
      "Epoch: 7 cost time: 10.684828042984009\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1966750 Vali Loss: 0.0515746 Test Loss: 0.1437420\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1198446\n",
      "\tspeed: 0.0591s/iter; left time: 431.9049s\n",
      "\titers: 200, epoch: 8 | loss: 0.1399414\n",
      "\tspeed: 0.0204s/iter; left time: 147.0753s\n",
      "\titers: 300, epoch: 8 | loss: 0.1922701\n",
      "\tspeed: 0.0204s/iter; left time: 145.2110s\n",
      "\titers: 400, epoch: 8 | loss: 0.2479041\n",
      "\tspeed: 0.0204s/iter; left time: 142.9566s\n",
      "\titers: 500, epoch: 8 | loss: 0.1562815\n",
      "\tspeed: 0.0204s/iter; left time: 141.0627s\n",
      "Epoch: 8 cost time: 11.939346551895142\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1939699 Vali Loss: 0.0508912 Test Loss: 0.1437241\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2116522\n",
      "\tspeed: 0.0612s/iter; left time: 412.5643s\n",
      "\titers: 200, epoch: 9 | loss: 0.2552299\n",
      "\tspeed: 0.0209s/iter; left time: 138.9193s\n",
      "\titers: 300, epoch: 9 | loss: 0.2623025\n",
      "\tspeed: 0.0209s/iter; left time: 136.7958s\n",
      "\titers: 400, epoch: 9 | loss: 0.3542818\n",
      "\tspeed: 0.0209s/iter; left time: 134.6630s\n",
      "\titers: 500, epoch: 9 | loss: 0.1560402\n",
      "\tspeed: 0.0209s/iter; left time: 132.5362s\n",
      "Epoch: 9 cost time: 12.205559015274048\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1923798 Vali Loss: 0.0505789 Test Loss: 0.1431689\n",
      "Validation loss decreased (0.050781 --> 0.050579).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1748650\n",
      "\tspeed: 0.0618s/iter; left time: 381.6656s\n",
      "\titers: 200, epoch: 10 | loss: 0.1834387\n",
      "\tspeed: 0.0209s/iter; left time: 127.1624s\n",
      "\titers: 300, epoch: 10 | loss: 0.2298550\n",
      "\tspeed: 0.0209s/iter; left time: 124.5912s\n",
      "\titers: 400, epoch: 10 | loss: 0.2008950\n",
      "\tspeed: 0.0209s/iter; left time: 122.4858s\n",
      "\titers: 500, epoch: 10 | loss: 0.2051430\n",
      "\tspeed: 0.0209s/iter; left time: 120.4190s\n",
      "Epoch: 10 cost time: 12.18502163887024\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1923975 Vali Loss: 0.0504339 Test Loss: 0.1432166\n",
      "Validation loss decreased (0.050579 --> 0.050434).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1846491\n",
      "\tspeed: 0.0599s/iter; left time: 335.5044s\n",
      "\titers: 200, epoch: 11 | loss: 0.2004280\n",
      "\tspeed: 0.0209s/iter; left time: 114.7853s\n",
      "\titers: 300, epoch: 11 | loss: 0.1794293\n",
      "\tspeed: 0.0209s/iter; left time: 112.6900s\n",
      "\titers: 400, epoch: 11 | loss: 0.1951572\n",
      "\tspeed: 0.0209s/iter; left time: 110.8263s\n",
      "\titers: 500, epoch: 11 | loss: 0.1286800\n",
      "\tspeed: 0.0209s/iter; left time: 108.6107s\n",
      "Epoch: 11 cost time: 12.176588296890259\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1942597 Vali Loss: 0.0503781 Test Loss: 0.1431744\n",
      "Validation loss decreased (0.050434 --> 0.050378).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1566080\n",
      "\tspeed: 0.0589s/iter; left time: 296.3549s\n",
      "\titers: 200, epoch: 12 | loss: 0.2663315\n",
      "\tspeed: 0.0182s/iter; left time: 89.6838s\n",
      "\titers: 300, epoch: 12 | loss: 0.1894167\n",
      "\tspeed: 0.0182s/iter; left time: 87.8640s\n",
      "\titers: 400, epoch: 12 | loss: 0.1221318\n",
      "\tspeed: 0.0182s/iter; left time: 86.0054s\n",
      "\titers: 500, epoch: 12 | loss: 0.2709911\n",
      "\tspeed: 0.0182s/iter; left time: 84.1019s\n",
      "Epoch: 12 cost time: 10.774144172668457\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1917216 Vali Loss: 0.0504367 Test Loss: 0.1431719\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1706440\n",
      "\tspeed: 0.0568s/iter; left time: 253.2221s\n",
      "\titers: 200, epoch: 13 | loss: 0.1085223\n",
      "\tspeed: 0.0186s/iter; left time: 80.9659s\n",
      "\titers: 300, epoch: 13 | loss: 0.1748090\n",
      "\tspeed: 0.0192s/iter; left time: 81.9142s\n",
      "\titers: 400, epoch: 13 | loss: 0.1320335\n",
      "\tspeed: 0.0204s/iter; left time: 84.8512s\n",
      "\titers: 500, epoch: 13 | loss: 0.1230229\n",
      "\tspeed: 0.0204s/iter; left time: 82.6921s\n",
      "Epoch: 13 cost time: 11.611348867416382\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1911351 Vali Loss: 0.0506621 Test Loss: 0.1431873\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1293672\n",
      "\tspeed: 0.0608s/iter; left time: 236.5323s\n",
      "\titers: 200, epoch: 14 | loss: 0.1320582\n",
      "\tspeed: 0.0209s/iter; left time: 79.1022s\n",
      "\titers: 300, epoch: 14 | loss: 0.1745252\n",
      "\tspeed: 0.0209s/iter; left time: 76.9651s\n",
      "\titers: 400, epoch: 14 | loss: 0.1491092\n",
      "\tspeed: 0.0209s/iter; left time: 74.9028s\n",
      "\titers: 500, epoch: 14 | loss: 0.1423359\n",
      "\tspeed: 0.0209s/iter; left time: 72.7985s\n",
      "Epoch: 14 cost time: 12.185254335403442\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1921796 Vali Loss: 0.0504895 Test Loss: 0.1431945\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14335070550441742, mae:0.24025031924247742\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2392795\n",
      "\tspeed: 0.0343s/iter; left time: 387.8176s\n",
      "\titers: 200, epoch: 1 | loss: 0.3235706\n",
      "\tspeed: 0.0222s/iter; left time: 248.1742s\n",
      "\titers: 300, epoch: 1 | loss: 0.2933561\n",
      "\tspeed: 0.0222s/iter; left time: 246.0044s\n",
      "\titers: 400, epoch: 1 | loss: 0.4406134\n",
      "\tspeed: 0.0222s/iter; left time: 243.8248s\n",
      "\titers: 500, epoch: 1 | loss: 0.4308271\n",
      "\tspeed: 0.0221s/iter; left time: 241.2581s\n",
      "Epoch: 1 cost time: 13.903225898742676\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3484697 Vali Loss: 0.0637454 Test Loss: 0.1881898\n",
      "Validation loss decreased (inf --> 0.063745).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3287198\n",
      "\tspeed: 0.0657s/iter; left time: 704.6571s\n",
      "\titers: 200, epoch: 2 | loss: 0.2529510\n",
      "\tspeed: 0.0240s/iter; left time: 255.1172s\n",
      "\titers: 300, epoch: 2 | loss: 0.2642837\n",
      "\tspeed: 0.0240s/iter; left time: 252.9995s\n",
      "\titers: 400, epoch: 2 | loss: 0.2760209\n",
      "\tspeed: 0.0240s/iter; left time: 250.3376s\n",
      "\titers: 500, epoch: 2 | loss: 0.2563839\n",
      "\tspeed: 0.0240s/iter; left time: 248.0970s\n",
      "Epoch: 2 cost time: 13.998886346817017\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2911943 Vali Loss: 0.0618177 Test Loss: 0.1784082\n",
      "Validation loss decreased (0.063745 --> 0.061818).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.5450932\n",
      "\tspeed: 0.0680s/iter; left time: 690.9515s\n",
      "\titers: 200, epoch: 3 | loss: 0.2047932\n",
      "\tspeed: 0.0220s/iter; left time: 221.7746s\n",
      "\titers: 300, epoch: 3 | loss: 0.3956991\n",
      "\tspeed: 0.0220s/iter; left time: 219.3778s\n",
      "\titers: 400, epoch: 3 | loss: 0.2368403\n",
      "\tspeed: 0.0220s/iter; left time: 217.3105s\n",
      "\titers: 500, epoch: 3 | loss: 0.2099386\n",
      "\tspeed: 0.0220s/iter; left time: 215.1210s\n",
      "Epoch: 3 cost time: 12.864713668823242\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2846029 Vali Loss: 0.0579255 Test Loss: 0.1641819\n",
      "Validation loss decreased (0.061818 --> 0.057925).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2046410\n",
      "\tspeed: 0.0629s/iter; left time: 603.7329s\n",
      "\titers: 200, epoch: 4 | loss: 0.2222010\n",
      "\tspeed: 0.0220s/iter; left time: 208.7906s\n",
      "\titers: 300, epoch: 4 | loss: 0.2790974\n",
      "\tspeed: 0.0220s/iter; left time: 206.4901s\n",
      "\titers: 400, epoch: 4 | loss: 0.2107026\n",
      "\tspeed: 0.0220s/iter; left time: 204.5055s\n",
      "\titers: 500, epoch: 4 | loss: 0.3143312\n",
      "\tspeed: 0.0220s/iter; left time: 202.4739s\n",
      "Epoch: 4 cost time: 12.887407541275024\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2583972 Vali Loss: 0.0568851 Test Loss: 0.1662834\n",
      "Validation loss decreased (0.057925 --> 0.056885).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2783979\n",
      "\tspeed: 0.0633s/iter; left time: 571.4105s\n",
      "\titers: 200, epoch: 5 | loss: 0.2492633\n",
      "\tspeed: 0.0221s/iter; left time: 197.0368s\n",
      "\titers: 300, epoch: 5 | loss: 0.1835204\n",
      "\tspeed: 0.0221s/iter; left time: 194.8929s\n",
      "\titers: 400, epoch: 5 | loss: 0.2364521\n",
      "\tspeed: 0.0221s/iter; left time: 192.6282s\n",
      "\titers: 500, epoch: 5 | loss: 0.2786642\n",
      "\tspeed: 0.0221s/iter; left time: 190.5702s\n",
      "Epoch: 5 cost time: 12.889318943023682\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2471538 Vali Loss: 0.0547942 Test Loss: 0.1584070\n",
      "Validation loss decreased (0.056885 --> 0.054794).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2043900\n",
      "\tspeed: 0.0652s/iter; left time: 550.6114s\n",
      "\titers: 200, epoch: 6 | loss: 0.3686359\n",
      "\tspeed: 0.0220s/iter; left time: 183.9377s\n",
      "\titers: 300, epoch: 6 | loss: 0.2122341\n",
      "\tspeed: 0.0220s/iter; left time: 181.7354s\n",
      "\titers: 400, epoch: 6 | loss: 0.1944636\n",
      "\tspeed: 0.0220s/iter; left time: 179.5275s\n",
      "\titers: 500, epoch: 6 | loss: 0.3643658\n",
      "\tspeed: 0.0220s/iter; left time: 177.2790s\n",
      "Epoch: 6 cost time: 12.86254596710205\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2404939 Vali Loss: 0.0540997 Test Loss: 0.1588474\n",
      "Validation loss decreased (0.054794 --> 0.054100).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2972192\n",
      "\tspeed: 0.0651s/iter; left time: 513.3508s\n",
      "\titers: 200, epoch: 7 | loss: 0.1989725\n",
      "\tspeed: 0.0221s/iter; left time: 172.3042s\n",
      "\titers: 300, epoch: 7 | loss: 0.1605517\n",
      "\tspeed: 0.0221s/iter; left time: 170.0443s\n",
      "\titers: 400, epoch: 7 | loss: 0.1990369\n",
      "\tspeed: 0.0221s/iter; left time: 167.9161s\n",
      "\titers: 500, epoch: 7 | loss: 0.1980347\n",
      "\tspeed: 0.0221s/iter; left time: 165.6793s\n",
      "Epoch: 7 cost time: 12.9523344039917\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2361313 Vali Loss: 0.0534070 Test Loss: 0.1574572\n",
      "Validation loss decreased (0.054100 --> 0.053407).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1609631\n",
      "\tspeed: 0.0634s/iter; left time: 463.7313s\n",
      "\titers: 200, epoch: 8 | loss: 0.6042265\n",
      "\tspeed: 0.0221s/iter; left time: 159.4751s\n",
      "\titers: 300, epoch: 8 | loss: 0.1642970\n",
      "\tspeed: 0.0221s/iter; left time: 157.3586s\n",
      "\titers: 400, epoch: 8 | loss: 0.2039292\n",
      "\tspeed: 0.0221s/iter; left time: 155.0844s\n",
      "\titers: 500, epoch: 8 | loss: 0.2035870\n",
      "\tspeed: 0.0221s/iter; left time: 152.8276s\n",
      "Epoch: 8 cost time: 12.929905891418457\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2348030 Vali Loss: 0.0534884 Test Loss: 0.1569922\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2348224\n",
      "\tspeed: 0.0619s/iter; left time: 417.1533s\n",
      "\titers: 200, epoch: 9 | loss: 0.1998831\n",
      "\tspeed: 0.0220s/iter; left time: 146.3185s\n",
      "\titers: 300, epoch: 9 | loss: 0.3595006\n",
      "\tspeed: 0.0220s/iter; left time: 144.1156s\n",
      "\titers: 400, epoch: 9 | loss: 0.1851474\n",
      "\tspeed: 0.0220s/iter; left time: 141.8334s\n",
      "\titers: 500, epoch: 9 | loss: 0.2093672\n",
      "\tspeed: 0.0220s/iter; left time: 139.6174s\n",
      "Epoch: 9 cost time: 12.820004940032959\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2342727 Vali Loss: 0.0532538 Test Loss: 0.1566123\n",
      "Validation loss decreased (0.053407 --> 0.053254).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2134637\n",
      "\tspeed: 0.0626s/iter; left time: 386.0652s\n",
      "\titers: 200, epoch: 10 | loss: 0.2708219\n",
      "\tspeed: 0.0221s/iter; left time: 133.9152s\n",
      "\titers: 300, epoch: 10 | loss: 0.2378317\n",
      "\tspeed: 0.0221s/iter; left time: 131.6851s\n",
      "\titers: 400, epoch: 10 | loss: 0.3833191\n",
      "\tspeed: 0.0220s/iter; left time: 129.3859s\n",
      "\titers: 500, epoch: 10 | loss: 0.2913111\n",
      "\tspeed: 0.0220s/iter; left time: 127.2452s\n",
      "Epoch: 10 cost time: 12.890914916992188\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2342428 Vali Loss: 0.0534238 Test Loss: 0.1564279\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1943801\n",
      "\tspeed: 0.0639s/iter; left time: 357.9947s\n",
      "\titers: 200, epoch: 11 | loss: 0.1655978\n",
      "\tspeed: 0.0221s/iter; left time: 121.4604s\n",
      "\titers: 300, epoch: 11 | loss: 0.1294974\n",
      "\tspeed: 0.0221s/iter; left time: 119.1229s\n",
      "\titers: 400, epoch: 11 | loss: 0.2129554\n",
      "\tspeed: 0.0221s/iter; left time: 116.9546s\n",
      "\titers: 500, epoch: 11 | loss: 0.1963433\n",
      "\tspeed: 0.0221s/iter; left time: 114.6920s\n",
      "Epoch: 11 cost time: 12.851131677627563\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2345260 Vali Loss: 0.0532203 Test Loss: 0.1563416\n",
      "Validation loss decreased (0.053254 --> 0.053220).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2545201\n",
      "\tspeed: 0.0654s/iter; left time: 329.2623s\n",
      "\titers: 200, epoch: 12 | loss: 0.1851709\n",
      "\tspeed: 0.0221s/iter; left time: 109.1787s\n",
      "\titers: 300, epoch: 12 | loss: 0.3137481\n",
      "\tspeed: 0.0221s/iter; left time: 106.8944s\n",
      "\titers: 400, epoch: 12 | loss: 0.1479778\n",
      "\tspeed: 0.0221s/iter; left time: 104.6490s\n",
      "\titers: 500, epoch: 12 | loss: 0.2617011\n",
      "\tspeed: 0.0221s/iter; left time: 102.4211s\n",
      "Epoch: 12 cost time: 12.937089443206787\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2333996 Vali Loss: 0.0531982 Test Loss: 0.1563964\n",
      "Validation loss decreased (0.053220 --> 0.053198).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.3771702\n",
      "\tspeed: 0.0633s/iter; left time: 282.6025s\n",
      "\titers: 200, epoch: 13 | loss: 0.1694183\n",
      "\tspeed: 0.0222s/iter; left time: 96.8051s\n",
      "\titers: 300, epoch: 13 | loss: 0.1437278\n",
      "\tspeed: 0.0222s/iter; left time: 94.5270s\n",
      "\titers: 400, epoch: 13 | loss: 0.2160377\n",
      "\tspeed: 0.0222s/iter; left time: 92.3110s\n",
      "\titers: 500, epoch: 13 | loss: 0.4250709\n",
      "\tspeed: 0.0222s/iter; left time: 90.2093s\n",
      "Epoch: 13 cost time: 12.963280200958252\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2338304 Vali Loss: 0.0532326 Test Loss: 0.1563705\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2406413\n",
      "\tspeed: 0.0625s/iter; left time: 243.2995s\n",
      "\titers: 200, epoch: 14 | loss: 0.3010644\n",
      "\tspeed: 0.0221s/iter; left time: 83.6922s\n",
      "\titers: 300, epoch: 14 | loss: 0.2287150\n",
      "\tspeed: 0.0221s/iter; left time: 81.4516s\n",
      "\titers: 400, epoch: 14 | loss: 0.1924923\n",
      "\tspeed: 0.0221s/iter; left time: 79.3316s\n",
      "\titers: 500, epoch: 14 | loss: 0.1605249\n",
      "\tspeed: 0.0221s/iter; left time: 77.2702s\n",
      "Epoch: 14 cost time: 12.875060319900513\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2321255 Vali Loss: 0.0534521 Test Loss: 0.1563781\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2224185\n",
      "\tspeed: 0.0643s/iter; left time: 213.7006s\n",
      "\titers: 200, epoch: 15 | loss: 0.2126149\n",
      "\tspeed: 0.0222s/iter; left time: 71.3906s\n",
      "\titers: 300, epoch: 15 | loss: 0.1061153\n",
      "\tspeed: 0.0221s/iter; left time: 69.0753s\n",
      "\titers: 400, epoch: 15 | loss: 0.2380839\n",
      "\tspeed: 0.0221s/iter; left time: 66.8805s\n",
      "\titers: 500, epoch: 15 | loss: 0.2103924\n",
      "\tspeed: 0.0221s/iter; left time: 64.6511s\n",
      "Epoch: 15 cost time: 12.938843488693237\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2347845 Vali Loss: 0.0533193 Test Loss: 0.1563731\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1566171944141388, mae:0.2542649507522583\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3377392\n",
      "\tspeed: 0.0330s/iter; left time: 373.4224s\n",
      "\titers: 200, epoch: 1 | loss: 0.1646500\n",
      "\tspeed: 0.0214s/iter; left time: 239.2884s\n",
      "\titers: 300, epoch: 1 | loss: 0.2190873\n",
      "\tspeed: 0.0214s/iter; left time: 237.4184s\n",
      "\titers: 400, epoch: 1 | loss: 0.2957370\n",
      "\tspeed: 0.0214s/iter; left time: 234.8769s\n",
      "\titers: 500, epoch: 1 | loss: 0.2291556\n",
      "\tspeed: 0.0214s/iter; left time: 233.2782s\n",
      "Epoch: 1 cost time: 13.408550500869751\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3399138 Vali Loss: 0.0699984 Test Loss: 0.1897158\n",
      "Validation loss decreased (inf --> 0.069998).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2779487\n",
      "\tspeed: 0.0642s/iter; left time: 688.6223s\n",
      "\titers: 200, epoch: 2 | loss: 0.2231273\n",
      "\tspeed: 0.0214s/iter; left time: 227.9705s\n",
      "\titers: 300, epoch: 2 | loss: 0.3795634\n",
      "\tspeed: 0.0214s/iter; left time: 225.6746s\n",
      "\titers: 400, epoch: 2 | loss: 0.2643101\n",
      "\tspeed: 0.0214s/iter; left time: 223.2013s\n",
      "\titers: 500, epoch: 2 | loss: 0.2047420\n",
      "\tspeed: 0.0214s/iter; left time: 221.5478s\n",
      "Epoch: 2 cost time: 12.529194831848145\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2844722 Vali Loss: 0.0626669 Test Loss: 0.1898932\n",
      "Validation loss decreased (0.069998 --> 0.062667).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2739033\n",
      "\tspeed: 0.0652s/iter; left time: 662.1446s\n",
      "\titers: 200, epoch: 3 | loss: 0.2690250\n",
      "\tspeed: 0.0214s/iter; left time: 214.8658s\n",
      "\titers: 300, epoch: 3 | loss: 0.1724478\n",
      "\tspeed: 0.0213s/iter; left time: 212.6239s\n",
      "\titers: 400, epoch: 3 | loss: 0.1621042\n",
      "\tspeed: 0.0213s/iter; left time: 210.3601s\n",
      "\titers: 500, epoch: 3 | loss: 0.2729535\n",
      "\tspeed: 0.0213s/iter; left time: 208.3490s\n",
      "Epoch: 3 cost time: 12.476640701293945\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2619222 Vali Loss: 0.0608310 Test Loss: 0.1616126\n",
      "Validation loss decreased (0.062667 --> 0.060831).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1626700\n",
      "\tspeed: 0.0616s/iter; left time: 590.9157s\n",
      "\titers: 200, epoch: 4 | loss: 0.2158362\n",
      "\tspeed: 0.0213s/iter; left time: 202.2244s\n",
      "\titers: 300, epoch: 4 | loss: 0.2240596\n",
      "\tspeed: 0.0213s/iter; left time: 200.2874s\n",
      "\titers: 400, epoch: 4 | loss: 0.3337071\n",
      "\tspeed: 0.0213s/iter; left time: 197.9642s\n",
      "\titers: 500, epoch: 4 | loss: 0.1876449\n",
      "\tspeed: 0.0213s/iter; left time: 195.8913s\n",
      "Epoch: 4 cost time: 12.451783180236816\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2398474 Vali Loss: 0.0581919 Test Loss: 0.1548366\n",
      "Validation loss decreased (0.060831 --> 0.058192).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.3035271\n",
      "\tspeed: 0.0615s/iter; left time: 554.6474s\n",
      "\titers: 200, epoch: 5 | loss: 0.1793818\n",
      "\tspeed: 0.0213s/iter; left time: 189.8101s\n",
      "\titers: 300, epoch: 5 | loss: 0.2417612\n",
      "\tspeed: 0.0213s/iter; left time: 187.8336s\n",
      "\titers: 400, epoch: 5 | loss: 0.2484414\n",
      "\tspeed: 0.0213s/iter; left time: 185.4268s\n",
      "\titers: 500, epoch: 5 | loss: 0.1850076\n",
      "\tspeed: 0.0213s/iter; left time: 183.4114s\n",
      "Epoch: 5 cost time: 12.440310955047607\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2259612 Vali Loss: 0.0581790 Test Loss: 0.1517878\n",
      "Validation loss decreased (0.058192 --> 0.058179).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2080330\n",
      "\tspeed: 0.0618s/iter; left time: 522.2266s\n",
      "\titers: 200, epoch: 6 | loss: 0.3204569\n",
      "\tspeed: 0.0214s/iter; left time: 179.1110s\n",
      "\titers: 300, epoch: 6 | loss: 0.2294517\n",
      "\tspeed: 0.0215s/iter; left time: 177.0775s\n",
      "\titers: 400, epoch: 6 | loss: 0.1415012\n",
      "\tspeed: 0.0214s/iter; left time: 174.6666s\n",
      "\titers: 500, epoch: 6 | loss: 0.1335572\n",
      "\tspeed: 0.0214s/iter; left time: 172.4935s\n",
      "Epoch: 6 cost time: 12.524194955825806\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2220644 Vali Loss: 0.0585967 Test Loss: 0.1529160\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1969465\n",
      "\tspeed: 0.0634s/iter; left time: 499.8249s\n",
      "\titers: 200, epoch: 7 | loss: 0.2675973\n",
      "\tspeed: 0.0214s/iter; left time: 166.5159s\n",
      "\titers: 300, epoch: 7 | loss: 0.2175160\n",
      "\tspeed: 0.0214s/iter; left time: 164.4299s\n",
      "\titers: 400, epoch: 7 | loss: 0.1372379\n",
      "\tspeed: 0.0214s/iter; left time: 162.2041s\n",
      "\titers: 500, epoch: 7 | loss: 0.1875855\n",
      "\tspeed: 0.0214s/iter; left time: 160.0782s\n",
      "Epoch: 7 cost time: 12.494813919067383\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2186270 Vali Loss: 0.0573958 Test Loss: 0.1515518\n",
      "Validation loss decreased (0.058179 --> 0.057396).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2150478\n",
      "\tspeed: 0.0621s/iter; left time: 453.8890s\n",
      "\titers: 200, epoch: 8 | loss: 0.2058710\n",
      "\tspeed: 0.0214s/iter; left time: 154.6078s\n",
      "\titers: 300, epoch: 8 | loss: 0.2225778\n",
      "\tspeed: 0.0214s/iter; left time: 152.4315s\n",
      "\titers: 400, epoch: 8 | loss: 0.2368623\n",
      "\tspeed: 0.0214s/iter; left time: 150.2796s\n",
      "\titers: 500, epoch: 8 | loss: 0.2915786\n",
      "\tspeed: 0.0214s/iter; left time: 148.1124s\n",
      "Epoch: 8 cost time: 12.534618616104126\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2145763 Vali Loss: 0.0559872 Test Loss: 0.1507481\n",
      "Validation loss decreased (0.057396 --> 0.055987).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1523317\n",
      "\tspeed: 0.0637s/iter; left time: 429.3513s\n",
      "\titers: 200, epoch: 9 | loss: 0.3080234\n",
      "\tspeed: 0.0233s/iter; left time: 154.6424s\n",
      "\titers: 300, epoch: 9 | loss: 0.1853736\n",
      "\tspeed: 0.0233s/iter; left time: 152.1523s\n",
      "\titers: 400, epoch: 9 | loss: 0.1888307\n",
      "\tspeed: 0.0233s/iter; left time: 149.8180s\n",
      "\titers: 500, epoch: 9 | loss: 0.1443109\n",
      "\tspeed: 0.0233s/iter; left time: 147.5173s\n",
      "Epoch: 9 cost time: 13.59677267074585\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2150063 Vali Loss: 0.0561404 Test Loss: 0.1507259\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1206441\n",
      "\tspeed: 0.0623s/iter; left time: 384.1791s\n",
      "\titers: 200, epoch: 10 | loss: 0.2277396\n",
      "\tspeed: 0.0215s/iter; left time: 130.3027s\n",
      "\titers: 300, epoch: 10 | loss: 0.1638017\n",
      "\tspeed: 0.0215s/iter; left time: 128.1167s\n",
      "\titers: 400, epoch: 10 | loss: 0.2121173\n",
      "\tspeed: 0.0214s/iter; left time: 125.8528s\n",
      "\titers: 500, epoch: 10 | loss: 0.2538242\n",
      "\tspeed: 0.0214s/iter; left time: 123.6605s\n",
      "Epoch: 10 cost time: 12.512019157409668\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2131337 Vali Loss: 0.0561769 Test Loss: 0.1508286\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2837865\n",
      "\tspeed: 0.0615s/iter; left time: 344.6421s\n",
      "\titers: 200, epoch: 11 | loss: 0.2379410\n",
      "\tspeed: 0.0214s/iter; left time: 117.7934s\n",
      "\titers: 300, epoch: 11 | loss: 0.1742121\n",
      "\tspeed: 0.0214s/iter; left time: 115.6572s\n",
      "\titers: 400, epoch: 11 | loss: 0.1710873\n",
      "\tspeed: 0.0214s/iter; left time: 113.2421s\n",
      "\titers: 500, epoch: 11 | loss: 0.2518443\n",
      "\tspeed: 0.0213s/iter; left time: 111.0156s\n",
      "Epoch: 11 cost time: 12.460595846176147\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2131534 Vali Loss: 0.0561333 Test Loss: 0.1507716\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15090881288051605, mae:0.2482869029045105\n",
      ">>> LR=1e-3,DO=0.1,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3404231\n",
      "\tspeed: 0.0320s/iter; left time: 361.5946s\n",
      "\titers: 200, epoch: 1 | loss: 0.2426745\n",
      "\tspeed: 0.0202s/iter; left time: 225.8984s\n",
      "\titers: 300, epoch: 1 | loss: 0.4168512\n",
      "\tspeed: 0.0202s/iter; left time: 223.6939s\n",
      "\titers: 400, epoch: 1 | loss: 0.3796895\n",
      "\tspeed: 0.0201s/iter; left time: 221.1340s\n",
      "\titers: 500, epoch: 1 | loss: 0.2751806\n",
      "\tspeed: 0.0200s/iter; left time: 218.0853s\n",
      "Epoch: 1 cost time: 12.824127435684204\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3250057 Vali Loss: 0.0661804 Test Loss: 0.1876831\n",
      "Validation loss decreased (inf --> 0.066180).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1786113\n",
      "\tspeed: 0.0608s/iter; left time: 651.9142s\n",
      "\titers: 200, epoch: 2 | loss: 0.5230356\n",
      "\tspeed: 0.0200s/iter; left time: 212.8684s\n",
      "\titers: 300, epoch: 2 | loss: 0.3196713\n",
      "\tspeed: 0.0200s/iter; left time: 210.9458s\n",
      "\titers: 400, epoch: 2 | loss: 0.1660084\n",
      "\tspeed: 0.0200s/iter; left time: 209.1379s\n",
      "\titers: 500, epoch: 2 | loss: 0.1820250\n",
      "\tspeed: 0.0200s/iter; left time: 207.1260s\n",
      "Epoch: 2 cost time: 11.740784168243408\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2875961 Vali Loss: 0.0598336 Test Loss: 0.1854444\n",
      "Validation loss decreased (0.066180 --> 0.059834).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1814147\n",
      "\tspeed: 0.0596s/iter; left time: 605.8869s\n",
      "\titers: 200, epoch: 3 | loss: 0.1833535\n",
      "\tspeed: 0.0201s/iter; left time: 201.9345s\n",
      "\titers: 300, epoch: 3 | loss: 0.2722208\n",
      "\tspeed: 0.0201s/iter; left time: 200.0327s\n",
      "\titers: 400, epoch: 3 | loss: 0.2516907\n",
      "\tspeed: 0.0201s/iter; left time: 197.7144s\n",
      "\titers: 500, epoch: 3 | loss: 0.2900542\n",
      "\tspeed: 0.0200s/iter; left time: 195.4708s\n",
      "Epoch: 3 cost time: 11.74783992767334\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2670503 Vali Loss: 0.0646542 Test Loss: 0.1743281\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1391644\n",
      "\tspeed: 0.0593s/iter; left time: 568.4444s\n",
      "\titers: 200, epoch: 4 | loss: 0.4543982\n",
      "\tspeed: 0.0202s/iter; left time: 191.2701s\n",
      "\titers: 300, epoch: 4 | loss: 0.1633909\n",
      "\tspeed: 0.0201s/iter; left time: 189.0386s\n",
      "\titers: 400, epoch: 4 | loss: 0.2394994\n",
      "\tspeed: 0.0201s/iter; left time: 186.5756s\n",
      "\titers: 500, epoch: 4 | loss: 0.2080919\n",
      "\tspeed: 0.0201s/iter; left time: 185.0088s\n",
      "Epoch: 4 cost time: 11.755877017974854\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2490871 Vali Loss: 0.0620873 Test Loss: 0.1656219\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2104890\n",
      "\tspeed: 0.0590s/iter; left time: 531.8204s\n",
      "\titers: 200, epoch: 5 | loss: 0.4366803\n",
      "\tspeed: 0.0202s/iter; left time: 180.1585s\n",
      "\titers: 300, epoch: 5 | loss: 0.2191622\n",
      "\tspeed: 0.0201s/iter; left time: 177.7410s\n",
      "\titers: 400, epoch: 5 | loss: 0.2156383\n",
      "\tspeed: 0.0202s/iter; left time: 175.7948s\n",
      "\titers: 500, epoch: 5 | loss: 0.2265786\n",
      "\tspeed: 0.0202s/iter; left time: 173.8781s\n",
      "Epoch: 5 cost time: 11.794955015182495\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2388819 Vali Loss: 0.0566011 Test Loss: 0.1586045\n",
      "Validation loss decreased (0.059834 --> 0.056601).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2559460\n",
      "\tspeed: 0.0600s/iter; left time: 507.4240s\n",
      "\titers: 200, epoch: 6 | loss: 0.2333039\n",
      "\tspeed: 0.0201s/iter; left time: 168.1653s\n",
      "\titers: 300, epoch: 6 | loss: 0.2261849\n",
      "\tspeed: 0.0201s/iter; left time: 166.2005s\n",
      "\titers: 400, epoch: 6 | loss: 0.1795142\n",
      "\tspeed: 0.0201s/iter; left time: 164.2310s\n",
      "\titers: 500, epoch: 6 | loss: 0.3686235\n",
      "\tspeed: 0.0202s/iter; left time: 162.3055s\n",
      "Epoch: 6 cost time: 11.75473928451538\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2322234 Vali Loss: 0.0568356 Test Loss: 0.1558356\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2439669\n",
      "\tspeed: 0.0580s/iter; left time: 456.9707s\n",
      "\titers: 200, epoch: 7 | loss: 0.2223335\n",
      "\tspeed: 0.0201s/iter; left time: 156.5674s\n",
      "\titers: 300, epoch: 7 | loss: 0.2759081\n",
      "\tspeed: 0.0201s/iter; left time: 154.3387s\n",
      "\titers: 400, epoch: 7 | loss: 0.4412869\n",
      "\tspeed: 0.0201s/iter; left time: 152.2563s\n",
      "\titers: 500, epoch: 7 | loss: 0.1374561\n",
      "\tspeed: 0.0201s/iter; left time: 150.0643s\n",
      "Epoch: 7 cost time: 11.749794483184814\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2276247 Vali Loss: 0.0568543 Test Loss: 0.1559284\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2687363\n",
      "\tspeed: 0.0588s/iter; left time: 429.7600s\n",
      "\titers: 200, epoch: 8 | loss: 0.3044820\n",
      "\tspeed: 0.0202s/iter; left time: 145.3933s\n",
      "\titers: 300, epoch: 8 | loss: 0.2953758\n",
      "\tspeed: 0.0201s/iter; left time: 143.2161s\n",
      "\titers: 400, epoch: 8 | loss: 0.1874395\n",
      "\tspeed: 0.0202s/iter; left time: 141.3380s\n",
      "\titers: 500, epoch: 8 | loss: 0.2586952\n",
      "\tspeed: 0.0202s/iter; left time: 139.3845s\n",
      "Epoch: 8 cost time: 11.819557428359985\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2268142 Vali Loss: 0.0565078 Test Loss: 0.1549985\n",
      "Validation loss decreased (0.056601 --> 0.056508).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2243491\n",
      "\tspeed: 0.0621s/iter; left time: 418.3213s\n",
      "\titers: 200, epoch: 9 | loss: 0.1885431\n",
      "\tspeed: 0.0201s/iter; left time: 133.7745s\n",
      "\titers: 300, epoch: 9 | loss: 0.2734574\n",
      "\tspeed: 0.0201s/iter; left time: 131.5854s\n",
      "\titers: 400, epoch: 9 | loss: 0.1793066\n",
      "\tspeed: 0.0201s/iter; left time: 129.4760s\n",
      "\titers: 500, epoch: 9 | loss: 0.1427067\n",
      "\tspeed: 0.0201s/iter; left time: 127.7219s\n",
      "Epoch: 9 cost time: 11.781221628189087\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2260832 Vali Loss: 0.0563967 Test Loss: 0.1547564\n",
      "Validation loss decreased (0.056508 --> 0.056397).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2403950\n",
      "\tspeed: 0.0596s/iter; left time: 367.7644s\n",
      "\titers: 200, epoch: 10 | loss: 0.1651448\n",
      "\tspeed: 0.0201s/iter; left time: 122.0587s\n",
      "\titers: 300, epoch: 10 | loss: 0.2896049\n",
      "\tspeed: 0.0201s/iter; left time: 119.8959s\n",
      "\titers: 400, epoch: 10 | loss: 0.3092924\n",
      "\tspeed: 0.0201s/iter; left time: 117.8504s\n",
      "\titers: 500, epoch: 10 | loss: 0.2361620\n",
      "\tspeed: 0.0201s/iter; left time: 115.8818s\n",
      "Epoch: 10 cost time: 11.741727352142334\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2255608 Vali Loss: 0.0564119 Test Loss: 0.1546019\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2059657\n",
      "\tspeed: 0.0627s/iter; left time: 351.0879s\n",
      "\titers: 200, epoch: 11 | loss: 0.1982250\n",
      "\tspeed: 0.0223s/iter; left time: 122.7408s\n",
      "\titers: 300, epoch: 11 | loss: 0.1643462\n",
      "\tspeed: 0.0223s/iter; left time: 120.1812s\n",
      "\titers: 400, epoch: 11 | loss: 0.2687558\n",
      "\tspeed: 0.0212s/iter; left time: 112.6201s\n",
      "\titers: 500, epoch: 11 | loss: 0.1734656\n",
      "\tspeed: 0.0202s/iter; left time: 104.8308s\n",
      "Epoch: 11 cost time: 12.523143529891968\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2242707 Vali Loss: 0.0564297 Test Loss: 0.1546341\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2062238\n",
      "\tspeed: 0.0583s/iter; left time: 293.3372s\n",
      "\titers: 200, epoch: 12 | loss: 0.1974917\n",
      "\tspeed: 0.0201s/iter; left time: 99.0675s\n",
      "\titers: 300, epoch: 12 | loss: 0.3693649\n",
      "\tspeed: 0.0201s/iter; left time: 97.1088s\n",
      "\titers: 400, epoch: 12 | loss: 0.2459300\n",
      "\tspeed: 0.0201s/iter; left time: 94.9773s\n",
      "\titers: 500, epoch: 12 | loss: 0.3968561\n",
      "\tspeed: 0.0201s/iter; left time: 93.0425s\n",
      "Epoch: 12 cost time: 11.709505796432495\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2242734 Vali Loss: 0.0563085 Test Loss: 0.1546287\n",
      "Validation loss decreased (0.056397 --> 0.056308).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2222096\n",
      "\tspeed: 0.0594s/iter; left time: 265.0717s\n",
      "\titers: 200, epoch: 13 | loss: 0.1883194\n",
      "\tspeed: 0.0202s/iter; left time: 87.9550s\n",
      "\titers: 300, epoch: 13 | loss: 0.2055316\n",
      "\tspeed: 0.0201s/iter; left time: 85.7852s\n",
      "\titers: 400, epoch: 13 | loss: 0.1621581\n",
      "\tspeed: 0.0202s/iter; left time: 83.8509s\n",
      "\titers: 500, epoch: 13 | loss: 0.3007304\n",
      "\tspeed: 0.0202s/iter; left time: 81.8581s\n",
      "Epoch: 13 cost time: 11.773485898971558\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2250128 Vali Loss: 0.0565566 Test Loss: 0.1546288\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.2919002\n",
      "\tspeed: 0.0583s/iter; left time: 227.0343s\n",
      "\titers: 200, epoch: 14 | loss: 0.2277680\n",
      "\tspeed: 0.0201s/iter; left time: 76.2789s\n",
      "\titers: 300, epoch: 14 | loss: 0.2307416\n",
      "\tspeed: 0.0201s/iter; left time: 74.3616s\n",
      "\titers: 400, epoch: 14 | loss: 0.1529334\n",
      "\tspeed: 0.0201s/iter; left time: 72.3353s\n",
      "\titers: 500, epoch: 14 | loss: 0.1951609\n",
      "\tspeed: 0.0201s/iter; left time: 70.2578s\n",
      "Epoch: 14 cost time: 11.743305921554565\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2229430 Vali Loss: 0.0564357 Test Loss: 0.1546331\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1179616\n",
      "\tspeed: 0.0597s/iter; left time: 198.3964s\n",
      "\titers: 200, epoch: 15 | loss: 0.2970698\n",
      "\tspeed: 0.0201s/iter; left time: 64.7917s\n",
      "\titers: 300, epoch: 15 | loss: 0.2058809\n",
      "\tspeed: 0.0201s/iter; left time: 62.6177s\n",
      "\titers: 400, epoch: 15 | loss: 0.2033745\n",
      "\tspeed: 0.0201s/iter; left time: 60.5911s\n",
      "\titers: 500, epoch: 15 | loss: 0.2084622\n",
      "\tspeed: 0.0200s/iter; left time: 58.5561s\n",
      "Epoch: 15 cost time: 11.737856388092041\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2243458 Vali Loss: 0.0565327 Test Loss: 0.1546283\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15482033789157867, mae:0.25201061367988586\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2117237\n",
      "\tspeed: 0.0220s/iter; left time: 248.8152s\n",
      "\titers: 200, epoch: 1 | loss: 0.1962214\n",
      "\tspeed: 0.0105s/iter; left time: 117.1434s\n",
      "\titers: 300, epoch: 1 | loss: 0.2084974\n",
      "\tspeed: 0.0105s/iter; left time: 116.3532s\n",
      "\titers: 400, epoch: 1 | loss: 0.2296560\n",
      "\tspeed: 0.0105s/iter; left time: 115.0132s\n",
      "\titers: 500, epoch: 1 | loss: 0.2272920\n",
      "\tspeed: 0.0105s/iter; left time: 114.2215s\n",
      "Epoch: 1 cost time: 7.171305179595947\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2698223 Vali Loss: 0.0505259 Test Loss: 0.1521214\n",
      "Validation loss decreased (inf --> 0.050526).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.5054663\n",
      "\tspeed: 0.0362s/iter; left time: 388.5442s\n",
      "\titers: 200, epoch: 2 | loss: 0.3730318\n",
      "\tspeed: 0.0104s/iter; left time: 110.9767s\n",
      "\titers: 300, epoch: 2 | loss: 0.2677314\n",
      "\tspeed: 0.0104s/iter; left time: 109.7926s\n",
      "\titers: 400, epoch: 2 | loss: 0.2034914\n",
      "\tspeed: 0.0104s/iter; left time: 108.8417s\n",
      "\titers: 500, epoch: 2 | loss: 0.2365679\n",
      "\tspeed: 0.0104s/iter; left time: 107.7636s\n",
      "Epoch: 2 cost time: 6.234140396118164\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2636143 Vali Loss: 0.0675508 Test Loss: 0.1951685\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2460580\n",
      "\tspeed: 0.0364s/iter; left time: 369.8823s\n",
      "\titers: 200, epoch: 3 | loss: 0.3752654\n",
      "\tspeed: 0.0104s/iter; left time: 104.8366s\n",
      "\titers: 300, epoch: 3 | loss: 0.2134835\n",
      "\tspeed: 0.0104s/iter; left time: 104.0087s\n",
      "\titers: 400, epoch: 3 | loss: 0.2111763\n",
      "\tspeed: 0.0104s/iter; left time: 102.8978s\n",
      "\titers: 500, epoch: 3 | loss: 0.2905028\n",
      "\tspeed: 0.0104s/iter; left time: 101.8983s\n",
      "Epoch: 3 cost time: 6.264663457870483\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2672945 Vali Loss: 0.0502162 Test Loss: 0.1495679\n",
      "Validation loss decreased (0.050526 --> 0.050216).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.4105504\n",
      "\tspeed: 0.0376s/iter; left time: 360.9838s\n",
      "\titers: 200, epoch: 4 | loss: 0.1554852\n",
      "\tspeed: 0.0104s/iter; left time: 98.8073s\n",
      "\titers: 300, epoch: 4 | loss: 0.2539790\n",
      "\tspeed: 0.0104s/iter; left time: 97.9731s\n",
      "\titers: 400, epoch: 4 | loss: 0.2332794\n",
      "\tspeed: 0.0104s/iter; left time: 96.9278s\n",
      "\titers: 500, epoch: 4 | loss: 0.2072173\n",
      "\tspeed: 0.0104s/iter; left time: 95.7301s\n",
      "Epoch: 4 cost time: 6.259564161300659\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2372657 Vali Loss: 0.0477580 Test Loss: 0.1426489\n",
      "Validation loss decreased (0.050216 --> 0.047758).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2328312\n",
      "\tspeed: 0.0356s/iter; left time: 320.7285s\n",
      "\titers: 200, epoch: 5 | loss: 0.2216953\n",
      "\tspeed: 0.0104s/iter; left time: 92.8863s\n",
      "\titers: 300, epoch: 5 | loss: 0.1907568\n",
      "\tspeed: 0.0104s/iter; left time: 91.7870s\n",
      "\titers: 400, epoch: 5 | loss: 0.1760155\n",
      "\tspeed: 0.0104s/iter; left time: 91.1016s\n",
      "\titers: 500, epoch: 5 | loss: 0.2505337\n",
      "\tspeed: 0.0105s/iter; left time: 90.2355s\n",
      "Epoch: 5 cost time: 6.247252464294434\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2250188 Vali Loss: 0.0460322 Test Loss: 0.1390025\n",
      "Validation loss decreased (0.047758 --> 0.046032).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1989035\n",
      "\tspeed: 0.0378s/iter; left time: 319.8456s\n",
      "\titers: 200, epoch: 6 | loss: 0.1535221\n",
      "\tspeed: 0.0105s/iter; left time: 87.2908s\n",
      "\titers: 300, epoch: 6 | loss: 0.2412390\n",
      "\tspeed: 0.0105s/iter; left time: 86.2707s\n",
      "\titers: 400, epoch: 6 | loss: 0.2632490\n",
      "\tspeed: 0.0105s/iter; left time: 85.1997s\n",
      "\titers: 500, epoch: 6 | loss: 0.2396662\n",
      "\tspeed: 0.0104s/iter; left time: 84.1275s\n",
      "Epoch: 6 cost time: 6.246135711669922\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2193443 Vali Loss: 0.0450908 Test Loss: 0.1399323\n",
      "Validation loss decreased (0.046032 --> 0.045091).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2661636\n",
      "\tspeed: 0.0366s/iter; left time: 288.7899s\n",
      "\titers: 200, epoch: 7 | loss: 0.2804905\n",
      "\tspeed: 0.0105s/iter; left time: 81.4527s\n",
      "\titers: 300, epoch: 7 | loss: 0.1554718\n",
      "\tspeed: 0.0105s/iter; left time: 80.3677s\n",
      "\titers: 400, epoch: 7 | loss: 0.1617900\n",
      "\tspeed: 0.0105s/iter; left time: 79.3304s\n",
      "\titers: 500, epoch: 7 | loss: 0.1971248\n",
      "\tspeed: 0.0105s/iter; left time: 78.2625s\n",
      "Epoch: 7 cost time: 6.25324559211731\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2169546 Vali Loss: 0.0455527 Test Loss: 0.1369280\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2547485\n",
      "\tspeed: 0.0357s/iter; left time: 261.2762s\n",
      "\titers: 200, epoch: 8 | loss: 0.2095753\n",
      "\tspeed: 0.0105s/iter; left time: 75.6768s\n",
      "\titers: 300, epoch: 8 | loss: 0.2478575\n",
      "\tspeed: 0.0105s/iter; left time: 74.6272s\n",
      "\titers: 400, epoch: 8 | loss: 0.2990359\n",
      "\tspeed: 0.0105s/iter; left time: 73.5842s\n",
      "\titers: 500, epoch: 8 | loss: 0.1562864\n",
      "\tspeed: 0.0105s/iter; left time: 72.4651s\n",
      "Epoch: 8 cost time: 6.3018388748168945\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2126635 Vali Loss: 0.0449546 Test Loss: 0.1366721\n",
      "Validation loss decreased (0.045091 --> 0.044955).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2795043\n",
      "\tspeed: 0.0364s/iter; left time: 245.5225s\n",
      "\titers: 200, epoch: 9 | loss: 0.1916270\n",
      "\tspeed: 0.0105s/iter; left time: 69.8932s\n",
      "\titers: 300, epoch: 9 | loss: 0.1030944\n",
      "\tspeed: 0.0105s/iter; left time: 68.5353s\n",
      "\titers: 400, epoch: 9 | loss: 0.1902943\n",
      "\tspeed: 0.0105s/iter; left time: 67.6586s\n",
      "\titers: 500, epoch: 9 | loss: 0.1884133\n",
      "\tspeed: 0.0105s/iter; left time: 66.8493s\n",
      "Epoch: 9 cost time: 6.327685117721558\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2139503 Vali Loss: 0.0453385 Test Loss: 0.1365460\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1995925\n",
      "\tspeed: 0.0367s/iter; left time: 226.5458s\n",
      "\titers: 200, epoch: 10 | loss: 0.1834594\n",
      "\tspeed: 0.0104s/iter; left time: 63.3855s\n",
      "\titers: 300, epoch: 10 | loss: 0.1525384\n",
      "\tspeed: 0.0105s/iter; left time: 62.4762s\n",
      "\titers: 400, epoch: 10 | loss: 0.1993958\n",
      "\tspeed: 0.0104s/iter; left time: 61.3487s\n",
      "\titers: 500, epoch: 10 | loss: 0.3330832\n",
      "\tspeed: 0.0105s/iter; left time: 60.4912s\n",
      "Epoch: 10 cost time: 6.369235992431641\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2125980 Vali Loss: 0.0449574 Test Loss: 0.1366582\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2353677\n",
      "\tspeed: 0.0367s/iter; left time: 205.4235s\n",
      "\titers: 200, epoch: 11 | loss: 0.1963727\n",
      "\tspeed: 0.0105s/iter; left time: 57.6356s\n",
      "\titers: 300, epoch: 11 | loss: 0.1679645\n",
      "\tspeed: 0.0105s/iter; left time: 56.4705s\n",
      "\titers: 400, epoch: 11 | loss: 0.3239220\n",
      "\tspeed: 0.0104s/iter; left time: 55.3630s\n",
      "\titers: 500, epoch: 11 | loss: 0.1685197\n",
      "\tspeed: 0.0104s/iter; left time: 54.2857s\n",
      "Epoch: 11 cost time: 6.238062143325806\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2125218 Vali Loss: 0.0449107 Test Loss: 0.1366072\n",
      "Validation loss decreased (0.044955 --> 0.044911).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2075755\n",
      "\tspeed: 0.0372s/iter; left time: 186.9113s\n",
      "\titers: 200, epoch: 12 | loss: 0.2189093\n",
      "\tspeed: 0.0105s/iter; left time: 51.7587s\n",
      "\titers: 300, epoch: 12 | loss: 0.1716382\n",
      "\tspeed: 0.0105s/iter; left time: 50.6757s\n",
      "\titers: 400, epoch: 12 | loss: 0.1820216\n",
      "\tspeed: 0.0105s/iter; left time: 49.6273s\n",
      "\titers: 500, epoch: 12 | loss: 0.3031299\n",
      "\tspeed: 0.0105s/iter; left time: 48.5636s\n",
      "Epoch: 12 cost time: 6.281940937042236\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2114540 Vali Loss: 0.0450061 Test Loss: 0.1366529\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1022834\n",
      "\tspeed: 0.0353s/iter; left time: 157.5550s\n",
      "\titers: 200, epoch: 13 | loss: 0.2675892\n",
      "\tspeed: 0.0105s/iter; left time: 45.7415s\n",
      "\titers: 300, epoch: 13 | loss: 0.1558159\n",
      "\tspeed: 0.0105s/iter; left time: 44.6781s\n",
      "\titers: 400, epoch: 13 | loss: 0.1571575\n",
      "\tspeed: 0.0105s/iter; left time: 43.6360s\n",
      "\titers: 500, epoch: 13 | loss: 0.2214960\n",
      "\tspeed: 0.0105s/iter; left time: 42.5514s\n",
      "Epoch: 13 cost time: 6.258947372436523\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2133635 Vali Loss: 0.0450953 Test Loss: 0.1366360\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1524723\n",
      "\tspeed: 0.0361s/iter; left time: 140.5357s\n",
      "\titers: 200, epoch: 14 | loss: 0.1994179\n",
      "\tspeed: 0.0105s/iter; left time: 39.7153s\n",
      "\titers: 300, epoch: 14 | loss: 0.2454288\n",
      "\tspeed: 0.0105s/iter; left time: 38.6802s\n",
      "\titers: 400, epoch: 14 | loss: 0.2762757\n",
      "\tspeed: 0.0105s/iter; left time: 37.6347s\n",
      "\titers: 500, epoch: 14 | loss: 0.2121911\n",
      "\tspeed: 0.0105s/iter; left time: 36.6224s\n",
      "Epoch: 14 cost time: 6.263035535812378\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2123307 Vali Loss: 0.0449673 Test Loss: 0.1366275\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13682235777378082, mae:0.23013094067573547\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2720160\n",
      "\tspeed: 0.0235s/iter; left time: 265.8249s\n",
      "\titers: 200, epoch: 1 | loss: 0.2550612\n",
      "\tspeed: 0.0116s/iter; left time: 130.2905s\n",
      "\titers: 300, epoch: 1 | loss: 0.2996303\n",
      "\tspeed: 0.0111s/iter; left time: 123.7416s\n",
      "\titers: 400, epoch: 1 | loss: 0.2450242\n",
      "\tspeed: 0.0104s/iter; left time: 114.0813s\n",
      "\titers: 500, epoch: 1 | loss: 0.2024878\n",
      "\tspeed: 0.0104s/iter; left time: 112.9372s\n",
      "Epoch: 1 cost time: 7.476855278015137\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2292420 Vali Loss: 0.0406269 Test Loss: 0.1289614\n",
      "Validation loss decreased (inf --> 0.040627).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1660670\n",
      "\tspeed: 0.0369s/iter; left time: 395.9932s\n",
      "\titers: 200, epoch: 2 | loss: 0.2225348\n",
      "\tspeed: 0.0103s/iter; left time: 109.0103s\n",
      "\titers: 300, epoch: 2 | loss: 0.1696781\n",
      "\tspeed: 0.0103s/iter; left time: 108.2404s\n",
      "\titers: 400, epoch: 2 | loss: 0.2024043\n",
      "\tspeed: 0.0103s/iter; left time: 107.0188s\n",
      "\titers: 500, epoch: 2 | loss: 0.2608290\n",
      "\tspeed: 0.0103s/iter; left time: 106.2111s\n",
      "Epoch: 2 cost time: 6.1616668701171875\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2151188 Vali Loss: 0.0432774 Test Loss: 0.1319082\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1073155\n",
      "\tspeed: 0.0384s/iter; left time: 390.2057s\n",
      "\titers: 200, epoch: 3 | loss: 0.2045198\n",
      "\tspeed: 0.0118s/iter; left time: 118.7316s\n",
      "\titers: 300, epoch: 3 | loss: 0.3562557\n",
      "\tspeed: 0.0119s/iter; left time: 118.1520s\n",
      "\titers: 400, epoch: 3 | loss: 0.1984722\n",
      "\tspeed: 0.0118s/iter; left time: 116.2431s\n",
      "\titers: 500, epoch: 3 | loss: 0.1355830\n",
      "\tspeed: 0.0118s/iter; left time: 115.0998s\n",
      "Epoch: 3 cost time: 7.003190994262695\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1942021 Vali Loss: 0.0354472 Test Loss: 0.1168530\n",
      "Validation loss decreased (0.040627 --> 0.035447).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1595048\n",
      "\tspeed: 0.0382s/iter; left time: 366.5038s\n",
      "\titers: 200, epoch: 4 | loss: 0.1545954\n",
      "\tspeed: 0.0118s/iter; left time: 111.8431s\n",
      "\titers: 300, epoch: 4 | loss: 0.1039984\n",
      "\tspeed: 0.0118s/iter; left time: 110.9109s\n",
      "\titers: 400, epoch: 4 | loss: 0.1309091\n",
      "\tspeed: 0.0118s/iter; left time: 109.7568s\n",
      "\titers: 500, epoch: 4 | loss: 0.2197086\n",
      "\tspeed: 0.0118s/iter; left time: 108.6915s\n",
      "Epoch: 4 cost time: 7.0193305015563965\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1774322 Vali Loss: 0.0345319 Test Loss: 0.1144773\n",
      "Validation loss decreased (0.035447 --> 0.034532).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1512218\n",
      "\tspeed: 0.0380s/iter; left time: 342.7783s\n",
      "\titers: 200, epoch: 5 | loss: 0.2394757\n",
      "\tspeed: 0.0118s/iter; left time: 105.1550s\n",
      "\titers: 300, epoch: 5 | loss: 0.1671902\n",
      "\tspeed: 0.0118s/iter; left time: 103.7674s\n",
      "\titers: 400, epoch: 5 | loss: 0.1594759\n",
      "\tspeed: 0.0118s/iter; left time: 102.9081s\n",
      "\titers: 500, epoch: 5 | loss: 0.1756693\n",
      "\tspeed: 0.0118s/iter; left time: 101.6771s\n",
      "Epoch: 5 cost time: 6.997888565063477\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1706449 Vali Loss: 0.0340808 Test Loss: 0.1143026\n",
      "Validation loss decreased (0.034532 --> 0.034081).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1132660\n",
      "\tspeed: 0.0390s/iter; left time: 329.7496s\n",
      "\titers: 200, epoch: 6 | loss: 0.2210791\n",
      "\tspeed: 0.0103s/iter; left time: 86.1760s\n",
      "\titers: 300, epoch: 6 | loss: 0.1718553\n",
      "\tspeed: 0.0103s/iter; left time: 85.2981s\n",
      "\titers: 400, epoch: 6 | loss: 0.1357596\n",
      "\tspeed: 0.0107s/iter; left time: 87.2708s\n",
      "\titers: 500, epoch: 6 | loss: 0.1047990\n",
      "\tspeed: 0.0109s/iter; left time: 87.9465s\n",
      "Epoch: 6 cost time: 6.389994382858276\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1650022 Vali Loss: 0.0340683 Test Loss: 0.1121477\n",
      "Validation loss decreased (0.034081 --> 0.034068).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1775665\n",
      "\tspeed: 0.0366s/iter; left time: 288.7427s\n",
      "\titers: 200, epoch: 7 | loss: 0.1840083\n",
      "\tspeed: 0.0103s/iter; left time: 80.0612s\n",
      "\titers: 300, epoch: 7 | loss: 0.3098571\n",
      "\tspeed: 0.0103s/iter; left time: 79.0619s\n",
      "\titers: 400, epoch: 7 | loss: 0.2166393\n",
      "\tspeed: 0.0103s/iter; left time: 78.0445s\n",
      "\titers: 500, epoch: 7 | loss: 0.1808318\n",
      "\tspeed: 0.0103s/iter; left time: 76.9403s\n",
      "Epoch: 7 cost time: 6.1598474979400635\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1634188 Vali Loss: 0.0334270 Test Loss: 0.1127313\n",
      "Validation loss decreased (0.034068 --> 0.033427).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1337667\n",
      "\tspeed: 0.0381s/iter; left time: 278.7151s\n",
      "\titers: 200, epoch: 8 | loss: 0.1220730\n",
      "\tspeed: 0.0103s/iter; left time: 74.2404s\n",
      "\titers: 300, epoch: 8 | loss: 0.1137987\n",
      "\tspeed: 0.0103s/iter; left time: 73.1457s\n",
      "\titers: 400, epoch: 8 | loss: 0.1611990\n",
      "\tspeed: 0.0103s/iter; left time: 72.3715s\n",
      "\titers: 500, epoch: 8 | loss: 0.1009670\n",
      "\tspeed: 0.0103s/iter; left time: 70.9555s\n",
      "Epoch: 8 cost time: 6.242177248001099\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1629380 Vali Loss: 0.0332468 Test Loss: 0.1121480\n",
      "Validation loss decreased (0.033427 --> 0.033247).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1463883\n",
      "\tspeed: 0.0404s/iter; left time: 272.1419s\n",
      "\titers: 200, epoch: 9 | loss: 0.1533737\n",
      "\tspeed: 0.0105s/iter; left time: 69.4982s\n",
      "\titers: 300, epoch: 9 | loss: 0.1479795\n",
      "\tspeed: 0.0103s/iter; left time: 67.4730s\n",
      "\titers: 400, epoch: 9 | loss: 0.2016435\n",
      "\tspeed: 0.0103s/iter; left time: 66.4097s\n",
      "\titers: 500, epoch: 9 | loss: 0.1387911\n",
      "\tspeed: 0.0103s/iter; left time: 65.4533s\n",
      "Epoch: 9 cost time: 6.322438955307007\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1612467 Vali Loss: 0.0334340 Test Loss: 0.1120927\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2028267\n",
      "\tspeed: 0.0373s/iter; left time: 229.8721s\n",
      "\titers: 200, epoch: 10 | loss: 0.1325454\n",
      "\tspeed: 0.0116s/iter; left time: 70.3941s\n",
      "\titers: 300, epoch: 10 | loss: 0.1501399\n",
      "\tspeed: 0.0116s/iter; left time: 69.1704s\n",
      "\titers: 400, epoch: 10 | loss: 0.1044933\n",
      "\tspeed: 0.0116s/iter; left time: 68.0528s\n",
      "\titers: 500, epoch: 10 | loss: 0.1074002\n",
      "\tspeed: 0.0116s/iter; left time: 66.9036s\n",
      "Epoch: 10 cost time: 6.903021574020386\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1624086 Vali Loss: 0.0333549 Test Loss: 0.1120792\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1523762\n",
      "\tspeed: 0.0370s/iter; left time: 207.4273s\n",
      "\titers: 200, epoch: 11 | loss: 0.1533564\n",
      "\tspeed: 0.0104s/iter; left time: 56.9614s\n",
      "\titers: 300, epoch: 11 | loss: 0.1951946\n",
      "\tspeed: 0.0103s/iter; left time: 55.8297s\n",
      "\titers: 400, epoch: 11 | loss: 0.1608341\n",
      "\tspeed: 0.0103s/iter; left time: 54.7370s\n",
      "\titers: 500, epoch: 11 | loss: 0.1590894\n",
      "\tspeed: 0.0103s/iter; left time: 53.6618s\n",
      "Epoch: 11 cost time: 6.180590629577637\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1606826 Vali Loss: 0.0332173 Test Loss: 0.1120477\n",
      "Validation loss decreased (0.033247 --> 0.033217).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1488593\n",
      "\tspeed: 0.0369s/iter; left time: 185.4573s\n",
      "\titers: 200, epoch: 12 | loss: 0.1528861\n",
      "\tspeed: 0.0103s/iter; left time: 51.0015s\n",
      "\titers: 300, epoch: 12 | loss: 0.1186375\n",
      "\tspeed: 0.0104s/iter; left time: 50.0501s\n",
      "\titers: 400, epoch: 12 | loss: 0.2596448\n",
      "\tspeed: 0.0103s/iter; left time: 48.9312s\n",
      "\titers: 500, epoch: 12 | loss: 0.1406447\n",
      "\tspeed: 0.0104s/iter; left time: 47.9749s\n",
      "Epoch: 12 cost time: 6.193445920944214\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1614960 Vali Loss: 0.0332660 Test Loss: 0.1121420\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2797479\n",
      "\tspeed: 0.0376s/iter; left time: 167.5491s\n",
      "\titers: 200, epoch: 13 | loss: 0.2159488\n",
      "\tspeed: 0.0103s/iter; left time: 44.7488s\n",
      "\titers: 300, epoch: 13 | loss: 0.1232705\n",
      "\tspeed: 0.0104s/iter; left time: 44.5129s\n",
      "\titers: 400, epoch: 13 | loss: 0.1761489\n",
      "\tspeed: 0.0116s/iter; left time: 48.0704s\n",
      "\titers: 500, epoch: 13 | loss: 0.1888717\n",
      "\tspeed: 0.0116s/iter; left time: 46.9914s\n",
      "Epoch: 13 cost time: 6.527817964553833\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1612591 Vali Loss: 0.0332204 Test Loss: 0.1121158\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1898113\n",
      "\tspeed: 0.0368s/iter; left time: 143.1451s\n",
      "\titers: 200, epoch: 14 | loss: 0.1162388\n",
      "\tspeed: 0.0103s/iter; left time: 39.1596s\n",
      "\titers: 300, epoch: 14 | loss: 0.1605986\n",
      "\tspeed: 0.0103s/iter; left time: 38.0982s\n",
      "\titers: 400, epoch: 14 | loss: 0.1655512\n",
      "\tspeed: 0.0103s/iter; left time: 37.1168s\n",
      "\titers: 500, epoch: 14 | loss: 0.1636503\n",
      "\tspeed: 0.0106s/iter; left time: 37.0080s\n",
      "Epoch: 14 cost time: 6.282956123352051\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1613626 Vali Loss: 0.0332831 Test Loss: 0.1121289\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11220263689756393, mae:0.20432178676128387\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2665744\n",
      "\tspeed: 0.0221s/iter; left time: 249.3126s\n",
      "\titers: 200, epoch: 1 | loss: 0.1625039\n",
      "\tspeed: 0.0102s/iter; left time: 114.6148s\n",
      "\titers: 300, epoch: 1 | loss: 0.1715275\n",
      "\tspeed: 0.0102s/iter; left time: 113.7618s\n",
      "\titers: 400, epoch: 1 | loss: 0.2192196\n",
      "\tspeed: 0.0102s/iter; left time: 112.3898s\n",
      "\titers: 500, epoch: 1 | loss: 0.2436446\n",
      "\tspeed: 0.0102s/iter; left time: 111.6684s\n",
      "Epoch: 1 cost time: 7.059645891189575\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2679370 Vali Loss: 0.0518775 Test Loss: 0.1528189\n",
      "Validation loss decreased (inf --> 0.051878).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3995857\n",
      "\tspeed: 0.0363s/iter; left time: 389.7174s\n",
      "\titers: 200, epoch: 2 | loss: 0.3813215\n",
      "\tspeed: 0.0103s/iter; left time: 109.8620s\n",
      "\titers: 300, epoch: 2 | loss: 0.2108474\n",
      "\tspeed: 0.0104s/iter; left time: 109.1688s\n",
      "\titers: 400, epoch: 2 | loss: 0.3423056\n",
      "\tspeed: 0.0103s/iter; left time: 107.7693s\n",
      "\titers: 500, epoch: 2 | loss: 0.3010347\n",
      "\tspeed: 0.0103s/iter; left time: 106.5234s\n",
      "Epoch: 2 cost time: 6.186953067779541\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2640337 Vali Loss: 0.0516810 Test Loss: 0.1591369\n",
      "Validation loss decreased (0.051878 --> 0.051681).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1683755\n",
      "\tspeed: 0.0378s/iter; left time: 383.7513s\n",
      "\titers: 200, epoch: 3 | loss: 0.2236179\n",
      "\tspeed: 0.0102s/iter; left time: 103.0676s\n",
      "\titers: 300, epoch: 3 | loss: 0.2537792\n",
      "\tspeed: 0.0102s/iter; left time: 101.7648s\n",
      "\titers: 400, epoch: 3 | loss: 0.1891318\n",
      "\tspeed: 0.0103s/iter; left time: 101.2828s\n",
      "\titers: 500, epoch: 3 | loss: 0.1693430\n",
      "\tspeed: 0.0103s/iter; left time: 100.0649s\n",
      "Epoch: 3 cost time: 6.13559365272522\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2363307 Vali Loss: 0.0479477 Test Loss: 0.1425689\n",
      "Validation loss decreased (0.051681 --> 0.047948).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1491272\n",
      "\tspeed: 0.0360s/iter; left time: 345.6987s\n",
      "\titers: 200, epoch: 4 | loss: 0.1335760\n",
      "\tspeed: 0.0103s/iter; left time: 97.9722s\n",
      "\titers: 300, epoch: 4 | loss: 0.1788428\n",
      "\tspeed: 0.0103s/iter; left time: 96.6884s\n",
      "\titers: 400, epoch: 4 | loss: 0.2666694\n",
      "\tspeed: 0.0103s/iter; left time: 95.5644s\n",
      "\titers: 500, epoch: 4 | loss: 0.1612497\n",
      "\tspeed: 0.0103s/iter; left time: 94.4866s\n",
      "Epoch: 4 cost time: 6.171074390411377\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2194571 Vali Loss: 0.0480028 Test Loss: 0.1399659\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1819476\n",
      "\tspeed: 0.0380s/iter; left time: 342.4094s\n",
      "\titers: 200, epoch: 5 | loss: 0.1414169\n",
      "\tspeed: 0.0102s/iter; left time: 91.3245s\n",
      "\titers: 300, epoch: 5 | loss: 0.2882839\n",
      "\tspeed: 0.0103s/iter; left time: 90.5997s\n",
      "\titers: 400, epoch: 5 | loss: 0.1805757\n",
      "\tspeed: 0.0102s/iter; left time: 89.2999s\n",
      "\titers: 500, epoch: 5 | loss: 0.3003975\n",
      "\tspeed: 0.0102s/iter; left time: 88.2836s\n",
      "Epoch: 5 cost time: 6.185537338256836\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2111736 Vali Loss: 0.0444598 Test Loss: 0.1373426\n",
      "Validation loss decreased (0.047948 --> 0.044460).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1481559\n",
      "\tspeed: 0.0370s/iter; left time: 312.7998s\n",
      "\titers: 200, epoch: 6 | loss: 0.2157323\n",
      "\tspeed: 0.0103s/iter; left time: 85.7641s\n",
      "\titers: 300, epoch: 6 | loss: 0.2078682\n",
      "\tspeed: 0.0103s/iter; left time: 84.8313s\n",
      "\titers: 400, epoch: 6 | loss: 0.1530166\n",
      "\tspeed: 0.0103s/iter; left time: 83.7108s\n",
      "\titers: 500, epoch: 6 | loss: 0.1808219\n",
      "\tspeed: 0.0103s/iter; left time: 82.7146s\n",
      "Epoch: 6 cost time: 6.147995948791504\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2026788 Vali Loss: 0.0443465 Test Loss: 0.1363007\n",
      "Validation loss decreased (0.044460 --> 0.044347).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2109874\n",
      "\tspeed: 0.0356s/iter; left time: 280.8406s\n",
      "\titers: 200, epoch: 7 | loss: 0.1694206\n",
      "\tspeed: 0.0103s/iter; left time: 79.9159s\n",
      "\titers: 300, epoch: 7 | loss: 0.1643476\n",
      "\tspeed: 0.0103s/iter; left time: 78.8259s\n",
      "\titers: 400, epoch: 7 | loss: 0.1253503\n",
      "\tspeed: 0.0103s/iter; left time: 77.9279s\n",
      "\titers: 500, epoch: 7 | loss: 0.2321343\n",
      "\tspeed: 0.0117s/iter; left time: 87.1845s\n",
      "Epoch: 7 cost time: 6.3664140701293945\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2003247 Vali Loss: 0.0449832 Test Loss: 0.1369952\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1437660\n",
      "\tspeed: 0.0389s/iter; left time: 284.6403s\n",
      "\titers: 200, epoch: 8 | loss: 0.1740367\n",
      "\tspeed: 0.0117s/iter; left time: 84.6288s\n",
      "\titers: 300, epoch: 8 | loss: 0.2138445\n",
      "\tspeed: 0.0118s/iter; left time: 83.8251s\n",
      "\titers: 400, epoch: 8 | loss: 0.2274852\n",
      "\tspeed: 0.0118s/iter; left time: 82.5468s\n",
      "\titers: 500, epoch: 8 | loss: 0.2640571\n",
      "\tspeed: 0.0117s/iter; left time: 81.2022s\n",
      "Epoch: 8 cost time: 6.819082260131836\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1992223 Vali Loss: 0.0450322 Test Loss: 0.1366178\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1558319\n",
      "\tspeed: 0.0361s/iter; left time: 243.2239s\n",
      "\titers: 200, epoch: 9 | loss: 0.2155426\n",
      "\tspeed: 0.0103s/iter; left time: 68.1510s\n",
      "\titers: 300, epoch: 9 | loss: 0.1660830\n",
      "\tspeed: 0.0102s/iter; left time: 66.9928s\n",
      "\titers: 400, epoch: 9 | loss: 0.1353658\n",
      "\tspeed: 0.0103s/iter; left time: 66.2040s\n",
      "\titers: 500, epoch: 9 | loss: 0.2189843\n",
      "\tspeed: 0.0115s/iter; left time: 73.1943s\n",
      "Epoch: 9 cost time: 6.383540153503418\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1988193 Vali Loss: 0.0453159 Test Loss: 0.1367790\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1364724338054657, mae:0.23290593922138214\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3146958\n",
      "\tspeed: 0.0242s/iter; left time: 273.4582s\n",
      "\titers: 200, epoch: 1 | loss: 0.2838693\n",
      "\tspeed: 0.0127s/iter; left time: 142.4738s\n",
      "\titers: 300, epoch: 1 | loss: 0.1965312\n",
      "\tspeed: 0.0126s/iter; left time: 140.1263s\n",
      "\titers: 400, epoch: 1 | loss: 0.1757296\n",
      "\tspeed: 0.0118s/iter; left time: 129.3803s\n",
      "\titers: 500, epoch: 1 | loss: 0.2426057\n",
      "\tspeed: 0.0118s/iter; left time: 128.2061s\n",
      "Epoch: 1 cost time: 8.176191806793213\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3048955 Vali Loss: 0.0569524 Test Loss: 0.1670164\n",
      "Validation loss decreased (inf --> 0.056952).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3564685\n",
      "\tspeed: 0.0403s/iter; left time: 432.7631s\n",
      "\titers: 200, epoch: 2 | loss: 0.2490194\n",
      "\tspeed: 0.0118s/iter; left time: 125.8648s\n",
      "\titers: 300, epoch: 2 | loss: 0.2912205\n",
      "\tspeed: 0.0118s/iter; left time: 124.4098s\n",
      "\titers: 400, epoch: 2 | loss: 0.2075615\n",
      "\tspeed: 0.0118s/iter; left time: 123.3239s\n",
      "\titers: 500, epoch: 2 | loss: 0.3259264\n",
      "\tspeed: 0.0118s/iter; left time: 121.9133s\n",
      "Epoch: 2 cost time: 7.056912660598755\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3621097 Vali Loss: 0.0655616 Test Loss: 0.2010690\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.4145622\n",
      "\tspeed: 0.0392s/iter; left time: 397.9353s\n",
      "\titers: 200, epoch: 3 | loss: 0.4263114\n",
      "\tspeed: 0.0119s/iter; left time: 119.6298s\n",
      "\titers: 300, epoch: 3 | loss: 0.1655468\n",
      "\tspeed: 0.0119s/iter; left time: 118.1937s\n",
      "\titers: 400, epoch: 3 | loss: 0.3101531\n",
      "\tspeed: 0.0119s/iter; left time: 116.9034s\n",
      "\titers: 500, epoch: 3 | loss: 0.2383604\n",
      "\tspeed: 0.0119s/iter; left time: 115.7429s\n",
      "Epoch: 3 cost time: 7.07227087020874\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3143142 Vali Loss: 0.0663429 Test Loss: 0.1853718\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2161061\n",
      "\tspeed: 0.0381s/iter; left time: 365.8391s\n",
      "\titers: 200, epoch: 4 | loss: 0.3722821\n",
      "\tspeed: 0.0119s/iter; left time: 112.5736s\n",
      "\titers: 300, epoch: 4 | loss: 0.2015136\n",
      "\tspeed: 0.0118s/iter; left time: 111.1767s\n",
      "\titers: 400, epoch: 4 | loss: 0.1925986\n",
      "\tspeed: 0.0119s/iter; left time: 110.1929s\n",
      "\titers: 500, epoch: 4 | loss: 0.2151652\n",
      "\tspeed: 0.0119s/iter; left time: 109.0253s\n",
      "Epoch: 4 cost time: 7.038574695587158\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2983768 Vali Loss: 0.0602570 Test Loss: 0.1767666\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16728396713733673, mae:0.26014572381973267\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1621534\n",
      "\tspeed: 0.0234s/iter; left time: 264.6818s\n",
      "\titers: 200, epoch: 1 | loss: 0.4235111\n",
      "\tspeed: 0.0114s/iter; left time: 127.9585s\n",
      "\titers: 300, epoch: 1 | loss: 0.2352856\n",
      "\tspeed: 0.0114s/iter; left time: 127.0495s\n",
      "\titers: 400, epoch: 1 | loss: 0.4525290\n",
      "\tspeed: 0.0114s/iter; left time: 125.6618s\n",
      "\titers: 500, epoch: 1 | loss: 0.2929428\n",
      "\tspeed: 0.0114s/iter; left time: 124.6577s\n",
      "Epoch: 1 cost time: 7.771124601364136\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3089340 Vali Loss: 0.0703744 Test Loss: 0.2047924\n",
      "Validation loss decreased (inf --> 0.070374).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2462121\n",
      "\tspeed: 0.0386s/iter; left time: 413.7616s\n",
      "\titers: 200, epoch: 2 | loss: 0.1961503\n",
      "\tspeed: 0.0114s/iter; left time: 121.5595s\n",
      "\titers: 300, epoch: 2 | loss: 0.1915278\n",
      "\tspeed: 0.0114s/iter; left time: 120.5134s\n",
      "\titers: 400, epoch: 2 | loss: 0.3650100\n",
      "\tspeed: 0.0114s/iter; left time: 119.2229s\n",
      "\titers: 500, epoch: 2 | loss: 0.5563440\n",
      "\tspeed: 0.0114s/iter; left time: 117.9995s\n",
      "Epoch: 2 cost time: 6.828376531600952\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3197718 Vali Loss: 0.0647909 Test Loss: 0.1861613\n",
      "Validation loss decreased (0.070374 --> 0.064791).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3419395\n",
      "\tspeed: 0.0407s/iter; left time: 413.1509s\n",
      "\titers: 200, epoch: 3 | loss: 0.1741049\n",
      "\tspeed: 0.0117s/iter; left time: 117.6734s\n",
      "\titers: 300, epoch: 3 | loss: 0.3266390\n",
      "\tspeed: 0.0115s/iter; left time: 114.6748s\n",
      "\titers: 400, epoch: 3 | loss: 0.2794829\n",
      "\tspeed: 0.0115s/iter; left time: 113.4718s\n",
      "\titers: 500, epoch: 3 | loss: 0.4134959\n",
      "\tspeed: 0.0115s/iter; left time: 112.2739s\n",
      "Epoch: 3 cost time: 6.994849920272827\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3013770 Vali Loss: 0.0672429 Test Loss: 0.1973960\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2597378\n",
      "\tspeed: 0.0384s/iter; left time: 367.9706s\n",
      "\titers: 200, epoch: 4 | loss: 0.1957911\n",
      "\tspeed: 0.0115s/iter; left time: 109.1189s\n",
      "\titers: 300, epoch: 4 | loss: 0.2042681\n",
      "\tspeed: 0.0115s/iter; left time: 107.9369s\n",
      "\titers: 400, epoch: 4 | loss: 0.2713029\n",
      "\tspeed: 0.0115s/iter; left time: 106.7900s\n",
      "\titers: 500, epoch: 4 | loss: 0.2831668\n",
      "\tspeed: 0.0114s/iter; left time: 105.1600s\n",
      "Epoch: 4 cost time: 6.843238592147827\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.3059611 Vali Loss: 0.0696376 Test Loss: 0.1842965\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.3049182\n",
      "\tspeed: 0.0395s/iter; left time: 356.1554s\n",
      "\titers: 200, epoch: 5 | loss: 0.2351806\n",
      "\tspeed: 0.0116s/iter; left time: 103.5231s\n",
      "\titers: 300, epoch: 5 | loss: 0.4495172\n",
      "\tspeed: 0.0114s/iter; left time: 100.7109s\n",
      "\titers: 400, epoch: 5 | loss: 0.2376069\n",
      "\tspeed: 0.0114s/iter; left time: 99.6074s\n",
      "\titers: 500, epoch: 5 | loss: 0.2409071\n",
      "\tspeed: 0.0114s/iter; left time: 98.4990s\n",
      "Epoch: 5 cost time: 6.929037570953369\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2914947 Vali Loss: 0.0645789 Test Loss: 0.1771692\n",
      "Validation loss decreased (0.064791 --> 0.064579).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3137941\n",
      "\tspeed: 0.0386s/iter; left time: 326.4873s\n",
      "\titers: 200, epoch: 6 | loss: 0.3051760\n",
      "\tspeed: 0.0114s/iter; left time: 95.5179s\n",
      "\titers: 300, epoch: 6 | loss: 0.1748664\n",
      "\tspeed: 0.0115s/iter; left time: 94.5557s\n",
      "\titers: 400, epoch: 6 | loss: 0.2137271\n",
      "\tspeed: 0.0114s/iter; left time: 93.2933s\n",
      "\titers: 500, epoch: 6 | loss: 0.2155635\n",
      "\tspeed: 0.0114s/iter; left time: 92.0205s\n",
      "Epoch: 6 cost time: 6.855357646942139\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2856273 Vali Loss: 0.0634681 Test Loss: 0.1763937\n",
      "Validation loss decreased (0.064579 --> 0.063468).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2200189\n",
      "\tspeed: 0.0393s/iter; left time: 309.6146s\n",
      "\titers: 200, epoch: 7 | loss: 0.3610333\n",
      "\tspeed: 0.0118s/iter; left time: 92.1319s\n",
      "\titers: 300, epoch: 7 | loss: 0.2429709\n",
      "\tspeed: 0.0115s/iter; left time: 88.0622s\n",
      "\titers: 400, epoch: 7 | loss: 0.2551625\n",
      "\tspeed: 0.0115s/iter; left time: 87.0207s\n",
      "\titers: 500, epoch: 7 | loss: 0.2693877\n",
      "\tspeed: 0.0115s/iter; left time: 85.8250s\n",
      "Epoch: 7 cost time: 6.9770965576171875\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2844154 Vali Loss: 0.0636506 Test Loss: 0.1754195\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1919257\n",
      "\tspeed: 0.0372s/iter; left time: 272.1269s\n",
      "\titers: 200, epoch: 8 | loss: 0.2944022\n",
      "\tspeed: 0.0114s/iter; left time: 82.2368s\n",
      "\titers: 300, epoch: 8 | loss: 0.2542833\n",
      "\tspeed: 0.0114s/iter; left time: 81.1956s\n",
      "\titers: 400, epoch: 8 | loss: 0.2601533\n",
      "\tspeed: 0.0114s/iter; left time: 80.0300s\n",
      "\titers: 500, epoch: 8 | loss: 0.2570642\n",
      "\tspeed: 0.0114s/iter; left time: 78.7942s\n",
      "Epoch: 8 cost time: 6.788907766342163\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2814559 Vali Loss: 0.0634190 Test Loss: 0.1761353\n",
      "Validation loss decreased (0.063468 --> 0.063419).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3800250\n",
      "\tspeed: 0.0383s/iter; left time: 258.1400s\n",
      "\titers: 200, epoch: 9 | loss: 0.1612675\n",
      "\tspeed: 0.0115s/iter; left time: 76.4463s\n",
      "\titers: 300, epoch: 9 | loss: 0.3236064\n",
      "\tspeed: 0.0115s/iter; left time: 75.2265s\n",
      "\titers: 400, epoch: 9 | loss: 0.3164397\n",
      "\tspeed: 0.0115s/iter; left time: 73.9621s\n",
      "\titers: 500, epoch: 9 | loss: 0.3997166\n",
      "\tspeed: 0.0115s/iter; left time: 72.8525s\n",
      "Epoch: 9 cost time: 6.874089956283569\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2817927 Vali Loss: 0.0627447 Test Loss: 0.1758721\n",
      "Validation loss decreased (0.063419 --> 0.062745).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2104461\n",
      "\tspeed: 0.0400s/iter; left time: 246.9523s\n",
      "\titers: 200, epoch: 10 | loss: 0.2363057\n",
      "\tspeed: 0.0114s/iter; left time: 69.3621s\n",
      "\titers: 300, epoch: 10 | loss: 0.2046684\n",
      "\tspeed: 0.0114s/iter; left time: 68.1243s\n",
      "\titers: 400, epoch: 10 | loss: 0.2702019\n",
      "\tspeed: 0.0114s/iter; left time: 66.9980s\n",
      "\titers: 500, epoch: 10 | loss: 0.2903891\n",
      "\tspeed: 0.0114s/iter; left time: 65.7189s\n",
      "Epoch: 10 cost time: 6.789546966552734\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2824371 Vali Loss: 0.0631095 Test Loss: 0.1755162\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1698792\n",
      "\tspeed: 0.0397s/iter; left time: 222.1401s\n",
      "\titers: 200, epoch: 11 | loss: 0.2862641\n",
      "\tspeed: 0.0114s/iter; left time: 62.7640s\n",
      "\titers: 300, epoch: 11 | loss: 0.2626987\n",
      "\tspeed: 0.0114s/iter; left time: 61.6441s\n",
      "\titers: 400, epoch: 11 | loss: 0.3237939\n",
      "\tspeed: 0.0114s/iter; left time: 60.5997s\n",
      "\titers: 500, epoch: 11 | loss: 0.4603405\n",
      "\tspeed: 0.0114s/iter; left time: 59.2909s\n",
      "Epoch: 11 cost time: 6.833332777023315\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2805423 Vali Loss: 0.0631211 Test Loss: 0.1755228\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2366856\n",
      "\tspeed: 0.0401s/iter; left time: 201.7135s\n",
      "\titers: 200, epoch: 12 | loss: 0.2605909\n",
      "\tspeed: 0.0114s/iter; left time: 56.4373s\n",
      "\titers: 300, epoch: 12 | loss: 0.1787814\n",
      "\tspeed: 0.0115s/iter; left time: 55.3514s\n",
      "\titers: 400, epoch: 12 | loss: 0.3991050\n",
      "\tspeed: 0.0115s/iter; left time: 54.2596s\n",
      "\titers: 500, epoch: 12 | loss: 0.2452077\n",
      "\tspeed: 0.0114s/iter; left time: 52.9997s\n",
      "Epoch: 12 cost time: 6.804385423660278\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2828113 Vali Loss: 0.0631685 Test Loss: 0.1755223\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17613042891025543, mae:0.26716727018356323\n",
      ">>> LR=1e-3,DO=0.2,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3310875\n",
      "\tspeed: 0.0242s/iter; left time: 273.8603s\n",
      "\titers: 200, epoch: 1 | loss: 0.3205411\n",
      "\tspeed: 0.0118s/iter; left time: 131.9012s\n",
      "\titers: 300, epoch: 1 | loss: 0.4598767\n",
      "\tspeed: 0.0109s/iter; left time: 120.6346s\n",
      "\titers: 400, epoch: 1 | loss: 0.3615898\n",
      "\tspeed: 0.0109s/iter; left time: 119.8226s\n",
      "\titers: 500, epoch: 1 | loss: 0.2669631\n",
      "\tspeed: 0.0109s/iter; left time: 118.6929s\n",
      "Epoch: 1 cost time: 7.677866220474243\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3139204 Vali Loss: 0.0577420 Test Loss: 0.1718128\n",
      "Validation loss decreased (inf --> 0.057742).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2738406\n",
      "\tspeed: 0.0376s/iter; left time: 403.8103s\n",
      "\titers: 200, epoch: 2 | loss: 0.3333329\n",
      "\tspeed: 0.0109s/iter; left time: 116.1550s\n",
      "\titers: 300, epoch: 2 | loss: 0.1958640\n",
      "\tspeed: 0.0109s/iter; left time: 115.1513s\n",
      "\titers: 400, epoch: 2 | loss: 0.2421278\n",
      "\tspeed: 0.0109s/iter; left time: 114.2103s\n",
      "\titers: 500, epoch: 2 | loss: 0.2657471\n",
      "\tspeed: 0.0109s/iter; left time: 112.7485s\n",
      "Epoch: 2 cost time: 6.545705795288086\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3191139 Vali Loss: 0.0683331 Test Loss: 0.1963060\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1974169\n",
      "\tspeed: 0.0376s/iter; left time: 382.4663s\n",
      "\titers: 200, epoch: 3 | loss: 0.2441244\n",
      "\tspeed: 0.0109s/iter; left time: 109.3179s\n",
      "\titers: 300, epoch: 3 | loss: 0.3068202\n",
      "\tspeed: 0.0109s/iter; left time: 108.3313s\n",
      "\titers: 400, epoch: 3 | loss: 0.3356909\n",
      "\tspeed: 0.0109s/iter; left time: 107.4054s\n",
      "\titers: 500, epoch: 3 | loss: 0.3248774\n",
      "\tspeed: 0.0109s/iter; left time: 106.0497s\n",
      "Epoch: 3 cost time: 6.51710844039917\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2982767 Vali Loss: 0.0630827 Test Loss: 0.1717639\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2849285\n",
      "\tspeed: 0.0367s/iter; left time: 351.6023s\n",
      "\titers: 200, epoch: 4 | loss: 0.1341906\n",
      "\tspeed: 0.0109s/iter; left time: 103.7643s\n",
      "\titers: 300, epoch: 4 | loss: 0.3374710\n",
      "\tspeed: 0.0109s/iter; left time: 102.5677s\n",
      "\titers: 400, epoch: 4 | loss: 0.4772728\n",
      "\tspeed: 0.0109s/iter; left time: 101.2983s\n",
      "\titers: 500, epoch: 4 | loss: 0.3229633\n",
      "\tspeed: 0.0109s/iter; left time: 100.4007s\n",
      "Epoch: 4 cost time: 6.5179595947265625\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2769645 Vali Loss: 0.0623952 Test Loss: 0.1707577\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17203237116336823, mae:0.27029332518577576\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2435709\n",
      "\tspeed: 0.0261s/iter; left time: 295.0062s\n",
      "\titers: 200, epoch: 1 | loss: 0.1911342\n",
      "\tspeed: 0.0145s/iter; left time: 162.0989s\n",
      "\titers: 300, epoch: 1 | loss: 0.3567736\n",
      "\tspeed: 0.0144s/iter; left time: 160.3979s\n",
      "\titers: 400, epoch: 1 | loss: 0.2810073\n",
      "\tspeed: 0.0144s/iter; left time: 158.8049s\n",
      "\titers: 500, epoch: 1 | loss: 0.2252716\n",
      "\tspeed: 0.0144s/iter; left time: 157.2939s\n",
      "Epoch: 1 cost time: 9.455031871795654\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2713187 Vali Loss: 0.0512629 Test Loss: 0.1473980\n",
      "Validation loss decreased (inf --> 0.051263).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3252781\n",
      "\tspeed: 0.0463s/iter; left time: 497.3670s\n",
      "\titers: 200, epoch: 2 | loss: 0.2741295\n",
      "\tspeed: 0.0145s/iter; left time: 153.8609s\n",
      "\titers: 300, epoch: 2 | loss: 0.2366938\n",
      "\tspeed: 0.0145s/iter; left time: 152.3350s\n",
      "\titers: 400, epoch: 2 | loss: 0.1830042\n",
      "\tspeed: 0.0145s/iter; left time: 151.1744s\n",
      "\titers: 500, epoch: 2 | loss: 0.3432406\n",
      "\tspeed: 0.0145s/iter; left time: 149.3079s\n",
      "Epoch: 2 cost time: 8.576072931289673\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2852060 Vali Loss: 0.0700450 Test Loss: 0.2050198\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.5130218\n",
      "\tspeed: 0.0457s/iter; left time: 464.2633s\n",
      "\titers: 200, epoch: 3 | loss: 0.3435275\n",
      "\tspeed: 0.0145s/iter; left time: 146.3541s\n",
      "\titers: 300, epoch: 3 | loss: 0.1621601\n",
      "\tspeed: 0.0145s/iter; left time: 144.4962s\n",
      "\titers: 400, epoch: 3 | loss: 0.2179836\n",
      "\tspeed: 0.0145s/iter; left time: 142.8208s\n",
      "\titers: 500, epoch: 3 | loss: 0.2560741\n",
      "\tspeed: 0.0145s/iter; left time: 141.4166s\n",
      "Epoch: 3 cost time: 8.536221027374268\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3024391 Vali Loss: 0.0615400 Test Loss: 0.1726220\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2690701\n",
      "\tspeed: 0.0457s/iter; left time: 438.0855s\n",
      "\titers: 200, epoch: 4 | loss: 0.2230475\n",
      "\tspeed: 0.0144s/iter; left time: 136.7752s\n",
      "\titers: 300, epoch: 4 | loss: 0.2580901\n",
      "\tspeed: 0.0144s/iter; left time: 135.4510s\n",
      "\titers: 400, epoch: 4 | loss: 0.2845585\n",
      "\tspeed: 0.0144s/iter; left time: 134.0913s\n",
      "\titers: 500, epoch: 4 | loss: 0.2436915\n",
      "\tspeed: 0.0144s/iter; left time: 132.5368s\n",
      "Epoch: 4 cost time: 8.505971908569336\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2748295 Vali Loss: 0.0567464 Test Loss: 0.1649906\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14764295518398285, mae:0.2447245717048645\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3579422\n",
      "\tspeed: 0.0274s/iter; left time: 310.0821s\n",
      "\titers: 200, epoch: 1 | loss: 0.3188561\n",
      "\tspeed: 0.0151s/iter; left time: 169.5132s\n",
      "\titers: 300, epoch: 1 | loss: 0.1412449\n",
      "\tspeed: 0.0160s/iter; left time: 177.7719s\n",
      "\titers: 400, epoch: 1 | loss: 0.3408230\n",
      "\tspeed: 0.0160s/iter; left time: 176.4226s\n",
      "\titers: 500, epoch: 1 | loss: 0.2369466\n",
      "\tspeed: 0.0161s/iter; left time: 175.3783s\n",
      "Epoch: 1 cost time: 10.247586965560913\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2422368 Vali Loss: 0.0484057 Test Loss: 0.1495427\n",
      "Validation loss decreased (inf --> 0.048406).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1942520\n",
      "\tspeed: 0.0499s/iter; left time: 535.7525s\n",
      "\titers: 200, epoch: 2 | loss: 0.2967184\n",
      "\tspeed: 0.0161s/iter; left time: 171.0242s\n",
      "\titers: 300, epoch: 2 | loss: 0.2182838\n",
      "\tspeed: 0.0161s/iter; left time: 169.2719s\n",
      "\titers: 400, epoch: 2 | loss: 0.2311323\n",
      "\tspeed: 0.0160s/iter; left time: 167.3987s\n",
      "\titers: 500, epoch: 2 | loss: 0.2057789\n",
      "\tspeed: 0.0160s/iter; left time: 165.7068s\n",
      "Epoch: 2 cost time: 9.442594289779663\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2377276 Vali Loss: 0.0728795 Test Loss: 0.2161092\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3237036\n",
      "\tspeed: 0.0459s/iter; left time: 466.8108s\n",
      "\titers: 200, epoch: 3 | loss: 0.1564135\n",
      "\tspeed: 0.0143s/iter; left time: 143.4069s\n",
      "\titers: 300, epoch: 3 | loss: 0.3661249\n",
      "\tspeed: 0.0142s/iter; left time: 141.7708s\n",
      "\titers: 400, epoch: 3 | loss: 0.1727637\n",
      "\tspeed: 0.0142s/iter; left time: 140.0997s\n",
      "\titers: 500, epoch: 3 | loss: 0.2421882\n",
      "\tspeed: 0.0142s/iter; left time: 139.0593s\n",
      "Epoch: 3 cost time: 8.394636631011963\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2786699 Vali Loss: 0.0570485 Test Loss: 0.1763236\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2777259\n",
      "\tspeed: 0.0452s/iter; left time: 433.1026s\n",
      "\titers: 200, epoch: 4 | loss: 0.2660138\n",
      "\tspeed: 0.0144s/iter; left time: 136.2677s\n",
      "\titers: 300, epoch: 4 | loss: 0.3011681\n",
      "\tspeed: 0.0143s/iter; left time: 134.5965s\n",
      "\titers: 400, epoch: 4 | loss: 0.1803908\n",
      "\tspeed: 0.0143s/iter; left time: 133.1639s\n",
      "\titers: 500, epoch: 4 | loss: 0.1449819\n",
      "\tspeed: 0.0143s/iter; left time: 131.8775s\n",
      "Epoch: 4 cost time: 8.459372997283936\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2481087 Vali Loss: 0.0525879 Test Loss: 0.1558835\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.149746373295784, mae:0.24280180037021637\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1354365\n",
      "\tspeed: 0.0259s/iter; left time: 292.9644s\n",
      "\titers: 200, epoch: 1 | loss: 0.3682787\n",
      "\tspeed: 0.0143s/iter; left time: 159.9192s\n",
      "\titers: 300, epoch: 1 | loss: 0.2754350\n",
      "\tspeed: 0.0142s/iter; left time: 158.1848s\n",
      "\titers: 400, epoch: 1 | loss: 0.2626590\n",
      "\tspeed: 0.0143s/iter; left time: 156.9163s\n",
      "\titers: 500, epoch: 1 | loss: 0.2970988\n",
      "\tspeed: 0.0143s/iter; left time: 155.4790s\n",
      "Epoch: 1 cost time: 9.34536099433899\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2840688 Vali Loss: 0.0640136 Test Loss: 0.1812901\n",
      "Validation loss decreased (inf --> 0.064014).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2767709\n",
      "\tspeed: 0.0469s/iter; left time: 503.2788s\n",
      "\titers: 200, epoch: 2 | loss: 0.1961362\n",
      "\tspeed: 0.0142s/iter; left time: 150.6724s\n",
      "\titers: 300, epoch: 2 | loss: 0.1926715\n",
      "\tspeed: 0.0142s/iter; left time: 149.9131s\n",
      "\titers: 400, epoch: 2 | loss: 0.2650085\n",
      "\tspeed: 0.0142s/iter; left time: 148.2225s\n",
      "\titers: 500, epoch: 2 | loss: 0.3407041\n",
      "\tspeed: 0.0142s/iter; left time: 146.2601s\n",
      "Epoch: 2 cost time: 8.407214164733887\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2870881 Vali Loss: 0.0582459 Test Loss: 0.1693398\n",
      "Validation loss decreased (0.064014 --> 0.058246).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2637312\n",
      "\tspeed: 0.0459s/iter; left time: 465.9714s\n",
      "\titers: 200, epoch: 3 | loss: 0.3161782\n",
      "\tspeed: 0.0143s/iter; left time: 143.9818s\n",
      "\titers: 300, epoch: 3 | loss: 0.2022959\n",
      "\tspeed: 0.0143s/iter; left time: 142.4436s\n",
      "\titers: 400, epoch: 3 | loss: 0.2054545\n",
      "\tspeed: 0.0142s/iter; left time: 140.5093s\n",
      "\titers: 500, epoch: 3 | loss: 0.2705434\n",
      "\tspeed: 0.0143s/iter; left time: 139.4784s\n",
      "Epoch: 3 cost time: 8.445943117141724\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2598173 Vali Loss: 0.0578059 Test Loss: 0.1630261\n",
      "Validation loss decreased (0.058246 --> 0.057806).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2894474\n",
      "\tspeed: 0.0453s/iter; left time: 434.7688s\n",
      "\titers: 200, epoch: 4 | loss: 0.2260659\n",
      "\tspeed: 0.0143s/iter; left time: 135.8088s\n",
      "\titers: 300, epoch: 4 | loss: 0.2424885\n",
      "\tspeed: 0.0143s/iter; left time: 134.3188s\n",
      "\titers: 400, epoch: 4 | loss: 0.2020031\n",
      "\tspeed: 0.0143s/iter; left time: 132.6442s\n",
      "\titers: 500, epoch: 4 | loss: 0.1781809\n",
      "\tspeed: 0.0143s/iter; left time: 131.1891s\n",
      "Epoch: 4 cost time: 8.451119661331177\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2434683 Vali Loss: 0.0540485 Test Loss: 0.1607744\n",
      "Validation loss decreased (0.057806 --> 0.054049).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2027901\n",
      "\tspeed: 0.0462s/iter; left time: 416.3939s\n",
      "\titers: 200, epoch: 5 | loss: 0.3091655\n",
      "\tspeed: 0.0143s/iter; left time: 127.1686s\n",
      "\titers: 300, epoch: 5 | loss: 0.1973173\n",
      "\tspeed: 0.0142s/iter; left time: 125.2724s\n",
      "\titers: 400, epoch: 5 | loss: 0.3333797\n",
      "\tspeed: 0.0142s/iter; left time: 123.7756s\n",
      "\titers: 500, epoch: 5 | loss: 0.2070854\n",
      "\tspeed: 0.0142s/iter; left time: 122.3613s\n",
      "Epoch: 5 cost time: 8.393794298171997\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2325874 Vali Loss: 0.0519195 Test Loss: 0.1577916\n",
      "Validation loss decreased (0.054049 --> 0.051919).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2825751\n",
      "\tspeed: 0.0457s/iter; left time: 386.2805s\n",
      "\titers: 200, epoch: 6 | loss: 0.2050829\n",
      "\tspeed: 0.0143s/iter; left time: 119.1941s\n",
      "\titers: 300, epoch: 6 | loss: 0.1434190\n",
      "\tspeed: 0.0143s/iter; left time: 117.7702s\n",
      "\titers: 400, epoch: 6 | loss: 0.2691714\n",
      "\tspeed: 0.0143s/iter; left time: 116.3568s\n",
      "\titers: 500, epoch: 6 | loss: 0.1810805\n",
      "\tspeed: 0.0143s/iter; left time: 114.8823s\n",
      "Epoch: 6 cost time: 8.42814564704895\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2260295 Vali Loss: 0.0501509 Test Loss: 0.1582185\n",
      "Validation loss decreased (0.051919 --> 0.050151).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3239396\n",
      "\tspeed: 0.0460s/iter; left time: 362.1967s\n",
      "\titers: 200, epoch: 7 | loss: 0.1413668\n",
      "\tspeed: 0.0142s/iter; left time: 110.8559s\n",
      "\titers: 300, epoch: 7 | loss: 0.1768094\n",
      "\tspeed: 0.0142s/iter; left time: 109.4038s\n",
      "\titers: 400, epoch: 7 | loss: 0.3414020\n",
      "\tspeed: 0.0142s/iter; left time: 107.9637s\n",
      "\titers: 500, epoch: 7 | loss: 0.2191450\n",
      "\tspeed: 0.0142s/iter; left time: 106.5354s\n",
      "Epoch: 7 cost time: 8.431467771530151\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2243734 Vali Loss: 0.0499334 Test Loss: 0.1556317\n",
      "Validation loss decreased (0.050151 --> 0.049933).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1709580\n",
      "\tspeed: 0.0490s/iter; left time: 357.9293s\n",
      "\titers: 200, epoch: 8 | loss: 0.2004823\n",
      "\tspeed: 0.0161s/iter; left time: 116.0104s\n",
      "\titers: 300, epoch: 8 | loss: 0.2172453\n",
      "\tspeed: 0.0161s/iter; left time: 114.3865s\n",
      "\titers: 400, epoch: 8 | loss: 0.1888614\n",
      "\tspeed: 0.0161s/iter; left time: 112.6980s\n",
      "\titers: 500, epoch: 8 | loss: 0.1446288\n",
      "\tspeed: 0.0161s/iter; left time: 111.1555s\n",
      "Epoch: 8 cost time: 9.511318683624268\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2232421 Vali Loss: 0.0503342 Test Loss: 0.1561412\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2895650\n",
      "\tspeed: 0.0490s/iter; left time: 330.0108s\n",
      "\titers: 200, epoch: 9 | loss: 0.1684789\n",
      "\tspeed: 0.0142s/iter; left time: 94.3206s\n",
      "\titers: 300, epoch: 9 | loss: 0.1635505\n",
      "\tspeed: 0.0142s/iter; left time: 92.8680s\n",
      "\titers: 400, epoch: 9 | loss: 0.1515073\n",
      "\tspeed: 0.0142s/iter; left time: 91.2289s\n",
      "\titers: 500, epoch: 9 | loss: 0.2116920\n",
      "\tspeed: 0.0142s/iter; left time: 89.8840s\n",
      "Epoch: 9 cost time: 8.374700546264648\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2210441 Vali Loss: 0.0499185 Test Loss: 0.1554581\n",
      "Validation loss decreased (0.049933 --> 0.049918).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2955706\n",
      "\tspeed: 0.0464s/iter; left time: 286.5725s\n",
      "\titers: 200, epoch: 10 | loss: 0.2255763\n",
      "\tspeed: 0.0142s/iter; left time: 85.9365s\n",
      "\titers: 300, epoch: 10 | loss: 0.1939872\n",
      "\tspeed: 0.0142s/iter; left time: 84.6102s\n",
      "\titers: 400, epoch: 10 | loss: 0.2501310\n",
      "\tspeed: 0.0142s/iter; left time: 83.2627s\n",
      "\titers: 500, epoch: 10 | loss: 0.2223947\n",
      "\tspeed: 0.0142s/iter; left time: 81.7398s\n",
      "Epoch: 10 cost time: 8.37479043006897\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2197289 Vali Loss: 0.0500757 Test Loss: 0.1554331\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2479482\n",
      "\tspeed: 0.0471s/iter; left time: 264.0562s\n",
      "\titers: 200, epoch: 11 | loss: 0.1726053\n",
      "\tspeed: 0.0161s/iter; left time: 88.7794s\n",
      "\titers: 300, epoch: 11 | loss: 0.1634731\n",
      "\tspeed: 0.0161s/iter; left time: 86.9523s\n",
      "\titers: 400, epoch: 11 | loss: 0.2541782\n",
      "\tspeed: 0.0161s/iter; left time: 85.1654s\n",
      "\titers: 500, epoch: 11 | loss: 0.1446469\n",
      "\tspeed: 0.0161s/iter; left time: 83.6115s\n",
      "Epoch: 11 cost time: 9.45587420463562\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2199666 Vali Loss: 0.0500236 Test Loss: 0.1554699\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1390455\n",
      "\tspeed: 0.0498s/iter; left time: 250.3068s\n",
      "\titers: 200, epoch: 12 | loss: 0.1907740\n",
      "\tspeed: 0.0142s/iter; left time: 70.2486s\n",
      "\titers: 300, epoch: 12 | loss: 0.2275511\n",
      "\tspeed: 0.0142s/iter; left time: 68.6784s\n",
      "\titers: 400, epoch: 12 | loss: 0.1400598\n",
      "\tspeed: 0.0143s/iter; left time: 67.6628s\n",
      "\titers: 500, epoch: 12 | loss: 0.2891672\n",
      "\tspeed: 0.0143s/iter; left time: 66.1112s\n",
      "Epoch: 12 cost time: 8.400589227676392\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2222741 Vali Loss: 0.0498362 Test Loss: 0.1554103\n",
      "Validation loss decreased (0.049918 --> 0.049836).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1693378\n",
      "\tspeed: 0.0463s/iter; left time: 206.3497s\n",
      "\titers: 200, epoch: 13 | loss: 0.1841998\n",
      "\tspeed: 0.0144s/iter; left time: 62.5835s\n",
      "\titers: 300, epoch: 13 | loss: 0.2378055\n",
      "\tspeed: 0.0143s/iter; left time: 61.0003s\n",
      "\titers: 400, epoch: 13 | loss: 0.2329025\n",
      "\tspeed: 0.0143s/iter; left time: 59.5530s\n",
      "\titers: 500, epoch: 13 | loss: 0.2395816\n",
      "\tspeed: 0.0143s/iter; left time: 57.9883s\n",
      "Epoch: 13 cost time: 8.464636325836182\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2195402 Vali Loss: 0.0501228 Test Loss: 0.1554241\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1925913\n",
      "\tspeed: 0.0459s/iter; left time: 178.7762s\n",
      "\titers: 200, epoch: 14 | loss: 0.2345678\n",
      "\tspeed: 0.0143s/iter; left time: 54.1413s\n",
      "\titers: 300, epoch: 14 | loss: 0.1716644\n",
      "\tspeed: 0.0142s/iter; left time: 52.5894s\n",
      "\titers: 400, epoch: 14 | loss: 0.2786288\n",
      "\tspeed: 0.0143s/iter; left time: 51.1817s\n",
      "\titers: 500, epoch: 14 | loss: 0.2081250\n",
      "\tspeed: 0.0143s/iter; left time: 49.7656s\n",
      "Epoch: 14 cost time: 8.430323600769043\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2213041 Vali Loss: 0.0503268 Test Loss: 0.1554163\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2340338\n",
      "\tspeed: 0.0461s/iter; left time: 153.1772s\n",
      "\titers: 200, epoch: 15 | loss: 0.1625172\n",
      "\tspeed: 0.0142s/iter; left time: 45.8491s\n",
      "\titers: 300, epoch: 15 | loss: 0.2733411\n",
      "\tspeed: 0.0142s/iter; left time: 44.4438s\n",
      "\titers: 400, epoch: 15 | loss: 0.2272985\n",
      "\tspeed: 0.0142s/iter; left time: 42.9286s\n",
      "\titers: 500, epoch: 15 | loss: 0.1665039\n",
      "\tspeed: 0.0141s/iter; left time: 41.2492s\n",
      "Epoch: 15 cost time: 8.40393328666687\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2200371 Vali Loss: 0.0499670 Test Loss: 0.1554104\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15562692284584045, mae:0.2503296732902527\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4231477\n",
      "\tspeed: 0.0306s/iter; left time: 345.5087s\n",
      "\titers: 200, epoch: 1 | loss: 0.2990322\n",
      "\tspeed: 0.0184s/iter; left time: 205.6859s\n",
      "\titers: 300, epoch: 1 | loss: 0.4513031\n",
      "\tspeed: 0.0184s/iter; left time: 204.2628s\n",
      "\titers: 400, epoch: 1 | loss: 0.2904619\n",
      "\tspeed: 0.0184s/iter; left time: 202.2220s\n",
      "\titers: 500, epoch: 1 | loss: 0.2761419\n",
      "\tspeed: 0.0183s/iter; left time: 200.0073s\n",
      "Epoch: 1 cost time: 11.746707916259766\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3566688 Vali Loss: 0.0702681 Test Loss: 0.1946494\n",
      "Validation loss decreased (inf --> 0.070268).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4136405\n",
      "\tspeed: 0.0552s/iter; left time: 592.3144s\n",
      "\titers: 200, epoch: 2 | loss: 0.3237491\n",
      "\tspeed: 0.0170s/iter; left time: 180.9636s\n",
      "\titers: 300, epoch: 2 | loss: 0.3927438\n",
      "\tspeed: 0.0169s/iter; left time: 178.3693s\n",
      "\titers: 400, epoch: 2 | loss: 0.2460304\n",
      "\tspeed: 0.0169s/iter; left time: 176.4321s\n",
      "\titers: 500, epoch: 2 | loss: 0.3138577\n",
      "\tspeed: 0.0169s/iter; left time: 174.7214s\n",
      "Epoch: 2 cost time: 10.150765180587769\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3107000 Vali Loss: 0.0631987 Test Loss: 0.1842450\n",
      "Validation loss decreased (0.070268 --> 0.063199).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2380202\n",
      "\tspeed: 0.0510s/iter; left time: 517.8157s\n",
      "\titers: 200, epoch: 3 | loss: 0.2695333\n",
      "\tspeed: 0.0174s/iter; left time: 175.3056s\n",
      "\titers: 300, epoch: 3 | loss: 0.4293661\n",
      "\tspeed: 0.0180s/iter; left time: 179.4267s\n",
      "\titers: 400, epoch: 3 | loss: 0.2429115\n",
      "\tspeed: 0.0180s/iter; left time: 177.5047s\n",
      "\titers: 500, epoch: 3 | loss: 0.2631704\n",
      "\tspeed: 0.0180s/iter; left time: 175.8834s\n",
      "Epoch: 3 cost time: 10.416537523269653\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2844806 Vali Loss: 0.0596799 Test Loss: 0.1749096\n",
      "Validation loss decreased (0.063199 --> 0.059680).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.3597705\n",
      "\tspeed: 0.0536s/iter; left time: 514.0755s\n",
      "\titers: 200, epoch: 4 | loss: 0.3170551\n",
      "\tspeed: 0.0170s/iter; left time: 161.4208s\n",
      "\titers: 300, epoch: 4 | loss: 0.2862722\n",
      "\tspeed: 0.0170s/iter; left time: 159.6554s\n",
      "\titers: 400, epoch: 4 | loss: 0.3588545\n",
      "\tspeed: 0.0170s/iter; left time: 157.9697s\n",
      "\titers: 500, epoch: 4 | loss: 0.2317751\n",
      "\tspeed: 0.0170s/iter; left time: 156.2252s\n",
      "Epoch: 4 cost time: 10.15993070602417\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2729959 Vali Loss: 0.0586416 Test Loss: 0.1634108\n",
      "Validation loss decreased (0.059680 --> 0.058642).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.1480240\n",
      "\tspeed: 0.0536s/iter; left time: 483.4674s\n",
      "\titers: 200, epoch: 5 | loss: 0.2854924\n",
      "\tspeed: 0.0169s/iter; left time: 150.5535s\n",
      "\titers: 300, epoch: 5 | loss: 0.2601712\n",
      "\tspeed: 0.0180s/iter; left time: 158.4383s\n",
      "\titers: 400, epoch: 5 | loss: 0.2432022\n",
      "\tspeed: 0.0193s/iter; left time: 168.2534s\n",
      "\titers: 500, epoch: 5 | loss: 0.2031509\n",
      "\tspeed: 0.0191s/iter; left time: 164.5058s\n",
      "Epoch: 5 cost time: 10.707136631011963\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2617277 Vali Loss: 0.0552872 Test Loss: 0.1606847\n",
      "Validation loss decreased (0.058642 --> 0.055287).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3094109\n",
      "\tspeed: 0.0524s/iter; left time: 442.4519s\n",
      "\titers: 200, epoch: 6 | loss: 0.2309000\n",
      "\tspeed: 0.0169s/iter; left time: 141.3523s\n",
      "\titers: 300, epoch: 6 | loss: 0.2965054\n",
      "\tspeed: 0.0169s/iter; left time: 139.6711s\n",
      "\titers: 400, epoch: 6 | loss: 0.3793553\n",
      "\tspeed: 0.0169s/iter; left time: 137.8828s\n",
      "\titers: 500, epoch: 6 | loss: 0.1989893\n",
      "\tspeed: 0.0169s/iter; left time: 136.1486s\n",
      "Epoch: 6 cost time: 9.964446067810059\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2568656 Vali Loss: 0.0557706 Test Loss: 0.1589244\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1992728\n",
      "\tspeed: 0.0516s/iter; left time: 406.6682s\n",
      "\titers: 200, epoch: 7 | loss: 0.2386451\n",
      "\tspeed: 0.0168s/iter; left time: 131.1062s\n",
      "\titers: 300, epoch: 7 | loss: 0.3433176\n",
      "\tspeed: 0.0168s/iter; left time: 129.3997s\n",
      "\titers: 400, epoch: 7 | loss: 0.1856261\n",
      "\tspeed: 0.0168s/iter; left time: 127.7197s\n",
      "\titers: 500, epoch: 7 | loss: 0.2049061\n",
      "\tspeed: 0.0168s/iter; left time: 126.0427s\n",
      "Epoch: 7 cost time: 9.898459911346436\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2552189 Vali Loss: 0.0553833 Test Loss: 0.1578540\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2772514\n",
      "\tspeed: 0.0501s/iter; left time: 365.9273s\n",
      "\titers: 200, epoch: 8 | loss: 0.1852691\n",
      "\tspeed: 0.0169s/iter; left time: 122.0043s\n",
      "\titers: 300, epoch: 8 | loss: 0.2553893\n",
      "\tspeed: 0.0169s/iter; left time: 120.3039s\n",
      "\titers: 400, epoch: 8 | loss: 0.1523726\n",
      "\tspeed: 0.0169s/iter; left time: 118.6058s\n",
      "\titers: 500, epoch: 8 | loss: 0.1697097\n",
      "\tspeed: 0.0169s/iter; left time: 116.9172s\n",
      "Epoch: 8 cost time: 9.953924417495728\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2544855 Vali Loss: 0.0552160 Test Loss: 0.1583025\n",
      "Validation loss decreased (0.055287 --> 0.055216).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2468323\n",
      "\tspeed: 0.0534s/iter; left time: 360.1663s\n",
      "\titers: 200, epoch: 9 | loss: 0.1402135\n",
      "\tspeed: 0.0169s/iter; left time: 112.3977s\n",
      "\titers: 300, epoch: 9 | loss: 0.2766612\n",
      "\tspeed: 0.0169s/iter; left time: 110.6968s\n",
      "\titers: 400, epoch: 9 | loss: 0.2861530\n",
      "\tspeed: 0.0169s/iter; left time: 109.0347s\n",
      "\titers: 500, epoch: 9 | loss: 0.3543449\n",
      "\tspeed: 0.0169s/iter; left time: 107.4426s\n",
      "Epoch: 9 cost time: 9.971363544464111\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2515577 Vali Loss: 0.0554824 Test Loss: 0.1580071\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3629105\n",
      "\tspeed: 0.0524s/iter; left time: 323.4762s\n",
      "\titers: 200, epoch: 10 | loss: 0.1137268\n",
      "\tspeed: 0.0184s/iter; left time: 111.9460s\n",
      "\titers: 300, epoch: 10 | loss: 0.3536044\n",
      "\tspeed: 0.0184s/iter; left time: 109.8502s\n",
      "\titers: 400, epoch: 10 | loss: 0.3374973\n",
      "\tspeed: 0.0184s/iter; left time: 108.1767s\n",
      "\titers: 500, epoch: 10 | loss: 0.2181917\n",
      "\tspeed: 0.0184s/iter; left time: 106.3519s\n",
      "Epoch: 10 cost time: 10.836272954940796\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2517068 Vali Loss: 0.0553969 Test Loss: 0.1578377\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1954637\n",
      "\tspeed: 0.0514s/iter; left time: 287.7707s\n",
      "\titers: 200, epoch: 11 | loss: 0.2611381\n",
      "\tspeed: 0.0169s/iter; left time: 93.0945s\n",
      "\titers: 300, epoch: 11 | loss: 0.1358562\n",
      "\tspeed: 0.0169s/iter; left time: 91.3509s\n",
      "\titers: 400, epoch: 11 | loss: 0.2665876\n",
      "\tspeed: 0.0169s/iter; left time: 89.7095s\n",
      "\titers: 500, epoch: 11 | loss: 0.3374287\n",
      "\tspeed: 0.0169s/iter; left time: 87.9472s\n",
      "Epoch: 11 cost time: 9.93829345703125\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2517991 Vali Loss: 0.0553704 Test Loss: 0.1576126\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15853843092918396, mae:0.2533927857875824\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2764733\n",
      "\tspeed: 0.0284s/iter; left time: 321.2808s\n",
      "\titers: 200, epoch: 1 | loss: 0.2327543\n",
      "\tspeed: 0.0165s/iter; left time: 184.5093s\n",
      "\titers: 300, epoch: 1 | loss: 0.3357033\n",
      "\tspeed: 0.0165s/iter; left time: 182.9302s\n",
      "\titers: 400, epoch: 1 | loss: 0.3047566\n",
      "\tspeed: 0.0165s/iter; left time: 181.5058s\n",
      "\titers: 500, epoch: 1 | loss: 0.1972861\n",
      "\tspeed: 0.0164s/iter; left time: 179.1272s\n",
      "Epoch: 1 cost time: 10.633033037185669\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3256827 Vali Loss: 0.0674371 Test Loss: 0.1923324\n",
      "Validation loss decreased (inf --> 0.067437).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2472515\n",
      "\tspeed: 0.0528s/iter; left time: 566.6611s\n",
      "\titers: 200, epoch: 2 | loss: 0.1821082\n",
      "\tspeed: 0.0164s/iter; left time: 174.7526s\n",
      "\titers: 300, epoch: 2 | loss: 0.3611526\n",
      "\tspeed: 0.0170s/iter; left time: 178.6492s\n",
      "\titers: 400, epoch: 2 | loss: 0.4608039\n",
      "\tspeed: 0.0179s/iter; left time: 186.9937s\n",
      "\titers: 500, epoch: 2 | loss: 0.3331698\n",
      "\tspeed: 0.0179s/iter; left time: 185.3542s\n",
      "Epoch: 2 cost time: 10.206190586090088\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3138315 Vali Loss: 0.0646692 Test Loss: 0.1826266\n",
      "Validation loss decreased (0.067437 --> 0.064669).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3125826\n",
      "\tspeed: 0.0515s/iter; left time: 523.7181s\n",
      "\titers: 200, epoch: 3 | loss: 0.3133857\n",
      "\tspeed: 0.0165s/iter; left time: 165.6985s\n",
      "\titers: 300, epoch: 3 | loss: 0.1997578\n",
      "\tspeed: 0.0165s/iter; left time: 164.3144s\n",
      "\titers: 400, epoch: 3 | loss: 0.3287965\n",
      "\tspeed: 0.0165s/iter; left time: 162.4516s\n",
      "\titers: 500, epoch: 3 | loss: 0.1674982\n",
      "\tspeed: 0.0165s/iter; left time: 160.7747s\n",
      "Epoch: 3 cost time: 9.714487314224243\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2841374 Vali Loss: 0.0582497 Test Loss: 0.1715780\n",
      "Validation loss decreased (0.064669 --> 0.058250).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1959866\n",
      "\tspeed: 0.0527s/iter; left time: 505.8210s\n",
      "\titers: 200, epoch: 4 | loss: 0.6555514\n",
      "\tspeed: 0.0164s/iter; left time: 155.6889s\n",
      "\titers: 300, epoch: 4 | loss: 0.2739381\n",
      "\tspeed: 0.0164s/iter; left time: 154.0855s\n",
      "\titers: 400, epoch: 4 | loss: 0.1755477\n",
      "\tspeed: 0.0164s/iter; left time: 152.3424s\n",
      "\titers: 500, epoch: 4 | loss: 0.3965024\n",
      "\tspeed: 0.0164s/iter; left time: 150.6855s\n",
      "Epoch: 4 cost time: 9.721477270126343\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2689576 Vali Loss: 0.0583956 Test Loss: 0.1655976\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2582152\n",
      "\tspeed: 0.0502s/iter; left time: 452.6956s\n",
      "\titers: 200, epoch: 5 | loss: 0.3659150\n",
      "\tspeed: 0.0165s/iter; left time: 147.1776s\n",
      "\titers: 300, epoch: 5 | loss: 0.2310751\n",
      "\tspeed: 0.0165s/iter; left time: 145.5506s\n",
      "\titers: 400, epoch: 5 | loss: 0.1816646\n",
      "\tspeed: 0.0169s/iter; left time: 147.3480s\n",
      "\titers: 500, epoch: 5 | loss: 0.3596244\n",
      "\tspeed: 0.0179s/iter; left time: 154.4648s\n",
      "Epoch: 5 cost time: 9.969182968139648\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2582919 Vali Loss: 0.0567488 Test Loss: 0.1616790\n",
      "Validation loss decreased (0.058250 --> 0.056749).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2957802\n",
      "\tspeed: 0.0530s/iter; left time: 448.1704s\n",
      "\titers: 200, epoch: 6 | loss: 0.3656701\n",
      "\tspeed: 0.0165s/iter; left time: 137.4922s\n",
      "\titers: 300, epoch: 6 | loss: 0.5005087\n",
      "\tspeed: 0.0164s/iter; left time: 135.7051s\n",
      "\titers: 400, epoch: 6 | loss: 0.2646871\n",
      "\tspeed: 0.0164s/iter; left time: 134.0102s\n",
      "\titers: 500, epoch: 6 | loss: 0.2224196\n",
      "\tspeed: 0.0164s/iter; left time: 132.3379s\n",
      "Epoch: 6 cost time: 9.686028003692627\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2525463 Vali Loss: 0.0566615 Test Loss: 0.1613017\n",
      "Validation loss decreased (0.056749 --> 0.056661).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2991766\n",
      "\tspeed: 0.0504s/iter; left time: 396.9280s\n",
      "\titers: 200, epoch: 7 | loss: 0.1717590\n",
      "\tspeed: 0.0164s/iter; left time: 127.5285s\n",
      "\titers: 300, epoch: 7 | loss: 0.2409506\n",
      "\tspeed: 0.0164s/iter; left time: 125.8645s\n",
      "\titers: 400, epoch: 7 | loss: 0.3314831\n",
      "\tspeed: 0.0164s/iter; left time: 124.2097s\n",
      "\titers: 500, epoch: 7 | loss: 0.3442245\n",
      "\tspeed: 0.0164s/iter; left time: 122.5719s\n",
      "Epoch: 7 cost time: 9.657699346542358\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2513187 Vali Loss: 0.0563184 Test Loss: 0.1605487\n",
      "Validation loss decreased (0.056661 --> 0.056318).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1689795\n",
      "\tspeed: 0.0524s/iter; left time: 382.9443s\n",
      "\titers: 200, epoch: 8 | loss: 0.3971741\n",
      "\tspeed: 0.0170s/iter; left time: 122.6406s\n",
      "\titers: 300, epoch: 8 | loss: 0.3039874\n",
      "\tspeed: 0.0170s/iter; left time: 120.9830s\n",
      "\titers: 400, epoch: 8 | loss: 0.2689097\n",
      "\tspeed: 0.0170s/iter; left time: 119.1412s\n",
      "\titers: 500, epoch: 8 | loss: 0.2332681\n",
      "\tspeed: 0.0170s/iter; left time: 117.2994s\n",
      "Epoch: 8 cost time: 10.080722093582153\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2480591 Vali Loss: 0.0560904 Test Loss: 0.1599616\n",
      "Validation loss decreased (0.056318 --> 0.056090).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1714244\n",
      "\tspeed: 0.0537s/iter; left time: 362.0410s\n",
      "\titers: 200, epoch: 9 | loss: 0.2399662\n",
      "\tspeed: 0.0170s/iter; left time: 112.8379s\n",
      "\titers: 300, epoch: 9 | loss: 0.1432478\n",
      "\tspeed: 0.0170s/iter; left time: 111.2237s\n",
      "\titers: 400, epoch: 9 | loss: 0.2058955\n",
      "\tspeed: 0.0170s/iter; left time: 109.4103s\n",
      "\titers: 500, epoch: 9 | loss: 0.1150765\n",
      "\tspeed: 0.0170s/iter; left time: 107.7701s\n",
      "Epoch: 9 cost time: 10.075819730758667\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2479948 Vali Loss: 0.0558472 Test Loss: 0.1595002\n",
      "Validation loss decreased (0.056090 --> 0.055847).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3212492\n",
      "\tspeed: 0.0539s/iter; left time: 332.3499s\n",
      "\titers: 200, epoch: 10 | loss: 0.2343052\n",
      "\tspeed: 0.0171s/iter; left time: 103.5956s\n",
      "\titers: 300, epoch: 10 | loss: 0.1787103\n",
      "\tspeed: 0.0171s/iter; left time: 101.8208s\n",
      "\titers: 400, epoch: 10 | loss: 0.2474824\n",
      "\tspeed: 0.0171s/iter; left time: 100.1453s\n",
      "\titers: 500, epoch: 10 | loss: 0.1732036\n",
      "\tspeed: 0.0170s/iter; left time: 98.3844s\n",
      "Epoch: 10 cost time: 10.119998216629028\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2465933 Vali Loss: 0.0556785 Test Loss: 0.1596540\n",
      "Validation loss decreased (0.055847 --> 0.055678).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1643394\n",
      "\tspeed: 0.0508s/iter; left time: 284.6066s\n",
      "\titers: 200, epoch: 11 | loss: 0.1678609\n",
      "\tspeed: 0.0165s/iter; left time: 90.5127s\n",
      "\titers: 300, epoch: 11 | loss: 0.2018326\n",
      "\tspeed: 0.0165s/iter; left time: 88.9557s\n",
      "\titers: 400, epoch: 11 | loss: 0.2276821\n",
      "\tspeed: 0.0165s/iter; left time: 87.2942s\n",
      "\titers: 500, epoch: 11 | loss: 0.1991215\n",
      "\tspeed: 0.0165s/iter; left time: 85.5826s\n",
      "Epoch: 11 cost time: 9.675696849822998\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2476586 Vali Loss: 0.0559690 Test Loss: 0.1596856\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1573327\n",
      "\tspeed: 0.0508s/iter; left time: 255.4432s\n",
      "\titers: 200, epoch: 12 | loss: 0.2785705\n",
      "\tspeed: 0.0165s/iter; left time: 81.4327s\n",
      "\titers: 300, epoch: 12 | loss: 0.1592060\n",
      "\tspeed: 0.0165s/iter; left time: 79.6397s\n",
      "\titers: 400, epoch: 12 | loss: 0.2363529\n",
      "\tspeed: 0.0164s/iter; left time: 77.7211s\n",
      "\titers: 500, epoch: 12 | loss: 0.3511350\n",
      "\tspeed: 0.0164s/iter; left time: 76.0188s\n",
      "Epoch: 12 cost time: 9.695243120193481\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2443652 Vali Loss: 0.0558417 Test Loss: 0.1596739\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.3338568\n",
      "\tspeed: 0.0507s/iter; left time: 226.3470s\n",
      "\titers: 200, epoch: 13 | loss: 0.2935806\n",
      "\tspeed: 0.0165s/iter; left time: 72.0322s\n",
      "\titers: 300, epoch: 13 | loss: 0.2703342\n",
      "\tspeed: 0.0165s/iter; left time: 70.3909s\n",
      "\titers: 400, epoch: 13 | loss: 0.1846772\n",
      "\tspeed: 0.0165s/iter; left time: 68.7028s\n",
      "\titers: 500, epoch: 13 | loss: 0.2558920\n",
      "\tspeed: 0.0165s/iter; left time: 67.0546s\n",
      "Epoch: 13 cost time: 9.709242343902588\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2451861 Vali Loss: 0.0559950 Test Loss: 0.1596886\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15988914668560028, mae:0.255709171295166\n",
      ">>> LR=1e-3,DO=0.2,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3528516\n",
      "\tspeed: 0.0271s/iter; left time: 306.0206s\n",
      "\titers: 200, epoch: 1 | loss: 0.3574461\n",
      "\tspeed: 0.0154s/iter; left time: 172.3646s\n",
      "\titers: 300, epoch: 1 | loss: 0.2459876\n",
      "\tspeed: 0.0154s/iter; left time: 171.1494s\n",
      "\titers: 400, epoch: 1 | loss: 0.3437310\n",
      "\tspeed: 0.0154s/iter; left time: 169.8759s\n",
      "\titers: 500, epoch: 1 | loss: 0.5735512\n",
      "\tspeed: 0.0154s/iter; left time: 168.1850s\n",
      "Epoch: 1 cost time: 10.006650447845459\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3299547 Vali Loss: 0.0726970 Test Loss: 0.1894750\n",
      "Validation loss decreased (inf --> 0.072697).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.4155256\n",
      "\tspeed: 0.0489s/iter; left time: 524.3369s\n",
      "\titers: 200, epoch: 2 | loss: 0.1897063\n",
      "\tspeed: 0.0155s/iter; left time: 164.4406s\n",
      "\titers: 300, epoch: 2 | loss: 0.3088529\n",
      "\tspeed: 0.0154s/iter; left time: 162.2002s\n",
      "\titers: 400, epoch: 2 | loss: 0.3127601\n",
      "\tspeed: 0.0154s/iter; left time: 160.7928s\n",
      "\titers: 500, epoch: 2 | loss: 0.4591195\n",
      "\tspeed: 0.0154s/iter; left time: 158.8397s\n",
      "Epoch: 2 cost time: 9.121246814727783\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3048152 Vali Loss: 0.0728215 Test Loss: 0.1978776\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.3362475\n",
      "\tspeed: 0.0481s/iter; left time: 488.5240s\n",
      "\titers: 200, epoch: 3 | loss: 0.1796288\n",
      "\tspeed: 0.0154s/iter; left time: 154.6876s\n",
      "\titers: 300, epoch: 3 | loss: 0.3259010\n",
      "\tspeed: 0.0154s/iter; left time: 152.9149s\n",
      "\titers: 400, epoch: 3 | loss: 0.3156268\n",
      "\tspeed: 0.0153s/iter; left time: 151.2696s\n",
      "\titers: 500, epoch: 3 | loss: 0.2664908\n",
      "\tspeed: 0.0153s/iter; left time: 149.7044s\n",
      "Epoch: 3 cost time: 9.010859251022339\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.3005098 Vali Loss: 0.0633868 Test Loss: 0.1851613\n",
      "Validation loss decreased (0.072697 --> 0.063387).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2100977\n",
      "\tspeed: 0.0486s/iter; left time: 466.1330s\n",
      "\titers: 200, epoch: 4 | loss: 0.3616856\n",
      "\tspeed: 0.0154s/iter; left time: 146.1287s\n",
      "\titers: 300, epoch: 4 | loss: 0.4346203\n",
      "\tspeed: 0.0154s/iter; left time: 144.6251s\n",
      "\titers: 400, epoch: 4 | loss: 0.2474310\n",
      "\tspeed: 0.0154s/iter; left time: 143.1588s\n",
      "\titers: 500, epoch: 4 | loss: 0.3295820\n",
      "\tspeed: 0.0154s/iter; left time: 141.3630s\n",
      "Epoch: 4 cost time: 9.090102910995483\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2913144 Vali Loss: 0.0619122 Test Loss: 0.1752462\n",
      "Validation loss decreased (0.063387 --> 0.061912).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.3288084\n",
      "\tspeed: 0.0505s/iter; left time: 455.7724s\n",
      "\titers: 200, epoch: 5 | loss: 0.2195111\n",
      "\tspeed: 0.0170s/iter; left time: 151.6599s\n",
      "\titers: 300, epoch: 5 | loss: 0.2494730\n",
      "\tspeed: 0.0170s/iter; left time: 149.9155s\n",
      "\titers: 400, epoch: 5 | loss: 0.1984664\n",
      "\tspeed: 0.0170s/iter; left time: 148.2146s\n",
      "\titers: 500, epoch: 5 | loss: 0.2411065\n",
      "\tspeed: 0.0170s/iter; left time: 146.6223s\n",
      "Epoch: 5 cost time: 10.013183116912842\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2832449 Vali Loss: 0.0608572 Test Loss: 0.1716351\n",
      "Validation loss decreased (0.061912 --> 0.060857).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3289462\n",
      "\tspeed: 0.0490s/iter; left time: 414.3797s\n",
      "\titers: 200, epoch: 6 | loss: 0.2251569\n",
      "\tspeed: 0.0155s/iter; left time: 129.1671s\n",
      "\titers: 300, epoch: 6 | loss: 0.1530811\n",
      "\tspeed: 0.0154s/iter; left time: 127.3723s\n",
      "\titers: 400, epoch: 6 | loss: 0.1735361\n",
      "\tspeed: 0.0154s/iter; left time: 125.6056s\n",
      "\titers: 500, epoch: 6 | loss: 0.2480948\n",
      "\tspeed: 0.0154s/iter; left time: 123.9927s\n",
      "Epoch: 6 cost time: 9.09894061088562\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2769288 Vali Loss: 0.0607782 Test Loss: 0.1672092\n",
      "Validation loss decreased (0.060857 --> 0.060778).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2894515\n",
      "\tspeed: 0.0534s/iter; left time: 420.8073s\n",
      "\titers: 200, epoch: 7 | loss: 0.2232478\n",
      "\tspeed: 0.0170s/iter; left time: 132.6380s\n",
      "\titers: 300, epoch: 7 | loss: 0.2083797\n",
      "\tspeed: 0.0170s/iter; left time: 130.8375s\n",
      "\titers: 400, epoch: 7 | loss: 0.2925494\n",
      "\tspeed: 0.0170s/iter; left time: 129.0303s\n",
      "\titers: 500, epoch: 7 | loss: 0.2986919\n",
      "\tspeed: 0.0170s/iter; left time: 127.3218s\n",
      "Epoch: 7 cost time: 10.030033826828003\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2726304 Vali Loss: 0.0607205 Test Loss: 0.1663236\n",
      "Validation loss decreased (0.060778 --> 0.060720).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2765966\n",
      "\tspeed: 0.0503s/iter; left time: 367.6474s\n",
      "\titers: 200, epoch: 8 | loss: 0.3518615\n",
      "\tspeed: 0.0154s/iter; left time: 111.1154s\n",
      "\titers: 300, epoch: 8 | loss: 0.1765554\n",
      "\tspeed: 0.0154s/iter; left time: 109.6330s\n",
      "\titers: 400, epoch: 8 | loss: 0.2702326\n",
      "\tspeed: 0.0155s/iter; left time: 108.3424s\n",
      "\titers: 500, epoch: 8 | loss: 0.2131765\n",
      "\tspeed: 0.0155s/iter; left time: 106.8930s\n",
      "Epoch: 8 cost time: 9.13086462020874\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2694661 Vali Loss: 0.0601330 Test Loss: 0.1655186\n",
      "Validation loss decreased (0.060720 --> 0.060133).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2739267\n",
      "\tspeed: 0.0490s/iter; left time: 330.1429s\n",
      "\titers: 200, epoch: 9 | loss: 0.3057661\n",
      "\tspeed: 0.0154s/iter; left time: 102.5001s\n",
      "\titers: 300, epoch: 9 | loss: 0.2766374\n",
      "\tspeed: 0.0154s/iter; left time: 100.9189s\n",
      "\titers: 400, epoch: 9 | loss: 0.1855141\n",
      "\tspeed: 0.0154s/iter; left time: 99.3716s\n",
      "\titers: 500, epoch: 9 | loss: 0.2378395\n",
      "\tspeed: 0.0154s/iter; left time: 97.7737s\n",
      "Epoch: 9 cost time: 9.108590126037598\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2694652 Vali Loss: 0.0601589 Test Loss: 0.1652751\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2965716\n",
      "\tspeed: 0.0488s/iter; left time: 301.0228s\n",
      "\titers: 200, epoch: 10 | loss: 0.3119071\n",
      "\tspeed: 0.0154s/iter; left time: 93.6897s\n",
      "\titers: 300, epoch: 10 | loss: 0.1638429\n",
      "\tspeed: 0.0154s/iter; left time: 92.1525s\n",
      "\titers: 400, epoch: 10 | loss: 0.3317775\n",
      "\tspeed: 0.0154s/iter; left time: 90.5573s\n",
      "\titers: 500, epoch: 10 | loss: 0.5118732\n",
      "\tspeed: 0.0154s/iter; left time: 89.0324s\n",
      "Epoch: 10 cost time: 9.111079931259155\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2700157 Vali Loss: 0.0602742 Test Loss: 0.1647697\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2693446\n",
      "\tspeed: 0.0490s/iter; left time: 274.3350s\n",
      "\titers: 200, epoch: 11 | loss: 0.1961205\n",
      "\tspeed: 0.0154s/iter; left time: 84.8021s\n",
      "\titers: 300, epoch: 11 | loss: 0.2544862\n",
      "\tspeed: 0.0154s/iter; left time: 83.2305s\n",
      "\titers: 400, epoch: 11 | loss: 0.2897861\n",
      "\tspeed: 0.0154s/iter; left time: 81.7152s\n",
      "\titers: 500, epoch: 11 | loss: 0.3234709\n",
      "\tspeed: 0.0154s/iter; left time: 80.2791s\n",
      "Epoch: 11 cost time: 9.070013284683228\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2692987 Vali Loss: 0.0602649 Test Loss: 0.1648742\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16577757894992828, mae:0.26132142543792725\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2132039\n",
      "\tspeed: 0.0323s/iter; left time: 364.6002s\n",
      "\titers: 200, epoch: 1 | loss: 0.2482248\n",
      "\tspeed: 0.0186s/iter; left time: 208.0448s\n",
      "\titers: 300, epoch: 1 | loss: 0.4155103\n",
      "\tspeed: 0.0186s/iter; left time: 206.5562s\n",
      "\titers: 400, epoch: 1 | loss: 0.2788582\n",
      "\tspeed: 0.0186s/iter; left time: 204.3457s\n",
      "\titers: 500, epoch: 1 | loss: 0.2421324\n",
      "\tspeed: 0.0186s/iter; left time: 202.8496s\n",
      "Epoch: 1 cost time: 12.01831841468811\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2878056 Vali Loss: 0.0654367 Test Loss: 0.1771272\n",
      "Validation loss decreased (inf --> 0.065437).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.5202433\n",
      "\tspeed: 0.0581s/iter; left time: 623.8222s\n",
      "\titers: 200, epoch: 2 | loss: 0.2306121\n",
      "\tspeed: 0.0187s/iter; left time: 198.4425s\n",
      "\titers: 300, epoch: 2 | loss: 0.2242718\n",
      "\tspeed: 0.0187s/iter; left time: 196.5491s\n",
      "\titers: 400, epoch: 2 | loss: 0.2614449\n",
      "\tspeed: 0.0193s/iter; left time: 201.2080s\n",
      "\titers: 500, epoch: 2 | loss: 0.2152333\n",
      "\tspeed: 0.0189s/iter; left time: 194.8100s\n",
      "Epoch: 2 cost time: 11.017516613006592\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2904823 Vali Loss: 0.0628499 Test Loss: 0.1861763\n",
      "Validation loss decreased (0.065437 --> 0.062850).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2092441\n",
      "\tspeed: 0.0582s/iter; left time: 591.5626s\n",
      "\titers: 200, epoch: 3 | loss: 0.3344850\n",
      "\tspeed: 0.0188s/iter; left time: 188.8330s\n",
      "\titers: 300, epoch: 3 | loss: 0.2349855\n",
      "\tspeed: 0.0188s/iter; left time: 186.9083s\n",
      "\titers: 400, epoch: 3 | loss: 0.1700952\n",
      "\tspeed: 0.0188s/iter; left time: 185.0032s\n",
      "\titers: 500, epoch: 3 | loss: 0.2035912\n",
      "\tspeed: 0.0188s/iter; left time: 183.0751s\n",
      "Epoch: 3 cost time: 10.990633726119995\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2683181 Vali Loss: 0.0603431 Test Loss: 0.1769986\n",
      "Validation loss decreased (0.062850 --> 0.060343).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2909709\n",
      "\tspeed: 0.0559s/iter; left time: 535.7454s\n",
      "\titers: 200, epoch: 4 | loss: 0.3411694\n",
      "\tspeed: 0.0186s/iter; left time: 176.1713s\n",
      "\titers: 300, epoch: 4 | loss: 0.3257471\n",
      "\tspeed: 0.0186s/iter; left time: 174.5046s\n",
      "\titers: 400, epoch: 4 | loss: 0.1559393\n",
      "\tspeed: 0.0186s/iter; left time: 172.5918s\n",
      "\titers: 500, epoch: 4 | loss: 0.2574477\n",
      "\tspeed: 0.0186s/iter; left time: 170.6779s\n",
      "Epoch: 4 cost time: 10.881583452224731\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2628267 Vali Loss: 0.0567002 Test Loss: 0.1610373\n",
      "Validation loss decreased (0.060343 --> 0.056700).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2836421\n",
      "\tspeed: 0.0585s/iter; left time: 527.4131s\n",
      "\titers: 200, epoch: 5 | loss: 0.2339277\n",
      "\tspeed: 0.0187s/iter; left time: 166.7652s\n",
      "\titers: 300, epoch: 5 | loss: 0.2372324\n",
      "\tspeed: 0.0188s/iter; left time: 165.5256s\n",
      "\titers: 400, epoch: 5 | loss: 0.2867775\n",
      "\tspeed: 0.0187s/iter; left time: 163.1377s\n",
      "\titers: 500, epoch: 5 | loss: 0.2466409\n",
      "\tspeed: 0.0197s/iter; left time: 170.1039s\n",
      "Epoch: 5 cost time: 11.265970945358276\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2554568 Vali Loss: 0.0538172 Test Loss: 0.1566114\n",
      "Validation loss decreased (0.056700 --> 0.053817).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1382849\n",
      "\tspeed: 0.0637s/iter; left time: 538.5668s\n",
      "\titers: 200, epoch: 6 | loss: 0.2312297\n",
      "\tspeed: 0.0216s/iter; left time: 180.5285s\n",
      "\titers: 300, epoch: 6 | loss: 0.2006404\n",
      "\tspeed: 0.0216s/iter; left time: 178.3596s\n",
      "\titers: 400, epoch: 6 | loss: 0.2580297\n",
      "\tspeed: 0.0196s/iter; left time: 159.4689s\n",
      "\titers: 500, epoch: 6 | loss: 0.2003629\n",
      "\tspeed: 0.0188s/iter; left time: 151.1968s\n",
      "Epoch: 6 cost time: 11.943485021591187\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2458913 Vali Loss: 0.0522738 Test Loss: 0.1558172\n",
      "Validation loss decreased (0.053817 --> 0.052274).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1853729\n",
      "\tspeed: 0.0567s/iter; left time: 447.0929s\n",
      "\titers: 200, epoch: 7 | loss: 0.2679348\n",
      "\tspeed: 0.0187s/iter; left time: 145.5315s\n",
      "\titers: 300, epoch: 7 | loss: 0.1970098\n",
      "\tspeed: 0.0187s/iter; left time: 143.4610s\n",
      "\titers: 400, epoch: 7 | loss: 0.2792156\n",
      "\tspeed: 0.0187s/iter; left time: 141.4528s\n",
      "\titers: 500, epoch: 7 | loss: 0.3566693\n",
      "\tspeed: 0.0187s/iter; left time: 139.7199s\n",
      "Epoch: 7 cost time: 10.927425384521484\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2409843 Vali Loss: 0.0515541 Test Loss: 0.1536989\n",
      "Validation loss decreased (0.052274 --> 0.051554).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2299775\n",
      "\tspeed: 0.0576s/iter; left time: 421.4555s\n",
      "\titers: 200, epoch: 8 | loss: 0.2109481\n",
      "\tspeed: 0.0186s/iter; left time: 134.0440s\n",
      "\titers: 300, epoch: 8 | loss: 0.1976876\n",
      "\tspeed: 0.0185s/iter; left time: 131.8753s\n",
      "\titers: 400, epoch: 8 | loss: 0.3727143\n",
      "\tspeed: 0.0186s/iter; left time: 130.2679s\n",
      "\titers: 500, epoch: 8 | loss: 0.2535819\n",
      "\tspeed: 0.0186s/iter; left time: 128.2140s\n",
      "Epoch: 8 cost time: 10.883481740951538\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2374367 Vali Loss: 0.0513114 Test Loss: 0.1529048\n",
      "Validation loss decreased (0.051554 --> 0.051311).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1614419\n",
      "\tspeed: 0.0584s/iter; left time: 393.7680s\n",
      "\titers: 200, epoch: 9 | loss: 0.3201469\n",
      "\tspeed: 0.0187s/iter; left time: 124.0993s\n",
      "\titers: 300, epoch: 9 | loss: 0.1190581\n",
      "\tspeed: 0.0187s/iter; left time: 122.3018s\n",
      "\titers: 400, epoch: 9 | loss: 0.2792606\n",
      "\tspeed: 0.0187s/iter; left time: 120.1952s\n",
      "\titers: 500, epoch: 9 | loss: 0.1153495\n",
      "\tspeed: 0.0187s/iter; left time: 118.4546s\n",
      "Epoch: 9 cost time: 10.930221796035767\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2353253 Vali Loss: 0.0511915 Test Loss: 0.1528212\n",
      "Validation loss decreased (0.051311 --> 0.051191).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2203282\n",
      "\tspeed: 0.0559s/iter; left time: 344.9658s\n",
      "\titers: 200, epoch: 10 | loss: 0.2659106\n",
      "\tspeed: 0.0186s/iter; left time: 112.9778s\n",
      "\titers: 300, epoch: 10 | loss: 0.1446058\n",
      "\tspeed: 0.0186s/iter; left time: 111.1993s\n",
      "\titers: 400, epoch: 10 | loss: 0.2124428\n",
      "\tspeed: 0.0186s/iter; left time: 109.3725s\n",
      "\titers: 500, epoch: 10 | loss: 0.2000332\n",
      "\tspeed: 0.0186s/iter; left time: 107.3548s\n",
      "Epoch: 10 cost time: 10.883831262588501\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2360373 Vali Loss: 0.0511895 Test Loss: 0.1525097\n",
      "Validation loss decreased (0.051191 --> 0.051189).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1884583\n",
      "\tspeed: 0.0571s/iter; left time: 319.9631s\n",
      "\titers: 200, epoch: 11 | loss: 0.3364536\n",
      "\tspeed: 0.0186s/iter; left time: 102.5493s\n",
      "\titers: 300, epoch: 11 | loss: 0.2422140\n",
      "\tspeed: 0.0187s/iter; left time: 100.9253s\n",
      "\titers: 400, epoch: 11 | loss: 0.1363387\n",
      "\tspeed: 0.0187s/iter; left time: 98.9280s\n",
      "\titers: 500, epoch: 11 | loss: 0.3151762\n",
      "\tspeed: 0.0187s/iter; left time: 97.0557s\n",
      "Epoch: 11 cost time: 10.955337762832642\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2365175 Vali Loss: 0.0511866 Test Loss: 0.1524168\n",
      "Validation loss decreased (0.051189 --> 0.051187).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.3024474\n",
      "\tspeed: 0.0592s/iter; left time: 297.9320s\n",
      "\titers: 200, epoch: 12 | loss: 0.3122122\n",
      "\tspeed: 0.0216s/iter; left time: 106.3847s\n",
      "\titers: 300, epoch: 12 | loss: 0.1375531\n",
      "\tspeed: 0.0216s/iter; left time: 104.1938s\n",
      "\titers: 400, epoch: 12 | loss: 0.2285809\n",
      "\tspeed: 0.0216s/iter; left time: 102.0114s\n",
      "\titers: 500, epoch: 12 | loss: 0.1865078\n",
      "\tspeed: 0.0216s/iter; left time: 99.9848s\n",
      "Epoch: 12 cost time: 12.59106993675232\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2352742 Vali Loss: 0.0509963 Test Loss: 0.1524491\n",
      "Validation loss decreased (0.051187 --> 0.050996).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.3926801\n",
      "\tspeed: 0.0614s/iter; left time: 274.0999s\n",
      "\titers: 200, epoch: 13 | loss: 0.2231462\n",
      "\tspeed: 0.0214s/iter; left time: 93.4529s\n",
      "\titers: 300, epoch: 13 | loss: 0.2874231\n",
      "\tspeed: 0.0197s/iter; left time: 83.8805s\n",
      "\titers: 400, epoch: 13 | loss: 0.1770340\n",
      "\tspeed: 0.0187s/iter; left time: 77.6139s\n",
      "\titers: 500, epoch: 13 | loss: 0.2780897\n",
      "\tspeed: 0.0187s/iter; left time: 75.7574s\n",
      "Epoch: 13 cost time: 11.596874237060547\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2350211 Vali Loss: 0.0510677 Test Loss: 0.1524323\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.3067077\n",
      "\tspeed: 0.0574s/iter; left time: 223.2718s\n",
      "\titers: 200, epoch: 14 | loss: 0.1708578\n",
      "\tspeed: 0.0208s/iter; left time: 79.0421s\n",
      "\titers: 300, epoch: 14 | loss: 0.1416878\n",
      "\tspeed: 0.0208s/iter; left time: 76.9228s\n",
      "\titers: 400, epoch: 14 | loss: 0.2449211\n",
      "\tspeed: 0.0208s/iter; left time: 74.8480s\n",
      "\titers: 500, epoch: 14 | loss: 0.2723967\n",
      "\tspeed: 0.0208s/iter; left time: 72.7538s\n",
      "Epoch: 14 cost time: 12.155897855758667\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2351534 Vali Loss: 0.0510622 Test Loss: 0.1524382\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.1750597\n",
      "\tspeed: 0.0570s/iter; left time: 189.4467s\n",
      "\titers: 200, epoch: 15 | loss: 0.2392311\n",
      "\tspeed: 0.0186s/iter; left time: 59.9501s\n",
      "\titers: 300, epoch: 15 | loss: 0.1884887\n",
      "\tspeed: 0.0186s/iter; left time: 58.1498s\n",
      "\titers: 400, epoch: 15 | loss: 0.2420480\n",
      "\tspeed: 0.0186s/iter; left time: 56.2470s\n",
      "\titers: 500, epoch: 15 | loss: 0.1719001\n",
      "\tspeed: 0.0186s/iter; left time: 54.3605s\n",
      "Epoch: 15 cost time: 10.87500262260437\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2347518 Vali Loss: 0.0510754 Test Loss: 0.1524345\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15269137918949127, mae:0.24526135623455048\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4440405\n",
      "\tspeed: 0.0299s/iter; left time: 338.0613s\n",
      "\titers: 200, epoch: 1 | loss: 0.2563414\n",
      "\tspeed: 0.0181s/iter; left time: 203.0248s\n",
      "\titers: 300, epoch: 1 | loss: 0.2045213\n",
      "\tspeed: 0.0181s/iter; left time: 201.1423s\n",
      "\titers: 400, epoch: 1 | loss: 0.1961407\n",
      "\tspeed: 0.0181s/iter; left time: 199.5981s\n",
      "\titers: 500, epoch: 1 | loss: 0.2555127\n",
      "\tspeed: 0.0182s/iter; left time: 197.8534s\n",
      "Epoch: 1 cost time: 11.566752672195435\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2561358 Vali Loss: 0.0489382 Test Loss: 0.1559181\n",
      "Validation loss decreased (inf --> 0.048938).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1675851\n",
      "\tspeed: 0.0579s/iter; left time: 621.4693s\n",
      "\titers: 200, epoch: 2 | loss: 0.3353744\n",
      "\tspeed: 0.0183s/iter; left time: 194.5502s\n",
      "\titers: 300, epoch: 2 | loss: 0.5020998\n",
      "\tspeed: 0.0182s/iter; left time: 192.0259s\n",
      "\titers: 400, epoch: 2 | loss: 0.2674504\n",
      "\tspeed: 0.0182s/iter; left time: 189.9468s\n",
      "\titers: 500, epoch: 2 | loss: 0.2010202\n",
      "\tspeed: 0.0182s/iter; left time: 188.2211s\n",
      "Epoch: 2 cost time: 10.695012092590332\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2872678 Vali Loss: 0.0682885 Test Loss: 0.1902094\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2125782\n",
      "\tspeed: 0.0553s/iter; left time: 561.9494s\n",
      "\titers: 200, epoch: 3 | loss: 0.2854894\n",
      "\tspeed: 0.0180s/iter; left time: 181.1935s\n",
      "\titers: 300, epoch: 3 | loss: 0.2461788\n",
      "\tspeed: 0.0180s/iter; left time: 179.1923s\n",
      "\titers: 400, epoch: 3 | loss: 0.2213762\n",
      "\tspeed: 0.0180s/iter; left time: 177.4602s\n",
      "\titers: 500, epoch: 3 | loss: 0.1950367\n",
      "\tspeed: 0.0180s/iter; left time: 175.3926s\n",
      "Epoch: 3 cost time: 10.515527486801147\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2838300 Vali Loss: 0.0612267 Test Loss: 0.1770139\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2848371\n",
      "\tspeed: 0.0550s/iter; left time: 527.1302s\n",
      "\titers: 200, epoch: 4 | loss: 0.1449135\n",
      "\tspeed: 0.0182s/iter; left time: 172.5543s\n",
      "\titers: 300, epoch: 4 | loss: 0.4600289\n",
      "\tspeed: 0.0182s/iter; left time: 170.9985s\n",
      "\titers: 400, epoch: 4 | loss: 0.1936486\n",
      "\tspeed: 0.0181s/iter; left time: 168.3854s\n",
      "\titers: 500, epoch: 4 | loss: 0.1924767\n",
      "\tspeed: 0.0181s/iter; left time: 166.5596s\n",
      "Epoch: 4 cost time: 10.63205075263977\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2692256 Vali Loss: 0.0593378 Test Loss: 0.1638209\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15614116191864014, mae:0.2531272768974304\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3439945\n",
      "\tspeed: 0.0325s/iter; left time: 367.7251s\n",
      "\titers: 200, epoch: 1 | loss: 0.2224072\n",
      "\tspeed: 0.0206s/iter; left time: 230.3047s\n",
      "\titers: 300, epoch: 1 | loss: 0.2982511\n",
      "\tspeed: 0.0207s/iter; left time: 229.3575s\n",
      "\titers: 400, epoch: 1 | loss: 0.3607741\n",
      "\tspeed: 0.0206s/iter; left time: 226.8367s\n",
      "\titers: 500, epoch: 1 | loss: 0.4477174\n",
      "\tspeed: 0.0206s/iter; left time: 224.5508s\n",
      "Epoch: 1 cost time: 12.995243072509766\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2980871 Vali Loss: 0.0661756 Test Loss: 0.1882649\n",
      "Validation loss decreased (inf --> 0.066176).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3172938\n",
      "\tspeed: 0.0605s/iter; left time: 648.8206s\n",
      "\titers: 200, epoch: 2 | loss: 0.4120645\n",
      "\tspeed: 0.0184s/iter; left time: 195.4045s\n",
      "\titers: 300, epoch: 2 | loss: 0.5728456\n",
      "\tspeed: 0.0184s/iter; left time: 193.4565s\n",
      "\titers: 400, epoch: 2 | loss: 0.2337411\n",
      "\tspeed: 0.0184s/iter; left time: 191.4185s\n",
      "\titers: 500, epoch: 2 | loss: 0.2605494\n",
      "\tspeed: 0.0187s/iter; left time: 193.5824s\n",
      "Epoch: 2 cost time: 11.002652168273926\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3149009 Vali Loss: 0.0684825 Test Loss: 0.1888304\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1760447\n",
      "\tspeed: 0.0625s/iter; left time: 634.5696s\n",
      "\titers: 200, epoch: 3 | loss: 0.3244613\n",
      "\tspeed: 0.0211s/iter; left time: 212.3612s\n",
      "\titers: 300, epoch: 3 | loss: 0.3146085\n",
      "\tspeed: 0.0211s/iter; left time: 210.5356s\n",
      "\titers: 400, epoch: 3 | loss: 0.3546068\n",
      "\tspeed: 0.0211s/iter; left time: 208.4058s\n",
      "\titers: 500, epoch: 3 | loss: 0.3117248\n",
      "\tspeed: 0.0206s/iter; left time: 201.4381s\n",
      "Epoch: 3 cost time: 12.095808744430542\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2925788 Vali Loss: 0.0609396 Test Loss: 0.1662711\n",
      "Validation loss decreased (0.066176 --> 0.060940).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2310022\n",
      "\tspeed: 0.0581s/iter; left time: 557.6276s\n",
      "\titers: 200, epoch: 4 | loss: 0.2679229\n",
      "\tspeed: 0.0185s/iter; left time: 175.9002s\n",
      "\titers: 300, epoch: 4 | loss: 0.2043655\n",
      "\tspeed: 0.0185s/iter; left time: 173.8590s\n",
      "\titers: 400, epoch: 4 | loss: 0.2407873\n",
      "\tspeed: 0.0185s/iter; left time: 171.8941s\n",
      "\titers: 500, epoch: 4 | loss: 0.1919197\n",
      "\tspeed: 0.0185s/iter; left time: 169.8261s\n",
      "Epoch: 4 cost time: 10.939177751541138\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2716240 Vali Loss: 0.0596362 Test Loss: 0.1630546\n",
      "Validation loss decreased (0.060940 --> 0.059636).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2280511\n",
      "\tspeed: 0.0559s/iter; left time: 504.1322s\n",
      "\titers: 200, epoch: 5 | loss: 0.2428658\n",
      "\tspeed: 0.0184s/iter; left time: 164.4224s\n",
      "\titers: 300, epoch: 5 | loss: 0.2224826\n",
      "\tspeed: 0.0184s/iter; left time: 162.4681s\n",
      "\titers: 400, epoch: 5 | loss: 0.2465171\n",
      "\tspeed: 0.0184s/iter; left time: 160.6571s\n",
      "\titers: 500, epoch: 5 | loss: 0.1690178\n",
      "\tspeed: 0.0184s/iter; left time: 158.7230s\n",
      "Epoch: 5 cost time: 10.788768768310547\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2609641 Vali Loss: 0.0581607 Test Loss: 0.1598530\n",
      "Validation loss decreased (0.059636 --> 0.058161).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4152997\n",
      "\tspeed: 0.0578s/iter; left time: 488.4949s\n",
      "\titers: 200, epoch: 6 | loss: 0.1902053\n",
      "\tspeed: 0.0184s/iter; left time: 153.3553s\n",
      "\titers: 300, epoch: 6 | loss: 0.4684936\n",
      "\tspeed: 0.0183s/iter; left time: 151.3576s\n",
      "\titers: 400, epoch: 6 | loss: 0.1891903\n",
      "\tspeed: 0.0184s/iter; left time: 149.5965s\n",
      "\titers: 500, epoch: 6 | loss: 0.2699345\n",
      "\tspeed: 0.0184s/iter; left time: 147.8087s\n",
      "Epoch: 6 cost time: 10.771824836730957\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2563190 Vali Loss: 0.0575713 Test Loss: 0.1564931\n",
      "Validation loss decreased (0.058161 --> 0.057571).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1933302\n",
      "\tspeed: 0.0568s/iter; left time: 447.3859s\n",
      "\titers: 200, epoch: 7 | loss: 0.2402638\n",
      "\tspeed: 0.0185s/iter; left time: 143.7249s\n",
      "\titers: 300, epoch: 7 | loss: 0.2761043\n",
      "\tspeed: 0.0185s/iter; left time: 141.8148s\n",
      "\titers: 400, epoch: 7 | loss: 0.2309266\n",
      "\tspeed: 0.0185s/iter; left time: 139.9671s\n",
      "\titers: 500, epoch: 7 | loss: 0.1958035\n",
      "\tspeed: 0.0184s/iter; left time: 138.0105s\n",
      "Epoch: 7 cost time: 10.824069499969482\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2547132 Vali Loss: 0.0579028 Test Loss: 0.1565990\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1519305\n",
      "\tspeed: 0.0591s/iter; left time: 431.8992s\n",
      "\titers: 200, epoch: 8 | loss: 0.1581205\n",
      "\tspeed: 0.0206s/iter; left time: 148.7560s\n",
      "\titers: 300, epoch: 8 | loss: 0.2114730\n",
      "\tspeed: 0.0206s/iter; left time: 146.6769s\n",
      "\titers: 400, epoch: 8 | loss: 0.2588448\n",
      "\tspeed: 0.0206s/iter; left time: 144.5011s\n",
      "\titers: 500, epoch: 8 | loss: 0.1707231\n",
      "\tspeed: 0.0206s/iter; left time: 142.4705s\n",
      "Epoch: 8 cost time: 12.071088075637817\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2521554 Vali Loss: 0.0575652 Test Loss: 0.1560879\n",
      "Validation loss decreased (0.057571 --> 0.057565).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2606703\n",
      "\tspeed: 0.0598s/iter; left time: 403.0335s\n",
      "\titers: 200, epoch: 9 | loss: 0.2921563\n",
      "\tspeed: 0.0182s/iter; left time: 120.9184s\n",
      "\titers: 300, epoch: 9 | loss: 0.2700184\n",
      "\tspeed: 0.0182s/iter; left time: 119.0223s\n",
      "\titers: 400, epoch: 9 | loss: 0.4152758\n",
      "\tspeed: 0.0182s/iter; left time: 117.4950s\n",
      "\titers: 500, epoch: 9 | loss: 0.2096159\n",
      "\tspeed: 0.0182s/iter; left time: 115.6548s\n",
      "Epoch: 9 cost time: 10.68292236328125\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2526055 Vali Loss: 0.0574870 Test Loss: 0.1556351\n",
      "Validation loss decreased (0.057565 --> 0.057487).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2231581\n",
      "\tspeed: 0.0565s/iter; left time: 348.4104s\n",
      "\titers: 200, epoch: 10 | loss: 0.2138534\n",
      "\tspeed: 0.0185s/iter; left time: 112.0533s\n",
      "\titers: 300, epoch: 10 | loss: 0.3480861\n",
      "\tspeed: 0.0185s/iter; left time: 110.3376s\n",
      "\titers: 400, epoch: 10 | loss: 0.2435866\n",
      "\tspeed: 0.0185s/iter; left time: 108.3702s\n",
      "\titers: 500, epoch: 10 | loss: 0.2342066\n",
      "\tspeed: 0.0185s/iter; left time: 106.5149s\n",
      "Epoch: 10 cost time: 10.852468729019165\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2514910 Vali Loss: 0.0572551 Test Loss: 0.1555971\n",
      "Validation loss decreased (0.057487 --> 0.057255).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2281349\n",
      "\tspeed: 0.0569s/iter; left time: 318.5186s\n",
      "\titers: 200, epoch: 11 | loss: 0.2650180\n",
      "\tspeed: 0.0184s/iter; left time: 101.2528s\n",
      "\titers: 300, epoch: 11 | loss: 0.2456894\n",
      "\tspeed: 0.0184s/iter; left time: 99.4122s\n",
      "\titers: 400, epoch: 11 | loss: 0.2605615\n",
      "\tspeed: 0.0184s/iter; left time: 97.5569s\n",
      "\titers: 500, epoch: 11 | loss: 0.1960399\n",
      "\tspeed: 0.0184s/iter; left time: 95.6858s\n",
      "Epoch: 11 cost time: 10.805077314376831\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2525947 Vali Loss: 0.0572932 Test Loss: 0.1556307\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1464265\n",
      "\tspeed: 0.0594s/iter; left time: 298.8416s\n",
      "\titers: 200, epoch: 12 | loss: 0.3264920\n",
      "\tspeed: 0.0196s/iter; left time: 96.7420s\n",
      "\titers: 300, epoch: 12 | loss: 0.2778126\n",
      "\tspeed: 0.0184s/iter; left time: 88.9651s\n",
      "\titers: 400, epoch: 12 | loss: 0.1597119\n",
      "\tspeed: 0.0191s/iter; left time: 90.5441s\n",
      "\titers: 500, epoch: 12 | loss: 0.3305517\n",
      "\tspeed: 0.0206s/iter; left time: 95.3987s\n",
      "Epoch: 12 cost time: 11.557821273803711\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2501300 Vali Loss: 0.0573288 Test Loss: 0.1556164\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2810948\n",
      "\tspeed: 0.0597s/iter; left time: 266.3176s\n",
      "\titers: 200, epoch: 13 | loss: 0.1267352\n",
      "\tspeed: 0.0206s/iter; left time: 89.7114s\n",
      "\titers: 300, epoch: 13 | loss: 0.1804745\n",
      "\tspeed: 0.0206s/iter; left time: 87.7820s\n",
      "\titers: 400, epoch: 13 | loss: 0.1703719\n",
      "\tspeed: 0.0206s/iter; left time: 85.5666s\n",
      "\titers: 500, epoch: 13 | loss: 0.1641101\n",
      "\tspeed: 0.0205s/iter; left time: 83.3649s\n",
      "Epoch: 13 cost time: 12.047001838684082\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2510339 Vali Loss: 0.0575678 Test Loss: 0.1556001\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15581247210502625, mae:0.2524816393852234\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2820024\n",
      "\tspeed: 0.0341s/iter; left time: 385.6921s\n",
      "\titers: 200, epoch: 1 | loss: 0.3506850\n",
      "\tspeed: 0.0220s/iter; left time: 246.3864s\n",
      "\titers: 300, epoch: 1 | loss: 0.3158914\n",
      "\tspeed: 0.0228s/iter; left time: 252.6402s\n",
      "\titers: 400, epoch: 1 | loss: 0.4694290\n",
      "\tspeed: 0.0240s/iter; left time: 263.6715s\n",
      "\titers: 500, epoch: 1 | loss: 0.4384179\n",
      "\tspeed: 0.0239s/iter; left time: 260.6969s\n",
      "Epoch: 1 cost time: 14.407434225082397\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3687771 Vali Loss: 0.0628052 Test Loss: 0.1830931\n",
      "Validation loss decreased (inf --> 0.062805).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3938585\n",
      "\tspeed: 0.0648s/iter; left time: 695.0129s\n",
      "\titers: 200, epoch: 2 | loss: 0.2715237\n",
      "\tspeed: 0.0236s/iter; left time: 250.7559s\n",
      "\titers: 300, epoch: 2 | loss: 0.2369220\n",
      "\tspeed: 0.0237s/iter; left time: 249.0882s\n",
      "\titers: 400, epoch: 2 | loss: 0.2923615\n",
      "\tspeed: 0.0236s/iter; left time: 246.6088s\n",
      "\titers: 500, epoch: 2 | loss: 0.2706907\n",
      "\tspeed: 0.0237s/iter; left time: 244.3491s\n",
      "Epoch: 2 cost time: 13.652963399887085\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3097597 Vali Loss: 0.0641888 Test Loss: 0.1786072\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.6045175\n",
      "\tspeed: 0.0651s/iter; left time: 661.1789s\n",
      "\titers: 200, epoch: 3 | loss: 0.1884619\n",
      "\tspeed: 0.0236s/iter; left time: 237.8792s\n",
      "\titers: 300, epoch: 3 | loss: 0.3644444\n",
      "\tspeed: 0.0236s/iter; left time: 235.3123s\n",
      "\titers: 400, epoch: 3 | loss: 0.2941984\n",
      "\tspeed: 0.0236s/iter; left time: 232.7604s\n",
      "\titers: 500, epoch: 3 | loss: 0.2039631\n",
      "\tspeed: 0.0236s/iter; left time: 230.6823s\n",
      "Epoch: 3 cost time: 13.785388469696045\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2877696 Vali Loss: 0.0594068 Test Loss: 0.1651690\n",
      "Validation loss decreased (0.062805 --> 0.059407).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2447802\n",
      "\tspeed: 0.0693s/iter; left time: 664.1949s\n",
      "\titers: 200, epoch: 4 | loss: 0.1950940\n",
      "\tspeed: 0.0240s/iter; left time: 227.5534s\n",
      "\titers: 300, epoch: 4 | loss: 0.2962869\n",
      "\tspeed: 0.0240s/iter; left time: 225.0400s\n",
      "\titers: 400, epoch: 4 | loss: 0.2045498\n",
      "\tspeed: 0.0240s/iter; left time: 223.1150s\n",
      "\titers: 500, epoch: 4 | loss: 0.3095154\n",
      "\tspeed: 0.0240s/iter; left time: 220.2865s\n",
      "Epoch: 4 cost time: 13.991017818450928\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2705669 Vali Loss: 0.0570208 Test Loss: 0.1616229\n",
      "Validation loss decreased (0.059407 --> 0.057021).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2788117\n",
      "\tspeed: 0.0645s/iter; left time: 582.0475s\n",
      "\titers: 200, epoch: 5 | loss: 0.2860644\n",
      "\tspeed: 0.0220s/iter; left time: 196.6196s\n",
      "\titers: 300, epoch: 5 | loss: 0.2103918\n",
      "\tspeed: 0.0220s/iter; left time: 194.3719s\n",
      "\titers: 400, epoch: 5 | loss: 0.2173960\n",
      "\tspeed: 0.0220s/iter; left time: 192.0365s\n",
      "\titers: 500, epoch: 5 | loss: 0.2649921\n",
      "\tspeed: 0.0220s/iter; left time: 190.0333s\n",
      "Epoch: 5 cost time: 12.887232542037964\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2607152 Vali Loss: 0.0576824 Test Loss: 0.1600697\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1931576\n",
      "\tspeed: 0.0633s/iter; left time: 535.1060s\n",
      "\titers: 200, epoch: 6 | loss: 0.3377065\n",
      "\tspeed: 0.0220s/iter; left time: 184.0431s\n",
      "\titers: 300, epoch: 6 | loss: 0.1976932\n",
      "\tspeed: 0.0220s/iter; left time: 181.7521s\n",
      "\titers: 400, epoch: 6 | loss: 0.1933704\n",
      "\tspeed: 0.0228s/iter; left time: 185.5289s\n",
      "\titers: 500, epoch: 6 | loss: 0.3341262\n",
      "\tspeed: 0.0240s/iter; left time: 193.1173s\n",
      "Epoch: 6 cost time: 13.282084465026855\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2561844 Vali Loss: 0.0569983 Test Loss: 0.1571478\n",
      "Validation loss decreased (0.057021 --> 0.056998).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3503615\n",
      "\tspeed: 0.0649s/iter; left time: 511.7222s\n",
      "\titers: 200, epoch: 7 | loss: 0.2650831\n",
      "\tspeed: 0.0221s/iter; left time: 172.1655s\n",
      "\titers: 300, epoch: 7 | loss: 0.1854428\n",
      "\tspeed: 0.0221s/iter; left time: 169.6822s\n",
      "\titers: 400, epoch: 7 | loss: 0.2032392\n",
      "\tspeed: 0.0221s/iter; left time: 167.4948s\n",
      "\titers: 500, epoch: 7 | loss: 0.2495519\n",
      "\tspeed: 0.0221s/iter; left time: 165.5382s\n",
      "Epoch: 7 cost time: 12.925457954406738\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2529105 Vali Loss: 0.0565248 Test Loss: 0.1552147\n",
      "Validation loss decreased (0.056998 --> 0.056525).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1709098\n",
      "\tspeed: 0.0642s/iter; left time: 469.6872s\n",
      "\titers: 200, epoch: 8 | loss: 0.5014006\n",
      "\tspeed: 0.0220s/iter; left time: 158.5240s\n",
      "\titers: 300, epoch: 8 | loss: 0.1560250\n",
      "\tspeed: 0.0220s/iter; left time: 156.2533s\n",
      "\titers: 400, epoch: 8 | loss: 0.2622548\n",
      "\tspeed: 0.0220s/iter; left time: 154.0322s\n",
      "\titers: 500, epoch: 8 | loss: 0.2482225\n",
      "\tspeed: 0.0220s/iter; left time: 151.8253s\n",
      "Epoch: 8 cost time: 12.829472303390503\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2526525 Vali Loss: 0.0563946 Test Loss: 0.1553344\n",
      "Validation loss decreased (0.056525 --> 0.056395).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2847161\n",
      "\tspeed: 0.0655s/iter; left time: 441.7660s\n",
      "\titers: 200, epoch: 9 | loss: 0.2171539\n",
      "\tspeed: 0.0221s/iter; left time: 147.0125s\n",
      "\titers: 300, epoch: 9 | loss: 0.3929270\n",
      "\tspeed: 0.0221s/iter; left time: 144.6263s\n",
      "\titers: 400, epoch: 9 | loss: 0.2341013\n",
      "\tspeed: 0.0221s/iter; left time: 142.6536s\n",
      "\titers: 500, epoch: 9 | loss: 0.1879824\n",
      "\tspeed: 0.0221s/iter; left time: 140.2170s\n",
      "Epoch: 9 cost time: 12.910869598388672\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2493533 Vali Loss: 0.0561724 Test Loss: 0.1555994\n",
      "Validation loss decreased (0.056395 --> 0.056172).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2559296\n",
      "\tspeed: 0.0660s/iter; left time: 407.2262s\n",
      "\titers: 200, epoch: 10 | loss: 0.2823784\n",
      "\tspeed: 0.0221s/iter; left time: 134.2113s\n",
      "\titers: 300, epoch: 10 | loss: 0.2442188\n",
      "\tspeed: 0.0221s/iter; left time: 131.7520s\n",
      "\titers: 400, epoch: 10 | loss: 0.3683121\n",
      "\tspeed: 0.0221s/iter; left time: 129.7680s\n",
      "\titers: 500, epoch: 10 | loss: 0.2910262\n",
      "\tspeed: 0.0220s/iter; left time: 127.1494s\n",
      "Epoch: 10 cost time: 12.931173086166382\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2496015 Vali Loss: 0.0566815 Test Loss: 0.1553975\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.1941482\n",
      "\tspeed: 0.0637s/iter; left time: 356.6519s\n",
      "\titers: 200, epoch: 11 | loss: 0.1915396\n",
      "\tspeed: 0.0220s/iter; left time: 121.2064s\n",
      "\titers: 300, epoch: 11 | loss: 0.1241986\n",
      "\tspeed: 0.0220s/iter; left time: 118.9062s\n",
      "\titers: 400, epoch: 11 | loss: 0.1996565\n",
      "\tspeed: 0.0220s/iter; left time: 116.8219s\n",
      "\titers: 500, epoch: 11 | loss: 0.1841637\n",
      "\tspeed: 0.0220s/iter; left time: 114.5714s\n",
      "Epoch: 11 cost time: 12.859290838241577\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2511690 Vali Loss: 0.0564752 Test Loss: 0.1553328\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2536335\n",
      "\tspeed: 0.0640s/iter; left time: 321.7890s\n",
      "\titers: 200, epoch: 12 | loss: 0.2307326\n",
      "\tspeed: 0.0221s/iter; left time: 109.1946s\n",
      "\titers: 300, epoch: 12 | loss: 0.3589673\n",
      "\tspeed: 0.0221s/iter; left time: 106.8491s\n",
      "\titers: 400, epoch: 12 | loss: 0.1117583\n",
      "\tspeed: 0.0221s/iter; left time: 104.6963s\n",
      "\titers: 500, epoch: 12 | loss: 0.2942688\n",
      "\tspeed: 0.0221s/iter; left time: 102.4175s\n",
      "Epoch: 12 cost time: 12.931370735168457\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2487588 Vali Loss: 0.0564325 Test Loss: 0.1553530\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15584561228752136, mae:0.2539593577384949\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3483191\n",
      "\tspeed: 0.0331s/iter; left time: 373.5096s\n",
      "\titers: 200, epoch: 1 | loss: 0.1764646\n",
      "\tspeed: 0.0213s/iter; left time: 238.7167s\n",
      "\titers: 300, epoch: 1 | loss: 0.2731398\n",
      "\tspeed: 0.0213s/iter; left time: 236.5706s\n",
      "\titers: 400, epoch: 1 | loss: 0.3596932\n",
      "\tspeed: 0.0213s/iter; left time: 234.6394s\n",
      "\titers: 500, epoch: 1 | loss: 0.2425520\n",
      "\tspeed: 0.0213s/iter; left time: 232.3121s\n",
      "Epoch: 1 cost time: 13.383516073226929\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3932763 Vali Loss: 0.0687599 Test Loss: 0.1930391\n",
      "Validation loss decreased (inf --> 0.068760).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.3420772\n",
      "\tspeed: 0.0639s/iter; left time: 685.5531s\n",
      "\titers: 200, epoch: 2 | loss: 0.2079389\n",
      "\tspeed: 0.0232s/iter; left time: 246.7263s\n",
      "\titers: 300, epoch: 2 | loss: 0.4470553\n",
      "\tspeed: 0.0232s/iter; left time: 244.6926s\n",
      "\titers: 400, epoch: 2 | loss: 0.2903394\n",
      "\tspeed: 0.0233s/iter; left time: 242.6212s\n",
      "\titers: 500, epoch: 2 | loss: 0.2193912\n",
      "\tspeed: 0.0221s/iter; left time: 228.1391s\n",
      "Epoch: 2 cost time: 13.170797348022461\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3195452 Vali Loss: 0.0677651 Test Loss: 0.1936957\n",
      "Validation loss decreased (0.068760 --> 0.067765).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.2322346\n",
      "\tspeed: 0.0626s/iter; left time: 635.9053s\n",
      "\titers: 200, epoch: 3 | loss: 0.2921307\n",
      "\tspeed: 0.0215s/iter; left time: 216.2617s\n",
      "\titers: 300, epoch: 3 | loss: 0.1781242\n",
      "\tspeed: 0.0215s/iter; left time: 213.9341s\n",
      "\titers: 400, epoch: 3 | loss: 0.1827128\n",
      "\tspeed: 0.0215s/iter; left time: 211.7461s\n",
      "\titers: 500, epoch: 3 | loss: 0.2208113\n",
      "\tspeed: 0.0215s/iter; left time: 209.5536s\n",
      "Epoch: 3 cost time: 12.586251258850098\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2932154 Vali Loss: 0.0649118 Test Loss: 0.1745840\n",
      "Validation loss decreased (0.067765 --> 0.064912).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.2069071\n",
      "\tspeed: 0.0629s/iter; left time: 603.2570s\n",
      "\titers: 200, epoch: 4 | loss: 0.2678215\n",
      "\tspeed: 0.0214s/iter; left time: 203.3189s\n",
      "\titers: 300, epoch: 4 | loss: 0.2591911\n",
      "\tspeed: 0.0214s/iter; left time: 201.3335s\n",
      "\titers: 400, epoch: 4 | loss: 0.3349337\n",
      "\tspeed: 0.0214s/iter; left time: 198.8994s\n",
      "\titers: 500, epoch: 4 | loss: 0.1894222\n",
      "\tspeed: 0.0214s/iter; left time: 196.7450s\n",
      "Epoch: 4 cost time: 12.52560806274414\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2769989 Vali Loss: 0.0618164 Test Loss: 0.1722599\n",
      "Validation loss decreased (0.064912 --> 0.061816).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.4016593\n",
      "\tspeed: 0.0652s/iter; left time: 587.8599s\n",
      "\titers: 200, epoch: 5 | loss: 0.1939733\n",
      "\tspeed: 0.0214s/iter; left time: 191.2290s\n",
      "\titers: 300, epoch: 5 | loss: 0.3224022\n",
      "\tspeed: 0.0214s/iter; left time: 189.1261s\n",
      "\titers: 400, epoch: 5 | loss: 0.2390254\n",
      "\tspeed: 0.0214s/iter; left time: 186.6897s\n",
      "\titers: 500, epoch: 5 | loss: 0.2287242\n",
      "\tspeed: 0.0214s/iter; left time: 184.6540s\n",
      "Epoch: 5 cost time: 12.553574085235596\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2684514 Vali Loss: 0.0615603 Test Loss: 0.1668281\n",
      "Validation loss decreased (0.061816 --> 0.061560).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2436883\n",
      "\tspeed: 0.0629s/iter; left time: 531.5315s\n",
      "\titers: 200, epoch: 6 | loss: 0.3324875\n",
      "\tspeed: 0.0214s/iter; left time: 178.5228s\n",
      "\titers: 300, epoch: 6 | loss: 0.2827042\n",
      "\tspeed: 0.0214s/iter; left time: 176.3970s\n",
      "\titers: 400, epoch: 6 | loss: 0.1656434\n",
      "\tspeed: 0.0214s/iter; left time: 174.2341s\n",
      "\titers: 500, epoch: 6 | loss: 0.1709646\n",
      "\tspeed: 0.0214s/iter; left time: 171.9657s\n",
      "Epoch: 6 cost time: 12.502390146255493\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2631734 Vali Loss: 0.0609450 Test Loss: 0.1660672\n",
      "Validation loss decreased (0.061560 --> 0.060945).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2456770\n",
      "\tspeed: 0.0626s/iter; left time: 493.2266s\n",
      "\titers: 200, epoch: 7 | loss: 0.3631036\n",
      "\tspeed: 0.0213s/iter; left time: 165.6497s\n",
      "\titers: 300, epoch: 7 | loss: 0.3073087\n",
      "\tspeed: 0.0213s/iter; left time: 163.5840s\n",
      "\titers: 400, epoch: 7 | loss: 0.1740722\n",
      "\tspeed: 0.0213s/iter; left time: 161.4689s\n",
      "\titers: 500, epoch: 7 | loss: 0.2432873\n",
      "\tspeed: 0.0213s/iter; left time: 159.3229s\n",
      "Epoch: 7 cost time: 12.454591989517212\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2611012 Vali Loss: 0.0604223 Test Loss: 0.1646789\n",
      "Validation loss decreased (0.060945 --> 0.060422).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2533799\n",
      "\tspeed: 0.0629s/iter; left time: 460.1789s\n",
      "\titers: 200, epoch: 8 | loss: 0.2918572\n",
      "\tspeed: 0.0213s/iter; left time: 153.8919s\n",
      "\titers: 300, epoch: 8 | loss: 0.2444859\n",
      "\tspeed: 0.0213s/iter; left time: 151.7778s\n",
      "\titers: 400, epoch: 8 | loss: 0.2939004\n",
      "\tspeed: 0.0213s/iter; left time: 149.6167s\n",
      "\titers: 500, epoch: 8 | loss: 0.3102149\n",
      "\tspeed: 0.0213s/iter; left time: 147.4563s\n",
      "Epoch: 8 cost time: 12.489131450653076\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2591132 Vali Loss: 0.0587517 Test Loss: 0.1642147\n",
      "Validation loss decreased (0.060422 --> 0.058752).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2027416\n",
      "\tspeed: 0.0631s/iter; left time: 425.2815s\n",
      "\titers: 200, epoch: 9 | loss: 0.3553377\n",
      "\tspeed: 0.0214s/iter; left time: 142.2365s\n",
      "\titers: 300, epoch: 9 | loss: 0.2911352\n",
      "\tspeed: 0.0214s/iter; left time: 140.1132s\n",
      "\titers: 400, epoch: 9 | loss: 0.2423984\n",
      "\tspeed: 0.0214s/iter; left time: 137.9988s\n",
      "\titers: 500, epoch: 9 | loss: 0.1473337\n",
      "\tspeed: 0.0214s/iter; left time: 135.8209s\n",
      "Epoch: 9 cost time: 12.532773733139038\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2591514 Vali Loss: 0.0583740 Test Loss: 0.1639684\n",
      "Validation loss decreased (0.058752 --> 0.058374).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1709135\n",
      "\tspeed: 0.0622s/iter; left time: 383.7393s\n",
      "\titers: 200, epoch: 10 | loss: 0.2414147\n",
      "\tspeed: 0.0213s/iter; left time: 129.3851s\n",
      "\titers: 300, epoch: 10 | loss: 0.1864398\n",
      "\tspeed: 0.0213s/iter; left time: 127.1677s\n",
      "\titers: 400, epoch: 10 | loss: 0.2411635\n",
      "\tspeed: 0.0213s/iter; left time: 125.0189s\n",
      "\titers: 500, epoch: 10 | loss: 0.3385921\n",
      "\tspeed: 0.0213s/iter; left time: 123.0806s\n",
      "Epoch: 10 cost time: 12.46562933921814\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2577685 Vali Loss: 0.0582898 Test Loss: 0.1638390\n",
      "Validation loss decreased (0.058374 --> 0.058290).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4224576\n",
      "\tspeed: 0.0632s/iter; left time: 354.2378s\n",
      "\titers: 200, epoch: 11 | loss: 0.2363219\n",
      "\tspeed: 0.0214s/iter; left time: 117.5685s\n",
      "\titers: 300, epoch: 11 | loss: 0.2084909\n",
      "\tspeed: 0.0214s/iter; left time: 115.3943s\n",
      "\titers: 400, epoch: 11 | loss: 0.1982952\n",
      "\tspeed: 0.0214s/iter; left time: 113.2963s\n",
      "\titers: 500, epoch: 11 | loss: 0.2825384\n",
      "\tspeed: 0.0214s/iter; left time: 111.0623s\n",
      "Epoch: 11 cost time: 12.593770503997803\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2576231 Vali Loss: 0.0584254 Test Loss: 0.1637842\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.3107935\n",
      "\tspeed: 0.0630s/iter; left time: 317.1349s\n",
      "\titers: 200, epoch: 12 | loss: 0.2872938\n",
      "\tspeed: 0.0213s/iter; left time: 105.1231s\n",
      "\titers: 300, epoch: 12 | loss: 0.2831005\n",
      "\tspeed: 0.0213s/iter; left time: 102.9150s\n",
      "\titers: 400, epoch: 12 | loss: 0.4618843\n",
      "\tspeed: 0.0213s/iter; left time: 100.8120s\n",
      "\titers: 500, epoch: 12 | loss: 0.1982855\n",
      "\tspeed: 0.0213s/iter; left time: 98.6468s\n",
      "Epoch: 12 cost time: 12.502469778060913\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2571457 Vali Loss: 0.0585422 Test Loss: 0.1637864\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1983078\n",
      "\tspeed: 0.0656s/iter; left time: 292.7464s\n",
      "\titers: 200, epoch: 13 | loss: 0.3412160\n",
      "\tspeed: 0.0233s/iter; left time: 101.4738s\n",
      "\titers: 300, epoch: 13 | loss: 0.3556422\n",
      "\tspeed: 0.0226s/iter; left time: 96.1091s\n",
      "\titers: 400, epoch: 13 | loss: 0.2023166\n",
      "\tspeed: 0.0214s/iter; left time: 89.0920s\n",
      "\titers: 500, epoch: 13 | loss: 0.2963874\n",
      "\tspeed: 0.0214s/iter; left time: 86.8644s\n",
      "Epoch: 13 cost time: 12.971540451049805\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2576762 Vali Loss: 0.0589034 Test Loss: 0.1637957\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16409872472286224, mae:0.2606174945831299\n",
      ">>> LR=1e-3,DO=0.2,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3804397\n",
      "\tspeed: 0.0341s/iter; left time: 385.0031s\n",
      "\titers: 200, epoch: 1 | loss: 0.3089974\n",
      "\tspeed: 0.0222s/iter; left time: 248.5842s\n",
      "\titers: 300, epoch: 1 | loss: 0.3638180\n",
      "\tspeed: 0.0222s/iter; left time: 246.2041s\n",
      "\titers: 400, epoch: 1 | loss: 0.4387281\n",
      "\tspeed: 0.0222s/iter; left time: 244.0192s\n",
      "\titers: 500, epoch: 1 | loss: 0.3337742\n",
      "\tspeed: 0.0222s/iter; left time: 241.7443s\n",
      "Epoch: 1 cost time: 13.891973972320557\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.3645407 Vali Loss: 0.0670480 Test Loss: 0.1957417\n",
      "Validation loss decreased (inf --> 0.067048).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.2270536\n",
      "\tspeed: 0.0615s/iter; left time: 659.9578s\n",
      "\titers: 200, epoch: 2 | loss: 0.5691278\n",
      "\tspeed: 0.0202s/iter; left time: 214.6213s\n",
      "\titers: 300, epoch: 2 | loss: 0.2426668\n",
      "\tspeed: 0.0202s/iter; left time: 212.4459s\n",
      "\titers: 400, epoch: 2 | loss: 0.2305209\n",
      "\tspeed: 0.0202s/iter; left time: 210.3474s\n",
      "\titers: 500, epoch: 2 | loss: 0.1968035\n",
      "\tspeed: 0.0202s/iter; left time: 208.2809s\n",
      "Epoch: 2 cost time: 11.82432770729065\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.3082443 Vali Loss: 0.0630910 Test Loss: 0.1835305\n",
      "Validation loss decreased (0.067048 --> 0.063091).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.1988384\n",
      "\tspeed: 0.0617s/iter; left time: 626.5061s\n",
      "\titers: 200, epoch: 3 | loss: 0.2185974\n",
      "\tspeed: 0.0202s/iter; left time: 202.7395s\n",
      "\titers: 300, epoch: 3 | loss: 0.2903170\n",
      "\tspeed: 0.0201s/iter; left time: 200.4387s\n",
      "\titers: 400, epoch: 3 | loss: 0.3049778\n",
      "\tspeed: 0.0201s/iter; left time: 198.5071s\n",
      "\titers: 500, epoch: 3 | loss: 0.2555400\n",
      "\tspeed: 0.0201s/iter; left time: 196.3211s\n",
      "Epoch: 3 cost time: 11.794127941131592\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2866641 Vali Loss: 0.0612436 Test Loss: 0.1738186\n",
      "Validation loss decreased (0.063091 --> 0.061244).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.1943121\n",
      "\tspeed: 0.0622s/iter; left time: 596.6749s\n",
      "\titers: 200, epoch: 4 | loss: 0.4831101\n",
      "\tspeed: 0.0201s/iter; left time: 190.6399s\n",
      "\titers: 300, epoch: 4 | loss: 0.1913214\n",
      "\tspeed: 0.0201s/iter; left time: 188.4711s\n",
      "\titers: 400, epoch: 4 | loss: 0.2798837\n",
      "\tspeed: 0.0201s/iter; left time: 186.4224s\n",
      "\titers: 500, epoch: 4 | loss: 0.2258324\n",
      "\tspeed: 0.0201s/iter; left time: 184.6538s\n",
      "Epoch: 4 cost time: 11.766197681427002\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2727965 Vali Loss: 0.0597988 Test Loss: 0.1625510\n",
      "Validation loss decreased (0.061244 --> 0.059799).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.2511033\n",
      "\tspeed: 0.0623s/iter; left time: 562.4297s\n",
      "\titers: 200, epoch: 5 | loss: 0.6101279\n",
      "\tspeed: 0.0233s/iter; left time: 207.7919s\n",
      "\titers: 300, epoch: 5 | loss: 0.2532804\n",
      "\tspeed: 0.0232s/iter; left time: 204.8087s\n",
      "\titers: 400, epoch: 5 | loss: 0.2133041\n",
      "\tspeed: 0.0232s/iter; left time: 202.4649s\n",
      "\titers: 500, epoch: 5 | loss: 0.2544225\n",
      "\tspeed: 0.0232s/iter; left time: 199.9604s\n",
      "Epoch: 5 cost time: 13.459649562835693\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2646309 Vali Loss: 0.0560531 Test Loss: 0.1614465\n",
      "Validation loss decreased (0.059799 --> 0.056053).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2450040\n",
      "\tspeed: 0.0677s/iter; left time: 572.4231s\n",
      "\titers: 200, epoch: 6 | loss: 0.3549562\n",
      "\tspeed: 0.0231s/iter; left time: 193.1098s\n",
      "\titers: 300, epoch: 6 | loss: 0.2124002\n",
      "\tspeed: 0.0231s/iter; left time: 190.7210s\n",
      "\titers: 400, epoch: 6 | loss: 0.1401052\n",
      "\tspeed: 0.0231s/iter; left time: 188.4062s\n",
      "\titers: 500, epoch: 6 | loss: 0.4268948\n",
      "\tspeed: 0.0231s/iter; left time: 186.0937s\n",
      "Epoch: 6 cost time: 13.490031719207764\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2608943 Vali Loss: 0.0560516 Test Loss: 0.1611834\n",
      "Validation loss decreased (0.056053 --> 0.056052).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2776586\n",
      "\tspeed: 0.0645s/iter; left time: 507.9943s\n",
      "\titers: 200, epoch: 7 | loss: 0.2870423\n",
      "\tspeed: 0.0232s/iter; left time: 180.7517s\n",
      "\titers: 300, epoch: 7 | loss: 0.3501499\n",
      "\tspeed: 0.0232s/iter; left time: 178.0090s\n",
      "\titers: 400, epoch: 7 | loss: 0.4562049\n",
      "\tspeed: 0.0232s/iter; left time: 175.7171s\n",
      "\titers: 500, epoch: 7 | loss: 0.2012505\n",
      "\tspeed: 0.0232s/iter; left time: 173.3941s\n",
      "Epoch: 7 cost time: 13.505961656570435\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2569121 Vali Loss: 0.0560550 Test Loss: 0.1617218\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3294926\n",
      "\tspeed: 0.0637s/iter; left time: 465.7291s\n",
      "\titers: 200, epoch: 8 | loss: 0.4140672\n",
      "\tspeed: 0.0230s/iter; left time: 166.0453s\n",
      "\titers: 300, epoch: 8 | loss: 0.3153934\n",
      "\tspeed: 0.0230s/iter; left time: 163.7205s\n",
      "\titers: 400, epoch: 8 | loss: 0.1939416\n",
      "\tspeed: 0.0230s/iter; left time: 161.3855s\n",
      "\titers: 500, epoch: 8 | loss: 0.2573278\n",
      "\tspeed: 0.0230s/iter; left time: 159.0880s\n",
      "Epoch: 8 cost time: 13.437071084976196\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2557399 Vali Loss: 0.0562486 Test Loss: 0.1602705\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2525150\n",
      "\tspeed: 0.0643s/iter; left time: 433.2965s\n",
      "\titers: 200, epoch: 9 | loss: 0.2477296\n",
      "\tspeed: 0.0231s/iter; left time: 153.5171s\n",
      "\titers: 300, epoch: 9 | loss: 0.3376626\n",
      "\tspeed: 0.0231s/iter; left time: 151.3282s\n",
      "\titers: 400, epoch: 9 | loss: 0.1935522\n",
      "\tspeed: 0.0231s/iter; left time: 148.9798s\n",
      "\titers: 500, epoch: 9 | loss: 0.1374224\n",
      "\tspeed: 0.0231s/iter; left time: 146.6622s\n",
      "Epoch: 9 cost time: 13.501768350601196\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2557171 Vali Loss: 0.0556010 Test Loss: 0.1605741\n",
      "Validation loss decreased (0.056052 --> 0.055601).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2737424\n",
      "\tspeed: 0.0653s/iter; left time: 402.7719s\n",
      "\titers: 200, epoch: 10 | loss: 0.1813244\n",
      "\tspeed: 0.0231s/iter; left time: 140.5382s\n",
      "\titers: 300, epoch: 10 | loss: 0.4009261\n",
      "\tspeed: 0.0231s/iter; left time: 138.2169s\n",
      "\titers: 400, epoch: 10 | loss: 0.3548692\n",
      "\tspeed: 0.0231s/iter; left time: 135.8010s\n",
      "\titers: 500, epoch: 10 | loss: 0.2752964\n",
      "\tspeed: 0.0231s/iter; left time: 133.4847s\n",
      "Epoch: 10 cost time: 13.479889631271362\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2539065 Vali Loss: 0.0560734 Test Loss: 0.1604954\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.2159983\n",
      "\tspeed: 0.0633s/iter; left time: 354.5469s\n",
      "\titers: 200, epoch: 11 | loss: 0.2328295\n",
      "\tspeed: 0.0230s/iter; left time: 126.7338s\n",
      "\titers: 300, epoch: 11 | loss: 0.1948394\n",
      "\tspeed: 0.0230s/iter; left time: 124.3820s\n",
      "\titers: 400, epoch: 11 | loss: 0.2906358\n",
      "\tspeed: 0.0230s/iter; left time: 121.9739s\n",
      "\titers: 500, epoch: 11 | loss: 0.2166879\n",
      "\tspeed: 0.0230s/iter; left time: 119.6646s\n",
      "Epoch: 11 cost time: 13.439470767974854\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2556766 Vali Loss: 0.0561369 Test Loss: 0.1604189\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2205788\n",
      "\tspeed: 0.0643s/iter; left time: 323.5204s\n",
      "\titers: 200, epoch: 12 | loss: 0.2239135\n",
      "\tspeed: 0.0231s/iter; left time: 113.8641s\n",
      "\titers: 300, epoch: 12 | loss: 0.4537135\n",
      "\tspeed: 0.0231s/iter; left time: 111.6032s\n",
      "\titers: 400, epoch: 12 | loss: 0.2867092\n",
      "\tspeed: 0.0231s/iter; left time: 109.2001s\n",
      "\titers: 500, epoch: 12 | loss: 0.3965144\n",
      "\tspeed: 0.0231s/iter; left time: 106.8991s\n",
      "Epoch: 12 cost time: 13.4809889793396\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2542800 Vali Loss: 0.0559142 Test Loss: 0.1604519\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16082032024860382, mae:0.2550908029079437\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1944437\n",
      "\tspeed: 0.0216s/iter; left time: 243.9524s\n",
      "\titers: 200, epoch: 1 | loss: 0.1642704\n",
      "\tspeed: 0.0099s/iter; left time: 111.0251s\n",
      "\titers: 300, epoch: 1 | loss: 0.2080029\n",
      "\tspeed: 0.0099s/iter; left time: 110.2856s\n",
      "\titers: 400, epoch: 1 | loss: 0.1865716\n",
      "\tspeed: 0.0099s/iter; left time: 108.9345s\n",
      "\titers: 500, epoch: 1 | loss: 0.1544829\n",
      "\tspeed: 0.0099s/iter; left time: 108.1734s\n",
      "Epoch: 1 cost time: 6.8672003746032715\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1804872 Vali Loss: 0.0488531 Test Loss: 0.1410827\n",
      "Validation loss decreased (inf --> 0.048853).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1664304\n",
      "\tspeed: 0.0367s/iter; left time: 393.8329s\n",
      "\titers: 200, epoch: 2 | loss: 0.2020042\n",
      "\tspeed: 0.0100s/iter; left time: 106.5745s\n",
      "\titers: 300, epoch: 2 | loss: 0.1241654\n",
      "\tspeed: 0.0100s/iter; left time: 105.0727s\n",
      "\titers: 400, epoch: 2 | loss: 0.1342875\n",
      "\tspeed: 0.0100s/iter; left time: 104.5519s\n",
      "\titers: 500, epoch: 2 | loss: 0.1565352\n",
      "\tspeed: 0.0100s/iter; left time: 103.5943s\n",
      "Epoch: 2 cost time: 5.9860358238220215\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1322044 Vali Loss: 0.0428261 Test Loss: 0.1458761\n",
      "Validation loss decreased (0.048853 --> 0.042826).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0983299\n",
      "\tspeed: 0.0383s/iter; left time: 389.2814s\n",
      "\titers: 200, epoch: 3 | loss: 0.0833045\n",
      "\tspeed: 0.0112s/iter; left time: 113.1309s\n",
      "\titers: 300, epoch: 3 | loss: 0.0775792\n",
      "\tspeed: 0.0112s/iter; left time: 111.6394s\n",
      "\titers: 400, epoch: 3 | loss: 0.1087999\n",
      "\tspeed: 0.0112s/iter; left time: 110.6030s\n",
      "\titers: 500, epoch: 3 | loss: 0.1334865\n",
      "\tspeed: 0.0112s/iter; left time: 109.0829s\n",
      "Epoch: 3 cost time: 6.70125937461853\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0894185 Vali Loss: 0.0405665 Test Loss: 0.1412212\n",
      "Validation loss decreased (0.042826 --> 0.040567).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0738157\n",
      "\tspeed: 0.0357s/iter; left time: 342.4441s\n",
      "\titers: 200, epoch: 4 | loss: 0.0421157\n",
      "\tspeed: 0.0099s/iter; left time: 94.4128s\n",
      "\titers: 300, epoch: 4 | loss: 0.0593118\n",
      "\tspeed: 0.0100s/iter; left time: 93.4989s\n",
      "\titers: 400, epoch: 4 | loss: 0.0594450\n",
      "\tspeed: 0.0100s/iter; left time: 92.5498s\n",
      "\titers: 500, epoch: 4 | loss: 0.0589572\n",
      "\tspeed: 0.0099s/iter; left time: 91.3903s\n",
      "Epoch: 4 cost time: 5.962709188461304\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0601401 Vali Loss: 0.0413942 Test Loss: 0.1406739\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0557960\n",
      "\tspeed: 0.0371s/iter; left time: 334.4861s\n",
      "\titers: 200, epoch: 5 | loss: 0.0553470\n",
      "\tspeed: 0.0099s/iter; left time: 88.2350s\n",
      "\titers: 300, epoch: 5 | loss: 0.0386101\n",
      "\tspeed: 0.0099s/iter; left time: 87.2026s\n",
      "\titers: 400, epoch: 5 | loss: 0.0521770\n",
      "\tspeed: 0.0099s/iter; left time: 86.3898s\n",
      "\titers: 500, epoch: 5 | loss: 0.0316364\n",
      "\tspeed: 0.0099s/iter; left time: 85.2588s\n",
      "Epoch: 5 cost time: 5.886400461196899\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0463121 Vali Loss: 0.0392300 Test Loss: 0.1354600\n",
      "Validation loss decreased (0.040567 --> 0.039230).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0314671\n",
      "\tspeed: 0.0365s/iter; left time: 308.3248s\n",
      "\titers: 200, epoch: 6 | loss: 0.0347356\n",
      "\tspeed: 0.0112s/iter; left time: 93.3634s\n",
      "\titers: 300, epoch: 6 | loss: 0.0368734\n",
      "\tspeed: 0.0112s/iter; left time: 92.1858s\n",
      "\titers: 400, epoch: 6 | loss: 0.0429091\n",
      "\tspeed: 0.0112s/iter; left time: 91.4468s\n",
      "\titers: 500, epoch: 6 | loss: 0.0451638\n",
      "\tspeed: 0.0100s/iter; left time: 80.7693s\n",
      "Epoch: 6 cost time: 6.474477529525757\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0389716 Vali Loss: 0.0409728 Test Loss: 0.1399496\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0390323\n",
      "\tspeed: 0.0352s/iter; left time: 277.4459s\n",
      "\titers: 200, epoch: 7 | loss: 0.0337189\n",
      "\tspeed: 0.0100s/iter; left time: 78.0340s\n",
      "\titers: 300, epoch: 7 | loss: 0.0284107\n",
      "\tspeed: 0.0100s/iter; left time: 76.8720s\n",
      "\titers: 400, epoch: 7 | loss: 0.0361575\n",
      "\tspeed: 0.0100s/iter; left time: 75.9162s\n",
      "\titers: 500, epoch: 7 | loss: 0.0334786\n",
      "\tspeed: 0.0100s/iter; left time: 74.8320s\n",
      "Epoch: 7 cost time: 6.009105443954468\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0354632 Vali Loss: 0.0407551 Test Loss: 0.1402271\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0376807\n",
      "\tspeed: 0.0368s/iter; left time: 269.2798s\n",
      "\titers: 200, epoch: 8 | loss: 0.0568481\n",
      "\tspeed: 0.0099s/iter; left time: 71.7046s\n",
      "\titers: 300, epoch: 8 | loss: 0.0314264\n",
      "\tspeed: 0.0099s/iter; left time: 70.5287s\n",
      "\titers: 400, epoch: 8 | loss: 0.0443626\n",
      "\tspeed: 0.0099s/iter; left time: 69.4805s\n",
      "\titers: 500, epoch: 8 | loss: 0.0328621\n",
      "\tspeed: 0.0099s/iter; left time: 68.4779s\n",
      "Epoch: 8 cost time: 5.9290711879730225\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0336702 Vali Loss: 0.0409307 Test Loss: 0.1423887\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.135650172829628, mae:0.23488344252109528\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1940793\n",
      "\tspeed: 0.0232s/iter; left time: 261.9338s\n",
      "\titers: 200, epoch: 1 | loss: 0.1388180\n",
      "\tspeed: 0.0110s/iter; left time: 122.6937s\n",
      "\titers: 300, epoch: 1 | loss: 0.2141243\n",
      "\tspeed: 0.0109s/iter; left time: 121.0699s\n",
      "\titers: 400, epoch: 1 | loss: 0.1099621\n",
      "\tspeed: 0.0109s/iter; left time: 120.2068s\n",
      "\titers: 500, epoch: 1 | loss: 0.1165200\n",
      "\tspeed: 0.0110s/iter; left time: 119.6131s\n",
      "Epoch: 1 cost time: 7.506651401519775\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1467942 Vali Loss: 0.0391663 Test Loss: 0.1277200\n",
      "Validation loss decreased (inf --> 0.039166).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1055574\n",
      "\tspeed: 0.0363s/iter; left time: 389.7991s\n",
      "\titers: 200, epoch: 2 | loss: 0.1487699\n",
      "\tspeed: 0.0098s/iter; left time: 104.2087s\n",
      "\titers: 300, epoch: 2 | loss: 0.0745109\n",
      "\tspeed: 0.0098s/iter; left time: 102.8560s\n",
      "\titers: 400, epoch: 2 | loss: 0.0588041\n",
      "\tspeed: 0.0098s/iter; left time: 101.8545s\n",
      "\titers: 500, epoch: 2 | loss: 0.0908444\n",
      "\tspeed: 0.0098s/iter; left time: 101.1650s\n",
      "Epoch: 2 cost time: 5.89277195930481\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1147852 Vali Loss: 0.0365444 Test Loss: 0.1178358\n",
      "Validation loss decreased (0.039166 --> 0.036544).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0494687\n",
      "\tspeed: 0.0355s/iter; left time: 360.7674s\n",
      "\titers: 200, epoch: 3 | loss: 0.0652063\n",
      "\tspeed: 0.0098s/iter; left time: 98.5235s\n",
      "\titers: 300, epoch: 3 | loss: 0.1403556\n",
      "\tspeed: 0.0098s/iter; left time: 97.2392s\n",
      "\titers: 400, epoch: 3 | loss: 0.0767204\n",
      "\tspeed: 0.0098s/iter; left time: 96.2017s\n",
      "\titers: 500, epoch: 3 | loss: 0.0613843\n",
      "\tspeed: 0.0097s/iter; left time: 94.8913s\n",
      "Epoch: 3 cost time: 5.855514764785767\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0810848 Vali Loss: 0.0354074 Test Loss: 0.1190029\n",
      "Validation loss decreased (0.036544 --> 0.035407).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0427368\n",
      "\tspeed: 0.0348s/iter; left time: 333.3381s\n",
      "\titers: 200, epoch: 4 | loss: 0.0439321\n",
      "\tspeed: 0.0097s/iter; left time: 92.4714s\n",
      "\titers: 300, epoch: 4 | loss: 0.0371526\n",
      "\tspeed: 0.0097s/iter; left time: 91.2928s\n",
      "\titers: 400, epoch: 4 | loss: 0.0446692\n",
      "\tspeed: 0.0097s/iter; left time: 90.3391s\n",
      "\titers: 500, epoch: 4 | loss: 0.0675589\n",
      "\tspeed: 0.0097s/iter; left time: 89.5732s\n",
      "Epoch: 4 cost time: 5.838128089904785\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0567281 Vali Loss: 0.0344380 Test Loss: 0.1204967\n",
      "Validation loss decreased (0.035407 --> 0.034438).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0475336\n",
      "\tspeed: 0.0360s/iter; left time: 324.7179s\n",
      "\titers: 200, epoch: 5 | loss: 0.0603414\n",
      "\tspeed: 0.0097s/iter; left time: 86.9224s\n",
      "\titers: 300, epoch: 5 | loss: 0.0557232\n",
      "\tspeed: 0.0097s/iter; left time: 86.0028s\n",
      "\titers: 400, epoch: 5 | loss: 0.0629306\n",
      "\tspeed: 0.0097s/iter; left time: 84.6850s\n",
      "\titers: 500, epoch: 5 | loss: 0.0599310\n",
      "\tspeed: 0.0097s/iter; left time: 83.8580s\n",
      "Epoch: 5 cost time: 5.8611767292022705\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0443423 Vali Loss: 0.0358920 Test Loss: 0.1261836\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0302408\n",
      "\tspeed: 0.0358s/iter; left time: 302.9484s\n",
      "\titers: 200, epoch: 6 | loss: 0.0627531\n",
      "\tspeed: 0.0109s/iter; left time: 91.3887s\n",
      "\titers: 300, epoch: 6 | loss: 0.0530466\n",
      "\tspeed: 0.0110s/iter; left time: 90.5554s\n",
      "\titers: 400, epoch: 6 | loss: 0.0300475\n",
      "\tspeed: 0.0110s/iter; left time: 89.2760s\n",
      "\titers: 500, epoch: 6 | loss: 0.0230651\n",
      "\tspeed: 0.0109s/iter; left time: 88.1561s\n",
      "Epoch: 6 cost time: 6.521501302719116\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0382273 Vali Loss: 0.0365799 Test Loss: 0.1277658\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0277006\n",
      "\tspeed: 0.0371s/iter; left time: 292.3464s\n",
      "\titers: 200, epoch: 7 | loss: 0.0329772\n",
      "\tspeed: 0.0109s/iter; left time: 84.8347s\n",
      "\titers: 300, epoch: 7 | loss: 0.0540259\n",
      "\tspeed: 0.0109s/iter; left time: 84.0546s\n",
      "\titers: 400, epoch: 7 | loss: 0.0743621\n",
      "\tspeed: 0.0110s/iter; left time: 83.0373s\n",
      "\titers: 500, epoch: 7 | loss: 0.0309425\n",
      "\tspeed: 0.0109s/iter; left time: 81.5113s\n",
      "Epoch: 7 cost time: 6.518216609954834\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0351306 Vali Loss: 0.0364887 Test Loss: 0.1271494\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12067621946334839, mae:0.216682568192482\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1714890\n",
      "\tspeed: 0.0230s/iter; left time: 259.6935s\n",
      "\titers: 200, epoch: 1 | loss: 0.1268095\n",
      "\tspeed: 0.0109s/iter; left time: 122.6289s\n",
      "\titers: 300, epoch: 1 | loss: 0.1375185\n",
      "\tspeed: 0.0109s/iter; left time: 121.1786s\n",
      "\titers: 400, epoch: 1 | loss: 0.0977060\n",
      "\tspeed: 0.0109s/iter; left time: 120.1579s\n",
      "\titers: 500, epoch: 1 | loss: 0.1776273\n",
      "\tspeed: 0.0110s/iter; left time: 119.4933s\n",
      "Epoch: 1 cost time: 7.435278415679932\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1847429 Vali Loss: 0.0443673 Test Loss: 0.1467872\n",
      "Validation loss decreased (inf --> 0.044367).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1578133\n",
      "\tspeed: 0.0351s/iter; left time: 376.6848s\n",
      "\titers: 200, epoch: 2 | loss: 0.2252475\n",
      "\tspeed: 0.0098s/iter; left time: 104.3801s\n",
      "\titers: 300, epoch: 2 | loss: 0.1275061\n",
      "\tspeed: 0.0098s/iter; left time: 103.3713s\n",
      "\titers: 400, epoch: 2 | loss: 0.1975546\n",
      "\tspeed: 0.0098s/iter; left time: 102.4979s\n",
      "\titers: 500, epoch: 2 | loss: 0.1235378\n",
      "\tspeed: 0.0098s/iter; left time: 101.5167s\n",
      "Epoch: 2 cost time: 5.892151594161987\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1313124 Vali Loss: 0.0441769 Test Loss: 0.1452591\n",
      "Validation loss decreased (0.044367 --> 0.044177).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0545728\n",
      "\tspeed: 0.0379s/iter; left time: 385.3396s\n",
      "\titers: 200, epoch: 3 | loss: 0.0621531\n",
      "\tspeed: 0.0110s/iter; left time: 110.5452s\n",
      "\titers: 300, epoch: 3 | loss: 0.0807057\n",
      "\tspeed: 0.0109s/iter; left time: 108.3753s\n",
      "\titers: 400, epoch: 3 | loss: 0.0659295\n",
      "\tspeed: 0.0097s/iter; left time: 95.8378s\n",
      "\titers: 500, epoch: 3 | loss: 0.0607469\n",
      "\tspeed: 0.0097s/iter; left time: 94.8027s\n",
      "Epoch: 3 cost time: 6.209687948226929\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0804944 Vali Loss: 0.0399675 Test Loss: 0.1428847\n",
      "Validation loss decreased (0.044177 --> 0.039968).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0398316\n",
      "\tspeed: 0.0345s/iter; left time: 331.2750s\n",
      "\titers: 200, epoch: 4 | loss: 0.0337781\n",
      "\tspeed: 0.0097s/iter; left time: 92.2535s\n",
      "\titers: 300, epoch: 4 | loss: 0.0411565\n",
      "\tspeed: 0.0098s/iter; left time: 91.6014s\n",
      "\titers: 400, epoch: 4 | loss: 0.0658094\n",
      "\tspeed: 0.0097s/iter; left time: 90.5797s\n",
      "\titers: 500, epoch: 4 | loss: 0.0430230\n",
      "\tspeed: 0.0098s/iter; left time: 89.6909s\n",
      "Epoch: 4 cost time: 5.823107719421387\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0502238 Vali Loss: 0.0407962 Test Loss: 0.1488430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0395215\n",
      "\tspeed: 0.0345s/iter; left time: 310.7845s\n",
      "\titers: 200, epoch: 5 | loss: 0.0381135\n",
      "\tspeed: 0.0098s/iter; left time: 87.1055s\n",
      "\titers: 300, epoch: 5 | loss: 0.0297308\n",
      "\tspeed: 0.0098s/iter; left time: 86.1259s\n",
      "\titers: 400, epoch: 5 | loss: 0.0274207\n",
      "\tspeed: 0.0098s/iter; left time: 85.0755s\n",
      "\titers: 500, epoch: 5 | loss: 0.0346355\n",
      "\tspeed: 0.0097s/iter; left time: 84.0463s\n",
      "Epoch: 5 cost time: 5.828102350234985\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0374292 Vali Loss: 0.0404922 Test Loss: 0.1463897\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0315148\n",
      "\tspeed: 0.0367s/iter; left time: 310.4911s\n",
      "\titers: 200, epoch: 6 | loss: 0.0427770\n",
      "\tspeed: 0.0098s/iter; left time: 81.7910s\n",
      "\titers: 300, epoch: 6 | loss: 0.0287391\n",
      "\tspeed: 0.0098s/iter; left time: 80.8093s\n",
      "\titers: 400, epoch: 6 | loss: 0.0242794\n",
      "\tspeed: 0.0098s/iter; left time: 80.2230s\n",
      "\titers: 500, epoch: 6 | loss: 0.0340353\n",
      "\tspeed: 0.0098s/iter; left time: 78.8886s\n",
      "Epoch: 6 cost time: 5.873657941818237\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0319297 Vali Loss: 0.0407948 Test Loss: 0.1484461\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.143092542886734, mae:0.2408505529165268\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2388572\n",
      "\tspeed: 0.0234s/iter; left time: 264.0797s\n",
      "\titers: 200, epoch: 1 | loss: 0.2025720\n",
      "\tspeed: 0.0114s/iter; left time: 127.8681s\n",
      "\titers: 300, epoch: 1 | loss: 0.1443961\n",
      "\tspeed: 0.0114s/iter; left time: 126.7699s\n",
      "\titers: 400, epoch: 1 | loss: 0.0870342\n",
      "\tspeed: 0.0114s/iter; left time: 125.7201s\n",
      "\titers: 500, epoch: 1 | loss: 0.1572416\n",
      "\tspeed: 0.0114s/iter; left time: 124.5562s\n",
      "Epoch: 1 cost time: 7.754876375198364\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2009764 Vali Loss: 0.0460886 Test Loss: 0.1417371\n",
      "Validation loss decreased (inf --> 0.046089).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1435807\n",
      "\tspeed: 0.0395s/iter; left time: 423.4959s\n",
      "\titers: 200, epoch: 2 | loss: 0.1392410\n",
      "\tspeed: 0.0115s/iter; left time: 122.0293s\n",
      "\titers: 300, epoch: 2 | loss: 0.1236219\n",
      "\tspeed: 0.0115s/iter; left time: 120.9795s\n",
      "\titers: 400, epoch: 2 | loss: 0.0819476\n",
      "\tspeed: 0.0115s/iter; left time: 119.7288s\n",
      "\titers: 500, epoch: 2 | loss: 0.1863607\n",
      "\tspeed: 0.0115s/iter; left time: 118.7881s\n",
      "Epoch: 2 cost time: 6.851367712020874\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1794212 Vali Loss: 0.0579344 Test Loss: 0.1662709\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1860282\n",
      "\tspeed: 0.0400s/iter; left time: 406.2660s\n",
      "\titers: 200, epoch: 3 | loss: 0.1703259\n",
      "\tspeed: 0.0125s/iter; left time: 125.2826s\n",
      "\titers: 300, epoch: 3 | loss: 0.1034399\n",
      "\tspeed: 0.0123s/iter; left time: 122.5921s\n",
      "\titers: 400, epoch: 3 | loss: 0.1257529\n",
      "\tspeed: 0.0114s/iter; left time: 112.4954s\n",
      "\titers: 500, epoch: 3 | loss: 0.1156912\n",
      "\tspeed: 0.0114s/iter; left time: 111.6375s\n",
      "Epoch: 3 cost time: 7.085433006286621\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1496666 Vali Loss: 0.0448545 Test Loss: 0.1353032\n",
      "Validation loss decreased (0.046089 --> 0.044854).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1272880\n",
      "\tspeed: 0.0392s/iter; left time: 375.6176s\n",
      "\titers: 200, epoch: 4 | loss: 0.1286890\n",
      "\tspeed: 0.0114s/iter; left time: 108.3654s\n",
      "\titers: 300, epoch: 4 | loss: 0.0969923\n",
      "\tspeed: 0.0114s/iter; left time: 107.3417s\n",
      "\titers: 400, epoch: 4 | loss: 0.0714660\n",
      "\tspeed: 0.0114s/iter; left time: 106.2038s\n",
      "\titers: 500, epoch: 4 | loss: 0.1516775\n",
      "\tspeed: 0.0114s/iter; left time: 104.9373s\n",
      "Epoch: 4 cost time: 6.8412535190582275\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1244113 Vali Loss: 0.0458936 Test Loss: 0.1368112\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0831007\n",
      "\tspeed: 0.0389s/iter; left time: 350.6518s\n",
      "\titers: 200, epoch: 5 | loss: 0.0894234\n",
      "\tspeed: 0.0114s/iter; left time: 101.6962s\n",
      "\titers: 300, epoch: 5 | loss: 0.1539751\n",
      "\tspeed: 0.0114s/iter; left time: 100.5978s\n",
      "\titers: 400, epoch: 5 | loss: 0.0966540\n",
      "\tspeed: 0.0114s/iter; left time: 99.4531s\n",
      "\titers: 500, epoch: 5 | loss: 0.1262841\n",
      "\tspeed: 0.0114s/iter; left time: 98.3530s\n",
      "Epoch: 5 cost time: 6.832409381866455\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1050426 Vali Loss: 0.0450158 Test Loss: 0.1372498\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0814576\n",
      "\tspeed: 0.0401s/iter; left time: 339.0279s\n",
      "\titers: 200, epoch: 6 | loss: 0.0685639\n",
      "\tspeed: 0.0125s/iter; left time: 104.0427s\n",
      "\titers: 300, epoch: 6 | loss: 0.0879787\n",
      "\tspeed: 0.0125s/iter; left time: 102.7554s\n",
      "\titers: 400, epoch: 6 | loss: 0.0606897\n",
      "\tspeed: 0.0125s/iter; left time: 101.6127s\n",
      "\titers: 500, epoch: 6 | loss: 0.0625465\n",
      "\tspeed: 0.0125s/iter; left time: 100.4680s\n",
      "Epoch: 6 cost time: 7.40555739402771\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0924215 Vali Loss: 0.0437972 Test Loss: 0.1383720\n",
      "Validation loss decreased (0.044854 --> 0.043797).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0925268\n",
      "\tspeed: 0.0411s/iter; left time: 323.5902s\n",
      "\titers: 200, epoch: 7 | loss: 0.0876379\n",
      "\tspeed: 0.0114s/iter; left time: 88.5102s\n",
      "\titers: 300, epoch: 7 | loss: 0.0903184\n",
      "\tspeed: 0.0114s/iter; left time: 87.4879s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643708\n",
      "\tspeed: 0.0114s/iter; left time: 86.2615s\n",
      "\titers: 500, epoch: 7 | loss: 0.1023967\n",
      "\tspeed: 0.0114s/iter; left time: 85.0131s\n",
      "Epoch: 7 cost time: 6.802591800689697\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0852080 Vali Loss: 0.0439345 Test Loss: 0.1393003\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0709743\n",
      "\tspeed: 0.0392s/iter; left time: 286.2819s\n",
      "\titers: 200, epoch: 8 | loss: 0.0723450\n",
      "\tspeed: 0.0114s/iter; left time: 82.5161s\n",
      "\titers: 300, epoch: 8 | loss: 0.0668660\n",
      "\tspeed: 0.0115s/iter; left time: 81.5556s\n",
      "\titers: 400, epoch: 8 | loss: 0.0706788\n",
      "\tspeed: 0.0114s/iter; left time: 80.1151s\n",
      "\titers: 500, epoch: 8 | loss: 0.0804036\n",
      "\tspeed: 0.0114s/iter; left time: 78.9973s\n",
      "Epoch: 8 cost time: 6.848761796951294\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0810429 Vali Loss: 0.0440970 Test Loss: 0.1393788\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0490901\n",
      "\tspeed: 0.0382s/iter; left time: 257.6584s\n",
      "\titers: 200, epoch: 9 | loss: 0.0777733\n",
      "\tspeed: 0.0115s/iter; left time: 76.1239s\n",
      "\titers: 300, epoch: 9 | loss: 0.0971335\n",
      "\tspeed: 0.0115s/iter; left time: 74.9420s\n",
      "\titers: 400, epoch: 9 | loss: 0.0591456\n",
      "\tspeed: 0.0114s/iter; left time: 73.6806s\n",
      "\titers: 500, epoch: 9 | loss: 0.0914075\n",
      "\tspeed: 0.0115s/iter; left time: 72.6064s\n",
      "Epoch: 9 cost time: 6.830945253372192\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0786846 Vali Loss: 0.0439969 Test Loss: 0.1393926\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13855166733264923, mae:0.23596133291721344\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.0974500\n",
      "\tspeed: 0.0240s/iter; left time: 271.7190s\n",
      "\titers: 200, epoch: 1 | loss: 0.2052382\n",
      "\tspeed: 0.0121s/iter; left time: 135.0763s\n",
      "\titers: 300, epoch: 1 | loss: 0.1459617\n",
      "\tspeed: 0.0120s/iter; left time: 133.5560s\n",
      "\titers: 400, epoch: 1 | loss: 0.1657365\n",
      "\tspeed: 0.0121s/iter; left time: 132.6139s\n",
      "\titers: 500, epoch: 1 | loss: 0.1082806\n",
      "\tspeed: 0.0121s/iter; left time: 131.6581s\n",
      "Epoch: 1 cost time: 8.123446226119995\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1601642 Vali Loss: 0.0422012 Test Loss: 0.1301945\n",
      "Validation loss decreased (inf --> 0.042201).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1238574\n",
      "\tspeed: 0.0389s/iter; left time: 417.9184s\n",
      "\titers: 200, epoch: 2 | loss: 0.0918113\n",
      "\tspeed: 0.0110s/iter; left time: 117.2048s\n",
      "\titers: 300, epoch: 2 | loss: 0.1213746\n",
      "\tspeed: 0.0110s/iter; left time: 116.1282s\n",
      "\titers: 400, epoch: 2 | loss: 0.1729004\n",
      "\tspeed: 0.0110s/iter; left time: 114.9492s\n",
      "\titers: 500, epoch: 2 | loss: 0.1869538\n",
      "\tspeed: 0.0110s/iter; left time: 113.8546s\n",
      "Epoch: 2 cost time: 6.615189552307129\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1370953 Vali Loss: 0.0412954 Test Loss: 0.1261017\n",
      "Validation loss decreased (0.042201 --> 0.041295).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1401310\n",
      "\tspeed: 0.0380s/iter; left time: 385.8802s\n",
      "\titers: 200, epoch: 3 | loss: 0.0646986\n",
      "\tspeed: 0.0110s/iter; left time: 111.0025s\n",
      "\titers: 300, epoch: 3 | loss: 0.1279343\n",
      "\tspeed: 0.0110s/iter; left time: 109.9729s\n",
      "\titers: 400, epoch: 3 | loss: 0.1239163\n",
      "\tspeed: 0.0110s/iter; left time: 108.8499s\n",
      "\titers: 500, epoch: 3 | loss: 0.1246570\n",
      "\tspeed: 0.0110s/iter; left time: 107.5986s\n",
      "Epoch: 3 cost time: 6.617619037628174\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1077242 Vali Loss: 0.0355741 Test Loss: 0.1213372\n",
      "Validation loss decreased (0.041295 --> 0.035574).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0936984\n",
      "\tspeed: 0.0394s/iter; left time: 378.0878s\n",
      "\titers: 200, epoch: 4 | loss: 0.0660693\n",
      "\tspeed: 0.0110s/iter; left time: 104.5968s\n",
      "\titers: 300, epoch: 4 | loss: 0.0422177\n",
      "\tspeed: 0.0110s/iter; left time: 103.4819s\n",
      "\titers: 400, epoch: 4 | loss: 0.1038613\n",
      "\tspeed: 0.0110s/iter; left time: 102.4296s\n",
      "\titers: 500, epoch: 4 | loss: 0.0759548\n",
      "\tspeed: 0.0110s/iter; left time: 101.2962s\n",
      "Epoch: 4 cost time: 6.601496696472168\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0860111 Vali Loss: 0.0350507 Test Loss: 0.1255417\n",
      "Validation loss decreased (0.035574 --> 0.035051).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0573087\n",
      "\tspeed: 0.0391s/iter; left time: 352.7485s\n",
      "\titers: 200, epoch: 5 | loss: 0.0602667\n",
      "\tspeed: 0.0110s/iter; left time: 98.0710s\n",
      "\titers: 300, epoch: 5 | loss: 0.0824649\n",
      "\tspeed: 0.0110s/iter; left time: 97.2937s\n",
      "\titers: 400, epoch: 5 | loss: 0.0633375\n",
      "\tspeed: 0.0111s/iter; left time: 96.3934s\n",
      "\titers: 500, epoch: 5 | loss: 0.0625824\n",
      "\tspeed: 0.0110s/iter; left time: 95.2429s\n",
      "Epoch: 5 cost time: 6.624646902084351\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0691570 Vali Loss: 0.0333579 Test Loss: 0.1244702\n",
      "Validation loss decreased (0.035051 --> 0.033358).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0775955\n",
      "\tspeed: 0.0391s/iter; left time: 330.3599s\n",
      "\titers: 200, epoch: 6 | loss: 0.0551258\n",
      "\tspeed: 0.0121s/iter; left time: 100.6359s\n",
      "\titers: 300, epoch: 6 | loss: 0.0558651\n",
      "\tspeed: 0.0121s/iter; left time: 99.4394s\n",
      "\titers: 400, epoch: 6 | loss: 0.0444722\n",
      "\tspeed: 0.0121s/iter; left time: 98.2616s\n",
      "\titers: 500, epoch: 6 | loss: 0.0528609\n",
      "\tspeed: 0.0121s/iter; left time: 97.1117s\n",
      "Epoch: 6 cost time: 7.18257212638855\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0584916 Vali Loss: 0.0337634 Test Loss: 0.1316425\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0433577\n",
      "\tspeed: 0.0381s/iter; left time: 300.0263s\n",
      "\titers: 200, epoch: 7 | loss: 0.0621938\n",
      "\tspeed: 0.0110s/iter; left time: 85.6881s\n",
      "\titers: 300, epoch: 7 | loss: 0.0465754\n",
      "\tspeed: 0.0110s/iter; left time: 84.5875s\n",
      "\titers: 400, epoch: 7 | loss: 0.0555614\n",
      "\tspeed: 0.0110s/iter; left time: 83.4877s\n",
      "\titers: 500, epoch: 7 | loss: 0.0376010\n",
      "\tspeed: 0.0110s/iter; left time: 82.3665s\n",
      "Epoch: 7 cost time: 6.654365539550781\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0522816 Vali Loss: 0.0336066 Test Loss: 0.1316548\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0479672\n",
      "\tspeed: 0.0392s/iter; left time: 286.8140s\n",
      "\titers: 200, epoch: 8 | loss: 0.0504123\n",
      "\tspeed: 0.0119s/iter; left time: 85.9681s\n",
      "\titers: 300, epoch: 8 | loss: 0.0374279\n",
      "\tspeed: 0.0119s/iter; left time: 84.8628s\n",
      "\titers: 400, epoch: 8 | loss: 0.0494475\n",
      "\tspeed: 0.0119s/iter; left time: 83.6622s\n",
      "\titers: 500, epoch: 8 | loss: 0.0504735\n",
      "\tspeed: 0.0119s/iter; left time: 82.5162s\n",
      "Epoch: 8 cost time: 7.094576120376587\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0490019 Vali Loss: 0.0342471 Test Loss: 0.1331143\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12465690076351166, mae:0.21787424385547638\n",
      ">>> LR=5e-4,DO=0.0,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2019889\n",
      "\tspeed: 0.0220s/iter; left time: 248.3764s\n",
      "\titers: 200, epoch: 1 | loss: 0.2130914\n",
      "\tspeed: 0.0102s/iter; left time: 114.6444s\n",
      "\titers: 300, epoch: 1 | loss: 0.1833379\n",
      "\tspeed: 0.0103s/iter; left time: 113.8172s\n",
      "\titers: 400, epoch: 1 | loss: 0.1353990\n",
      "\tspeed: 0.0102s/iter; left time: 112.5470s\n",
      "\titers: 500, epoch: 1 | loss: 0.1589779\n",
      "\tspeed: 0.0102s/iter; left time: 111.5283s\n",
      "Epoch: 1 cost time: 7.063090562820435\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1984546 Vali Loss: 0.0496909 Test Loss: 0.1584453\n",
      "Validation loss decreased (inf --> 0.049691).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1731774\n",
      "\tspeed: 0.0374s/iter; left time: 401.1218s\n",
      "\titers: 200, epoch: 2 | loss: 0.2615232\n",
      "\tspeed: 0.0107s/iter; left time: 113.9208s\n",
      "\titers: 300, epoch: 2 | loss: 0.1243211\n",
      "\tspeed: 0.0103s/iter; left time: 108.9235s\n",
      "\titers: 400, epoch: 2 | loss: 0.1666401\n",
      "\tspeed: 0.0104s/iter; left time: 108.1774s\n",
      "\titers: 500, epoch: 2 | loss: 0.1445317\n",
      "\tspeed: 0.0105s/iter; left time: 108.6392s\n",
      "Epoch: 2 cost time: 6.43419623374939\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2028869 Vali Loss: 0.0559001 Test Loss: 0.1606367\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1585160\n",
      "\tspeed: 0.0385s/iter; left time: 391.4463s\n",
      "\titers: 200, epoch: 3 | loss: 0.1345448\n",
      "\tspeed: 0.0103s/iter; left time: 103.8703s\n",
      "\titers: 300, epoch: 3 | loss: 0.1909739\n",
      "\tspeed: 0.0104s/iter; left time: 103.3879s\n",
      "\titers: 400, epoch: 3 | loss: 0.1253612\n",
      "\tspeed: 0.0104s/iter; left time: 102.1881s\n",
      "\titers: 500, epoch: 3 | loss: 0.1260650\n",
      "\tspeed: 0.0103s/iter; left time: 100.7332s\n",
      "Epoch: 3 cost time: 6.233942985534668\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1610141 Vali Loss: 0.0496181 Test Loss: 0.1464581\n",
      "Validation loss decreased (0.049691 --> 0.049618).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1535898\n",
      "\tspeed: 0.0377s/iter; left time: 361.2503s\n",
      "\titers: 200, epoch: 4 | loss: 0.1112829\n",
      "\tspeed: 0.0103s/iter; left time: 97.4193s\n",
      "\titers: 300, epoch: 4 | loss: 0.1107739\n",
      "\tspeed: 0.0103s/iter; left time: 96.3130s\n",
      "\titers: 400, epoch: 4 | loss: 0.2499674\n",
      "\tspeed: 0.0103s/iter; left time: 96.0191s\n",
      "\titers: 500, epoch: 4 | loss: 0.0918426\n",
      "\tspeed: 0.0103s/iter; left time: 94.9645s\n",
      "Epoch: 4 cost time: 6.194323778152466\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1255826 Vali Loss: 0.0450062 Test Loss: 0.1426192\n",
      "Validation loss decreased (0.049618 --> 0.045006).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1097186\n",
      "\tspeed: 0.0375s/iter; left time: 338.1759s\n",
      "\titers: 200, epoch: 5 | loss: 0.1317494\n",
      "\tspeed: 0.0113s/iter; left time: 100.7827s\n",
      "\titers: 300, epoch: 5 | loss: 0.1488588\n",
      "\tspeed: 0.0113s/iter; left time: 99.7319s\n",
      "\titers: 400, epoch: 5 | loss: 0.0857320\n",
      "\tspeed: 0.0113s/iter; left time: 98.5327s\n",
      "\titers: 500, epoch: 5 | loss: 0.1473603\n",
      "\tspeed: 0.0113s/iter; left time: 97.3864s\n",
      "Epoch: 5 cost time: 6.745983839035034\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1054959 Vali Loss: 0.0452785 Test Loss: 0.1415697\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0955923\n",
      "\tspeed: 0.0371s/iter; left time: 313.5378s\n",
      "\titers: 200, epoch: 6 | loss: 0.0796199\n",
      "\tspeed: 0.0102s/iter; left time: 85.5339s\n",
      "\titers: 300, epoch: 6 | loss: 0.0931741\n",
      "\tspeed: 0.0102s/iter; left time: 84.5007s\n",
      "\titers: 400, epoch: 6 | loss: 0.0931934\n",
      "\tspeed: 0.0102s/iter; left time: 83.4679s\n",
      "\titers: 500, epoch: 6 | loss: 0.1077420\n",
      "\tspeed: 0.0102s/iter; left time: 82.3767s\n",
      "Epoch: 6 cost time: 6.143591403961182\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0928161 Vali Loss: 0.0452328 Test Loss: 0.1478300\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0777740\n",
      "\tspeed: 0.0363s/iter; left time: 286.0977s\n",
      "\titers: 200, epoch: 7 | loss: 0.0906318\n",
      "\tspeed: 0.0102s/iter; left time: 79.4876s\n",
      "\titers: 300, epoch: 7 | loss: 0.0649016\n",
      "\tspeed: 0.0102s/iter; left time: 78.4390s\n",
      "\titers: 400, epoch: 7 | loss: 0.0970137\n",
      "\tspeed: 0.0102s/iter; left time: 77.4313s\n",
      "\titers: 500, epoch: 7 | loss: 0.0612615\n",
      "\tspeed: 0.0102s/iter; left time: 76.3589s\n",
      "Epoch: 7 cost time: 6.1871562004089355\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0846187 Vali Loss: 0.0453729 Test Loss: 0.1449361\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1428108662366867, mae:0.2442036122083664\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1978606\n",
      "\tspeed: 0.0252s/iter; left time: 284.5164s\n",
      "\titers: 200, epoch: 1 | loss: 0.1406488\n",
      "\tspeed: 0.0137s/iter; left time: 153.8186s\n",
      "\titers: 300, epoch: 1 | loss: 0.2377709\n",
      "\tspeed: 0.0137s/iter; left time: 152.1388s\n",
      "\titers: 400, epoch: 1 | loss: 0.1688309\n",
      "\tspeed: 0.0137s/iter; left time: 150.9829s\n",
      "\titers: 500, epoch: 1 | loss: 0.1429378\n",
      "\tspeed: 0.0137s/iter; left time: 149.6757s\n",
      "Epoch: 1 cost time: 9.015490531921387\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1824579 Vali Loss: 0.0461201 Test Loss: 0.1406951\n",
      "Validation loss decreased (inf --> 0.046120).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1808321\n",
      "\tspeed: 0.0451s/iter; left time: 484.4288s\n",
      "\titers: 200, epoch: 2 | loss: 0.1237853\n",
      "\tspeed: 0.0138s/iter; left time: 146.7176s\n",
      "\titers: 300, epoch: 2 | loss: 0.1161452\n",
      "\tspeed: 0.0138s/iter; left time: 145.3968s\n",
      "\titers: 400, epoch: 2 | loss: 0.0831951\n",
      "\tspeed: 0.0138s/iter; left time: 144.0438s\n",
      "\titers: 500, epoch: 2 | loss: 0.1708268\n",
      "\tspeed: 0.0138s/iter; left time: 142.7003s\n",
      "Epoch: 2 cost time: 8.162977933883667\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1403602 Vali Loss: 0.0479635 Test Loss: 0.1585671\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1754857\n",
      "\tspeed: 0.0469s/iter; left time: 476.3062s\n",
      "\titers: 200, epoch: 3 | loss: 0.0950794\n",
      "\tspeed: 0.0155s/iter; left time: 155.6985s\n",
      "\titers: 300, epoch: 3 | loss: 0.0790503\n",
      "\tspeed: 0.0155s/iter; left time: 154.1198s\n",
      "\titers: 400, epoch: 3 | loss: 0.0907891\n",
      "\tspeed: 0.0155s/iter; left time: 152.6054s\n",
      "\titers: 500, epoch: 3 | loss: 0.0887034\n",
      "\tspeed: 0.0155s/iter; left time: 151.0641s\n",
      "Epoch: 3 cost time: 9.13390302658081\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1063732 Vali Loss: 0.0409150 Test Loss: 0.1384491\n",
      "Validation loss decreased (0.046120 --> 0.040915).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0839169\n",
      "\tspeed: 0.0476s/iter; left time: 456.0979s\n",
      "\titers: 200, epoch: 4 | loss: 0.0629344\n",
      "\tspeed: 0.0139s/iter; left time: 131.6066s\n",
      "\titers: 300, epoch: 4 | loss: 0.0953755\n",
      "\tspeed: 0.0138s/iter; left time: 129.9858s\n",
      "\titers: 400, epoch: 4 | loss: 0.0753769\n",
      "\tspeed: 0.0139s/iter; left time: 128.7796s\n",
      "\titers: 500, epoch: 4 | loss: 0.0546866\n",
      "\tspeed: 0.0139s/iter; left time: 127.3964s\n",
      "Epoch: 4 cost time: 8.168796062469482\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0716987 Vali Loss: 0.0403713 Test Loss: 0.1423966\n",
      "Validation loss decreased (0.040915 --> 0.040371).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0682062\n",
      "\tspeed: 0.0474s/iter; left time: 427.8376s\n",
      "\titers: 200, epoch: 5 | loss: 0.0543592\n",
      "\tspeed: 0.0150s/iter; left time: 133.6711s\n",
      "\titers: 300, epoch: 5 | loss: 0.0492750\n",
      "\tspeed: 0.0138s/iter; left time: 122.1231s\n",
      "\titers: 400, epoch: 5 | loss: 0.0439108\n",
      "\tspeed: 0.0138s/iter; left time: 120.2250s\n",
      "\titers: 500, epoch: 5 | loss: 0.0325127\n",
      "\tspeed: 0.0138s/iter; left time: 118.9963s\n",
      "Epoch: 5 cost time: 8.467384576797485\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0540604 Vali Loss: 0.0405654 Test Loss: 0.1445131\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0450938\n",
      "\tspeed: 0.0468s/iter; left time: 395.5412s\n",
      "\titers: 200, epoch: 6 | loss: 0.0751078\n",
      "\tspeed: 0.0138s/iter; left time: 115.1195s\n",
      "\titers: 300, epoch: 6 | loss: 0.0407169\n",
      "\tspeed: 0.0138s/iter; left time: 113.9771s\n",
      "\titers: 400, epoch: 6 | loss: 0.0473953\n",
      "\tspeed: 0.0137s/iter; left time: 112.0704s\n",
      "\titers: 500, epoch: 6 | loss: 0.0438990\n",
      "\tspeed: 0.0138s/iter; left time: 110.7789s\n",
      "Epoch: 6 cost time: 8.157188177108765\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0449369 Vali Loss: 0.0414444 Test Loss: 0.1482243\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0427545\n",
      "\tspeed: 0.0441s/iter; left time: 347.6286s\n",
      "\titers: 200, epoch: 7 | loss: 0.0429754\n",
      "\tspeed: 0.0137s/iter; left time: 106.7805s\n",
      "\titers: 300, epoch: 7 | loss: 0.0315846\n",
      "\tspeed: 0.0137s/iter; left time: 105.4841s\n",
      "\titers: 400, epoch: 7 | loss: 0.0442837\n",
      "\tspeed: 0.0137s/iter; left time: 104.2180s\n",
      "\titers: 500, epoch: 7 | loss: 0.0533348\n",
      "\tspeed: 0.0137s/iter; left time: 102.7178s\n",
      "Epoch: 7 cost time: 8.095173835754395\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0403531 Vali Loss: 0.0424153 Test Loss: 0.1492263\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1425885260105133, mae:0.24156534671783447\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2268765\n",
      "\tspeed: 0.0266s/iter; left time: 300.4014s\n",
      "\titers: 200, epoch: 1 | loss: 0.1184546\n",
      "\tspeed: 0.0136s/iter; left time: 152.7133s\n",
      "\titers: 300, epoch: 1 | loss: 0.1239855\n",
      "\tspeed: 0.0136s/iter; left time: 150.9392s\n",
      "\titers: 400, epoch: 1 | loss: 0.1680717\n",
      "\tspeed: 0.0136s/iter; left time: 149.7068s\n",
      "\titers: 500, epoch: 1 | loss: 0.1292608\n",
      "\tspeed: 0.0137s/iter; left time: 148.8004s\n",
      "Epoch: 1 cost time: 9.116556882858276\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1520934 Vali Loss: 0.0372690 Test Loss: 0.1163540\n",
      "Validation loss decreased (inf --> 0.037269).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.0825776\n",
      "\tspeed: 0.0445s/iter; left time: 477.7519s\n",
      "\titers: 200, epoch: 2 | loss: 0.2555671\n",
      "\tspeed: 0.0135s/iter; left time: 143.3585s\n",
      "\titers: 300, epoch: 2 | loss: 0.1489368\n",
      "\tspeed: 0.0135s/iter; left time: 141.7336s\n",
      "\titers: 400, epoch: 2 | loss: 0.0923580\n",
      "\tspeed: 0.0135s/iter; left time: 140.5407s\n",
      "\titers: 500, epoch: 2 | loss: 0.0957127\n",
      "\tspeed: 0.0135s/iter; left time: 139.0985s\n",
      "Epoch: 2 cost time: 7.99338960647583\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1173969 Vali Loss: 0.0392548 Test Loss: 0.1194504\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1095200\n",
      "\tspeed: 0.0452s/iter; left time: 458.9152s\n",
      "\titers: 200, epoch: 3 | loss: 0.0508968\n",
      "\tspeed: 0.0134s/iter; left time: 134.9168s\n",
      "\titers: 300, epoch: 3 | loss: 0.0897601\n",
      "\tspeed: 0.0135s/iter; left time: 134.5506s\n",
      "\titers: 400, epoch: 3 | loss: 0.0699842\n",
      "\tspeed: 0.0135s/iter; left time: 133.4063s\n",
      "\titers: 500, epoch: 3 | loss: 0.0651250\n",
      "\tspeed: 0.0135s/iter; left time: 132.0639s\n",
      "Epoch: 3 cost time: 8.0017991065979\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0834549 Vali Loss: 0.0350599 Test Loss: 0.1154133\n",
      "Validation loss decreased (0.037269 --> 0.035060).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0440067\n",
      "\tspeed: 0.0448s/iter; left time: 430.0453s\n",
      "\titers: 200, epoch: 4 | loss: 0.0632474\n",
      "\tspeed: 0.0135s/iter; left time: 128.2031s\n",
      "\titers: 300, epoch: 4 | loss: 0.0656902\n",
      "\tspeed: 0.0135s/iter; left time: 127.0534s\n",
      "\titers: 400, epoch: 4 | loss: 0.0422224\n",
      "\tspeed: 0.0135s/iter; left time: 125.5288s\n",
      "\titers: 500, epoch: 4 | loss: 0.0366644\n",
      "\tspeed: 0.0135s/iter; left time: 124.0117s\n",
      "Epoch: 4 cost time: 8.020142555236816\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0564874 Vali Loss: 0.0340818 Test Loss: 0.1165414\n",
      "Validation loss decreased (0.035060 --> 0.034082).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0329393\n",
      "\tspeed: 0.0483s/iter; left time: 435.7397s\n",
      "\titers: 200, epoch: 5 | loss: 0.0378728\n",
      "\tspeed: 0.0135s/iter; left time: 120.5428s\n",
      "\titers: 300, epoch: 5 | loss: 0.0312857\n",
      "\tspeed: 0.0135s/iter; left time: 119.0727s\n",
      "\titers: 400, epoch: 5 | loss: 0.0266005\n",
      "\tspeed: 0.0135s/iter; left time: 117.9157s\n",
      "\titers: 500, epoch: 5 | loss: 0.0470453\n",
      "\tspeed: 0.0135s/iter; left time: 116.5017s\n",
      "Epoch: 5 cost time: 7.997188568115234\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0417427 Vali Loss: 0.0357250 Test Loss: 0.1175037\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0335595\n",
      "\tspeed: 0.0451s/iter; left time: 380.8601s\n",
      "\titers: 200, epoch: 6 | loss: 0.0529354\n",
      "\tspeed: 0.0135s/iter; left time: 112.8326s\n",
      "\titers: 300, epoch: 6 | loss: 0.0342036\n",
      "\tspeed: 0.0135s/iter; left time: 111.0444s\n",
      "\titers: 400, epoch: 6 | loss: 0.0658579\n",
      "\tspeed: 0.0135s/iter; left time: 109.8630s\n",
      "\titers: 500, epoch: 6 | loss: 0.0367821\n",
      "\tspeed: 0.0135s/iter; left time: 108.4982s\n",
      "Epoch: 6 cost time: 7.98458194732666\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0348177 Vali Loss: 0.0363659 Test Loss: 0.1205297\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0248893\n",
      "\tspeed: 0.0475s/iter; left time: 374.5458s\n",
      "\titers: 200, epoch: 7 | loss: 0.0280079\n",
      "\tspeed: 0.0143s/iter; left time: 111.4508s\n",
      "\titers: 300, epoch: 7 | loss: 0.0273204\n",
      "\tspeed: 0.0136s/iter; left time: 104.3841s\n",
      "\titers: 400, epoch: 7 | loss: 0.0482202\n",
      "\tspeed: 0.0136s/iter; left time: 102.9567s\n",
      "\titers: 500, epoch: 7 | loss: 0.0353472\n",
      "\tspeed: 0.0136s/iter; left time: 101.3804s\n",
      "Epoch: 7 cost time: 8.264543294906616\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0313630 Vali Loss: 0.0362975 Test Loss: 0.1222203\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11669544130563736, mae:0.2160900980234146\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1069971\n",
      "\tspeed: 0.0271s/iter; left time: 306.0480s\n",
      "\titers: 200, epoch: 1 | loss: 0.2587161\n",
      "\tspeed: 0.0151s/iter; left time: 169.3594s\n",
      "\titers: 300, epoch: 1 | loss: 0.2063680\n",
      "\tspeed: 0.0151s/iter; left time: 167.2523s\n",
      "\titers: 400, epoch: 1 | loss: 0.2087482\n",
      "\tspeed: 0.0151s/iter; left time: 165.9465s\n",
      "\titers: 500, epoch: 1 | loss: 0.2114511\n",
      "\tspeed: 0.0152s/iter; left time: 165.1563s\n",
      "Epoch: 1 cost time: 9.867110252380371\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1899124 Vali Loss: 0.0503171 Test Loss: 0.1435419\n",
      "Validation loss decreased (inf --> 0.050317).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1118510\n",
      "\tspeed: 0.0479s/iter; left time: 514.0981s\n",
      "\titers: 200, epoch: 2 | loss: 0.1333888\n",
      "\tspeed: 0.0135s/iter; left time: 143.2243s\n",
      "\titers: 300, epoch: 2 | loss: 0.0712786\n",
      "\tspeed: 0.0135s/iter; left time: 142.0308s\n",
      "\titers: 400, epoch: 2 | loss: 0.0988185\n",
      "\tspeed: 0.0135s/iter; left time: 140.3518s\n",
      "\titers: 500, epoch: 2 | loss: 0.1156343\n",
      "\tspeed: 0.0135s/iter; left time: 139.0606s\n",
      "Epoch: 2 cost time: 7.990076303482056\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1346016 Vali Loss: 0.0433708 Test Loss: 0.1410017\n",
      "Validation loss decreased (0.050317 --> 0.043371).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0975002\n",
      "\tspeed: 0.0444s/iter; left time: 451.4016s\n",
      "\titers: 200, epoch: 3 | loss: 0.1073149\n",
      "\tspeed: 0.0135s/iter; left time: 135.7469s\n",
      "\titers: 300, epoch: 3 | loss: 0.0861029\n",
      "\tspeed: 0.0135s/iter; left time: 134.0896s\n",
      "\titers: 400, epoch: 3 | loss: 0.0734350\n",
      "\tspeed: 0.0135s/iter; left time: 132.6987s\n",
      "\titers: 500, epoch: 3 | loss: 0.0743892\n",
      "\tspeed: 0.0135s/iter; left time: 131.9272s\n",
      "Epoch: 3 cost time: 7.98084020614624\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0793540 Vali Loss: 0.0424974 Test Loss: 0.1376587\n",
      "Validation loss decreased (0.043371 --> 0.042497).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0444206\n",
      "\tspeed: 0.0444s/iter; left time: 426.0962s\n",
      "\titers: 200, epoch: 4 | loss: 0.0393224\n",
      "\tspeed: 0.0138s/iter; left time: 131.2382s\n",
      "\titers: 300, epoch: 4 | loss: 0.0441419\n",
      "\tspeed: 0.0135s/iter; left time: 126.5927s\n",
      "\titers: 400, epoch: 4 | loss: 0.0433377\n",
      "\tspeed: 0.0134s/iter; left time: 124.5695s\n",
      "\titers: 500, epoch: 4 | loss: 0.0273443\n",
      "\tspeed: 0.0134s/iter; left time: 123.0173s\n",
      "Epoch: 4 cost time: 7.9930644035339355\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0454741 Vali Loss: 0.0429966 Test Loss: 0.1436175\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0456207\n",
      "\tspeed: 0.0438s/iter; left time: 395.3243s\n",
      "\titers: 200, epoch: 5 | loss: 0.0484772\n",
      "\tspeed: 0.0134s/iter; left time: 119.6799s\n",
      "\titers: 300, epoch: 5 | loss: 0.0314100\n",
      "\tspeed: 0.0134s/iter; left time: 118.3707s\n",
      "\titers: 400, epoch: 5 | loss: 0.0519954\n",
      "\tspeed: 0.0134s/iter; left time: 116.8141s\n",
      "\titers: 500, epoch: 5 | loss: 0.0293121\n",
      "\tspeed: 0.0134s/iter; left time: 115.7003s\n",
      "Epoch: 5 cost time: 7.9003684520721436\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0318291 Vali Loss: 0.0437204 Test Loss: 0.1449698\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0287018\n",
      "\tspeed: 0.0451s/iter; left time: 381.4086s\n",
      "\titers: 200, epoch: 6 | loss: 0.0228253\n",
      "\tspeed: 0.0135s/iter; left time: 112.3586s\n",
      "\titers: 300, epoch: 6 | loss: 0.0222545\n",
      "\tspeed: 0.0134s/iter; left time: 110.6221s\n",
      "\titers: 400, epoch: 6 | loss: 0.0427278\n",
      "\tspeed: 0.0134s/iter; left time: 109.4231s\n",
      "\titers: 500, epoch: 6 | loss: 0.0231968\n",
      "\tspeed: 0.0134s/iter; left time: 107.9060s\n",
      "Epoch: 6 cost time: 7.933849811553955\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0262436 Vali Loss: 0.0438920 Test Loss: 0.1476728\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13781242072582245, mae:0.24142491817474365\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1986155\n",
      "\tspeed: 0.0284s/iter; left time: 320.6945s\n",
      "\titers: 200, epoch: 1 | loss: 0.1691805\n",
      "\tspeed: 0.0164s/iter; left time: 183.6541s\n",
      "\titers: 300, epoch: 1 | loss: 0.2655474\n",
      "\tspeed: 0.0164s/iter; left time: 181.8990s\n",
      "\titers: 400, epoch: 1 | loss: 0.1940346\n",
      "\tspeed: 0.0164s/iter; left time: 180.4830s\n",
      "\titers: 500, epoch: 1 | loss: 0.1390540\n",
      "\tspeed: 0.0164s/iter; left time: 178.9719s\n",
      "Epoch: 1 cost time: 10.597198247909546\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1975997 Vali Loss: 0.0505389 Test Loss: 0.1476364\n",
      "Validation loss decreased (inf --> 0.050539).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2220080\n",
      "\tspeed: 0.0504s/iter; left time: 540.7710s\n",
      "\titers: 200, epoch: 2 | loss: 0.2834747\n",
      "\tspeed: 0.0163s/iter; left time: 173.7810s\n",
      "\titers: 300, epoch: 2 | loss: 0.3231107\n",
      "\tspeed: 0.0163s/iter; left time: 172.1140s\n",
      "\titers: 400, epoch: 2 | loss: 0.2974539\n",
      "\tspeed: 0.0164s/iter; left time: 170.6724s\n",
      "\titers: 500, epoch: 2 | loss: 0.2700153\n",
      "\tspeed: 0.0163s/iter; left time: 168.8354s\n",
      "Epoch: 2 cost time: 9.643572568893433\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2516783 Vali Loss: 0.0667279 Test Loss: 0.1820506\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1836330\n",
      "\tspeed: 0.0515s/iter; left time: 522.9636s\n",
      "\titers: 200, epoch: 3 | loss: 0.2148254\n",
      "\tspeed: 0.0163s/iter; left time: 164.4332s\n",
      "\titers: 300, epoch: 3 | loss: 0.3247426\n",
      "\tspeed: 0.0164s/iter; left time: 162.9853s\n",
      "\titers: 400, epoch: 3 | loss: 0.1836453\n",
      "\tspeed: 0.0163s/iter; left time: 161.0802s\n",
      "\titers: 500, epoch: 3 | loss: 0.1678694\n",
      "\tspeed: 0.0163s/iter; left time: 159.5793s\n",
      "Epoch: 3 cost time: 9.654511213302612\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2291430 Vali Loss: 0.0621862 Test Loss: 0.1663595\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2186916\n",
      "\tspeed: 0.0491s/iter; left time: 471.2547s\n",
      "\titers: 200, epoch: 4 | loss: 0.1787783\n",
      "\tspeed: 0.0166s/iter; left time: 157.2546s\n",
      "\titers: 300, epoch: 4 | loss: 0.1733173\n",
      "\tspeed: 0.0178s/iter; left time: 166.7911s\n",
      "\titers: 400, epoch: 4 | loss: 0.1957897\n",
      "\tspeed: 0.0178s/iter; left time: 164.9873s\n",
      "\titers: 500, epoch: 4 | loss: 0.1448365\n",
      "\tspeed: 0.0165s/iter; left time: 151.7243s\n",
      "Epoch: 4 cost time: 9.913108587265015\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1949281 Vali Loss: 0.0626483 Test Loss: 0.1643371\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14789031445980072, mae:0.24401988089084625\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1967783\n",
      "\tspeed: 0.0292s/iter; left time: 330.4450s\n",
      "\titers: 200, epoch: 1 | loss: 0.0812769\n",
      "\tspeed: 0.0173s/iter; left time: 193.2694s\n",
      "\titers: 300, epoch: 1 | loss: 0.2097766\n",
      "\tspeed: 0.0172s/iter; left time: 191.2928s\n",
      "\titers: 400, epoch: 1 | loss: 0.1258427\n",
      "\tspeed: 0.0173s/iter; left time: 190.1571s\n",
      "\titers: 500, epoch: 1 | loss: 0.0920935\n",
      "\tspeed: 0.0173s/iter; left time: 188.0967s\n",
      "Epoch: 1 cost time: 11.086338758468628\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1694094 Vali Loss: 0.0414194 Test Loss: 0.1318534\n",
      "Validation loss decreased (inf --> 0.041419).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1300433\n",
      "\tspeed: 0.0550s/iter; left time: 589.6944s\n",
      "\titers: 200, epoch: 2 | loss: 0.1766018\n",
      "\tspeed: 0.0173s/iter; left time: 184.0545s\n",
      "\titers: 300, epoch: 2 | loss: 0.1776148\n",
      "\tspeed: 0.0173s/iter; left time: 182.0856s\n",
      "\titers: 400, epoch: 2 | loss: 0.6121952\n",
      "\tspeed: 0.0173s/iter; left time: 180.0713s\n",
      "\titers: 500, epoch: 2 | loss: 0.2668276\n",
      "\tspeed: 0.0173s/iter; left time: 178.4023s\n",
      "Epoch: 2 cost time: 10.202077627182007\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2193054 Vali Loss: 0.0827282 Test Loss: 0.2201046\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2272870\n",
      "\tspeed: 0.0509s/iter; left time: 517.4685s\n",
      "\titers: 200, epoch: 3 | loss: 0.2946670\n",
      "\tspeed: 0.0172s/iter; left time: 173.5090s\n",
      "\titers: 300, epoch: 3 | loss: 0.1645177\n",
      "\tspeed: 0.0172s/iter; left time: 171.4346s\n",
      "\titers: 400, epoch: 3 | loss: 0.2846656\n",
      "\tspeed: 0.0172s/iter; left time: 169.6711s\n",
      "\titers: 500, epoch: 3 | loss: 0.1483205\n",
      "\tspeed: 0.0172s/iter; left time: 168.0480s\n",
      "Epoch: 3 cost time: 10.095296144485474\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2507399 Vali Loss: 0.0618857 Test Loss: 0.1737114\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1715246\n",
      "\tspeed: 0.0496s/iter; left time: 476.1442s\n",
      "\titers: 200, epoch: 4 | loss: 0.3059893\n",
      "\tspeed: 0.0158s/iter; left time: 149.9786s\n",
      "\titers: 300, epoch: 4 | loss: 0.2712645\n",
      "\tspeed: 0.0158s/iter; left time: 148.2795s\n",
      "\titers: 400, epoch: 4 | loss: 0.1277343\n",
      "\tspeed: 0.0158s/iter; left time: 146.6325s\n",
      "\titers: 500, epoch: 4 | loss: 0.2868744\n",
      "\tspeed: 0.0158s/iter; left time: 144.9743s\n",
      "Epoch: 4 cost time: 9.31934142112732\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2091032 Vali Loss: 0.0661804 Test Loss: 0.1758471\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1320623755455017, mae:0.23152689635753632\n",
      ">>> LR=5e-4,DO=0.0,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1846964\n",
      "\tspeed: 0.0264s/iter; left time: 297.9309s\n",
      "\titers: 200, epoch: 1 | loss: 0.1534819\n",
      "\tspeed: 0.0146s/iter; left time: 163.2188s\n",
      "\titers: 300, epoch: 1 | loss: 0.1390241\n",
      "\tspeed: 0.0146s/iter; left time: 161.5374s\n",
      "\titers: 400, epoch: 1 | loss: 0.1953725\n",
      "\tspeed: 0.0146s/iter; left time: 160.5391s\n",
      "\titers: 500, epoch: 1 | loss: 0.3213199\n",
      "\tspeed: 0.0146s/iter; left time: 159.0320s\n",
      "Epoch: 1 cost time: 9.53222370147705\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2022955 Vali Loss: 0.0596252 Test Loss: 0.1810754\n",
      "Validation loss decreased (inf --> 0.059625).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3014162\n",
      "\tspeed: 0.0473s/iter; left time: 507.8568s\n",
      "\titers: 200, epoch: 2 | loss: 0.1825785\n",
      "\tspeed: 0.0146s/iter; left time: 155.4732s\n",
      "\titers: 300, epoch: 2 | loss: 0.2656878\n",
      "\tspeed: 0.0146s/iter; left time: 153.6460s\n",
      "\titers: 400, epoch: 2 | loss: 0.2396755\n",
      "\tspeed: 0.0146s/iter; left time: 152.2644s\n",
      "\titers: 500, epoch: 2 | loss: 0.3102220\n",
      "\tspeed: 0.0146s/iter; left time: 150.4895s\n",
      "Epoch: 2 cost time: 8.654357433319092\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2341447 Vali Loss: 0.0562900 Test Loss: 0.1692586\n",
      "Validation loss decreased (0.059625 --> 0.056290).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2520309\n",
      "\tspeed: 0.0467s/iter; left time: 474.8179s\n",
      "\titers: 200, epoch: 3 | loss: 0.1126558\n",
      "\tspeed: 0.0146s/iter; left time: 146.9254s\n",
      "\titers: 300, epoch: 3 | loss: 0.2455842\n",
      "\tspeed: 0.0146s/iter; left time: 145.3135s\n",
      "\titers: 400, epoch: 3 | loss: 0.1633121\n",
      "\tspeed: 0.0146s/iter; left time: 144.0681s\n",
      "\titers: 500, epoch: 3 | loss: 0.1875406\n",
      "\tspeed: 0.0146s/iter; left time: 142.4853s\n",
      "Epoch: 3 cost time: 8.60222315788269\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1976280 Vali Loss: 0.0501224 Test Loss: 0.1469226\n",
      "Validation loss decreased (0.056290 --> 0.050122).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1423798\n",
      "\tspeed: 0.0484s/iter; left time: 464.3953s\n",
      "\titers: 200, epoch: 4 | loss: 0.1833380\n",
      "\tspeed: 0.0146s/iter; left time: 138.7320s\n",
      "\titers: 300, epoch: 4 | loss: 0.1767692\n",
      "\tspeed: 0.0146s/iter; left time: 137.3469s\n",
      "\titers: 400, epoch: 4 | loss: 0.1392000\n",
      "\tspeed: 0.0146s/iter; left time: 135.8123s\n",
      "\titers: 500, epoch: 4 | loss: 0.1467425\n",
      "\tspeed: 0.0146s/iter; left time: 134.2155s\n",
      "Epoch: 4 cost time: 8.607679843902588\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1538924 Vali Loss: 0.0479066 Test Loss: 0.1444010\n",
      "Validation loss decreased (0.050122 --> 0.047907).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0917977\n",
      "\tspeed: 0.0480s/iter; left time: 433.4023s\n",
      "\titers: 200, epoch: 5 | loss: 0.1277048\n",
      "\tspeed: 0.0145s/iter; left time: 129.7752s\n",
      "\titers: 300, epoch: 5 | loss: 0.1231754\n",
      "\tspeed: 0.0146s/iter; left time: 128.4556s\n",
      "\titers: 400, epoch: 5 | loss: 0.0834559\n",
      "\tspeed: 0.0146s/iter; left time: 126.9163s\n",
      "\titers: 500, epoch: 5 | loss: 0.1487637\n",
      "\tspeed: 0.0146s/iter; left time: 125.4476s\n",
      "Epoch: 5 cost time: 8.576813220977783\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1337128 Vali Loss: 0.0466142 Test Loss: 0.1435483\n",
      "Validation loss decreased (0.047907 --> 0.046614).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1269798\n",
      "\tspeed: 0.0461s/iter; left time: 389.7284s\n",
      "\titers: 200, epoch: 6 | loss: 0.1186455\n",
      "\tspeed: 0.0145s/iter; left time: 121.1454s\n",
      "\titers: 300, epoch: 6 | loss: 0.0984075\n",
      "\tspeed: 0.0145s/iter; left time: 119.6655s\n",
      "\titers: 400, epoch: 6 | loss: 0.0881625\n",
      "\tspeed: 0.0145s/iter; left time: 118.2629s\n",
      "\titers: 500, epoch: 6 | loss: 0.1166409\n",
      "\tspeed: 0.0145s/iter; left time: 116.8277s\n",
      "Epoch: 6 cost time: 8.550525665283203\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1221631 Vali Loss: 0.0458232 Test Loss: 0.1406332\n",
      "Validation loss decreased (0.046614 --> 0.045823).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1413343\n",
      "\tspeed: 0.0476s/iter; left time: 374.9272s\n",
      "\titers: 200, epoch: 7 | loss: 0.0839411\n",
      "\tspeed: 0.0146s/iter; left time: 113.2287s\n",
      "\titers: 300, epoch: 7 | loss: 0.1261159\n",
      "\tspeed: 0.0146s/iter; left time: 111.9662s\n",
      "\titers: 400, epoch: 7 | loss: 0.0824899\n",
      "\tspeed: 0.0146s/iter; left time: 110.4479s\n",
      "\titers: 500, epoch: 7 | loss: 0.1464380\n",
      "\tspeed: 0.0146s/iter; left time: 108.9528s\n",
      "Epoch: 7 cost time: 8.60797119140625\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1152028 Vali Loss: 0.0455213 Test Loss: 0.1399625\n",
      "Validation loss decreased (0.045823 --> 0.045521).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0983356\n",
      "\tspeed: 0.0469s/iter; left time: 343.0705s\n",
      "\titers: 200, epoch: 8 | loss: 0.1660734\n",
      "\tspeed: 0.0146s/iter; left time: 105.1737s\n",
      "\titers: 300, epoch: 8 | loss: 0.0982681\n",
      "\tspeed: 0.0146s/iter; left time: 103.6796s\n",
      "\titers: 400, epoch: 8 | loss: 0.1109750\n",
      "\tspeed: 0.0146s/iter; left time: 102.2066s\n",
      "\titers: 500, epoch: 8 | loss: 0.0811666\n",
      "\tspeed: 0.0146s/iter; left time: 100.7839s\n",
      "Epoch: 8 cost time: 8.598613739013672\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1107535 Vali Loss: 0.0446388 Test Loss: 0.1394486\n",
      "Validation loss decreased (0.045521 --> 0.044639).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0843178\n",
      "\tspeed: 0.0466s/iter; left time: 313.9934s\n",
      "\titers: 200, epoch: 9 | loss: 0.0957296\n",
      "\tspeed: 0.0146s/iter; left time: 96.8947s\n",
      "\titers: 300, epoch: 9 | loss: 0.0958310\n",
      "\tspeed: 0.0146s/iter; left time: 95.4071s\n",
      "\titers: 400, epoch: 9 | loss: 0.0913657\n",
      "\tspeed: 0.0146s/iter; left time: 93.9191s\n",
      "\titers: 500, epoch: 9 | loss: 0.1282018\n",
      "\tspeed: 0.0146s/iter; left time: 92.4556s\n",
      "Epoch: 9 cost time: 8.619352579116821\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1083913 Vali Loss: 0.0448575 Test Loss: 0.1397041\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1445305\n",
      "\tspeed: 0.0465s/iter; left time: 287.1902s\n",
      "\titers: 200, epoch: 10 | loss: 0.1169368\n",
      "\tspeed: 0.0146s/iter; left time: 88.4042s\n",
      "\titers: 300, epoch: 10 | loss: 0.0864425\n",
      "\tspeed: 0.0146s/iter; left time: 86.9185s\n",
      "\titers: 400, epoch: 10 | loss: 0.1313425\n",
      "\tspeed: 0.0145s/iter; left time: 85.4152s\n",
      "\titers: 500, epoch: 10 | loss: 0.1608877\n",
      "\tspeed: 0.0145s/iter; left time: 83.9432s\n",
      "Epoch: 10 cost time: 8.588130235671997\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1071333 Vali Loss: 0.0449487 Test Loss: 0.1401038\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1228852\n",
      "\tspeed: 0.0457s/iter; left time: 255.9256s\n",
      "\titers: 200, epoch: 11 | loss: 0.0852268\n",
      "\tspeed: 0.0145s/iter; left time: 80.0296s\n",
      "\titers: 300, epoch: 11 | loss: 0.0976405\n",
      "\tspeed: 0.0145s/iter; left time: 78.5214s\n",
      "\titers: 400, epoch: 11 | loss: 0.1279954\n",
      "\tspeed: 0.0145s/iter; left time: 77.0949s\n",
      "\titers: 500, epoch: 11 | loss: 0.1509469\n",
      "\tspeed: 0.0145s/iter; left time: 75.6197s\n",
      "Epoch: 11 cost time: 8.567837953567505\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1064138 Vali Loss: 0.0450237 Test Loss: 0.1398980\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13961860537528992, mae:0.2440221607685089\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1424365\n",
      "\tspeed: 0.0296s/iter; left time: 334.0397s\n",
      "\titers: 200, epoch: 1 | loss: 0.1840478\n",
      "\tspeed: 0.0175s/iter; left time: 196.0982s\n",
      "\titers: 300, epoch: 1 | loss: 0.2390462\n",
      "\tspeed: 0.0175s/iter; left time: 194.4126s\n",
      "\titers: 400, epoch: 1 | loss: 0.1257376\n",
      "\tspeed: 0.0175s/iter; left time: 192.7160s\n",
      "\titers: 500, epoch: 1 | loss: 0.1427562\n",
      "\tspeed: 0.0175s/iter; left time: 190.4853s\n",
      "Epoch: 1 cost time: 11.234099864959717\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1878913 Vali Loss: 0.0468647 Test Loss: 0.1412132\n",
      "Validation loss decreased (inf --> 0.046865).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2610076\n",
      "\tspeed: 0.0562s/iter; left time: 602.6827s\n",
      "\titers: 200, epoch: 2 | loss: 0.1107580\n",
      "\tspeed: 0.0176s/iter; left time: 186.8321s\n",
      "\titers: 300, epoch: 2 | loss: 0.1036385\n",
      "\tspeed: 0.0176s/iter; left time: 185.0130s\n",
      "\titers: 400, epoch: 2 | loss: 0.1070925\n",
      "\tspeed: 0.0176s/iter; left time: 183.0846s\n",
      "\titers: 500, epoch: 2 | loss: 0.1219436\n",
      "\tspeed: 0.0176s/iter; left time: 181.3749s\n",
      "Epoch: 2 cost time: 10.31496787071228\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1391301 Vali Loss: 0.0438123 Test Loss: 0.1367771\n",
      "Validation loss decreased (0.046865 --> 0.043812).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0830769\n",
      "\tspeed: 0.0560s/iter; left time: 568.7082s\n",
      "\titers: 200, epoch: 3 | loss: 0.0651449\n",
      "\tspeed: 0.0174s/iter; left time: 174.7894s\n",
      "\titers: 300, epoch: 3 | loss: 0.0848362\n",
      "\tspeed: 0.0173s/iter; left time: 172.6054s\n",
      "\titers: 400, epoch: 3 | loss: 0.0720099\n",
      "\tspeed: 0.0173s/iter; left time: 170.8772s\n",
      "\titers: 500, epoch: 3 | loss: 0.0553302\n",
      "\tspeed: 0.0174s/iter; left time: 169.6702s\n",
      "Epoch: 3 cost time: 10.217609167098999\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0927919 Vali Loss: 0.0406958 Test Loss: 0.1434521\n",
      "Validation loss decreased (0.043812 --> 0.040696).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0647112\n",
      "\tspeed: 0.0569s/iter; left time: 546.1732s\n",
      "\titers: 200, epoch: 4 | loss: 0.0716239\n",
      "\tspeed: 0.0175s/iter; left time: 166.2836s\n",
      "\titers: 300, epoch: 4 | loss: 0.0661048\n",
      "\tspeed: 0.0175s/iter; left time: 164.6064s\n",
      "\titers: 400, epoch: 4 | loss: 0.0359052\n",
      "\tspeed: 0.0175s/iter; left time: 163.0471s\n",
      "\titers: 500, epoch: 4 | loss: 0.0596870\n",
      "\tspeed: 0.0175s/iter; left time: 161.0673s\n",
      "Epoch: 4 cost time: 10.272650480270386\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0583398 Vali Loss: 0.0424002 Test Loss: 0.1405669\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0493537\n",
      "\tspeed: 0.0566s/iter; left time: 510.2105s\n",
      "\titers: 200, epoch: 5 | loss: 0.0440921\n",
      "\tspeed: 0.0175s/iter; left time: 156.4399s\n",
      "\titers: 300, epoch: 5 | loss: 0.0384487\n",
      "\tspeed: 0.0175s/iter; left time: 154.1832s\n",
      "\titers: 400, epoch: 5 | loss: 0.0319154\n",
      "\tspeed: 0.0175s/iter; left time: 152.5485s\n",
      "\titers: 500, epoch: 5 | loss: 0.0568911\n",
      "\tspeed: 0.0175s/iter; left time: 150.8815s\n",
      "Epoch: 5 cost time: 10.264096021652222\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0422725 Vali Loss: 0.0417912 Test Loss: 0.1395553\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0277847\n",
      "\tspeed: 0.0563s/iter; left time: 475.5051s\n",
      "\titers: 200, epoch: 6 | loss: 0.0349614\n",
      "\tspeed: 0.0210s/iter; left time: 175.3171s\n",
      "\titers: 300, epoch: 6 | loss: 0.0368796\n",
      "\tspeed: 0.0175s/iter; left time: 144.5615s\n",
      "\titers: 400, epoch: 6 | loss: 0.0483236\n",
      "\tspeed: 0.0175s/iter; left time: 142.8929s\n",
      "\titers: 500, epoch: 6 | loss: 0.0325147\n",
      "\tspeed: 0.0175s/iter; left time: 141.2039s\n",
      "Epoch: 6 cost time: 10.864749193191528\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0343979 Vali Loss: 0.0434447 Test Loss: 0.1454348\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14360164105892181, mae:0.24396653473377228\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3013273\n",
      "\tspeed: 0.0291s/iter; left time: 328.9242s\n",
      "\titers: 200, epoch: 1 | loss: 0.1627599\n",
      "\tspeed: 0.0173s/iter; left time: 193.8114s\n",
      "\titers: 300, epoch: 1 | loss: 0.1636733\n",
      "\tspeed: 0.0173s/iter; left time: 191.7704s\n",
      "\titers: 400, epoch: 1 | loss: 0.1275357\n",
      "\tspeed: 0.0172s/iter; left time: 189.5011s\n",
      "\titers: 500, epoch: 1 | loss: 0.1798341\n",
      "\tspeed: 0.0172s/iter; left time: 187.6757s\n",
      "Epoch: 1 cost time: 11.068463802337646\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1556122 Vali Loss: 0.0425841 Test Loss: 0.1366059\n",
      "Validation loss decreased (inf --> 0.042584).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1080536\n",
      "\tspeed: 0.0568s/iter; left time: 609.4106s\n",
      "\titers: 200, epoch: 2 | loss: 0.1725593\n",
      "\tspeed: 0.0174s/iter; left time: 185.0891s\n",
      "\titers: 300, epoch: 2 | loss: 0.1873654\n",
      "\tspeed: 0.0174s/iter; left time: 183.3103s\n",
      "\titers: 400, epoch: 2 | loss: 0.1795883\n",
      "\tspeed: 0.0174s/iter; left time: 181.4925s\n",
      "\titers: 500, epoch: 2 | loss: 0.1130972\n",
      "\tspeed: 0.0172s/iter; left time: 178.0487s\n",
      "Epoch: 2 cost time: 10.214637994766235\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1257417 Vali Loss: 0.0436001 Test Loss: 0.1318278\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0872726\n",
      "\tspeed: 0.0525s/iter; left time: 533.0209s\n",
      "\titers: 200, epoch: 3 | loss: 0.0909489\n",
      "\tspeed: 0.0172s/iter; left time: 172.9166s\n",
      "\titers: 300, epoch: 3 | loss: 0.1098259\n",
      "\tspeed: 0.0172s/iter; left time: 171.4080s\n",
      "\titers: 400, epoch: 3 | loss: 0.0617533\n",
      "\tspeed: 0.0172s/iter; left time: 169.2316s\n",
      "\titers: 500, epoch: 3 | loss: 0.0701032\n",
      "\tspeed: 0.0172s/iter; left time: 168.0404s\n",
      "Epoch: 3 cost time: 10.081186056137085\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0934521 Vali Loss: 0.0351826 Test Loss: 0.1203531\n",
      "Validation loss decreased (0.042584 --> 0.035183).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0649448\n",
      "\tspeed: 0.0537s/iter; left time: 515.4898s\n",
      "\titers: 200, epoch: 4 | loss: 0.0549003\n",
      "\tspeed: 0.0172s/iter; left time: 163.5482s\n",
      "\titers: 300, epoch: 4 | loss: 0.0712037\n",
      "\tspeed: 0.0172s/iter; left time: 161.9404s\n",
      "\titers: 400, epoch: 4 | loss: 0.0707857\n",
      "\tspeed: 0.0172s/iter; left time: 159.9485s\n",
      "\titers: 500, epoch: 4 | loss: 0.0471515\n",
      "\tspeed: 0.0172s/iter; left time: 158.3487s\n",
      "Epoch: 4 cost time: 10.118118286132812\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0624750 Vali Loss: 0.0335348 Test Loss: 0.1206524\n",
      "Validation loss decreased (0.035183 --> 0.033535).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0518756\n",
      "\tspeed: 0.0540s/iter; left time: 487.3786s\n",
      "\titers: 200, epoch: 5 | loss: 0.0462244\n",
      "\tspeed: 0.0173s/iter; left time: 153.9782s\n",
      "\titers: 300, epoch: 5 | loss: 0.0481729\n",
      "\tspeed: 0.0172s/iter; left time: 152.1555s\n",
      "\titers: 400, epoch: 5 | loss: 0.0416372\n",
      "\tspeed: 0.0172s/iter; left time: 150.3791s\n",
      "\titers: 500, epoch: 5 | loss: 0.0582077\n",
      "\tspeed: 0.0172s/iter; left time: 148.5392s\n",
      "Epoch: 5 cost time: 10.142864465713501\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0454446 Vali Loss: 0.0346301 Test Loss: 0.1202264\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0512787\n",
      "\tspeed: 0.0565s/iter; left time: 477.6111s\n",
      "\titers: 200, epoch: 6 | loss: 0.0321706\n",
      "\tspeed: 0.0193s/iter; left time: 161.1671s\n",
      "\titers: 300, epoch: 6 | loss: 0.0402475\n",
      "\tspeed: 0.0182s/iter; left time: 150.0400s\n",
      "\titers: 400, epoch: 6 | loss: 0.0389622\n",
      "\tspeed: 0.0173s/iter; left time: 140.7214s\n",
      "\titers: 500, epoch: 6 | loss: 0.0262756\n",
      "\tspeed: 0.0173s/iter; left time: 139.0547s\n",
      "Epoch: 6 cost time: 10.614771842956543\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0372056 Vali Loss: 0.0347524 Test Loss: 0.1218830\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0415928\n",
      "\tspeed: 0.0534s/iter; left time: 420.8277s\n",
      "\titers: 200, epoch: 7 | loss: 0.0410634\n",
      "\tspeed: 0.0172s/iter; left time: 134.0206s\n",
      "\titers: 300, epoch: 7 | loss: 0.0310007\n",
      "\tspeed: 0.0172s/iter; left time: 132.2524s\n",
      "\titers: 400, epoch: 7 | loss: 0.0346273\n",
      "\tspeed: 0.0172s/iter; left time: 130.5292s\n",
      "\titers: 500, epoch: 7 | loss: 0.0262612\n",
      "\tspeed: 0.0172s/iter; left time: 128.8181s\n",
      "Epoch: 7 cost time: 10.124592781066895\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0329937 Vali Loss: 0.0353262 Test Loss: 0.1226116\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12082037329673767, mae:0.21284867823123932\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2177885\n",
      "\tspeed: 0.0314s/iter; left time: 354.9021s\n",
      "\titers: 200, epoch: 1 | loss: 0.1740898\n",
      "\tspeed: 0.0193s/iter; left time: 216.6819s\n",
      "\titers: 300, epoch: 1 | loss: 0.2318988\n",
      "\tspeed: 0.0193s/iter; left time: 214.2933s\n",
      "\titers: 400, epoch: 1 | loss: 0.1778409\n",
      "\tspeed: 0.0194s/iter; left time: 213.1538s\n",
      "\titers: 500, epoch: 1 | loss: 0.2104654\n",
      "\tspeed: 0.0193s/iter; left time: 210.8893s\n",
      "Epoch: 1 cost time: 12.274818181991577\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1894304 Vali Loss: 0.0450520 Test Loss: 0.1393649\n",
      "Validation loss decreased (inf --> 0.045052).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2218399\n",
      "\tspeed: 0.0593s/iter; left time: 636.1939s\n",
      "\titers: 200, epoch: 2 | loss: 0.2031770\n",
      "\tspeed: 0.0194s/iter; left time: 206.7164s\n",
      "\titers: 300, epoch: 2 | loss: 0.2917352\n",
      "\tspeed: 0.0194s/iter; left time: 203.9995s\n",
      "\titers: 400, epoch: 2 | loss: 0.1115078\n",
      "\tspeed: 0.0193s/iter; left time: 201.7982s\n",
      "\titers: 500, epoch: 2 | loss: 0.0887230\n",
      "\tspeed: 0.0193s/iter; left time: 199.8775s\n",
      "Epoch: 2 cost time: 11.36694049835205\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1400354 Vali Loss: 0.0453597 Test Loss: 0.1389147\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0976101\n",
      "\tspeed: 0.0554s/iter; left time: 563.3419s\n",
      "\titers: 200, epoch: 3 | loss: 0.0790695\n",
      "\tspeed: 0.0172s/iter; left time: 173.4214s\n",
      "\titers: 300, epoch: 3 | loss: 0.0755557\n",
      "\tspeed: 0.0172s/iter; left time: 171.5011s\n",
      "\titers: 400, epoch: 3 | loss: 0.0531154\n",
      "\tspeed: 0.0172s/iter; left time: 170.0479s\n",
      "\titers: 500, epoch: 3 | loss: 0.0669597\n",
      "\tspeed: 0.0172s/iter; left time: 168.2180s\n",
      "Epoch: 3 cost time: 10.140873193740845\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0817688 Vali Loss: 0.0419895 Test Loss: 0.1375994\n",
      "Validation loss decreased (0.045052 --> 0.041990).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0431753\n",
      "\tspeed: 0.0537s/iter; left time: 514.8466s\n",
      "\titers: 200, epoch: 4 | loss: 0.0574122\n",
      "\tspeed: 0.0172s/iter; left time: 163.3992s\n",
      "\titers: 300, epoch: 4 | loss: 0.0522037\n",
      "\tspeed: 0.0171s/iter; left time: 161.0555s\n",
      "\titers: 400, epoch: 4 | loss: 0.0496517\n",
      "\tspeed: 0.0171s/iter; left time: 159.1858s\n",
      "\titers: 500, epoch: 4 | loss: 0.0428037\n",
      "\tspeed: 0.0172s/iter; left time: 158.2593s\n",
      "Epoch: 4 cost time: 10.105591535568237\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0467967 Vali Loss: 0.0405782 Test Loss: 0.1369759\n",
      "Validation loss decreased (0.041990 --> 0.040578).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0329710\n",
      "\tspeed: 0.0543s/iter; left time: 489.8735s\n",
      "\titers: 200, epoch: 5 | loss: 0.0373534\n",
      "\tspeed: 0.0172s/iter; left time: 153.5074s\n",
      "\titers: 300, epoch: 5 | loss: 0.0254652\n",
      "\tspeed: 0.0172s/iter; left time: 151.7855s\n",
      "\titers: 400, epoch: 5 | loss: 0.0320666\n",
      "\tspeed: 0.0172s/iter; left time: 149.7968s\n",
      "\titers: 500, epoch: 5 | loss: 0.0248615\n",
      "\tspeed: 0.0172s/iter; left time: 147.8571s\n",
      "Epoch: 5 cost time: 10.10209035873413\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0323576 Vali Loss: 0.0408936 Test Loss: 0.1396430\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0270176\n",
      "\tspeed: 0.0553s/iter; left time: 466.9849s\n",
      "\titers: 200, epoch: 6 | loss: 0.0235493\n",
      "\tspeed: 0.0196s/iter; left time: 163.5587s\n",
      "\titers: 300, epoch: 6 | loss: 0.0301357\n",
      "\tspeed: 0.0200s/iter; left time: 165.0559s\n",
      "\titers: 400, epoch: 6 | loss: 0.0253702\n",
      "\tspeed: 0.0200s/iter; left time: 162.9960s\n",
      "\titers: 500, epoch: 6 | loss: 0.0251508\n",
      "\tspeed: 0.0200s/iter; left time: 161.3113s\n",
      "Epoch: 6 cost time: 11.5802640914917\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0259659 Vali Loss: 0.0409562 Test Loss: 0.1410119\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0220625\n",
      "\tspeed: 0.0583s/iter; left time: 459.0770s\n",
      "\titers: 200, epoch: 7 | loss: 0.0194717\n",
      "\tspeed: 0.0200s/iter; left time: 155.7008s\n",
      "\titers: 300, epoch: 7 | loss: 0.0231270\n",
      "\tspeed: 0.0200s/iter; left time: 153.3603s\n",
      "\titers: 400, epoch: 7 | loss: 0.0367678\n",
      "\tspeed: 0.0200s/iter; left time: 151.3739s\n",
      "\titers: 500, epoch: 7 | loss: 0.0201000\n",
      "\tspeed: 0.0200s/iter; left time: 149.3371s\n",
      "Epoch: 7 cost time: 11.680776357650757\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0229394 Vali Loss: 0.0413790 Test Loss: 0.1412562\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13710694015026093, mae:0.2393602728843689\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1982598\n",
      "\tspeed: 0.0351s/iter; left time: 397.0552s\n",
      "\titers: 200, epoch: 1 | loss: 0.1722663\n",
      "\tspeed: 0.0231s/iter; left time: 258.5510s\n",
      "\titers: 300, epoch: 1 | loss: 0.2368596\n",
      "\tspeed: 0.0231s/iter; left time: 256.5817s\n",
      "\titers: 400, epoch: 1 | loss: 0.2784756\n",
      "\tspeed: 0.0231s/iter; left time: 254.6179s\n",
      "\titers: 500, epoch: 1 | loss: 0.2624359\n",
      "\tspeed: 0.0231s/iter; left time: 252.2754s\n",
      "Epoch: 1 cost time: 14.436309814453125\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2215046 Vali Loss: 0.0689446 Test Loss: 0.2047751\n",
      "Validation loss decreased (inf --> 0.068945).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3135594\n",
      "\tspeed: 0.0641s/iter; left time: 687.7538s\n",
      "\titers: 200, epoch: 2 | loss: 0.2461600\n",
      "\tspeed: 0.0213s/iter; left time: 226.8361s\n",
      "\titers: 300, epoch: 2 | loss: 0.1640294\n",
      "\tspeed: 0.0213s/iter; left time: 224.5727s\n",
      "\titers: 400, epoch: 2 | loss: 0.2334460\n",
      "\tspeed: 0.0213s/iter; left time: 222.4586s\n",
      "\titers: 500, epoch: 2 | loss: 0.1787507\n",
      "\tspeed: 0.0213s/iter; left time: 220.2469s\n",
      "Epoch: 2 cost time: 12.496492147445679\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2409333 Vali Loss: 0.0579337 Test Loss: 0.1650611\n",
      "Validation loss decreased (0.068945 --> 0.057934).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3445709\n",
      "\tspeed: 0.0634s/iter; left time: 643.7861s\n",
      "\titers: 200, epoch: 3 | loss: 0.1344387\n",
      "\tspeed: 0.0214s/iter; left time: 215.4934s\n",
      "\titers: 300, epoch: 3 | loss: 0.1870665\n",
      "\tspeed: 0.0214s/iter; left time: 212.9924s\n",
      "\titers: 400, epoch: 3 | loss: 0.1502970\n",
      "\tspeed: 0.0214s/iter; left time: 210.7641s\n",
      "\titers: 500, epoch: 3 | loss: 0.1235990\n",
      "\tspeed: 0.0214s/iter; left time: 208.4927s\n",
      "Epoch: 3 cost time: 12.529727220535278\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1725269 Vali Loss: 0.0474227 Test Loss: 0.1467401\n",
      "Validation loss decreased (0.057934 --> 0.047423).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1178306\n",
      "\tspeed: 0.0648s/iter; left time: 621.3064s\n",
      "\titers: 200, epoch: 4 | loss: 0.1037992\n",
      "\tspeed: 0.0214s/iter; left time: 202.7695s\n",
      "\titers: 300, epoch: 4 | loss: 0.1877244\n",
      "\tspeed: 0.0213s/iter; left time: 200.4343s\n",
      "\titers: 400, epoch: 4 | loss: 0.1446683\n",
      "\tspeed: 0.0213s/iter; left time: 198.0124s\n",
      "\titers: 500, epoch: 4 | loss: 0.1847490\n",
      "\tspeed: 0.0213s/iter; left time: 195.9914s\n",
      "Epoch: 4 cost time: 12.534107208251953\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1489754 Vali Loss: 0.0459652 Test Loss: 0.1424586\n",
      "Validation loss decreased (0.047423 --> 0.045965).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1684721\n",
      "\tspeed: 0.0672s/iter; left time: 606.3727s\n",
      "\titers: 200, epoch: 5 | loss: 0.2174387\n",
      "\tspeed: 0.0214s/iter; left time: 190.5443s\n",
      "\titers: 300, epoch: 5 | loss: 0.1022878\n",
      "\tspeed: 0.0219s/iter; left time: 193.1059s\n",
      "\titers: 400, epoch: 5 | loss: 0.1460482\n",
      "\tspeed: 0.0231s/iter; left time: 201.4535s\n",
      "\titers: 500, epoch: 5 | loss: 0.1213383\n",
      "\tspeed: 0.0231s/iter; left time: 199.1193s\n",
      "Epoch: 5 cost time: 13.19725251197815\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1338308 Vali Loss: 0.0453069 Test Loss: 0.1409828\n",
      "Validation loss decreased (0.045965 --> 0.045307).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1173596\n",
      "\tspeed: 0.0681s/iter; left time: 575.8399s\n",
      "\titers: 200, epoch: 6 | loss: 0.1838098\n",
      "\tspeed: 0.0220s/iter; left time: 183.3305s\n",
      "\titers: 300, epoch: 6 | loss: 0.1131455\n",
      "\tspeed: 0.0214s/iter; left time: 176.4253s\n",
      "\titers: 400, epoch: 6 | loss: 0.0865408\n",
      "\tspeed: 0.0213s/iter; left time: 173.9709s\n",
      "\titers: 500, epoch: 6 | loss: 0.1666597\n",
      "\tspeed: 0.0213s/iter; left time: 171.7410s\n",
      "Epoch: 6 cost time: 12.76200532913208\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1246720 Vali Loss: 0.0451699 Test Loss: 0.1443853\n",
      "Validation loss decreased (0.045307 --> 0.045170).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1408811\n",
      "\tspeed: 0.0679s/iter; left time: 535.4304s\n",
      "\titers: 200, epoch: 7 | loss: 0.1244936\n",
      "\tspeed: 0.0222s/iter; left time: 172.4591s\n",
      "\titers: 300, epoch: 7 | loss: 0.1025622\n",
      "\tspeed: 0.0214s/iter; left time: 164.4443s\n",
      "\titers: 400, epoch: 7 | loss: 0.1524026\n",
      "\tspeed: 0.0214s/iter; left time: 162.2732s\n",
      "\titers: 500, epoch: 7 | loss: 0.1064116\n",
      "\tspeed: 0.0214s/iter; left time: 159.9515s\n",
      "Epoch: 7 cost time: 12.810815572738647\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1183561 Vali Loss: 0.0456586 Test Loss: 0.1454745\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1128823\n",
      "\tspeed: 0.0631s/iter; left time: 461.5600s\n",
      "\titers: 200, epoch: 8 | loss: 0.1403389\n",
      "\tspeed: 0.0214s/iter; left time: 154.4493s\n",
      "\titers: 300, epoch: 8 | loss: 0.0930431\n",
      "\tspeed: 0.0214s/iter; left time: 152.0799s\n",
      "\titers: 400, epoch: 8 | loss: 0.0984493\n",
      "\tspeed: 0.0214s/iter; left time: 149.7835s\n",
      "\titers: 500, epoch: 8 | loss: 0.0983002\n",
      "\tspeed: 0.0214s/iter; left time: 147.7480s\n",
      "Epoch: 8 cost time: 12.589613676071167\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1149548 Vali Loss: 0.0459172 Test Loss: 0.1456573\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1132303\n",
      "\tspeed: 0.0640s/iter; left time: 431.1019s\n",
      "\titers: 200, epoch: 9 | loss: 0.1282349\n",
      "\tspeed: 0.0213s/iter; left time: 141.7252s\n",
      "\titers: 300, epoch: 9 | loss: 0.1326363\n",
      "\tspeed: 0.0213s/iter; left time: 139.4704s\n",
      "\titers: 400, epoch: 9 | loss: 0.0945258\n",
      "\tspeed: 0.0213s/iter; left time: 137.3532s\n",
      "\titers: 500, epoch: 9 | loss: 0.1243167\n",
      "\tspeed: 0.0213s/iter; left time: 135.0818s\n",
      "Epoch: 9 cost time: 12.48659372329712\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1132426 Vali Loss: 0.0457194 Test Loss: 0.1453414\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14455336332321167, mae:0.23860737681388855\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1611827\n",
      "\tspeed: 0.0350s/iter; left time: 395.9074s\n",
      "\titers: 200, epoch: 1 | loss: 0.0880886\n",
      "\tspeed: 0.0224s/iter; left time: 250.9847s\n",
      "\titers: 300, epoch: 1 | loss: 0.1551008\n",
      "\tspeed: 0.0224s/iter; left time: 248.6656s\n",
      "\titers: 400, epoch: 1 | loss: 0.1174971\n",
      "\tspeed: 0.0224s/iter; left time: 246.5601s\n",
      "\titers: 500, epoch: 1 | loss: 0.1570380\n",
      "\tspeed: 0.0224s/iter; left time: 244.2053s\n",
      "Epoch: 1 cost time: 14.085506677627563\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1819967 Vali Loss: 0.0417195 Test Loss: 0.1302326\n",
      "Validation loss decreased (inf --> 0.041720).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2005797\n",
      "\tspeed: 0.0621s/iter; left time: 666.4194s\n",
      "\titers: 200, epoch: 2 | loss: 0.1317277\n",
      "\tspeed: 0.0207s/iter; left time: 219.6524s\n",
      "\titers: 300, epoch: 2 | loss: 0.2992740\n",
      "\tspeed: 0.0206s/iter; left time: 217.0322s\n",
      "\titers: 400, epoch: 2 | loss: 0.2557242\n",
      "\tspeed: 0.0206s/iter; left time: 214.7338s\n",
      "\titers: 500, epoch: 2 | loss: 0.1844640\n",
      "\tspeed: 0.0206s/iter; left time: 212.6493s\n",
      "Epoch: 2 cost time: 12.067293643951416\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2159472 Vali Loss: 0.0691888 Test Loss: 0.2015038\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1917845\n",
      "\tspeed: 0.0598s/iter; left time: 607.9796s\n",
      "\titers: 200, epoch: 3 | loss: 0.1939210\n",
      "\tspeed: 0.0206s/iter; left time: 207.2197s\n",
      "\titers: 300, epoch: 3 | loss: 0.1543978\n",
      "\tspeed: 0.0206s/iter; left time: 205.0060s\n",
      "\titers: 400, epoch: 3 | loss: 0.1578405\n",
      "\tspeed: 0.0206s/iter; left time: 202.9015s\n",
      "\titers: 500, epoch: 3 | loss: 0.1485855\n",
      "\tspeed: 0.0206s/iter; left time: 201.1393s\n",
      "Epoch: 3 cost time: 12.027798414230347\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2254620 Vali Loss: 0.0597600 Test Loss: 0.1643411\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1238583\n",
      "\tspeed: 0.0597s/iter; left time: 572.6615s\n",
      "\titers: 200, epoch: 4 | loss: 0.1780287\n",
      "\tspeed: 0.0206s/iter; left time: 195.8218s\n",
      "\titers: 300, epoch: 4 | loss: 0.1466343\n",
      "\tspeed: 0.0206s/iter; left time: 193.5541s\n",
      "\titers: 400, epoch: 4 | loss: 0.2122630\n",
      "\tspeed: 0.0206s/iter; left time: 191.4763s\n",
      "\titers: 500, epoch: 4 | loss: 0.1696354\n",
      "\tspeed: 0.0206s/iter; left time: 189.4052s\n",
      "Epoch: 4 cost time: 12.049728870391846\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1947377 Vali Loss: 0.0546327 Test Loss: 0.1529316\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13044115900993347, mae:0.23281407356262207\n",
      ">>> LR=5e-4,DO=0.0,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2434345\n",
      "\tspeed: 0.0308s/iter; left time: 348.2356s\n",
      "\titers: 200, epoch: 1 | loss: 0.1531063\n",
      "\tspeed: 0.0191s/iter; left time: 213.6830s\n",
      "\titers: 300, epoch: 1 | loss: 0.2530334\n",
      "\tspeed: 0.0191s/iter; left time: 211.4828s\n",
      "\titers: 400, epoch: 1 | loss: 0.2281839\n",
      "\tspeed: 0.0191s/iter; left time: 209.7404s\n",
      "\titers: 500, epoch: 1 | loss: 0.2358771\n",
      "\tspeed: 0.0191s/iter; left time: 207.9804s\n",
      "Epoch: 1 cost time: 12.098169326782227\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2170983 Vali Loss: 0.0559171 Test Loss: 0.1756478\n",
      "Validation loss decreased (inf --> 0.055917).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1163736\n",
      "\tspeed: 0.0589s/iter; left time: 631.8317s\n",
      "\titers: 200, epoch: 2 | loss: 0.4850007\n",
      "\tspeed: 0.0191s/iter; left time: 203.1564s\n",
      "\titers: 300, epoch: 2 | loss: 0.2708116\n",
      "\tspeed: 0.0191s/iter; left time: 201.5027s\n",
      "\titers: 400, epoch: 2 | loss: 0.1452087\n",
      "\tspeed: 0.0191s/iter; left time: 199.3243s\n",
      "\titers: 500, epoch: 2 | loss: 0.1954087\n",
      "\tspeed: 0.0191s/iter; left time: 197.2320s\n",
      "Epoch: 2 cost time: 11.210079193115234\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2521133 Vali Loss: 0.0596861 Test Loss: 0.1777243\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1769210\n",
      "\tspeed: 0.0565s/iter; left time: 574.0230s\n",
      "\titers: 200, epoch: 3 | loss: 0.1579297\n",
      "\tspeed: 0.0191s/iter; left time: 192.2845s\n",
      "\titers: 300, epoch: 3 | loss: 0.2444570\n",
      "\tspeed: 0.0191s/iter; left time: 190.0683s\n",
      "\titers: 400, epoch: 3 | loss: 0.1686455\n",
      "\tspeed: 0.0205s/iter; left time: 202.0262s\n",
      "\titers: 500, epoch: 3 | loss: 0.2149357\n",
      "\tspeed: 0.0203s/iter; left time: 198.1292s\n",
      "Epoch: 3 cost time: 11.398840427398682\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2108717 Vali Loss: 0.0600512 Test Loss: 0.1710273\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1616878\n",
      "\tspeed: 0.0583s/iter; left time: 559.1112s\n",
      "\titers: 200, epoch: 4 | loss: 0.2954238\n",
      "\tspeed: 0.0191s/iter; left time: 181.2606s\n",
      "\titers: 300, epoch: 4 | loss: 0.1252136\n",
      "\tspeed: 0.0191s/iter; left time: 179.1363s\n",
      "\titers: 400, epoch: 4 | loss: 0.1872400\n",
      "\tspeed: 0.0191s/iter; left time: 177.3856s\n",
      "\titers: 500, epoch: 4 | loss: 0.1753382\n",
      "\tspeed: 0.0192s/iter; left time: 176.0083s\n",
      "Epoch: 4 cost time: 11.14454460144043\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1836865 Vali Loss: 0.0583106 Test Loss: 0.1626222\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.17591072618961334, mae:0.27432191371917725\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1875591\n",
      "\tspeed: 0.0234s/iter; left time: 264.9097s\n",
      "\titers: 200, epoch: 1 | loss: 0.1685902\n",
      "\tspeed: 0.0121s/iter; left time: 135.0400s\n",
      "\titers: 300, epoch: 1 | loss: 0.2426253\n",
      "\tspeed: 0.0120s/iter; left time: 133.2741s\n",
      "\titers: 400, epoch: 1 | loss: 0.2055227\n",
      "\tspeed: 0.0120s/iter; left time: 132.1271s\n",
      "\titers: 500, epoch: 1 | loss: 0.1781254\n",
      "\tspeed: 0.0120s/iter; left time: 131.3377s\n",
      "Epoch: 1 cost time: 8.046347618103027\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2129296 Vali Loss: 0.0461688 Test Loss: 0.1384224\n",
      "Validation loss decreased (inf --> 0.046169).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2534581\n",
      "\tspeed: 0.0401s/iter; left time: 429.8816s\n",
      "\titers: 200, epoch: 2 | loss: 0.2095187\n",
      "\tspeed: 0.0120s/iter; left time: 127.4822s\n",
      "\titers: 300, epoch: 2 | loss: 0.1648936\n",
      "\tspeed: 0.0120s/iter; left time: 126.3407s\n",
      "\titers: 400, epoch: 2 | loss: 0.1565098\n",
      "\tspeed: 0.0120s/iter; left time: 124.6881s\n",
      "\titers: 500, epoch: 2 | loss: 0.1552203\n",
      "\tspeed: 0.0120s/iter; left time: 123.6012s\n",
      "Epoch: 2 cost time: 7.1346213817596436\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1706752 Vali Loss: 0.0441416 Test Loss: 0.1367652\n",
      "Validation loss decreased (0.046169 --> 0.044142).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1195780\n",
      "\tspeed: 0.0396s/iter; left time: 402.1755s\n",
      "\titers: 200, epoch: 3 | loss: 0.1630878\n",
      "\tspeed: 0.0120s/iter; left time: 121.0289s\n",
      "\titers: 300, epoch: 3 | loss: 0.1316595\n",
      "\tspeed: 0.0120s/iter; left time: 119.7274s\n",
      "\titers: 400, epoch: 3 | loss: 0.1441677\n",
      "\tspeed: 0.0120s/iter; left time: 118.5215s\n",
      "\titers: 500, epoch: 3 | loss: 0.1832184\n",
      "\tspeed: 0.0120s/iter; left time: 117.1723s\n",
      "Epoch: 3 cost time: 7.129286766052246\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1405180 Vali Loss: 0.0383746 Test Loss: 0.1244795\n",
      "Validation loss decreased (0.044142 --> 0.038375).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1192791\n",
      "\tspeed: 0.0381s/iter; left time: 365.3903s\n",
      "\titers: 200, epoch: 4 | loss: 0.0985092\n",
      "\tspeed: 0.0119s/iter; left time: 113.3303s\n",
      "\titers: 300, epoch: 4 | loss: 0.1505321\n",
      "\tspeed: 0.0120s/iter; left time: 112.4095s\n",
      "\titers: 400, epoch: 4 | loss: 0.1254500\n",
      "\tspeed: 0.0120s/iter; left time: 111.1167s\n",
      "\titers: 500, epoch: 4 | loss: 0.1224056\n",
      "\tspeed: 0.0119s/iter; left time: 109.6381s\n",
      "Epoch: 4 cost time: 7.078660488128662\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1215223 Vali Loss: 0.0378357 Test Loss: 0.1281882\n",
      "Validation loss decreased (0.038375 --> 0.037836).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1180496\n",
      "\tspeed: 0.0381s/iter; left time: 343.3340s\n",
      "\titers: 200, epoch: 5 | loss: 0.1264121\n",
      "\tspeed: 0.0120s/iter; left time: 107.2321s\n",
      "\titers: 300, epoch: 5 | loss: 0.0851760\n",
      "\tspeed: 0.0120s/iter; left time: 105.8008s\n",
      "\titers: 400, epoch: 5 | loss: 0.1103657\n",
      "\tspeed: 0.0120s/iter; left time: 104.5831s\n",
      "\titers: 500, epoch: 5 | loss: 0.1040657\n",
      "\tspeed: 0.0120s/iter; left time: 103.5305s\n",
      "Epoch: 5 cost time: 7.130844831466675\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1098918 Vali Loss: 0.0366639 Test Loss: 0.1306551\n",
      "Validation loss decreased (0.037836 --> 0.036664).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0982700\n",
      "\tspeed: 0.0391s/iter; left time: 330.6801s\n",
      "\titers: 200, epoch: 6 | loss: 0.1011713\n",
      "\tspeed: 0.0120s/iter; left time: 100.5461s\n",
      "\titers: 300, epoch: 6 | loss: 0.1064663\n",
      "\tspeed: 0.0120s/iter; left time: 98.9876s\n",
      "\titers: 400, epoch: 6 | loss: 0.1059360\n",
      "\tspeed: 0.0120s/iter; left time: 97.9214s\n",
      "\titers: 500, epoch: 6 | loss: 0.1032415\n",
      "\tspeed: 0.0120s/iter; left time: 96.8903s\n",
      "Epoch: 6 cost time: 7.150566101074219\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1038395 Vali Loss: 0.0361007 Test Loss: 0.1349440\n",
      "Validation loss decreased (0.036664 --> 0.036101).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1149267\n",
      "\tspeed: 0.0394s/iter; left time: 310.6609s\n",
      "\titers: 200, epoch: 7 | loss: 0.1476670\n",
      "\tspeed: 0.0121s/iter; left time: 93.7614s\n",
      "\titers: 300, epoch: 7 | loss: 0.0790868\n",
      "\tspeed: 0.0120s/iter; left time: 92.5366s\n",
      "\titers: 400, epoch: 7 | loss: 0.0798678\n",
      "\tspeed: 0.0120s/iter; left time: 91.1381s\n",
      "\titers: 500, epoch: 7 | loss: 0.1045000\n",
      "\tspeed: 0.0120s/iter; left time: 89.7775s\n",
      "Epoch: 7 cost time: 7.115288734436035\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1021100 Vali Loss: 0.0367107 Test Loss: 0.1336253\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0942366\n",
      "\tspeed: 0.0365s/iter; left time: 266.6786s\n",
      "\titers: 200, epoch: 8 | loss: 0.0988189\n",
      "\tspeed: 0.0105s/iter; left time: 75.7822s\n",
      "\titers: 300, epoch: 8 | loss: 0.1260353\n",
      "\tspeed: 0.0105s/iter; left time: 74.6857s\n",
      "\titers: 400, epoch: 8 | loss: 0.1617311\n",
      "\tspeed: 0.0105s/iter; left time: 73.6057s\n",
      "\titers: 500, epoch: 8 | loss: 0.0782998\n",
      "\tspeed: 0.0105s/iter; left time: 72.4807s\n",
      "Epoch: 8 cost time: 6.265992641448975\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0995351 Vali Loss: 0.0362384 Test Loss: 0.1333975\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1689786\n",
      "\tspeed: 0.0359s/iter; left time: 241.8912s\n",
      "\titers: 200, epoch: 9 | loss: 0.0877496\n",
      "\tspeed: 0.0105s/iter; left time: 69.7528s\n",
      "\titers: 300, epoch: 9 | loss: 0.0617598\n",
      "\tspeed: 0.0105s/iter; left time: 68.7684s\n",
      "\titers: 400, epoch: 9 | loss: 0.0963161\n",
      "\tspeed: 0.0105s/iter; left time: 67.6133s\n",
      "\titers: 500, epoch: 9 | loss: 0.0842207\n",
      "\tspeed: 0.0105s/iter; left time: 66.5166s\n",
      "Epoch: 9 cost time: 6.270616769790649\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0986069 Vali Loss: 0.0366251 Test Loss: 0.1347573\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13512693345546722, mae:0.23280654847621918\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2258161\n",
      "\tspeed: 0.0217s/iter; left time: 245.0681s\n",
      "\titers: 200, epoch: 1 | loss: 0.1745539\n",
      "\tspeed: 0.0103s/iter; left time: 115.1452s\n",
      "\titers: 300, epoch: 1 | loss: 0.2272285\n",
      "\tspeed: 0.0103s/iter; left time: 114.1274s\n",
      "\titers: 400, epoch: 1 | loss: 0.1798825\n",
      "\tspeed: 0.0103s/iter; left time: 112.8720s\n",
      "\titers: 500, epoch: 1 | loss: 0.1475461\n",
      "\tspeed: 0.0103s/iter; left time: 112.0986s\n",
      "Epoch: 1 cost time: 7.050638198852539\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1818829 Vali Loss: 0.0384824 Test Loss: 0.1245723\n",
      "Validation loss decreased (inf --> 0.038482).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1269545\n",
      "\tspeed: 0.0368s/iter; left time: 394.6591s\n",
      "\titers: 200, epoch: 2 | loss: 0.1704410\n",
      "\tspeed: 0.0103s/iter; left time: 108.9933s\n",
      "\titers: 300, epoch: 2 | loss: 0.1202096\n",
      "\tspeed: 0.0103s/iter; left time: 108.3156s\n",
      "\titers: 400, epoch: 2 | loss: 0.1311851\n",
      "\tspeed: 0.0102s/iter; left time: 106.8829s\n",
      "\titers: 500, epoch: 2 | loss: 0.1385412\n",
      "\tspeed: 0.0103s/iter; left time: 105.9551s\n",
      "Epoch: 2 cost time: 6.250487565994263\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1543717 Vali Loss: 0.0365131 Test Loss: 0.1147417\n",
      "Validation loss decreased (0.038482 --> 0.036513).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0716504\n",
      "\tspeed: 0.0363s/iter; left time: 368.6101s\n",
      "\titers: 200, epoch: 3 | loss: 0.1160610\n",
      "\tspeed: 0.0103s/iter; left time: 103.9708s\n",
      "\titers: 300, epoch: 3 | loss: 0.2342663\n",
      "\tspeed: 0.0103s/iter; left time: 102.4367s\n",
      "\titers: 400, epoch: 3 | loss: 0.1401396\n",
      "\tspeed: 0.0103s/iter; left time: 101.5022s\n",
      "\titers: 500, epoch: 3 | loss: 0.0908380\n",
      "\tspeed: 0.0103s/iter; left time: 100.5357s\n",
      "Epoch: 3 cost time: 6.195517301559448\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1285174 Vali Loss: 0.0326620 Test Loss: 0.1128577\n",
      "Validation loss decreased (0.036513 --> 0.032662).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1006823\n",
      "\tspeed: 0.0359s/iter; left time: 344.7395s\n",
      "\titers: 200, epoch: 4 | loss: 0.0886865\n",
      "\tspeed: 0.0103s/iter; left time: 97.9268s\n",
      "\titers: 300, epoch: 4 | loss: 0.0701043\n",
      "\tspeed: 0.0104s/iter; left time: 97.2572s\n",
      "\titers: 400, epoch: 4 | loss: 0.0819835\n",
      "\tspeed: 0.0104s/iter; left time: 96.1739s\n",
      "\titers: 500, epoch: 4 | loss: 0.1388223\n",
      "\tspeed: 0.0103s/iter; left time: 94.7922s\n",
      "Epoch: 4 cost time: 6.181354284286499\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1119747 Vali Loss: 0.0325327 Test Loss: 0.1104302\n",
      "Validation loss decreased (0.032662 --> 0.032533).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0873269\n",
      "\tspeed: 0.0362s/iter; left time: 326.4210s\n",
      "\titers: 200, epoch: 5 | loss: 0.1224051\n",
      "\tspeed: 0.0103s/iter; left time: 91.6257s\n",
      "\titers: 300, epoch: 5 | loss: 0.0985460\n",
      "\tspeed: 0.0102s/iter; left time: 90.3039s\n",
      "\titers: 400, epoch: 5 | loss: 0.1158594\n",
      "\tspeed: 0.0102s/iter; left time: 89.2650s\n",
      "\titers: 500, epoch: 5 | loss: 0.0986964\n",
      "\tspeed: 0.0103s/iter; left time: 88.5719s\n",
      "Epoch: 5 cost time: 6.132014751434326\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1033130 Vali Loss: 0.0320356 Test Loss: 0.1119445\n",
      "Validation loss decreased (0.032533 --> 0.032036).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0776036\n",
      "\tspeed: 0.0367s/iter; left time: 310.4741s\n",
      "\titers: 200, epoch: 6 | loss: 0.1331430\n",
      "\tspeed: 0.0103s/iter; left time: 85.9255s\n",
      "\titers: 300, epoch: 6 | loss: 0.1295920\n",
      "\tspeed: 0.0103s/iter; left time: 85.2736s\n",
      "\titers: 400, epoch: 6 | loss: 0.0783443\n",
      "\tspeed: 0.0103s/iter; left time: 84.1185s\n",
      "\titers: 500, epoch: 6 | loss: 0.0676099\n",
      "\tspeed: 0.0103s/iter; left time: 82.7413s\n",
      "Epoch: 6 cost time: 6.169672966003418\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0982573 Vali Loss: 0.0314279 Test Loss: 0.1120153\n",
      "Validation loss decreased (0.032036 --> 0.031428).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0900342\n",
      "\tspeed: 0.0361s/iter; left time: 284.6052s\n",
      "\titers: 200, epoch: 7 | loss: 0.1217557\n",
      "\tspeed: 0.0103s/iter; left time: 80.1715s\n",
      "\titers: 300, epoch: 7 | loss: 0.1556553\n",
      "\tspeed: 0.0103s/iter; left time: 79.2896s\n",
      "\titers: 400, epoch: 7 | loss: 0.1425365\n",
      "\tspeed: 0.0103s/iter; left time: 78.4230s\n",
      "\titers: 500, epoch: 7 | loss: 0.0993003\n",
      "\tspeed: 0.0103s/iter; left time: 77.1838s\n",
      "Epoch: 7 cost time: 6.191269159317017\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0967970 Vali Loss: 0.0316864 Test Loss: 0.1134103\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0859262\n",
      "\tspeed: 0.0363s/iter; left time: 265.0621s\n",
      "\titers: 200, epoch: 8 | loss: 0.0744396\n",
      "\tspeed: 0.0103s/iter; left time: 74.6170s\n",
      "\titers: 300, epoch: 8 | loss: 0.0877784\n",
      "\tspeed: 0.0104s/iter; left time: 73.7141s\n",
      "\titers: 400, epoch: 8 | loss: 0.0967927\n",
      "\tspeed: 0.0104s/iter; left time: 72.7200s\n",
      "\titers: 500, epoch: 8 | loss: 0.0578659\n",
      "\tspeed: 0.0104s/iter; left time: 71.6744s\n",
      "Epoch: 8 cost time: 6.186060667037964\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0951235 Vali Loss: 0.0315706 Test Loss: 0.1138512\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0745744\n",
      "\tspeed: 0.0355s/iter; left time: 239.5742s\n",
      "\titers: 200, epoch: 9 | loss: 0.1081465\n",
      "\tspeed: 0.0104s/iter; left time: 68.9393s\n",
      "\titers: 300, epoch: 9 | loss: 0.0850372\n",
      "\tspeed: 0.0103s/iter; left time: 67.5631s\n",
      "\titers: 400, epoch: 9 | loss: 0.1032244\n",
      "\tspeed: 0.0103s/iter; left time: 66.3980s\n",
      "\titers: 500, epoch: 9 | loss: 0.0823691\n",
      "\tspeed: 0.0103s/iter; left time: 65.5128s\n",
      "Epoch: 9 cost time: 6.150758504867554\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0938647 Vali Loss: 0.0316149 Test Loss: 0.1139777\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11217200011014938, mae:0.20515543222427368\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2126052\n",
      "\tspeed: 0.0222s/iter; left time: 250.6611s\n",
      "\titers: 200, epoch: 1 | loss: 0.1374715\n",
      "\tspeed: 0.0104s/iter; left time: 116.0345s\n",
      "\titers: 300, epoch: 1 | loss: 0.1647153\n",
      "\tspeed: 0.0104s/iter; left time: 115.2550s\n",
      "\titers: 400, epoch: 1 | loss: 0.1097612\n",
      "\tspeed: 0.0104s/iter; left time: 113.9420s\n",
      "\titers: 500, epoch: 1 | loss: 0.2070301\n",
      "\tspeed: 0.0104s/iter; left time: 113.0576s\n",
      "Epoch: 1 cost time: 7.149724006652832\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2159055 Vali Loss: 0.0433398 Test Loss: 0.1446968\n",
      "Validation loss decreased (inf --> 0.043340).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1930389\n",
      "\tspeed: 0.0377s/iter; left time: 404.1384s\n",
      "\titers: 200, epoch: 2 | loss: 0.2482810\n",
      "\tspeed: 0.0103s/iter; left time: 109.7552s\n",
      "\titers: 300, epoch: 2 | loss: 0.1856881\n",
      "\tspeed: 0.0103s/iter; left time: 108.8988s\n",
      "\titers: 400, epoch: 2 | loss: 0.2534928\n",
      "\tspeed: 0.0103s/iter; left time: 107.6128s\n",
      "\titers: 500, epoch: 2 | loss: 0.1951734\n",
      "\tspeed: 0.0103s/iter; left time: 106.6489s\n",
      "Epoch: 2 cost time: 6.174974679946899\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1715210 Vali Loss: 0.0442632 Test Loss: 0.1414142\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0967074\n",
      "\tspeed: 0.0372s/iter; left time: 378.1847s\n",
      "\titers: 200, epoch: 3 | loss: 0.1278625\n",
      "\tspeed: 0.0104s/iter; left time: 104.6664s\n",
      "\titers: 300, epoch: 3 | loss: 0.1800192\n",
      "\tspeed: 0.0104s/iter; left time: 103.9152s\n",
      "\titers: 400, epoch: 3 | loss: 0.1153174\n",
      "\tspeed: 0.0104s/iter; left time: 102.8561s\n",
      "\titers: 500, epoch: 3 | loss: 0.0864907\n",
      "\tspeed: 0.0104s/iter; left time: 101.3042s\n",
      "Epoch: 3 cost time: 6.203191518783569\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1365773 Vali Loss: 0.0402464 Test Loss: 0.1316162\n",
      "Validation loss decreased (0.043340 --> 0.040246).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0689162\n",
      "\tspeed: 0.0357s/iter; left time: 342.7688s\n",
      "\titers: 200, epoch: 4 | loss: 0.0810238\n",
      "\tspeed: 0.0104s/iter; left time: 98.9623s\n",
      "\titers: 300, epoch: 4 | loss: 0.0791520\n",
      "\tspeed: 0.0104s/iter; left time: 97.8095s\n",
      "\titers: 400, epoch: 4 | loss: 0.1398570\n",
      "\tspeed: 0.0104s/iter; left time: 96.7048s\n",
      "\titers: 500, epoch: 4 | loss: 0.0956015\n",
      "\tspeed: 0.0104s/iter; left time: 96.0391s\n",
      "Epoch: 4 cost time: 6.252525329589844\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1136333 Vali Loss: 0.0403347 Test Loss: 0.1356540\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1071966\n",
      "\tspeed: 0.0365s/iter; left time: 329.0613s\n",
      "\titers: 200, epoch: 5 | loss: 0.0704997\n",
      "\tspeed: 0.0104s/iter; left time: 92.9254s\n",
      "\titers: 300, epoch: 5 | loss: 0.0892456\n",
      "\tspeed: 0.0104s/iter; left time: 91.7665s\n",
      "\titers: 400, epoch: 5 | loss: 0.1097680\n",
      "\tspeed: 0.0104s/iter; left time: 90.6897s\n",
      "\titers: 500, epoch: 5 | loss: 0.1410007\n",
      "\tspeed: 0.0104s/iter; left time: 89.4025s\n",
      "Epoch: 5 cost time: 6.179366588592529\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1049160 Vali Loss: 0.0393753 Test Loss: 0.1345298\n",
      "Validation loss decreased (0.040246 --> 0.039375).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0837624\n",
      "\tspeed: 0.0361s/iter; left time: 305.4277s\n",
      "\titers: 200, epoch: 6 | loss: 0.0961889\n",
      "\tspeed: 0.0103s/iter; left time: 86.2678s\n",
      "\titers: 300, epoch: 6 | loss: 0.1093323\n",
      "\tspeed: 0.0103s/iter; left time: 84.9472s\n",
      "\titers: 400, epoch: 6 | loss: 0.0809935\n",
      "\tspeed: 0.0103s/iter; left time: 84.1944s\n",
      "\titers: 500, epoch: 6 | loss: 0.1131487\n",
      "\tspeed: 0.0103s/iter; left time: 83.1896s\n",
      "Epoch: 6 cost time: 6.181368827819824\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0989067 Vali Loss: 0.0389056 Test Loss: 0.1327591\n",
      "Validation loss decreased (0.039375 --> 0.038906).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0908248\n",
      "\tspeed: 0.0382s/iter; left time: 301.3770s\n",
      "\titers: 200, epoch: 7 | loss: 0.0762231\n",
      "\tspeed: 0.0103s/iter; left time: 80.2606s\n",
      "\titers: 300, epoch: 7 | loss: 0.0909373\n",
      "\tspeed: 0.0104s/iter; left time: 79.5695s\n",
      "\titers: 400, epoch: 7 | loss: 0.0681608\n",
      "\tspeed: 0.0103s/iter; left time: 78.4035s\n",
      "\titers: 500, epoch: 7 | loss: 0.0986060\n",
      "\tspeed: 0.0103s/iter; left time: 77.1839s\n",
      "Epoch: 7 cost time: 6.180460453033447\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0962614 Vali Loss: 0.0390874 Test Loss: 0.1329040\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0775798\n",
      "\tspeed: 0.0367s/iter; left time: 268.1453s\n",
      "\titers: 200, epoch: 8 | loss: 0.1064886\n",
      "\tspeed: 0.0104s/iter; left time: 74.6881s\n",
      "\titers: 300, epoch: 8 | loss: 0.0902569\n",
      "\tspeed: 0.0103s/iter; left time: 73.5335s\n",
      "\titers: 400, epoch: 8 | loss: 0.0937475\n",
      "\tspeed: 0.0103s/iter; left time: 72.5195s\n",
      "\titers: 500, epoch: 8 | loss: 0.0717437\n",
      "\tspeed: 0.0103s/iter; left time: 71.4916s\n",
      "Epoch: 8 cost time: 6.224544286727905\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0940405 Vali Loss: 0.0391326 Test Loss: 0.1337127\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0720170\n",
      "\tspeed: 0.0353s/iter; left time: 237.7538s\n",
      "\titers: 200, epoch: 9 | loss: 0.1180055\n",
      "\tspeed: 0.0102s/iter; left time: 68.0475s\n",
      "\titers: 300, epoch: 9 | loss: 0.0763217\n",
      "\tspeed: 0.0102s/iter; left time: 66.9755s\n",
      "\titers: 400, epoch: 9 | loss: 0.0791094\n",
      "\tspeed: 0.0102s/iter; left time: 65.9521s\n",
      "\titers: 500, epoch: 9 | loss: 0.0849110\n",
      "\tspeed: 0.0102s/iter; left time: 64.9089s\n",
      "Epoch: 9 cost time: 6.103687524795532\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0928958 Vali Loss: 0.0394123 Test Loss: 0.1342105\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13295453786849976, mae:0.23362568020820618\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2748502\n",
      "\tspeed: 0.0235s/iter; left time: 265.7195s\n",
      "\titers: 200, epoch: 1 | loss: 0.2084380\n",
      "\tspeed: 0.0118s/iter; left time: 132.4330s\n",
      "\titers: 300, epoch: 1 | loss: 0.2147193\n",
      "\tspeed: 0.0118s/iter; left time: 131.1377s\n",
      "\titers: 400, epoch: 1 | loss: 0.1102636\n",
      "\tspeed: 0.0118s/iter; left time: 130.3029s\n",
      "\titers: 500, epoch: 1 | loss: 0.2449139\n",
      "\tspeed: 0.0119s/iter; left time: 129.1860s\n",
      "Epoch: 1 cost time: 7.964558362960815\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2353376 Vali Loss: 0.0468720 Test Loss: 0.1393561\n",
      "Validation loss decreased (inf --> 0.046872).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1956535\n",
      "\tspeed: 0.0409s/iter; left time: 439.3856s\n",
      "\titers: 200, epoch: 2 | loss: 0.1641064\n",
      "\tspeed: 0.0119s/iter; left time: 126.6514s\n",
      "\titers: 300, epoch: 2 | loss: 0.1314635\n",
      "\tspeed: 0.0119s/iter; left time: 125.7177s\n",
      "\titers: 400, epoch: 2 | loss: 0.1056886\n",
      "\tspeed: 0.0119s/iter; left time: 124.1607s\n",
      "\titers: 500, epoch: 2 | loss: 0.1730652\n",
      "\tspeed: 0.0119s/iter; left time: 122.9280s\n",
      "Epoch: 2 cost time: 7.1383562088012695\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1995923 Vali Loss: 0.0450163 Test Loss: 0.1359542\n",
      "Validation loss decreased (0.046872 --> 0.045016).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1934972\n",
      "\tspeed: 0.0407s/iter; left time: 413.5040s\n",
      "\titers: 200, epoch: 3 | loss: 0.2169630\n",
      "\tspeed: 0.0118s/iter; left time: 119.0773s\n",
      "\titers: 300, epoch: 3 | loss: 0.1213193\n",
      "\tspeed: 0.0118s/iter; left time: 118.0213s\n",
      "\titers: 400, epoch: 3 | loss: 0.1498200\n",
      "\tspeed: 0.0119s/iter; left time: 116.8669s\n",
      "\titers: 500, epoch: 3 | loss: 0.1662777\n",
      "\tspeed: 0.0118s/iter; left time: 115.6200s\n",
      "Epoch: 3 cost time: 7.070978403091431\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1741398 Vali Loss: 0.0422012 Test Loss: 0.1378659\n",
      "Validation loss decreased (0.045016 --> 0.042201).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1304682\n",
      "\tspeed: 0.0395s/iter; left time: 378.6890s\n",
      "\titers: 200, epoch: 4 | loss: 0.1854112\n",
      "\tspeed: 0.0118s/iter; left time: 112.3851s\n",
      "\titers: 300, epoch: 4 | loss: 0.1072130\n",
      "\tspeed: 0.0118s/iter; left time: 110.9767s\n",
      "\titers: 400, epoch: 4 | loss: 0.0934103\n",
      "\tspeed: 0.0118s/iter; left time: 109.8028s\n",
      "\titers: 500, epoch: 4 | loss: 0.1440407\n",
      "\tspeed: 0.0118s/iter; left time: 108.5896s\n",
      "Epoch: 4 cost time: 7.035829544067383\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1554324 Vali Loss: 0.0426190 Test Loss: 0.1270937\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1229717\n",
      "\tspeed: 0.0395s/iter; left time: 355.9402s\n",
      "\titers: 200, epoch: 5 | loss: 0.1247959\n",
      "\tspeed: 0.0119s/iter; left time: 105.9007s\n",
      "\titers: 300, epoch: 5 | loss: 0.1960773\n",
      "\tspeed: 0.0119s/iter; left time: 104.7369s\n",
      "\titers: 400, epoch: 5 | loss: 0.1387040\n",
      "\tspeed: 0.0119s/iter; left time: 103.5477s\n",
      "\titers: 500, epoch: 5 | loss: 0.1279780\n",
      "\tspeed: 0.0119s/iter; left time: 102.3289s\n",
      "Epoch: 5 cost time: 7.060994625091553\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1429132 Vali Loss: 0.0403843 Test Loss: 0.1261380\n",
      "Validation loss decreased (0.042201 --> 0.040384).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1534443\n",
      "\tspeed: 0.0402s/iter; left time: 339.7507s\n",
      "\titers: 200, epoch: 6 | loss: 0.0734791\n",
      "\tspeed: 0.0118s/iter; left time: 98.8749s\n",
      "\titers: 300, epoch: 6 | loss: 0.1539678\n",
      "\tspeed: 0.0118s/iter; left time: 97.6618s\n",
      "\titers: 400, epoch: 6 | loss: 0.1341914\n",
      "\tspeed: 0.0118s/iter; left time: 96.4895s\n",
      "\titers: 500, epoch: 6 | loss: 0.1050217\n",
      "\tspeed: 0.0118s/iter; left time: 95.2850s\n",
      "Epoch: 6 cost time: 7.065880298614502\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1340782 Vali Loss: 0.0394809 Test Loss: 0.1283349\n",
      "Validation loss decreased (0.040384 --> 0.039481).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1350735\n",
      "\tspeed: 0.0390s/iter; left time: 307.5795s\n",
      "\titers: 200, epoch: 7 | loss: 0.1328423\n",
      "\tspeed: 0.0119s/iter; left time: 92.3058s\n",
      "\titers: 300, epoch: 7 | loss: 0.1311206\n",
      "\tspeed: 0.0119s/iter; left time: 91.1159s\n",
      "\titers: 400, epoch: 7 | loss: 0.0903304\n",
      "\tspeed: 0.0118s/iter; left time: 89.8167s\n",
      "\titers: 500, epoch: 7 | loss: 0.1177615\n",
      "\tspeed: 0.0119s/iter; left time: 88.6957s\n",
      "Epoch: 7 cost time: 7.051419973373413\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1292391 Vali Loss: 0.0397825 Test Loss: 0.1285097\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0874919\n",
      "\tspeed: 0.0379s/iter; left time: 277.4223s\n",
      "\titers: 200, epoch: 8 | loss: 0.1236357\n",
      "\tspeed: 0.0119s/iter; left time: 85.8632s\n",
      "\titers: 300, epoch: 8 | loss: 0.1306811\n",
      "\tspeed: 0.0119s/iter; left time: 84.6447s\n",
      "\titers: 400, epoch: 8 | loss: 0.1075007\n",
      "\tspeed: 0.0119s/iter; left time: 83.4889s\n",
      "\titers: 500, epoch: 8 | loss: 0.1073734\n",
      "\tspeed: 0.0119s/iter; left time: 82.2770s\n",
      "Epoch: 8 cost time: 7.074612855911255\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1282189 Vali Loss: 0.0393154 Test Loss: 0.1281180\n",
      "Validation loss decreased (0.039481 --> 0.039315).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1067264\n",
      "\tspeed: 0.0404s/iter; left time: 272.3901s\n",
      "\titers: 200, epoch: 9 | loss: 0.1032476\n",
      "\tspeed: 0.0118s/iter; left time: 78.6119s\n",
      "\titers: 300, epoch: 9 | loss: 0.1405332\n",
      "\tspeed: 0.0118s/iter; left time: 77.4064s\n",
      "\titers: 400, epoch: 9 | loss: 0.0929061\n",
      "\tspeed: 0.0118s/iter; left time: 76.2429s\n",
      "\titers: 500, epoch: 9 | loss: 0.1268479\n",
      "\tspeed: 0.0118s/iter; left time: 75.0450s\n",
      "Epoch: 9 cost time: 7.064804315567017\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1259667 Vali Loss: 0.0393005 Test Loss: 0.1285784\n",
      "Validation loss decreased (0.039315 --> 0.039301).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1167146\n",
      "\tspeed: 0.0396s/iter; left time: 244.4144s\n",
      "\titers: 200, epoch: 10 | loss: 0.1376054\n",
      "\tspeed: 0.0119s/iter; left time: 72.2437s\n",
      "\titers: 300, epoch: 10 | loss: 0.1336108\n",
      "\tspeed: 0.0119s/iter; left time: 71.0661s\n",
      "\titers: 400, epoch: 10 | loss: 0.1067017\n",
      "\tspeed: 0.0119s/iter; left time: 69.8734s\n",
      "\titers: 500, epoch: 10 | loss: 0.1816334\n",
      "\tspeed: 0.0119s/iter; left time: 68.7409s\n",
      "Epoch: 10 cost time: 7.0794453620910645\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1260375 Vali Loss: 0.0393978 Test Loss: 0.1289789\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1116521\n",
      "\tspeed: 0.0391s/iter; left time: 219.2319s\n",
      "\titers: 200, epoch: 11 | loss: 0.1775654\n",
      "\tspeed: 0.0119s/iter; left time: 65.2114s\n",
      "\titers: 300, epoch: 11 | loss: 0.1322004\n",
      "\tspeed: 0.0118s/iter; left time: 63.9316s\n",
      "\titers: 400, epoch: 11 | loss: 0.1197584\n",
      "\tspeed: 0.0118s/iter; left time: 62.8013s\n",
      "\titers: 500, epoch: 11 | loss: 0.1856377\n",
      "\tspeed: 0.0119s/iter; left time: 61.6501s\n",
      "Epoch: 11 cost time: 7.029090642929077\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1244359 Vali Loss: 0.0394877 Test Loss: 0.1293370\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.0919617\n",
      "\tspeed: 0.0389s/iter; left time: 195.7739s\n",
      "\titers: 200, epoch: 12 | loss: 0.0905452\n",
      "\tspeed: 0.0119s/iter; left time: 58.5223s\n",
      "\titers: 300, epoch: 12 | loss: 0.1048527\n",
      "\tspeed: 0.0119s/iter; left time: 57.4372s\n",
      "\titers: 400, epoch: 12 | loss: 0.1121643\n",
      "\tspeed: 0.0119s/iter; left time: 56.2277s\n",
      "\titers: 500, epoch: 12 | loss: 0.1147581\n",
      "\tspeed: 0.0119s/iter; left time: 55.0299s\n",
      "Epoch: 12 cost time: 7.065691232681274\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1251730 Vali Loss: 0.0395623 Test Loss: 0.1292676\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12871778011322021, mae:0.22809365391731262\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1300711\n",
      "\tspeed: 0.0233s/iter; left time: 263.7516s\n",
      "\titers: 200, epoch: 1 | loss: 0.3094113\n",
      "\tspeed: 0.0115s/iter; left time: 128.4812s\n",
      "\titers: 300, epoch: 1 | loss: 0.1772321\n",
      "\tspeed: 0.0115s/iter; left time: 127.4936s\n",
      "\titers: 400, epoch: 1 | loss: 0.1927970\n",
      "\tspeed: 0.0115s/iter; left time: 126.1468s\n",
      "\titers: 500, epoch: 1 | loss: 0.1227823\n",
      "\tspeed: 0.0115s/iter; left time: 125.0119s\n",
      "Epoch: 1 cost time: 7.766777753829956\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1955146 Vali Loss: 0.0420059 Test Loss: 0.1293657\n",
      "Validation loss decreased (inf --> 0.042006).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1480882\n",
      "\tspeed: 0.0389s/iter; left time: 416.9936s\n",
      "\titers: 200, epoch: 2 | loss: 0.1168805\n",
      "\tspeed: 0.0115s/iter; left time: 122.5068s\n",
      "\titers: 300, epoch: 2 | loss: 0.1539083\n",
      "\tspeed: 0.0115s/iter; left time: 121.1540s\n",
      "\titers: 400, epoch: 2 | loss: 0.2182992\n",
      "\tspeed: 0.0115s/iter; left time: 120.1261s\n",
      "\titers: 500, epoch: 2 | loss: 0.6056554\n",
      "\tspeed: 0.0115s/iter; left time: 118.8211s\n",
      "Epoch: 2 cost time: 6.878265857696533\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1892001 Vali Loss: 0.0559730 Test Loss: 0.1621517\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2122157\n",
      "\tspeed: 0.0384s/iter; left time: 390.6663s\n",
      "\titers: 200, epoch: 3 | loss: 0.0970625\n",
      "\tspeed: 0.0114s/iter; left time: 114.5622s\n",
      "\titers: 300, epoch: 3 | loss: 0.1917423\n",
      "\tspeed: 0.0114s/iter; left time: 113.5637s\n",
      "\titers: 400, epoch: 3 | loss: 0.1587247\n",
      "\tspeed: 0.0114s/iter; left time: 112.5778s\n",
      "\titers: 500, epoch: 3 | loss: 0.2420566\n",
      "\tspeed: 0.0114s/iter; left time: 111.2139s\n",
      "Epoch: 3 cost time: 6.811446666717529\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1813456 Vali Loss: 0.0372267 Test Loss: 0.1250522\n",
      "Validation loss decreased (0.042006 --> 0.037227).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1855680\n",
      "\tspeed: 0.0387s/iter; left time: 371.2121s\n",
      "\titers: 200, epoch: 4 | loss: 0.0917098\n",
      "\tspeed: 0.0115s/iter; left time: 108.8252s\n",
      "\titers: 300, epoch: 4 | loss: 0.0880305\n",
      "\tspeed: 0.0115s/iter; left time: 107.9340s\n",
      "\titers: 400, epoch: 4 | loss: 0.1410631\n",
      "\tspeed: 0.0115s/iter; left time: 106.6410s\n",
      "\titers: 500, epoch: 4 | loss: 0.1505861\n",
      "\tspeed: 0.0115s/iter; left time: 105.7133s\n",
      "Epoch: 4 cost time: 6.865068674087524\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1574655 Vali Loss: 0.0376252 Test Loss: 0.1238940\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1433762\n",
      "\tspeed: 0.0376s/iter; left time: 339.1052s\n",
      "\titers: 200, epoch: 5 | loss: 0.1362560\n",
      "\tspeed: 0.0114s/iter; left time: 101.6624s\n",
      "\titers: 300, epoch: 5 | loss: 0.2190448\n",
      "\tspeed: 0.0114s/iter; left time: 100.6620s\n",
      "\titers: 400, epoch: 5 | loss: 0.1679020\n",
      "\tspeed: 0.0114s/iter; left time: 99.3978s\n",
      "\titers: 500, epoch: 5 | loss: 0.1148061\n",
      "\tspeed: 0.0114s/iter; left time: 98.4048s\n",
      "Epoch: 5 cost time: 6.768932819366455\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1475436 Vali Loss: 0.0346353 Test Loss: 0.1169233\n",
      "Validation loss decreased (0.037227 --> 0.034635).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1732468\n",
      "\tspeed: 0.0405s/iter; left time: 342.6527s\n",
      "\titers: 200, epoch: 6 | loss: 0.1484320\n",
      "\tspeed: 0.0125s/iter; left time: 104.6449s\n",
      "\titers: 300, epoch: 6 | loss: 0.0845025\n",
      "\tspeed: 0.0125s/iter; left time: 103.3248s\n",
      "\titers: 400, epoch: 6 | loss: 0.0988227\n",
      "\tspeed: 0.0125s/iter; left time: 102.1495s\n",
      "\titers: 500, epoch: 6 | loss: 0.1221070\n",
      "\tspeed: 0.0125s/iter; left time: 100.8603s\n",
      "Epoch: 6 cost time: 7.463322401046753\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1396978 Vali Loss: 0.0338739 Test Loss: 0.1158155\n",
      "Validation loss decreased (0.034635 --> 0.033874).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1004824\n",
      "\tspeed: 0.0395s/iter; left time: 311.3446s\n",
      "\titers: 200, epoch: 7 | loss: 0.2261177\n",
      "\tspeed: 0.0115s/iter; left time: 89.5220s\n",
      "\titers: 300, epoch: 7 | loss: 0.1112074\n",
      "\tspeed: 0.0115s/iter; left time: 88.3918s\n",
      "\titers: 400, epoch: 7 | loss: 0.1229349\n",
      "\tspeed: 0.0115s/iter; left time: 87.2351s\n",
      "\titers: 500, epoch: 7 | loss: 0.1122292\n",
      "\tspeed: 0.0115s/iter; left time: 86.0742s\n",
      "Epoch: 7 cost time: 6.886261701583862\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1377403 Vali Loss: 0.0337382 Test Loss: 0.1156678\n",
      "Validation loss decreased (0.033874 --> 0.033738).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1037824\n",
      "\tspeed: 0.0397s/iter; left time: 290.5652s\n",
      "\titers: 200, epoch: 8 | loss: 0.1524660\n",
      "\tspeed: 0.0122s/iter; left time: 88.0027s\n",
      "\titers: 300, epoch: 8 | loss: 0.1183857\n",
      "\tspeed: 0.0116s/iter; left time: 82.2604s\n",
      "\titers: 400, epoch: 8 | loss: 0.1234620\n",
      "\tspeed: 0.0115s/iter; left time: 80.5012s\n",
      "\titers: 500, epoch: 8 | loss: 0.1097233\n",
      "\tspeed: 0.0115s/iter; left time: 79.3966s\n",
      "Epoch: 8 cost time: 6.972746849060059\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1357162 Vali Loss: 0.0336374 Test Loss: 0.1146246\n",
      "Validation loss decreased (0.033738 --> 0.033637).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1797483\n",
      "\tspeed: 0.0382s/iter; left time: 257.6013s\n",
      "\titers: 200, epoch: 9 | loss: 0.0966282\n",
      "\tspeed: 0.0115s/iter; left time: 76.0498s\n",
      "\titers: 300, epoch: 9 | loss: 0.1592433\n",
      "\tspeed: 0.0115s/iter; left time: 74.9188s\n",
      "\titers: 400, epoch: 9 | loss: 0.1242249\n",
      "\tspeed: 0.0115s/iter; left time: 73.7696s\n",
      "\titers: 500, epoch: 9 | loss: 0.2368679\n",
      "\tspeed: 0.0115s/iter; left time: 72.6109s\n",
      "Epoch: 9 cost time: 6.836431264877319\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1345700 Vali Loss: 0.0333215 Test Loss: 0.1144174\n",
      "Validation loss decreased (0.033637 --> 0.033322).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1224813\n",
      "\tspeed: 0.0389s/iter; left time: 240.2340s\n",
      "\titers: 200, epoch: 10 | loss: 0.0959411\n",
      "\tspeed: 0.0114s/iter; left time: 69.3292s\n",
      "\titers: 300, epoch: 10 | loss: 0.1260364\n",
      "\tspeed: 0.0114s/iter; left time: 68.1881s\n",
      "\titers: 400, epoch: 10 | loss: 0.1690525\n",
      "\tspeed: 0.0114s/iter; left time: 67.0398s\n",
      "\titers: 500, epoch: 10 | loss: 0.1556181\n",
      "\tspeed: 0.0114s/iter; left time: 65.9190s\n",
      "Epoch: 10 cost time: 6.844830751419067\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1346377 Vali Loss: 0.0333686 Test Loss: 0.1149346\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1144761\n",
      "\tspeed: 0.0376s/iter; left time: 210.8094s\n",
      "\titers: 200, epoch: 11 | loss: 0.1198494\n",
      "\tspeed: 0.0114s/iter; left time: 62.8194s\n",
      "\titers: 300, epoch: 11 | loss: 0.1082775\n",
      "\tspeed: 0.0114s/iter; left time: 61.6691s\n",
      "\titers: 400, epoch: 11 | loss: 0.1738293\n",
      "\tspeed: 0.0114s/iter; left time: 60.6711s\n",
      "\titers: 500, epoch: 11 | loss: 0.1408115\n",
      "\tspeed: 0.0114s/iter; left time: 59.4628s\n",
      "Epoch: 11 cost time: 6.780963897705078\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1339782 Vali Loss: 0.0333226 Test Loss: 0.1150690\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1342557\n",
      "\tspeed: 0.0381s/iter; left time: 191.6814s\n",
      "\titers: 200, epoch: 12 | loss: 0.1216956\n",
      "\tspeed: 0.0115s/iter; left time: 56.6495s\n",
      "\titers: 300, epoch: 12 | loss: 0.1211447\n",
      "\tspeed: 0.0115s/iter; left time: 55.6580s\n",
      "\titers: 400, epoch: 12 | loss: 0.1809108\n",
      "\tspeed: 0.0115s/iter; left time: 54.2437s\n",
      "\titers: 500, epoch: 12 | loss: 0.1421872\n",
      "\tspeed: 0.0115s/iter; left time: 53.1163s\n",
      "Epoch: 12 cost time: 6.844978332519531\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1330528 Vali Loss: 0.0335861 Test Loss: 0.1150666\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11457356065511703, mae:0.2082524299621582\n",
      ">>> LR=5e-4,DO=0.1,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2563054\n",
      "\tspeed: 0.0226s/iter; left time: 255.3514s\n",
      "\titers: 200, epoch: 1 | loss: 0.3060882\n",
      "\tspeed: 0.0108s/iter; left time: 120.8458s\n",
      "\titers: 300, epoch: 1 | loss: 0.2427146\n",
      "\tspeed: 0.0108s/iter; left time: 119.4700s\n",
      "\titers: 400, epoch: 1 | loss: 0.1704900\n",
      "\tspeed: 0.0108s/iter; left time: 118.2840s\n",
      "\titers: 500, epoch: 1 | loss: 0.1884707\n",
      "\tspeed: 0.0108s/iter; left time: 117.4456s\n",
      "Epoch: 1 cost time: 7.3687968254089355\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2254649 Vali Loss: 0.0460251 Test Loss: 0.1410485\n",
      "Validation loss decreased (inf --> 0.046025).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2278082\n",
      "\tspeed: 0.0372s/iter; left time: 399.4518s\n",
      "\titers: 200, epoch: 2 | loss: 0.2863010\n",
      "\tspeed: 0.0109s/iter; left time: 115.5182s\n",
      "\titers: 300, epoch: 2 | loss: 0.1574631\n",
      "\tspeed: 0.0109s/iter; left time: 114.6582s\n",
      "\titers: 400, epoch: 2 | loss: 0.1781293\n",
      "\tspeed: 0.0109s/iter; left time: 113.4891s\n",
      "\titers: 500, epoch: 2 | loss: 0.3219774\n",
      "\tspeed: 0.0109s/iter; left time: 112.5544s\n",
      "Epoch: 2 cost time: 6.509297132492065\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2344164 Vali Loss: 0.0522266 Test Loss: 0.1590629\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1617962\n",
      "\tspeed: 0.0378s/iter; left time: 384.2378s\n",
      "\titers: 200, epoch: 3 | loss: 0.1521518\n",
      "\tspeed: 0.0120s/iter; left time: 121.1606s\n",
      "\titers: 300, epoch: 3 | loss: 0.2234536\n",
      "\tspeed: 0.0121s/iter; left time: 120.1279s\n",
      "\titers: 400, epoch: 3 | loss: 0.1817826\n",
      "\tspeed: 0.0120s/iter; left time: 118.3029s\n",
      "\titers: 500, epoch: 3 | loss: 0.2091747\n",
      "\tspeed: 0.0120s/iter; left time: 117.5001s\n",
      "Epoch: 3 cost time: 7.110620737075806\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2069548 Vali Loss: 0.0490118 Test Loss: 0.1398464\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2131798\n",
      "\tspeed: 0.0368s/iter; left time: 352.4722s\n",
      "\titers: 200, epoch: 4 | loss: 0.1151125\n",
      "\tspeed: 0.0108s/iter; left time: 102.7735s\n",
      "\titers: 300, epoch: 4 | loss: 0.1875534\n",
      "\tspeed: 0.0108s/iter; left time: 101.6030s\n",
      "\titers: 400, epoch: 4 | loss: 0.2876613\n",
      "\tspeed: 0.0108s/iter; left time: 100.4944s\n",
      "\titers: 500, epoch: 4 | loss: 0.1665280\n",
      "\tspeed: 0.0108s/iter; left time: 99.6420s\n",
      "Epoch: 4 cost time: 6.4387311935424805\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1805104 Vali Loss: 0.0436446 Test Loss: 0.1359658\n",
      "Validation loss decreased (0.046025 --> 0.043645).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1752025\n",
      "\tspeed: 0.0403s/iter; left time: 363.6989s\n",
      "\titers: 200, epoch: 5 | loss: 0.1700835\n",
      "\tspeed: 0.0118s/iter; left time: 105.1755s\n",
      "\titers: 300, epoch: 5 | loss: 0.1537457\n",
      "\tspeed: 0.0108s/iter; left time: 95.4229s\n",
      "\titers: 400, epoch: 5 | loss: 0.1103722\n",
      "\tspeed: 0.0108s/iter; left time: 94.3092s\n",
      "\titers: 500, epoch: 5 | loss: 0.2354374\n",
      "\tspeed: 0.0108s/iter; left time: 93.4912s\n",
      "Epoch: 5 cost time: 6.709837198257446\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1616194 Vali Loss: 0.0424842 Test Loss: 0.1335908\n",
      "Validation loss decreased (0.043645 --> 0.042484).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1213849\n",
      "\tspeed: 0.0390s/iter; left time: 329.2897s\n",
      "\titers: 200, epoch: 6 | loss: 0.1398307\n",
      "\tspeed: 0.0109s/iter; left time: 90.6425s\n",
      "\titers: 300, epoch: 6 | loss: 0.1669592\n",
      "\tspeed: 0.0108s/iter; left time: 89.0590s\n",
      "\titers: 400, epoch: 6 | loss: 0.1325021\n",
      "\tspeed: 0.0108s/iter; left time: 88.2550s\n",
      "\titers: 500, epoch: 6 | loss: 0.1601461\n",
      "\tspeed: 0.0108s/iter; left time: 87.2531s\n",
      "Epoch: 6 cost time: 6.489863395690918\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1514458 Vali Loss: 0.0423038 Test Loss: 0.1384318\n",
      "Validation loss decreased (0.042484 --> 0.042304).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1293175\n",
      "\tspeed: 0.0388s/iter; left time: 306.0930s\n",
      "\titers: 200, epoch: 7 | loss: 0.1011876\n",
      "\tspeed: 0.0120s/iter; left time: 92.9888s\n",
      "\titers: 300, epoch: 7 | loss: 0.0974072\n",
      "\tspeed: 0.0109s/iter; left time: 83.7856s\n",
      "\titers: 400, epoch: 7 | loss: 0.1650061\n",
      "\tspeed: 0.0108s/iter; left time: 82.1222s\n",
      "\titers: 500, epoch: 7 | loss: 0.1089765\n",
      "\tspeed: 0.0108s/iter; left time: 81.0422s\n",
      "Epoch: 7 cost time: 6.706374883651733\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1461564 Vali Loss: 0.0408922 Test Loss: 0.1339641\n",
      "Validation loss decreased (0.042304 --> 0.040892).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1181434\n",
      "\tspeed: 0.0380s/iter; left time: 277.8933s\n",
      "\titers: 200, epoch: 8 | loss: 0.1858453\n",
      "\tspeed: 0.0121s/iter; left time: 86.9867s\n",
      "\titers: 300, epoch: 8 | loss: 0.1627850\n",
      "\tspeed: 0.0120s/iter; left time: 85.1612s\n",
      "\titers: 400, epoch: 8 | loss: 0.1537389\n",
      "\tspeed: 0.0120s/iter; left time: 84.2176s\n",
      "\titers: 500, epoch: 8 | loss: 0.1267983\n",
      "\tspeed: 0.0120s/iter; left time: 83.0692s\n",
      "Epoch: 8 cost time: 7.147031545639038\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1429851 Vali Loss: 0.0401977 Test Loss: 0.1338758\n",
      "Validation loss decreased (0.040892 --> 0.040198).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1155324\n",
      "\tspeed: 0.0391s/iter; left time: 263.7250s\n",
      "\titers: 200, epoch: 9 | loss: 0.1059210\n",
      "\tspeed: 0.0108s/iter; left time: 71.6843s\n",
      "\titers: 300, epoch: 9 | loss: 0.1898012\n",
      "\tspeed: 0.0108s/iter; left time: 70.7281s\n",
      "\titers: 400, epoch: 9 | loss: 0.1801835\n",
      "\tspeed: 0.0108s/iter; left time: 69.4991s\n",
      "\titers: 500, epoch: 9 | loss: 0.0898061\n",
      "\tspeed: 0.0108s/iter; left time: 68.3072s\n",
      "Epoch: 9 cost time: 6.453861951828003\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1410465 Vali Loss: 0.0404282 Test Loss: 0.1336343\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1446062\n",
      "\tspeed: 0.0369s/iter; left time: 227.4205s\n",
      "\titers: 200, epoch: 10 | loss: 0.0783195\n",
      "\tspeed: 0.0109s/iter; left time: 66.1783s\n",
      "\titers: 300, epoch: 10 | loss: 0.1531917\n",
      "\tspeed: 0.0109s/iter; left time: 64.9092s\n",
      "\titers: 400, epoch: 10 | loss: 0.1077531\n",
      "\tspeed: 0.0108s/iter; left time: 63.6635s\n",
      "\titers: 500, epoch: 10 | loss: 0.1898513\n",
      "\tspeed: 0.0109s/iter; left time: 62.7093s\n",
      "Epoch: 10 cost time: 6.458868980407715\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1413932 Vali Loss: 0.0402531 Test Loss: 0.1331305\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0896061\n",
      "\tspeed: 0.0364s/iter; left time: 203.7285s\n",
      "\titers: 200, epoch: 11 | loss: 0.1391569\n",
      "\tspeed: 0.0108s/iter; left time: 59.5048s\n",
      "\titers: 300, epoch: 11 | loss: 0.1467878\n",
      "\tspeed: 0.0108s/iter; left time: 58.4358s\n",
      "\titers: 400, epoch: 11 | loss: 0.1015909\n",
      "\tspeed: 0.0108s/iter; left time: 57.3249s\n",
      "\titers: 500, epoch: 11 | loss: 0.1169378\n",
      "\tspeed: 0.0108s/iter; left time: 56.2655s\n",
      "Epoch: 11 cost time: 6.4444739818573\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1408087 Vali Loss: 0.0402770 Test Loss: 0.1330332\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13402952253818512, mae:0.23121598362922668\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1887528\n",
      "\tspeed: 0.0272s/iter; left time: 306.8602s\n",
      "\titers: 200, epoch: 1 | loss: 0.1745297\n",
      "\tspeed: 0.0146s/iter; left time: 163.0998s\n",
      "\titers: 300, epoch: 1 | loss: 0.2686448\n",
      "\tspeed: 0.0146s/iter; left time: 161.9206s\n",
      "\titers: 400, epoch: 1 | loss: 0.1905873\n",
      "\tspeed: 0.0146s/iter; left time: 160.7657s\n",
      "\titers: 500, epoch: 1 | loss: 0.1562005\n",
      "\tspeed: 0.0146s/iter; left time: 159.2930s\n",
      "Epoch: 1 cost time: 9.623996257781982\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2121014 Vali Loss: 0.0457071 Test Loss: 0.1369302\n",
      "Validation loss decreased (inf --> 0.045707).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2004817\n",
      "\tspeed: 0.0476s/iter; left time: 511.0625s\n",
      "\titers: 200, epoch: 2 | loss: 0.1453238\n",
      "\tspeed: 0.0147s/iter; left time: 156.1764s\n",
      "\titers: 300, epoch: 2 | loss: 0.1475892\n",
      "\tspeed: 0.0147s/iter; left time: 154.3497s\n",
      "\titers: 400, epoch: 2 | loss: 0.0873680\n",
      "\tspeed: 0.0146s/iter; left time: 152.7661s\n",
      "\titers: 500, epoch: 2 | loss: 0.1953811\n",
      "\tspeed: 0.0146s/iter; left time: 151.2690s\n",
      "Epoch: 2 cost time: 8.714062213897705\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1697742 Vali Loss: 0.0448095 Test Loss: 0.1532207\n",
      "Validation loss decreased (0.045707 --> 0.044810).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2172320\n",
      "\tspeed: 0.0521s/iter; left time: 528.9069s\n",
      "\titers: 200, epoch: 3 | loss: 0.1722004\n",
      "\tspeed: 0.0168s/iter; left time: 169.4061s\n",
      "\titers: 300, epoch: 3 | loss: 0.0974896\n",
      "\tspeed: 0.0168s/iter; left time: 167.5319s\n",
      "\titers: 400, epoch: 3 | loss: 0.1056065\n",
      "\tspeed: 0.0169s/iter; left time: 166.4217s\n",
      "\titers: 500, epoch: 3 | loss: 0.0902604\n",
      "\tspeed: 0.0169s/iter; left time: 164.5120s\n",
      "Epoch: 3 cost time: 9.910758018493652\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1363809 Vali Loss: 0.0369532 Test Loss: 0.1324762\n",
      "Validation loss decreased (0.044810 --> 0.036953).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1073803\n",
      "\tspeed: 0.0500s/iter; left time: 479.8787s\n",
      "\titers: 200, epoch: 4 | loss: 0.0982647\n",
      "\tspeed: 0.0164s/iter; left time: 155.9398s\n",
      "\titers: 300, epoch: 4 | loss: 0.1236442\n",
      "\tspeed: 0.0163s/iter; left time: 153.4475s\n",
      "\titers: 400, epoch: 4 | loss: 0.1236129\n",
      "\tspeed: 0.0163s/iter; left time: 151.8091s\n",
      "\titers: 500, epoch: 4 | loss: 0.0928323\n",
      "\tspeed: 0.0164s/iter; left time: 150.5421s\n",
      "Epoch: 4 cost time: 9.52298092842102\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1142296 Vali Loss: 0.0382205 Test Loss: 0.1314452\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1057231\n",
      "\tspeed: 0.0497s/iter; left time: 448.2169s\n",
      "\titers: 200, epoch: 5 | loss: 0.0959686\n",
      "\tspeed: 0.0147s/iter; left time: 130.9693s\n",
      "\titers: 300, epoch: 5 | loss: 0.1053961\n",
      "\tspeed: 0.0146s/iter; left time: 129.1204s\n",
      "\titers: 400, epoch: 5 | loss: 0.0760272\n",
      "\tspeed: 0.0147s/iter; left time: 127.9728s\n",
      "\titers: 500, epoch: 5 | loss: 0.0985045\n",
      "\tspeed: 0.0146s/iter; left time: 126.2629s\n",
      "Epoch: 5 cost time: 8.648949146270752\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1028115 Vali Loss: 0.0374991 Test Loss: 0.1352442\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0769622\n",
      "\tspeed: 0.0497s/iter; left time: 420.1575s\n",
      "\titers: 200, epoch: 6 | loss: 0.1111268\n",
      "\tspeed: 0.0165s/iter; left time: 138.0798s\n",
      "\titers: 300, epoch: 6 | loss: 0.0763125\n",
      "\tspeed: 0.0164s/iter; left time: 135.6954s\n",
      "\titers: 400, epoch: 6 | loss: 0.0836025\n",
      "\tspeed: 0.0149s/iter; left time: 121.6780s\n",
      "\titers: 500, epoch: 6 | loss: 0.0896173\n",
      "\tspeed: 0.0147s/iter; left time: 118.0556s\n",
      "Epoch: 6 cost time: 9.201226711273193\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0954362 Vali Loss: 0.0381863 Test Loss: 0.1341667\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1326751559972763, mae:0.23089633882045746\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2571278\n",
      "\tspeed: 0.0259s/iter; left time: 292.6718s\n",
      "\titers: 200, epoch: 1 | loss: 0.1795674\n",
      "\tspeed: 0.0143s/iter; left time: 159.7418s\n",
      "\titers: 300, epoch: 1 | loss: 0.1446693\n",
      "\tspeed: 0.0142s/iter; left time: 158.1180s\n",
      "\titers: 400, epoch: 1 | loss: 0.2394119\n",
      "\tspeed: 0.0143s/iter; left time: 156.8041s\n",
      "\titers: 500, epoch: 1 | loss: 0.1704829\n",
      "\tspeed: 0.0142s/iter; left time: 155.2685s\n",
      "Epoch: 1 cost time: 9.33791470527649\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1852389 Vali Loss: 0.0390686 Test Loss: 0.1200361\n",
      "Validation loss decreased (inf --> 0.039069).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1506557\n",
      "\tspeed: 0.0463s/iter; left time: 496.5390s\n",
      "\titers: 200, epoch: 2 | loss: 0.2355483\n",
      "\tspeed: 0.0142s/iter; left time: 151.3605s\n",
      "\titers: 300, epoch: 2 | loss: 0.1674860\n",
      "\tspeed: 0.0143s/iter; left time: 150.2184s\n",
      "\titers: 400, epoch: 2 | loss: 0.1498764\n",
      "\tspeed: 0.0143s/iter; left time: 148.8538s\n",
      "\titers: 500, epoch: 2 | loss: 0.1291158\n",
      "\tspeed: 0.0143s/iter; left time: 147.2481s\n",
      "Epoch: 2 cost time: 8.43352746963501\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1558052 Vali Loss: 0.0364324 Test Loss: 0.1229202\n",
      "Validation loss decreased (0.039069 --> 0.036432).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1597399\n",
      "\tspeed: 0.0459s/iter; left time: 466.4397s\n",
      "\titers: 200, epoch: 3 | loss: 0.0761815\n",
      "\tspeed: 0.0142s/iter; left time: 142.6813s\n",
      "\titers: 300, epoch: 3 | loss: 0.1524725\n",
      "\tspeed: 0.0142s/iter; left time: 141.2594s\n",
      "\titers: 400, epoch: 3 | loss: 0.1095894\n",
      "\tspeed: 0.0142s/iter; left time: 139.7952s\n",
      "\titers: 500, epoch: 3 | loss: 0.1125388\n",
      "\tspeed: 0.0142s/iter; left time: 138.4411s\n",
      "Epoch: 3 cost time: 8.398529529571533\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1273141 Vali Loss: 0.0354726 Test Loss: 0.1159626\n",
      "Validation loss decreased (0.036432 --> 0.035473).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1125436\n",
      "\tspeed: 0.0452s/iter; left time: 433.5622s\n",
      "\titers: 200, epoch: 4 | loss: 0.1218719\n",
      "\tspeed: 0.0142s/iter; left time: 135.0975s\n",
      "\titers: 300, epoch: 4 | loss: 0.1170438\n",
      "\tspeed: 0.0142s/iter; left time: 133.7025s\n",
      "\titers: 400, epoch: 4 | loss: 0.0681171\n",
      "\tspeed: 0.0142s/iter; left time: 132.2755s\n",
      "\titers: 500, epoch: 4 | loss: 0.0770603\n",
      "\tspeed: 0.0142s/iter; left time: 130.8619s\n",
      "Epoch: 4 cost time: 8.419173240661621\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1111331 Vali Loss: 0.0332167 Test Loss: 0.1165192\n",
      "Validation loss decreased (0.035473 --> 0.033217).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0582938\n",
      "\tspeed: 0.0486s/iter; left time: 438.5705s\n",
      "\titers: 200, epoch: 5 | loss: 0.0808735\n",
      "\tspeed: 0.0160s/iter; left time: 142.5927s\n",
      "\titers: 300, epoch: 5 | loss: 0.1255110\n",
      "\tspeed: 0.0160s/iter; left time: 141.0132s\n",
      "\titers: 400, epoch: 5 | loss: 0.0846190\n",
      "\tspeed: 0.0153s/iter; left time: 133.3303s\n",
      "\titers: 500, epoch: 5 | loss: 0.1239723\n",
      "\tspeed: 0.0143s/iter; left time: 123.6621s\n",
      "Epoch: 5 cost time: 9.097311735153198\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0998496 Vali Loss: 0.0335846 Test Loss: 0.1124407\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0829497\n",
      "\tspeed: 0.0460s/iter; left time: 388.5676s\n",
      "\titers: 200, epoch: 6 | loss: 0.1824692\n",
      "\tspeed: 0.0143s/iter; left time: 119.2851s\n",
      "\titers: 300, epoch: 6 | loss: 0.1384225\n",
      "\tspeed: 0.0143s/iter; left time: 117.8735s\n",
      "\titers: 400, epoch: 6 | loss: 0.1213723\n",
      "\tspeed: 0.0143s/iter; left time: 116.4018s\n",
      "\titers: 500, epoch: 6 | loss: 0.1057120\n",
      "\tspeed: 0.0143s/iter; left time: 114.9526s\n",
      "Epoch: 6 cost time: 8.416188716888428\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0947269 Vali Loss: 0.0343582 Test Loss: 0.1162853\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0796793\n",
      "\tspeed: 0.0450s/iter; left time: 354.3130s\n",
      "\titers: 200, epoch: 7 | loss: 0.1019718\n",
      "\tspeed: 0.0143s/iter; left time: 111.1999s\n",
      "\titers: 300, epoch: 7 | loss: 0.1382055\n",
      "\tspeed: 0.0143s/iter; left time: 109.8127s\n",
      "\titers: 400, epoch: 7 | loss: 0.1120844\n",
      "\tspeed: 0.0143s/iter; left time: 108.3584s\n",
      "\titers: 500, epoch: 7 | loss: 0.1253088\n",
      "\tspeed: 0.0143s/iter; left time: 106.9453s\n",
      "Epoch: 7 cost time: 8.43242883682251\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0917960 Vali Loss: 0.0335369 Test Loss: 0.1151851\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11667660623788834, mae:0.2073177695274353\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1167674\n",
      "\tspeed: 0.0261s/iter; left time: 294.4935s\n",
      "\titers: 200, epoch: 1 | loss: 0.2955054\n",
      "\tspeed: 0.0143s/iter; left time: 159.9117s\n",
      "\titers: 300, epoch: 1 | loss: 0.2616664\n",
      "\tspeed: 0.0143s/iter; left time: 158.4688s\n",
      "\titers: 400, epoch: 1 | loss: 0.2577615\n",
      "\tspeed: 0.0143s/iter; left time: 157.1499s\n",
      "\titers: 500, epoch: 1 | loss: 0.2450613\n",
      "\tspeed: 0.0142s/iter; left time: 155.3118s\n",
      "Epoch: 1 cost time: 9.366874933242798\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2198592 Vali Loss: 0.0458057 Test Loss: 0.1364354\n",
      "Validation loss decreased (inf --> 0.045806).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1457091\n",
      "\tspeed: 0.0466s/iter; left time: 499.8325s\n",
      "\titers: 200, epoch: 2 | loss: 0.1517224\n",
      "\tspeed: 0.0143s/iter; left time: 151.7486s\n",
      "\titers: 300, epoch: 2 | loss: 0.1078367\n",
      "\tspeed: 0.0142s/iter; left time: 149.6932s\n",
      "\titers: 400, epoch: 2 | loss: 0.1300821\n",
      "\tspeed: 0.0142s/iter; left time: 148.3622s\n",
      "\titers: 500, epoch: 2 | loss: 0.1867114\n",
      "\tspeed: 0.0142s/iter; left time: 146.6534s\n",
      "Epoch: 2 cost time: 8.44642162322998\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1734601 Vali Loss: 0.0465346 Test Loss: 0.1419535\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1603560\n",
      "\tspeed: 0.0470s/iter; left time: 477.6462s\n",
      "\titers: 200, epoch: 3 | loss: 0.1496567\n",
      "\tspeed: 0.0144s/iter; left time: 144.7196s\n",
      "\titers: 300, epoch: 3 | loss: 0.1131109\n",
      "\tspeed: 0.0144s/iter; left time: 143.1720s\n",
      "\titers: 400, epoch: 3 | loss: 0.1033980\n",
      "\tspeed: 0.0143s/iter; left time: 141.4619s\n",
      "\titers: 500, epoch: 3 | loss: 0.1396991\n",
      "\tspeed: 0.0144s/iter; left time: 140.2634s\n",
      "Epoch: 3 cost time: 8.475270509719849\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1330556 Vali Loss: 0.0399300 Test Loss: 0.1306822\n",
      "Validation loss decreased (0.045806 --> 0.039930).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1255874\n",
      "\tspeed: 0.0462s/iter; left time: 443.3614s\n",
      "\titers: 200, epoch: 4 | loss: 0.0922611\n",
      "\tspeed: 0.0143s/iter; left time: 136.0771s\n",
      "\titers: 300, epoch: 4 | loss: 0.1204252\n",
      "\tspeed: 0.0143s/iter; left time: 134.5702s\n",
      "\titers: 400, epoch: 4 | loss: 0.0807351\n",
      "\tspeed: 0.0143s/iter; left time: 133.0586s\n",
      "\titers: 500, epoch: 4 | loss: 0.0890441\n",
      "\tspeed: 0.0143s/iter; left time: 131.4948s\n",
      "Epoch: 4 cost time: 8.472286701202393\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1093582 Vali Loss: 0.0405465 Test Loss: 0.1377843\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1012835\n",
      "\tspeed: 0.0494s/iter; left time: 445.7559s\n",
      "\titers: 200, epoch: 5 | loss: 0.1233751\n",
      "\tspeed: 0.0143s/iter; left time: 127.5593s\n",
      "\titers: 300, epoch: 5 | loss: 0.0911026\n",
      "\tspeed: 0.0143s/iter; left time: 125.8734s\n",
      "\titers: 400, epoch: 5 | loss: 0.1299900\n",
      "\tspeed: 0.0143s/iter; left time: 124.4614s\n",
      "\titers: 500, epoch: 5 | loss: 0.0800860\n",
      "\tspeed: 0.0143s/iter; left time: 122.9775s\n",
      "Epoch: 5 cost time: 8.452622890472412\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0979579 Vali Loss: 0.0395616 Test Loss: 0.1374321\n",
      "Validation loss decreased (0.039930 --> 0.039562).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1015463\n",
      "\tspeed: 0.0482s/iter; left time: 407.3940s\n",
      "\titers: 200, epoch: 6 | loss: 0.0948733\n",
      "\tspeed: 0.0142s/iter; left time: 118.9988s\n",
      "\titers: 300, epoch: 6 | loss: 0.0730019\n",
      "\tspeed: 0.0142s/iter; left time: 117.5535s\n",
      "\titers: 400, epoch: 6 | loss: 0.1324817\n",
      "\tspeed: 0.0142s/iter; left time: 116.0931s\n",
      "\titers: 500, epoch: 6 | loss: 0.0903305\n",
      "\tspeed: 0.0142s/iter; left time: 114.5675s\n",
      "Epoch: 6 cost time: 8.445652723312378\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0920441 Vali Loss: 0.0392771 Test Loss: 0.1379205\n",
      "Validation loss decreased (0.039562 --> 0.039277).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1402810\n",
      "\tspeed: 0.0459s/iter; left time: 362.0360s\n",
      "\titers: 200, epoch: 7 | loss: 0.0584416\n",
      "\tspeed: 0.0143s/iter; left time: 111.4629s\n",
      "\titers: 300, epoch: 7 | loss: 0.0678023\n",
      "\tspeed: 0.0143s/iter; left time: 109.9770s\n",
      "\titers: 400, epoch: 7 | loss: 0.1021162\n",
      "\tspeed: 0.0143s/iter; left time: 108.5975s\n",
      "\titers: 500, epoch: 7 | loss: 0.0978183\n",
      "\tspeed: 0.0143s/iter; left time: 107.2040s\n",
      "Epoch: 7 cost time: 8.471689939498901\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0904201 Vali Loss: 0.0401387 Test Loss: 0.1376490\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0721863\n",
      "\tspeed: 0.0468s/iter; left time: 342.4328s\n",
      "\titers: 200, epoch: 8 | loss: 0.0762913\n",
      "\tspeed: 0.0143s/iter; left time: 103.3164s\n",
      "\titers: 300, epoch: 8 | loss: 0.0771560\n",
      "\tspeed: 0.0143s/iter; left time: 101.7902s\n",
      "\titers: 400, epoch: 8 | loss: 0.0905376\n",
      "\tspeed: 0.0143s/iter; left time: 100.4699s\n",
      "\titers: 500, epoch: 8 | loss: 0.0657823\n",
      "\tspeed: 0.0143s/iter; left time: 98.8914s\n",
      "Epoch: 8 cost time: 8.469254732131958\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0885404 Vali Loss: 0.0401814 Test Loss: 0.1384605\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1196062\n",
      "\tspeed: 0.0463s/iter; left time: 311.8071s\n",
      "\titers: 200, epoch: 9 | loss: 0.0744827\n",
      "\tspeed: 0.0144s/iter; left time: 95.6021s\n",
      "\titers: 300, epoch: 9 | loss: 0.0610878\n",
      "\tspeed: 0.0144s/iter; left time: 94.1285s\n",
      "\titers: 400, epoch: 9 | loss: 0.0683761\n",
      "\tspeed: 0.0144s/iter; left time: 92.5881s\n",
      "\titers: 500, epoch: 9 | loss: 0.0821749\n",
      "\tspeed: 0.0144s/iter; left time: 91.1061s\n",
      "Epoch: 9 cost time: 8.51870846748352\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0874677 Vali Loss: 0.0394991 Test Loss: 0.1376500\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1380898505449295, mae:0.2346818745136261\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2557442\n",
      "\tspeed: 0.0305s/iter; left time: 344.2001s\n",
      "\titers: 200, epoch: 1 | loss: 0.2155398\n",
      "\tspeed: 0.0172s/iter; left time: 192.7067s\n",
      "\titers: 300, epoch: 1 | loss: 0.2420019\n",
      "\tspeed: 0.0168s/iter; left time: 186.4554s\n",
      "\titers: 400, epoch: 1 | loss: 0.2215629\n",
      "\tspeed: 0.0168s/iter; left time: 185.0204s\n",
      "\titers: 500, epoch: 1 | loss: 0.1729687\n",
      "\tspeed: 0.0168s/iter; left time: 183.4209s\n",
      "Epoch: 1 cost time: 11.03813362121582\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2311400 Vali Loss: 0.0515341 Test Loss: 0.1655550\n",
      "Validation loss decreased (inf --> 0.051534).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2657224\n",
      "\tspeed: 0.0533s/iter; left time: 572.2477s\n",
      "\titers: 200, epoch: 2 | loss: 0.2257499\n",
      "\tspeed: 0.0185s/iter; left time: 196.3086s\n",
      "\titers: 300, epoch: 2 | loss: 0.3500930\n",
      "\tspeed: 0.0170s/iter; left time: 178.8398s\n",
      "\titers: 400, epoch: 2 | loss: 0.2537546\n",
      "\tspeed: 0.0170s/iter; left time: 176.8780s\n",
      "\titers: 500, epoch: 2 | loss: 0.2537118\n",
      "\tspeed: 0.0170s/iter; left time: 175.2521s\n",
      "Epoch: 2 cost time: 10.306124448776245\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2654837 Vali Loss: 0.0593745 Test Loss: 0.1681601\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2091331\n",
      "\tspeed: 0.0530s/iter; left time: 538.8381s\n",
      "\titers: 200, epoch: 3 | loss: 0.2144510\n",
      "\tspeed: 0.0170s/iter; left time: 170.8935s\n",
      "\titers: 300, epoch: 3 | loss: 0.3650085\n",
      "\tspeed: 0.0170s/iter; left time: 168.9901s\n",
      "\titers: 400, epoch: 3 | loss: 0.2017132\n",
      "\tspeed: 0.0170s/iter; left time: 167.4332s\n",
      "\titers: 500, epoch: 3 | loss: 0.1605293\n",
      "\tspeed: 0.0170s/iter; left time: 165.7372s\n",
      "Epoch: 3 cost time: 9.978131532669067\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2349367 Vali Loss: 0.0586023 Test Loss: 0.1637388\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1823092\n",
      "\tspeed: 0.0505s/iter; left time: 483.8852s\n",
      "\titers: 200, epoch: 4 | loss: 0.1900171\n",
      "\tspeed: 0.0169s/iter; left time: 160.6804s\n",
      "\titers: 300, epoch: 4 | loss: 0.2166175\n",
      "\tspeed: 0.0169s/iter; left time: 159.1010s\n",
      "\titers: 400, epoch: 4 | loss: 0.2219737\n",
      "\tspeed: 0.0169s/iter; left time: 157.2406s\n",
      "\titers: 500, epoch: 4 | loss: 0.2073140\n",
      "\tspeed: 0.0169s/iter; left time: 155.7220s\n",
      "Epoch: 4 cost time: 9.966561794281006\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2113202 Vali Loss: 0.0572437 Test Loss: 0.1619374\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.16583923995494843, mae:0.2654508650302887\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1760582\n",
      "\tspeed: 0.0301s/iter; left time: 340.3405s\n",
      "\titers: 200, epoch: 1 | loss: 0.1070482\n",
      "\tspeed: 0.0178s/iter; left time: 199.5382s\n",
      "\titers: 300, epoch: 1 | loss: 0.2112269\n",
      "\tspeed: 0.0179s/iter; left time: 198.4176s\n",
      "\titers: 400, epoch: 1 | loss: 0.1858908\n",
      "\tspeed: 0.0179s/iter; left time: 196.9589s\n",
      "\titers: 500, epoch: 1 | loss: 0.1099274\n",
      "\tspeed: 0.0167s/iter; left time: 182.1474s\n",
      "Epoch: 1 cost time: 11.24231219291687\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2009359 Vali Loss: 0.0405242 Test Loss: 0.1315651\n",
      "Validation loss decreased (inf --> 0.040524).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2052581\n",
      "\tspeed: 0.0524s/iter; left time: 562.5326s\n",
      "\titers: 200, epoch: 2 | loss: 0.1780758\n",
      "\tspeed: 0.0164s/iter; left time: 174.1761s\n",
      "\titers: 300, epoch: 2 | loss: 0.3228746\n",
      "\tspeed: 0.0164s/iter; left time: 172.3092s\n",
      "\titers: 400, epoch: 2 | loss: 0.3696241\n",
      "\tspeed: 0.0164s/iter; left time: 171.1124s\n",
      "\titers: 500, epoch: 2 | loss: 0.2204932\n",
      "\tspeed: 0.0164s/iter; left time: 169.0029s\n",
      "Epoch: 2 cost time: 9.785012245178223\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2766150 Vali Loss: 0.0629898 Test Loss: 0.1795397\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2809814\n",
      "\tspeed: 0.0538s/iter; left time: 546.5740s\n",
      "\titers: 200, epoch: 3 | loss: 0.2335446\n",
      "\tspeed: 0.0178s/iter; left time: 179.1486s\n",
      "\titers: 300, epoch: 3 | loss: 0.1699868\n",
      "\tspeed: 0.0164s/iter; left time: 163.5342s\n",
      "\titers: 400, epoch: 3 | loss: 0.2662534\n",
      "\tspeed: 0.0164s/iter; left time: 161.8228s\n",
      "\titers: 500, epoch: 3 | loss: 0.1413514\n",
      "\tspeed: 0.0164s/iter; left time: 160.1694s\n",
      "Epoch: 3 cost time: 10.005398273468018\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2469185 Vali Loss: 0.0556733 Test Loss: 0.1584507\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1767578\n",
      "\tspeed: 0.0497s/iter; left time: 476.9929s\n",
      "\titers: 200, epoch: 4 | loss: 0.4568399\n",
      "\tspeed: 0.0163s/iter; left time: 154.8795s\n",
      "\titers: 300, epoch: 4 | loss: 0.2135920\n",
      "\tspeed: 0.0163s/iter; left time: 153.1766s\n",
      "\titers: 400, epoch: 4 | loss: 0.1967251\n",
      "\tspeed: 0.0163s/iter; left time: 151.5406s\n",
      "\titers: 500, epoch: 4 | loss: 0.3326260\n",
      "\tspeed: 0.0163s/iter; left time: 149.9364s\n",
      "Epoch: 4 cost time: 9.641014814376831\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2256985 Vali Loss: 0.0552150 Test Loss: 0.1522728\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13178963959217072, mae:0.22601796686649323\n",
      ">>> LR=5e-4,DO=0.1,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2132356\n",
      "\tspeed: 0.0295s/iter; left time: 333.7509s\n",
      "\titers: 200, epoch: 1 | loss: 0.2197873\n",
      "\tspeed: 0.0155s/iter; left time: 173.5191s\n",
      "\titers: 300, epoch: 1 | loss: 0.1616414\n",
      "\tspeed: 0.0155s/iter; left time: 171.8612s\n",
      "\titers: 400, epoch: 1 | loss: 0.2144824\n",
      "\tspeed: 0.0155s/iter; left time: 170.3002s\n",
      "\titers: 500, epoch: 1 | loss: 0.3870519\n",
      "\tspeed: 0.0155s/iter; left time: 168.9087s\n",
      "Epoch: 1 cost time: 10.290083169937134\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2323277 Vali Loss: 0.0471214 Test Loss: 0.1549581\n",
      "Validation loss decreased (inf --> 0.047121).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2906809\n",
      "\tspeed: 0.0506s/iter; left time: 542.9137s\n",
      "\titers: 200, epoch: 2 | loss: 0.1560972\n",
      "\tspeed: 0.0156s/iter; left time: 166.1875s\n",
      "\titers: 300, epoch: 2 | loss: 0.3482560\n",
      "\tspeed: 0.0155s/iter; left time: 163.2635s\n",
      "\titers: 400, epoch: 2 | loss: 0.2620814\n",
      "\tspeed: 0.0155s/iter; left time: 161.5942s\n",
      "\titers: 500, epoch: 2 | loss: 0.3732356\n",
      "\tspeed: 0.0155s/iter; left time: 159.7182s\n",
      "Epoch: 2 cost time: 9.4132399559021\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2595969 Vali Loss: 0.0632168 Test Loss: 0.1850292\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2194456\n",
      "\tspeed: 0.0482s/iter; left time: 489.2828s\n",
      "\titers: 200, epoch: 3 | loss: 0.1570438\n",
      "\tspeed: 0.0154s/iter; left time: 154.4942s\n",
      "\titers: 300, epoch: 3 | loss: 0.4517294\n",
      "\tspeed: 0.0154s/iter; left time: 152.9053s\n",
      "\titers: 400, epoch: 3 | loss: 0.2757720\n",
      "\tspeed: 0.0153s/iter; left time: 151.2359s\n",
      "\titers: 500, epoch: 3 | loss: 0.2946413\n",
      "\tspeed: 0.0153s/iter; left time: 149.6427s\n",
      "Epoch: 3 cost time: 9.027586698532104\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2807978 Vali Loss: 0.0621568 Test Loss: 0.1712141\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2104102\n",
      "\tspeed: 0.0477s/iter; left time: 457.3395s\n",
      "\titers: 200, epoch: 4 | loss: 0.3161986\n",
      "\tspeed: 0.0177s/iter; left time: 167.8232s\n",
      "\titers: 300, epoch: 4 | loss: 0.3023233\n",
      "\tspeed: 0.0177s/iter; left time: 165.8644s\n",
      "\titers: 400, epoch: 4 | loss: 0.1917839\n",
      "\tspeed: 0.0177s/iter; left time: 164.1686s\n",
      "\titers: 500, epoch: 4 | loss: 0.2984703\n",
      "\tspeed: 0.0176s/iter; left time: 162.2195s\n",
      "Epoch: 4 cost time: 10.167400121688843\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2446291 Vali Loss: 0.0604046 Test Loss: 0.1626612\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15516559779644012, mae:0.24410508573055267\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1601215\n",
      "\tspeed: 0.0303s/iter; left time: 342.7496s\n",
      "\titers: 200, epoch: 1 | loss: 0.2679668\n",
      "\tspeed: 0.0187s/iter; left time: 209.6383s\n",
      "\titers: 300, epoch: 1 | loss: 0.3529364\n",
      "\tspeed: 0.0187s/iter; left time: 207.7015s\n",
      "\titers: 400, epoch: 1 | loss: 0.1937822\n",
      "\tspeed: 0.0187s/iter; left time: 205.6082s\n",
      "\titers: 500, epoch: 1 | loss: 0.1617287\n",
      "\tspeed: 0.0187s/iter; left time: 204.2164s\n",
      "Epoch: 1 cost time: 11.879122972488403\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2158539 Vali Loss: 0.0480296 Test Loss: 0.1407557\n",
      "Validation loss decreased (inf --> 0.048030).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3049391\n",
      "\tspeed: 0.0564s/iter; left time: 605.1569s\n",
      "\titers: 200, epoch: 2 | loss: 0.1487861\n",
      "\tspeed: 0.0187s/iter; left time: 198.3648s\n",
      "\titers: 300, epoch: 2 | loss: 0.1564662\n",
      "\tspeed: 0.0187s/iter; left time: 196.4306s\n",
      "\titers: 400, epoch: 2 | loss: 0.1327024\n",
      "\tspeed: 0.0186s/iter; left time: 194.4379s\n",
      "\titers: 500, epoch: 2 | loss: 0.1627572\n",
      "\tspeed: 0.0186s/iter; left time: 192.6105s\n",
      "Epoch: 2 cost time: 10.923125982284546\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1726822 Vali Loss: 0.0468829 Test Loss: 0.1471639\n",
      "Validation loss decreased (0.048030 --> 0.046883).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1179763\n",
      "\tspeed: 0.0572s/iter; left time: 581.6917s\n",
      "\titers: 200, epoch: 3 | loss: 0.1013117\n",
      "\tspeed: 0.0188s/iter; left time: 189.0720s\n",
      "\titers: 300, epoch: 3 | loss: 0.1304374\n",
      "\tspeed: 0.0188s/iter; left time: 187.1011s\n",
      "\titers: 400, epoch: 3 | loss: 0.1004486\n",
      "\tspeed: 0.0187s/iter; left time: 184.7347s\n",
      "\titers: 500, epoch: 3 | loss: 0.0968303\n",
      "\tspeed: 0.0187s/iter; left time: 182.8719s\n",
      "Epoch: 3 cost time: 10.9925057888031\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1370727 Vali Loss: 0.0421463 Test Loss: 0.1415687\n",
      "Validation loss decreased (0.046883 --> 0.042146).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1208971\n",
      "\tspeed: 0.0589s/iter; left time: 564.7744s\n",
      "\titers: 200, epoch: 4 | loss: 0.1769108\n",
      "\tspeed: 0.0186s/iter; left time: 176.6597s\n",
      "\titers: 300, epoch: 4 | loss: 0.1254952\n",
      "\tspeed: 0.0186s/iter; left time: 174.4812s\n",
      "\titers: 400, epoch: 4 | loss: 0.0638355\n",
      "\tspeed: 0.0186s/iter; left time: 172.8335s\n",
      "\titers: 500, epoch: 4 | loss: 0.1190091\n",
      "\tspeed: 0.0186s/iter; left time: 171.1606s\n",
      "Epoch: 4 cost time: 10.879269123077393\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1120965 Vali Loss: 0.0393526 Test Loss: 0.1339446\n",
      "Validation loss decreased (0.042146 --> 0.039353).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1096135\n",
      "\tspeed: 0.0571s/iter; left time: 515.2317s\n",
      "\titers: 200, epoch: 5 | loss: 0.1083995\n",
      "\tspeed: 0.0187s/iter; left time: 166.7830s\n",
      "\titers: 300, epoch: 5 | loss: 0.1029091\n",
      "\tspeed: 0.0187s/iter; left time: 165.1256s\n",
      "\titers: 400, epoch: 5 | loss: 0.0780801\n",
      "\tspeed: 0.0188s/iter; left time: 163.7382s\n",
      "\titers: 500, epoch: 5 | loss: 0.0935497\n",
      "\tspeed: 0.0188s/iter; left time: 162.2470s\n",
      "Epoch: 5 cost time: 10.982906103134155\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0984703 Vali Loss: 0.0383031 Test Loss: 0.1369257\n",
      "Validation loss decreased (0.039353 --> 0.038303).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0612615\n",
      "\tspeed: 0.0561s/iter; left time: 473.8905s\n",
      "\titers: 200, epoch: 6 | loss: 0.0969343\n",
      "\tspeed: 0.0188s/iter; left time: 156.5931s\n",
      "\titers: 300, epoch: 6 | loss: 0.0809905\n",
      "\tspeed: 0.0187s/iter; left time: 154.4263s\n",
      "\titers: 400, epoch: 6 | loss: 0.1350560\n",
      "\tspeed: 0.0187s/iter; left time: 152.6021s\n",
      "\titers: 500, epoch: 6 | loss: 0.0834133\n",
      "\tspeed: 0.0187s/iter; left time: 150.8452s\n",
      "Epoch: 6 cost time: 10.982553720474243\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0927784 Vali Loss: 0.0386098 Test Loss: 0.1366741\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0775667\n",
      "\tspeed: 0.0572s/iter; left time: 450.8748s\n",
      "\titers: 200, epoch: 7 | loss: 0.0709670\n",
      "\tspeed: 0.0187s/iter; left time: 145.6733s\n",
      "\titers: 300, epoch: 7 | loss: 0.0941858\n",
      "\tspeed: 0.0187s/iter; left time: 143.7602s\n",
      "\titers: 400, epoch: 7 | loss: 0.0993690\n",
      "\tspeed: 0.0187s/iter; left time: 141.7781s\n",
      "\titers: 500, epoch: 7 | loss: 0.1505994\n",
      "\tspeed: 0.0187s/iter; left time: 139.9394s\n",
      "Epoch: 7 cost time: 10.964020252227783\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0896315 Vali Loss: 0.0384480 Test Loss: 0.1356376\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0899162\n",
      "\tspeed: 0.0554s/iter; left time: 404.7903s\n",
      "\titers: 200, epoch: 8 | loss: 0.0799357\n",
      "\tspeed: 0.0185s/iter; left time: 133.6262s\n",
      "\titers: 300, epoch: 8 | loss: 0.0750812\n",
      "\tspeed: 0.0185s/iter; left time: 131.8597s\n",
      "\titers: 400, epoch: 8 | loss: 0.1160712\n",
      "\tspeed: 0.0185s/iter; left time: 129.9446s\n",
      "\titers: 500, epoch: 8 | loss: 0.0674134\n",
      "\tspeed: 0.0185s/iter; left time: 128.1110s\n",
      "Epoch: 8 cost time: 10.824750423431396\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0882210 Vali Loss: 0.0388550 Test Loss: 0.1363302\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13711073994636536, mae:0.23102308809757233\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3742481\n",
      "\tspeed: 0.0325s/iter; left time: 367.4734s\n",
      "\titers: 200, epoch: 1 | loss: 0.1943786\n",
      "\tspeed: 0.0204s/iter; left time: 229.0242s\n",
      "\titers: 300, epoch: 1 | loss: 0.1684622\n",
      "\tspeed: 0.0204s/iter; left time: 226.9080s\n",
      "\titers: 400, epoch: 1 | loss: 0.1639359\n",
      "\tspeed: 0.0205s/iter; left time: 225.2671s\n",
      "\titers: 500, epoch: 1 | loss: 0.1772284\n",
      "\tspeed: 0.0205s/iter; left time: 223.2409s\n",
      "Epoch: 1 cost time: 12.921253681182861\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1876974 Vali Loss: 0.0407946 Test Loss: 0.1276755\n",
      "Validation loss decreased (inf --> 0.040795).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1380969\n",
      "\tspeed: 0.0572s/iter; left time: 613.9304s\n",
      "\titers: 200, epoch: 2 | loss: 0.2264183\n",
      "\tspeed: 0.0184s/iter; left time: 195.3691s\n",
      "\titers: 300, epoch: 2 | loss: 0.2026057\n",
      "\tspeed: 0.0184s/iter; left time: 193.5174s\n",
      "\titers: 400, epoch: 2 | loss: 0.2185692\n",
      "\tspeed: 0.0184s/iter; left time: 191.8396s\n",
      "\titers: 500, epoch: 2 | loss: 0.1197596\n",
      "\tspeed: 0.0184s/iter; left time: 189.8151s\n",
      "Epoch: 2 cost time: 10.784347295761108\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1580204 Vali Loss: 0.0412605 Test Loss: 0.1215693\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0871293\n",
      "\tspeed: 0.0582s/iter; left time: 590.9478s\n",
      "\titers: 200, epoch: 3 | loss: 0.1306126\n",
      "\tspeed: 0.0183s/iter; left time: 183.9942s\n",
      "\titers: 300, epoch: 3 | loss: 0.1405563\n",
      "\tspeed: 0.0183s/iter; left time: 182.1344s\n",
      "\titers: 400, epoch: 3 | loss: 0.0955729\n",
      "\tspeed: 0.0183s/iter; left time: 180.2149s\n",
      "\titers: 500, epoch: 3 | loss: 0.1021546\n",
      "\tspeed: 0.0182s/iter; left time: 178.0728s\n",
      "Epoch: 3 cost time: 10.696298360824585\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1316289 Vali Loss: 0.0364323 Test Loss: 0.1165626\n",
      "Validation loss decreased (0.040795 --> 0.036432).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1236294\n",
      "\tspeed: 0.0571s/iter; left time: 547.6592s\n",
      "\titers: 200, epoch: 4 | loss: 0.0730140\n",
      "\tspeed: 0.0182s/iter; left time: 172.7960s\n",
      "\titers: 300, epoch: 4 | loss: 0.1182805\n",
      "\tspeed: 0.0182s/iter; left time: 170.7003s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956322\n",
      "\tspeed: 0.0182s/iter; left time: 168.9297s\n",
      "\titers: 500, epoch: 4 | loss: 0.0843183\n",
      "\tspeed: 0.0182s/iter; left time: 167.0674s\n",
      "Epoch: 4 cost time: 10.656893491744995\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1119665 Vali Loss: 0.0349799 Test Loss: 0.1142847\n",
      "Validation loss decreased (0.036432 --> 0.034980).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0831127\n",
      "\tspeed: 0.0568s/iter; left time: 512.1725s\n",
      "\titers: 200, epoch: 5 | loss: 0.0967013\n",
      "\tspeed: 0.0181s/iter; left time: 161.6125s\n",
      "\titers: 300, epoch: 5 | loss: 0.0894479\n",
      "\tspeed: 0.0181s/iter; left time: 159.8880s\n",
      "\titers: 400, epoch: 5 | loss: 0.0886109\n",
      "\tspeed: 0.0181s/iter; left time: 157.8841s\n",
      "\titers: 500, epoch: 5 | loss: 0.0990796\n",
      "\tspeed: 0.0181s/iter; left time: 156.0124s\n",
      "Epoch: 5 cost time: 10.655960083007812\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0984995 Vali Loss: 0.0343408 Test Loss: 0.1158925\n",
      "Validation loss decreased (0.034980 --> 0.034341).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1077913\n",
      "\tspeed: 0.0603s/iter; left time: 509.8612s\n",
      "\titers: 200, epoch: 6 | loss: 0.0714055\n",
      "\tspeed: 0.0211s/iter; left time: 176.4485s\n",
      "\titers: 300, epoch: 6 | loss: 0.1109908\n",
      "\tspeed: 0.0211s/iter; left time: 173.8855s\n",
      "\titers: 400, epoch: 6 | loss: 0.0810228\n",
      "\tspeed: 0.0211s/iter; left time: 171.9710s\n",
      "\titers: 500, epoch: 6 | loss: 0.0629373\n",
      "\tspeed: 0.0211s/iter; left time: 169.5412s\n",
      "Epoch: 6 cost time: 12.360519409179688\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0925016 Vali Loss: 0.0345410 Test Loss: 0.1169708\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0780927\n",
      "\tspeed: 0.0616s/iter; left time: 485.4844s\n",
      "\titers: 200, epoch: 7 | loss: 0.1124896\n",
      "\tspeed: 0.0210s/iter; left time: 163.1867s\n",
      "\titers: 300, epoch: 7 | loss: 0.0596608\n",
      "\tspeed: 0.0209s/iter; left time: 160.8299s\n",
      "\titers: 400, epoch: 7 | loss: 0.1044410\n",
      "\tspeed: 0.0209s/iter; left time: 158.6093s\n",
      "\titers: 500, epoch: 7 | loss: 0.0788359\n",
      "\tspeed: 0.0209s/iter; left time: 156.5751s\n",
      "Epoch: 7 cost time: 12.26069974899292\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0886925 Vali Loss: 0.0339803 Test Loss: 0.1164838\n",
      "Validation loss decreased (0.034341 --> 0.033980).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0700560\n",
      "\tspeed: 0.0630s/iter; left time: 460.5910s\n",
      "\titers: 200, epoch: 8 | loss: 0.0891792\n",
      "\tspeed: 0.0210s/iter; left time: 151.3859s\n",
      "\titers: 300, epoch: 8 | loss: 0.0794379\n",
      "\tspeed: 0.0210s/iter; left time: 149.0178s\n",
      "\titers: 400, epoch: 8 | loss: 0.0846472\n",
      "\tspeed: 0.0209s/iter; left time: 146.7876s\n",
      "\titers: 500, epoch: 8 | loss: 0.1003609\n",
      "\tspeed: 0.0210s/iter; left time: 144.8850s\n",
      "Epoch: 8 cost time: 12.238821268081665\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0865921 Vali Loss: 0.0339551 Test Loss: 0.1166554\n",
      "Validation loss decreased (0.033980 --> 0.033955).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0524424\n",
      "\tspeed: 0.0612s/iter; left time: 412.5142s\n",
      "\titers: 200, epoch: 9 | loss: 0.0777009\n",
      "\tspeed: 0.0211s/iter; left time: 140.0419s\n",
      "\titers: 300, epoch: 9 | loss: 0.0828364\n",
      "\tspeed: 0.0210s/iter; left time: 137.6133s\n",
      "\titers: 400, epoch: 9 | loss: 0.0816082\n",
      "\tspeed: 0.0210s/iter; left time: 135.3715s\n",
      "\titers: 500, epoch: 9 | loss: 0.0778431\n",
      "\tspeed: 0.0210s/iter; left time: 133.2647s\n",
      "Epoch: 9 cost time: 12.32104778289795\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0863239 Vali Loss: 0.0340197 Test Loss: 0.1164078\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.0703812\n",
      "\tspeed: 0.0605s/iter; left time: 373.3626s\n",
      "\titers: 200, epoch: 10 | loss: 0.0600727\n",
      "\tspeed: 0.0211s/iter; left time: 127.9195s\n",
      "\titers: 300, epoch: 10 | loss: 0.1222803\n",
      "\tspeed: 0.0211s/iter; left time: 125.9477s\n",
      "\titers: 400, epoch: 10 | loss: 0.0940767\n",
      "\tspeed: 0.0211s/iter; left time: 123.7525s\n",
      "\titers: 500, epoch: 10 | loss: 0.0836515\n",
      "\tspeed: 0.0211s/iter; left time: 121.6088s\n",
      "Epoch: 10 cost time: 12.307207822799683\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0867250 Vali Loss: 0.0338425 Test Loss: 0.1164990\n",
      "Validation loss decreased (0.033955 --> 0.033843).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0886704\n",
      "\tspeed: 0.0616s/iter; left time: 345.1280s\n",
      "\titers: 200, epoch: 11 | loss: 0.0949598\n",
      "\tspeed: 0.0211s/iter; left time: 116.2663s\n",
      "\titers: 300, epoch: 11 | loss: 0.0609087\n",
      "\tspeed: 0.0211s/iter; left time: 114.0834s\n",
      "\titers: 400, epoch: 11 | loss: 0.1235714\n",
      "\tspeed: 0.0211s/iter; left time: 111.7734s\n",
      "\titers: 500, epoch: 11 | loss: 0.1064821\n",
      "\tspeed: 0.0210s/iter; left time: 109.1169s\n",
      "Epoch: 11 cost time: 12.351275444030762\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0859769 Vali Loss: 0.0337616 Test Loss: 0.1164080\n",
      "Validation loss decreased (0.033843 --> 0.033762).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1195657\n",
      "\tspeed: 0.0606s/iter; left time: 305.0120s\n",
      "\titers: 200, epoch: 12 | loss: 0.0771970\n",
      "\tspeed: 0.0210s/iter; left time: 103.3625s\n",
      "\titers: 300, epoch: 12 | loss: 0.0692294\n",
      "\tspeed: 0.0209s/iter; left time: 101.2034s\n",
      "\titers: 400, epoch: 12 | loss: 0.0837279\n",
      "\tspeed: 0.0210s/iter; left time: 99.1873s\n",
      "\titers: 500, epoch: 12 | loss: 0.0855555\n",
      "\tspeed: 0.0209s/iter; left time: 96.9875s\n",
      "Epoch: 12 cost time: 12.267935991287231\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0858482 Vali Loss: 0.0338301 Test Loss: 0.1163966\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.0759818\n",
      "\tspeed: 0.0629s/iter; left time: 280.4835s\n",
      "\titers: 200, epoch: 13 | loss: 0.0878199\n",
      "\tspeed: 0.0205s/iter; left time: 89.4098s\n",
      "\titers: 300, epoch: 13 | loss: 0.0670783\n",
      "\tspeed: 0.0205s/iter; left time: 87.3390s\n",
      "\titers: 400, epoch: 13 | loss: 0.1085088\n",
      "\tspeed: 0.0205s/iter; left time: 85.1557s\n",
      "\titers: 500, epoch: 13 | loss: 0.1070844\n",
      "\tspeed: 0.0205s/iter; left time: 83.2459s\n",
      "Epoch: 13 cost time: 12.063522815704346\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0867677 Vali Loss: 0.0338175 Test Loss: 0.1164519\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.0902542\n",
      "\tspeed: 0.0579s/iter; left time: 225.3214s\n",
      "\titers: 200, epoch: 14 | loss: 0.0839087\n",
      "\tspeed: 0.0182s/iter; left time: 69.1584s\n",
      "\titers: 300, epoch: 14 | loss: 0.0895629\n",
      "\tspeed: 0.0182s/iter; left time: 67.3035s\n",
      "\titers: 400, epoch: 14 | loss: 0.0804068\n",
      "\tspeed: 0.0182s/iter; left time: 65.4194s\n",
      "\titers: 500, epoch: 14 | loss: 0.0657523\n",
      "\tspeed: 0.0182s/iter; left time: 63.5942s\n",
      "Epoch: 14 cost time: 10.650235414505005\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.0862531 Vali Loss: 0.0338502 Test Loss: 0.1164589\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11655744165182114, mae:0.20830540359020233\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2465455\n",
      "\tspeed: 0.0324s/iter; left time: 366.1038s\n",
      "\titers: 200, epoch: 1 | loss: 0.1742174\n",
      "\tspeed: 0.0198s/iter; left time: 221.3247s\n",
      "\titers: 300, epoch: 1 | loss: 0.2763492\n",
      "\tspeed: 0.0183s/iter; left time: 203.2631s\n",
      "\titers: 400, epoch: 1 | loss: 0.2178470\n",
      "\tspeed: 0.0183s/iter; left time: 201.3144s\n",
      "\titers: 500, epoch: 1 | loss: 0.2626901\n",
      "\tspeed: 0.0183s/iter; left time: 199.7825s\n",
      "Epoch: 1 cost time: 12.047883749008179\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2196197 Vali Loss: 0.0462815 Test Loss: 0.1330467\n",
      "Validation loss decreased (inf --> 0.046282).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2293462\n",
      "\tspeed: 0.0565s/iter; left time: 606.7097s\n",
      "\titers: 200, epoch: 2 | loss: 0.1865129\n",
      "\tspeed: 0.0182s/iter; left time: 193.7774s\n",
      "\titers: 300, epoch: 2 | loss: 0.3665048\n",
      "\tspeed: 0.0183s/iter; left time: 192.2446s\n",
      "\titers: 400, epoch: 2 | loss: 0.1814012\n",
      "\tspeed: 0.0182s/iter; left time: 190.1764s\n",
      "\titers: 500, epoch: 2 | loss: 0.1074840\n",
      "\tspeed: 0.0182s/iter; left time: 188.1639s\n",
      "Epoch: 2 cost time: 10.697796821594238\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1843974 Vali Loss: 0.0468489 Test Loss: 0.1434171\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1040126\n",
      "\tspeed: 0.0557s/iter; left time: 565.9234s\n",
      "\titers: 200, epoch: 3 | loss: 0.1511192\n",
      "\tspeed: 0.0203s/iter; left time: 204.6775s\n",
      "\titers: 300, epoch: 3 | loss: 0.1757332\n",
      "\tspeed: 0.0211s/iter; left time: 210.5087s\n",
      "\titers: 400, epoch: 3 | loss: 0.1398604\n",
      "\tspeed: 0.0211s/iter; left time: 207.8076s\n",
      "\titers: 500, epoch: 3 | loss: 0.1326717\n",
      "\tspeed: 0.0210s/iter; left time: 204.5822s\n",
      "Epoch: 3 cost time: 11.935862064361572\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1427732 Vali Loss: 0.0422266 Test Loss: 0.1323110\n",
      "Validation loss decreased (0.046282 --> 0.042227).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0990643\n",
      "\tspeed: 0.0609s/iter; left time: 583.8546s\n",
      "\titers: 200, epoch: 4 | loss: 0.1284356\n",
      "\tspeed: 0.0211s/iter; left time: 200.4051s\n",
      "\titers: 300, epoch: 4 | loss: 0.1189185\n",
      "\tspeed: 0.0211s/iter; left time: 197.8686s\n",
      "\titers: 400, epoch: 4 | loss: 0.1148597\n",
      "\tspeed: 0.0211s/iter; left time: 195.9909s\n",
      "\titers: 500, epoch: 4 | loss: 0.0875525\n",
      "\tspeed: 0.0212s/iter; left time: 194.6029s\n",
      "Epoch: 4 cost time: 12.266483068466187\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1152169 Vali Loss: 0.0403422 Test Loss: 0.1315215\n",
      "Validation loss decreased (0.042227 --> 0.040342).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0930144\n",
      "\tspeed: 0.0589s/iter; left time: 531.2176s\n",
      "\titers: 200, epoch: 5 | loss: 0.1091283\n",
      "\tspeed: 0.0204s/iter; left time: 182.1463s\n",
      "\titers: 300, epoch: 5 | loss: 0.0675936\n",
      "\tspeed: 0.0204s/iter; left time: 180.1528s\n",
      "\titers: 400, epoch: 5 | loss: 0.0938081\n",
      "\tspeed: 0.0204s/iter; left time: 177.9487s\n",
      "\titers: 500, epoch: 5 | loss: 0.0558497\n",
      "\tspeed: 0.0191s/iter; left time: 164.8052s\n",
      "Epoch: 5 cost time: 11.660802841186523\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1007919 Vali Loss: 0.0410147 Test Loss: 0.1349144\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1119724\n",
      "\tspeed: 0.0582s/iter; left time: 491.9399s\n",
      "\titers: 200, epoch: 6 | loss: 0.0632188\n",
      "\tspeed: 0.0181s/iter; left time: 151.5656s\n",
      "\titers: 300, epoch: 6 | loss: 0.1177115\n",
      "\tspeed: 0.0181s/iter; left time: 149.5166s\n",
      "\titers: 400, epoch: 6 | loss: 0.0846691\n",
      "\tspeed: 0.0181s/iter; left time: 147.6917s\n",
      "\titers: 500, epoch: 6 | loss: 0.0950823\n",
      "\tspeed: 0.0181s/iter; left time: 145.8109s\n",
      "Epoch: 6 cost time: 10.661482810974121\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0944268 Vali Loss: 0.0401300 Test Loss: 0.1352709\n",
      "Validation loss decreased (0.040342 --> 0.040130).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0799287\n",
      "\tspeed: 0.0571s/iter; left time: 450.1676s\n",
      "\titers: 200, epoch: 7 | loss: 0.0747626\n",
      "\tspeed: 0.0182s/iter; left time: 141.3067s\n",
      "\titers: 300, epoch: 7 | loss: 0.1034898\n",
      "\tspeed: 0.0182s/iter; left time: 139.5888s\n",
      "\titers: 400, epoch: 7 | loss: 0.0954349\n",
      "\tspeed: 0.0182s/iter; left time: 137.6212s\n",
      "\titers: 500, epoch: 7 | loss: 0.0578673\n",
      "\tspeed: 0.0181s/iter; left time: 135.7694s\n",
      "Epoch: 7 cost time: 10.66940450668335\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0923948 Vali Loss: 0.0397917 Test Loss: 0.1350439\n",
      "Validation loss decreased (0.040130 --> 0.039792).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0772300\n",
      "\tspeed: 0.0563s/iter; left time: 411.6069s\n",
      "\titers: 200, epoch: 8 | loss: 0.0666272\n",
      "\tspeed: 0.0181s/iter; left time: 130.6269s\n",
      "\titers: 300, epoch: 8 | loss: 0.0906120\n",
      "\tspeed: 0.0181s/iter; left time: 128.6785s\n",
      "\titers: 400, epoch: 8 | loss: 0.1087127\n",
      "\tspeed: 0.0181s/iter; left time: 126.8598s\n",
      "\titers: 500, epoch: 8 | loss: 0.0791122\n",
      "\tspeed: 0.0181s/iter; left time: 124.9690s\n",
      "Epoch: 8 cost time: 10.652371168136597\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0900067 Vali Loss: 0.0397980 Test Loss: 0.1351777\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0902390\n",
      "\tspeed: 0.0582s/iter; left time: 392.0867s\n",
      "\titers: 200, epoch: 9 | loss: 0.1244761\n",
      "\tspeed: 0.0182s/iter; left time: 121.1688s\n",
      "\titers: 300, epoch: 9 | loss: 0.1000864\n",
      "\tspeed: 0.0182s/iter; left time: 119.1941s\n",
      "\titers: 400, epoch: 9 | loss: 0.1546550\n",
      "\tspeed: 0.0182s/iter; left time: 117.4679s\n",
      "\titers: 500, epoch: 9 | loss: 0.0599694\n",
      "\tspeed: 0.0182s/iter; left time: 115.6064s\n",
      "Epoch: 9 cost time: 10.722315788269043\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0894911 Vali Loss: 0.0397211 Test Loss: 0.1346833\n",
      "Validation loss decreased (0.039792 --> 0.039721).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.0847079\n",
      "\tspeed: 0.0589s/iter; left time: 363.4001s\n",
      "\titers: 200, epoch: 10 | loss: 0.0722764\n",
      "\tspeed: 0.0182s/iter; left time: 110.5794s\n",
      "\titers: 300, epoch: 10 | loss: 0.0958148\n",
      "\tspeed: 0.0182s/iter; left time: 108.8148s\n",
      "\titers: 400, epoch: 10 | loss: 0.0959902\n",
      "\tspeed: 0.0182s/iter; left time: 106.8564s\n",
      "\titers: 500, epoch: 10 | loss: 0.0901856\n",
      "\tspeed: 0.0182s/iter; left time: 105.0173s\n",
      "Epoch: 10 cost time: 10.688402891159058\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0890617 Vali Loss: 0.0394520 Test Loss: 0.1355194\n",
      "Validation loss decreased (0.039721 --> 0.039452).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0883052\n",
      "\tspeed: 0.0586s/iter; left time: 328.1863s\n",
      "\titers: 200, epoch: 11 | loss: 0.0981652\n",
      "\tspeed: 0.0182s/iter; left time: 100.0518s\n",
      "\titers: 300, epoch: 11 | loss: 0.0668945\n",
      "\tspeed: 0.0182s/iter; left time: 98.2616s\n",
      "\titers: 400, epoch: 11 | loss: 0.1120059\n",
      "\tspeed: 0.0182s/iter; left time: 96.4966s\n",
      "\titers: 500, epoch: 11 | loss: 0.0745076\n",
      "\tspeed: 0.0182s/iter; left time: 94.5600s\n",
      "Epoch: 11 cost time: 10.71007227897644\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0892274 Vali Loss: 0.0395513 Test Loss: 0.1355649\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.0855307\n",
      "\tspeed: 0.0555s/iter; left time: 279.0495s\n",
      "\titers: 200, epoch: 12 | loss: 0.1050664\n",
      "\tspeed: 0.0182s/iter; left time: 89.7752s\n",
      "\titers: 300, epoch: 12 | loss: 0.0800592\n",
      "\tspeed: 0.0182s/iter; left time: 87.8551s\n",
      "\titers: 400, epoch: 12 | loss: 0.0910716\n",
      "\tspeed: 0.0182s/iter; left time: 86.0254s\n",
      "\titers: 500, epoch: 12 | loss: 0.0960518\n",
      "\tspeed: 0.0182s/iter; left time: 84.2407s\n",
      "Epoch: 12 cost time: 10.63636040687561\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0887536 Vali Loss: 0.0395426 Test Loss: 0.1356473\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.0758656\n",
      "\tspeed: 0.0548s/iter; left time: 244.3714s\n",
      "\titers: 200, epoch: 13 | loss: 0.0575939\n",
      "\tspeed: 0.0181s/iter; left time: 78.9768s\n",
      "\titers: 300, epoch: 13 | loss: 0.0811126\n",
      "\tspeed: 0.0181s/iter; left time: 77.1289s\n",
      "\titers: 400, epoch: 13 | loss: 0.0636982\n",
      "\tspeed: 0.0181s/iter; left time: 75.2688s\n",
      "\titers: 500, epoch: 13 | loss: 0.0718686\n",
      "\tspeed: 0.0181s/iter; left time: 73.4426s\n",
      "Epoch: 13 cost time: 10.613798141479492\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0880837 Vali Loss: 0.0396148 Test Loss: 0.1357707\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1356518715620041, mae:0.23420971632003784\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2044321\n",
      "\tspeed: 0.0348s/iter; left time: 393.7906s\n",
      "\titers: 200, epoch: 1 | loss: 0.1907370\n",
      "\tspeed: 0.0222s/iter; left time: 248.1550s\n",
      "\titers: 300, epoch: 1 | loss: 0.2209329\n",
      "\tspeed: 0.0222s/iter; left time: 246.3069s\n",
      "\titers: 400, epoch: 1 | loss: 0.3088663\n",
      "\tspeed: 0.0222s/iter; left time: 244.3902s\n",
      "\titers: 500, epoch: 1 | loss: 0.3752808\n",
      "\tspeed: 0.0222s/iter; left time: 242.0083s\n",
      "Epoch: 1 cost time: 13.964753866195679\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2527241 Vali Loss: 0.0623574 Test Loss: 0.1859929\n",
      "Validation loss decreased (inf --> 0.062357).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3072649\n",
      "\tspeed: 0.0677s/iter; left time: 726.4734s\n",
      "\titers: 200, epoch: 2 | loss: 0.2502496\n",
      "\tspeed: 0.0221s/iter; left time: 235.4238s\n",
      "\titers: 300, epoch: 2 | loss: 0.1918114\n",
      "\tspeed: 0.0221s/iter; left time: 232.5461s\n",
      "\titers: 400, epoch: 2 | loss: 0.3376081\n",
      "\tspeed: 0.0221s/iter; left time: 230.8149s\n",
      "\titers: 500, epoch: 2 | loss: 0.2363885\n",
      "\tspeed: 0.0221s/iter; left time: 228.6066s\n",
      "Epoch: 2 cost time: 12.943378210067749\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2683277 Vali Loss: 0.0661320 Test Loss: 0.1890021\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4923572\n",
      "\tspeed: 0.0629s/iter; left time: 639.4905s\n",
      "\titers: 200, epoch: 3 | loss: 0.1829364\n",
      "\tspeed: 0.0222s/iter; left time: 223.0454s\n",
      "\titers: 300, epoch: 3 | loss: 0.2508573\n",
      "\tspeed: 0.0222s/iter; left time: 220.7379s\n",
      "\titers: 400, epoch: 3 | loss: 0.2537628\n",
      "\tspeed: 0.0222s/iter; left time: 218.5207s\n",
      "\titers: 500, epoch: 3 | loss: 0.1679934\n",
      "\tspeed: 0.0222s/iter; left time: 216.8828s\n",
      "Epoch: 3 cost time: 12.941113233566284\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2455218 Vali Loss: 0.0570564 Test Loss: 0.1525956\n",
      "Validation loss decreased (0.062357 --> 0.057056).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1775687\n",
      "\tspeed: 0.0644s/iter; left time: 617.4761s\n",
      "\titers: 200, epoch: 4 | loss: 0.1459089\n",
      "\tspeed: 0.0237s/iter; left time: 224.5874s\n",
      "\titers: 300, epoch: 4 | loss: 0.2227755\n",
      "\tspeed: 0.0231s/iter; left time: 216.9227s\n",
      "\titers: 400, epoch: 4 | loss: 0.1869705\n",
      "\tspeed: 0.0222s/iter; left time: 206.1780s\n",
      "\titers: 500, epoch: 4 | loss: 0.2770719\n",
      "\tspeed: 0.0223s/iter; left time: 204.6866s\n",
      "Epoch: 4 cost time: 13.20621657371521\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2214344 Vali Loss: 0.0565339 Test Loss: 0.1514897\n",
      "Validation loss decreased (0.057056 --> 0.056534).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2272343\n",
      "\tspeed: 0.0642s/iter; left time: 578.9842s\n",
      "\titers: 200, epoch: 5 | loss: 0.1697168\n",
      "\tspeed: 0.0222s/iter; left time: 198.3256s\n",
      "\titers: 300, epoch: 5 | loss: 0.1575013\n",
      "\tspeed: 0.0222s/iter; left time: 196.0691s\n",
      "\titers: 400, epoch: 5 | loss: 0.2007736\n",
      "\tspeed: 0.0222s/iter; left time: 193.4654s\n",
      "\titers: 500, epoch: 5 | loss: 0.2119050\n",
      "\tspeed: 0.0222s/iter; left time: 191.3156s\n",
      "Epoch: 5 cost time: 12.96788763999939\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2082672 Vali Loss: 0.0547199 Test Loss: 0.1453940\n",
      "Validation loss decreased (0.056534 --> 0.054720).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1648124\n",
      "\tspeed: 0.0645s/iter; left time: 545.4505s\n",
      "\titers: 200, epoch: 6 | loss: 0.3070776\n",
      "\tspeed: 0.0221s/iter; left time: 184.8704s\n",
      "\titers: 300, epoch: 6 | loss: 0.1929084\n",
      "\tspeed: 0.0222s/iter; left time: 182.9220s\n",
      "\titers: 400, epoch: 6 | loss: 0.1536869\n",
      "\tspeed: 0.0221s/iter; left time: 180.3386s\n",
      "\titers: 500, epoch: 6 | loss: 0.2804589\n",
      "\tspeed: 0.0222s/iter; left time: 178.4076s\n",
      "Epoch: 6 cost time: 12.936241388320923\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2023327 Vali Loss: 0.0543583 Test Loss: 0.1462831\n",
      "Validation loss decreased (0.054720 --> 0.054358).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2472715\n",
      "\tspeed: 0.0630s/iter; left time: 496.2784s\n",
      "\titers: 200, epoch: 7 | loss: 0.1912365\n",
      "\tspeed: 0.0222s/iter; left time: 172.5475s\n",
      "\titers: 300, epoch: 7 | loss: 0.1569330\n",
      "\tspeed: 0.0222s/iter; left time: 170.2641s\n",
      "\titers: 400, epoch: 7 | loss: 0.1843344\n",
      "\tspeed: 0.0222s/iter; left time: 167.9421s\n",
      "\titers: 500, epoch: 7 | loss: 0.2040082\n",
      "\tspeed: 0.0222s/iter; left time: 165.7570s\n",
      "Epoch: 7 cost time: 12.928608179092407\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1976341 Vali Loss: 0.0550024 Test Loss: 0.1458890\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1246326\n",
      "\tspeed: 0.0623s/iter; left time: 455.4281s\n",
      "\titers: 200, epoch: 8 | loss: 0.4012249\n",
      "\tspeed: 0.0222s/iter; left time: 159.8991s\n",
      "\titers: 300, epoch: 8 | loss: 0.1279386\n",
      "\tspeed: 0.0222s/iter; left time: 157.5786s\n",
      "\titers: 400, epoch: 8 | loss: 0.1646966\n",
      "\tspeed: 0.0222s/iter; left time: 155.5474s\n",
      "\titers: 500, epoch: 8 | loss: 0.1723369\n",
      "\tspeed: 0.0222s/iter; left time: 153.1679s\n",
      "Epoch: 8 cost time: 12.937358617782593\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1953455 Vali Loss: 0.0543657 Test Loss: 0.1452569\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1745945\n",
      "\tspeed: 0.0636s/iter; left time: 428.4245s\n",
      "\titers: 200, epoch: 9 | loss: 0.1344898\n",
      "\tspeed: 0.0221s/iter; left time: 146.9791s\n",
      "\titers: 300, epoch: 9 | loss: 0.2274153\n",
      "\tspeed: 0.0221s/iter; left time: 144.2567s\n",
      "\titers: 400, epoch: 9 | loss: 0.1888725\n",
      "\tspeed: 0.0221s/iter; left time: 142.0337s\n",
      "\titers: 500, epoch: 9 | loss: 0.1694360\n",
      "\tspeed: 0.0221s/iter; left time: 139.8344s\n",
      "Epoch: 9 cost time: 13.030063152313232\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1939532 Vali Loss: 0.0545372 Test Loss: 0.1449776\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14646118879318237, mae:0.24671439826488495\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2160419\n",
      "\tspeed: 0.0332s/iter; left time: 374.8642s\n",
      "\titers: 200, epoch: 1 | loss: 0.1162213\n",
      "\tspeed: 0.0213s/iter; left time: 238.8341s\n",
      "\titers: 300, epoch: 1 | loss: 0.1844167\n",
      "\tspeed: 0.0213s/iter; left time: 236.5788s\n",
      "\titers: 400, epoch: 1 | loss: 0.1534699\n",
      "\tspeed: 0.0213s/iter; left time: 234.5786s\n",
      "\titers: 500, epoch: 1 | loss: 0.1646366\n",
      "\tspeed: 0.0213s/iter; left time: 232.5264s\n",
      "Epoch: 1 cost time: 13.395633697509766\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2125935 Vali Loss: 0.0405905 Test Loss: 0.1299499\n",
      "Validation loss decreased (inf --> 0.040591).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2203218\n",
      "\tspeed: 0.0624s/iter; left time: 669.8842s\n",
      "\titers: 200, epoch: 2 | loss: 0.2456415\n",
      "\tspeed: 0.0215s/iter; left time: 228.0922s\n",
      "\titers: 300, epoch: 2 | loss: 0.5099041\n",
      "\tspeed: 0.0214s/iter; left time: 225.5234s\n",
      "\titers: 400, epoch: 2 | loss: 0.2630272\n",
      "\tspeed: 0.0214s/iter; left time: 223.7348s\n",
      "\titers: 500, epoch: 2 | loss: 0.2143973\n",
      "\tspeed: 0.0214s/iter; left time: 221.5816s\n",
      "Epoch: 2 cost time: 12.534577369689941\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2649165 Vali Loss: 0.0637003 Test Loss: 0.1830065\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2386678\n",
      "\tspeed: 0.0625s/iter; left time: 635.1895s\n",
      "\titers: 200, epoch: 3 | loss: 0.2139221\n",
      "\tspeed: 0.0213s/iter; left time: 214.5852s\n",
      "\titers: 300, epoch: 3 | loss: 0.1634926\n",
      "\tspeed: 0.0213s/iter; left time: 212.3975s\n",
      "\titers: 400, epoch: 3 | loss: 0.1910556\n",
      "\tspeed: 0.0213s/iter; left time: 210.2869s\n",
      "\titers: 500, epoch: 3 | loss: 0.2340789\n",
      "\tspeed: 0.0213s/iter; left time: 208.0941s\n",
      "Epoch: 3 cost time: 12.42756175994873\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2446887 Vali Loss: 0.0576729 Test Loss: 0.1636531\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1887436\n",
      "\tspeed: 0.0635s/iter; left time: 609.0140s\n",
      "\titers: 200, epoch: 4 | loss: 0.2005028\n",
      "\tspeed: 0.0233s/iter; left time: 220.9902s\n",
      "\titers: 300, epoch: 4 | loss: 0.2122133\n",
      "\tspeed: 0.0233s/iter; left time: 218.6237s\n",
      "\titers: 400, epoch: 4 | loss: 0.2909198\n",
      "\tspeed: 0.0233s/iter; left time: 216.3010s\n",
      "\titers: 500, epoch: 4 | loss: 0.1725634\n",
      "\tspeed: 0.0233s/iter; left time: 213.9175s\n",
      "Epoch: 4 cost time: 13.571258783340454\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2258179 Vali Loss: 0.0539668 Test Loss: 0.1522814\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13016191124916077, mae:0.22739572823047638\n",
      ">>> LR=5e-4,DO=0.1,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2422605\n",
      "\tspeed: 0.0344s/iter; left time: 389.2361s\n",
      "\titers: 200, epoch: 1 | loss: 0.1829510\n",
      "\tspeed: 0.0222s/iter; left time: 249.0684s\n",
      "\titers: 300, epoch: 1 | loss: 0.2373776\n",
      "\tspeed: 0.0223s/iter; left time: 247.1206s\n",
      "\titers: 400, epoch: 1 | loss: 0.3229593\n",
      "\tspeed: 0.0223s/iter; left time: 244.8079s\n",
      "\titers: 500, epoch: 1 | loss: 0.4051868\n",
      "\tspeed: 0.0222s/iter; left time: 242.0636s\n",
      "Epoch: 1 cost time: 13.946796655654907\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2792140 Vali Loss: 0.0759798 Test Loss: 0.2162345\n",
      "Validation loss decreased (inf --> 0.075980).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2276985\n",
      "\tspeed: 0.0616s/iter; left time: 661.3075s\n",
      "\titers: 200, epoch: 2 | loss: 0.4476156\n",
      "\tspeed: 0.0203s/iter; left time: 215.5625s\n",
      "\titers: 300, epoch: 2 | loss: 0.2548041\n",
      "\tspeed: 0.0203s/iter; left time: 213.6173s\n",
      "\titers: 400, epoch: 2 | loss: 0.1816117\n",
      "\tspeed: 0.0203s/iter; left time: 211.4127s\n",
      "\titers: 500, epoch: 2 | loss: 0.1685787\n",
      "\tspeed: 0.0203s/iter; left time: 209.2311s\n",
      "Epoch: 2 cost time: 11.863142013549805\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2823152 Vali Loss: 0.0623125 Test Loss: 0.1725155\n",
      "Validation loss decreased (0.075980 --> 0.062312).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1956048\n",
      "\tspeed: 0.0625s/iter; left time: 635.3620s\n",
      "\titers: 200, epoch: 3 | loss: 0.1986732\n",
      "\tspeed: 0.0202s/iter; left time: 203.0452s\n",
      "\titers: 300, epoch: 3 | loss: 0.2300066\n",
      "\tspeed: 0.0202s/iter; left time: 201.4185s\n",
      "\titers: 400, epoch: 3 | loss: 0.2127498\n",
      "\tspeed: 0.0202s/iter; left time: 199.0181s\n",
      "\titers: 500, epoch: 3 | loss: 0.2054931\n",
      "\tspeed: 0.0202s/iter; left time: 196.9022s\n",
      "Epoch: 3 cost time: 11.829272508621216\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2416417 Vali Loss: 0.0628371 Test Loss: 0.1696558\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1648703\n",
      "\tspeed: 0.0590s/iter; left time: 565.5856s\n",
      "\titers: 200, epoch: 4 | loss: 0.4601944\n",
      "\tspeed: 0.0202s/iter; left time: 191.4833s\n",
      "\titers: 300, epoch: 4 | loss: 0.1544110\n",
      "\tspeed: 0.0202s/iter; left time: 189.3047s\n",
      "\titers: 400, epoch: 4 | loss: 0.2180265\n",
      "\tspeed: 0.0202s/iter; left time: 187.2272s\n",
      "\titers: 500, epoch: 4 | loss: 0.1886454\n",
      "\tspeed: 0.0201s/iter; left time: 185.0209s\n",
      "Epoch: 4 cost time: 11.774162769317627\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2286725 Vali Loss: 0.0597612 Test Loss: 0.1620128\n",
      "Validation loss decreased (0.062312 --> 0.059761).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1822467\n",
      "\tspeed: 0.0619s/iter; left time: 558.3106s\n",
      "\titers: 200, epoch: 5 | loss: 0.2959248\n",
      "\tspeed: 0.0216s/iter; left time: 192.9844s\n",
      "\titers: 300, epoch: 5 | loss: 0.1727977\n",
      "\tspeed: 0.0232s/iter; left time: 204.4879s\n",
      "\titers: 400, epoch: 5 | loss: 0.1904349\n",
      "\tspeed: 0.0232s/iter; left time: 202.3166s\n",
      "\titers: 500, epoch: 5 | loss: 0.2039821\n",
      "\tspeed: 0.0232s/iter; left time: 200.0187s\n",
      "Epoch: 5 cost time: 13.081918239593506\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2188623 Vali Loss: 0.0574483 Test Loss: 0.1568991\n",
      "Validation loss decreased (0.059761 --> 0.057448).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2260060\n",
      "\tspeed: 0.0670s/iter; left time: 566.1330s\n",
      "\titers: 200, epoch: 6 | loss: 0.2270817\n",
      "\tspeed: 0.0233s/iter; left time: 194.5966s\n",
      "\titers: 300, epoch: 6 | loss: 0.2142158\n",
      "\tspeed: 0.0232s/iter; left time: 191.5469s\n",
      "\titers: 400, epoch: 6 | loss: 0.1900075\n",
      "\tspeed: 0.0232s/iter; left time: 189.2476s\n",
      "\titers: 500, epoch: 6 | loss: 0.2526564\n",
      "\tspeed: 0.0232s/iter; left time: 186.6272s\n",
      "Epoch: 6 cost time: 13.553377389907837\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2127382 Vali Loss: 0.0570407 Test Loss: 0.1548477\n",
      "Validation loss decreased (0.057448 --> 0.057041).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1727831\n",
      "\tspeed: 0.0650s/iter; left time: 511.8919s\n",
      "\titers: 200, epoch: 7 | loss: 0.2374889\n",
      "\tspeed: 0.0232s/iter; left time: 180.3978s\n",
      "\titers: 300, epoch: 7 | loss: 0.2728970\n",
      "\tspeed: 0.0232s/iter; left time: 178.0920s\n",
      "\titers: 400, epoch: 7 | loss: 0.2354636\n",
      "\tspeed: 0.0232s/iter; left time: 176.0367s\n",
      "\titers: 500, epoch: 7 | loss: 0.1577698\n",
      "\tspeed: 0.0232s/iter; left time: 173.6389s\n",
      "Epoch: 7 cost time: 13.518157243728638\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2092387 Vali Loss: 0.0582214 Test Loss: 0.1559023\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2333621\n",
      "\tspeed: 0.0644s/iter; left time: 471.0419s\n",
      "\titers: 200, epoch: 8 | loss: 0.3190812\n",
      "\tspeed: 0.0232s/iter; left time: 167.0594s\n",
      "\titers: 300, epoch: 8 | loss: 0.2758017\n",
      "\tspeed: 0.0232s/iter; left time: 164.9546s\n",
      "\titers: 400, epoch: 8 | loss: 0.1584807\n",
      "\tspeed: 0.0231s/iter; left time: 162.3019s\n",
      "\titers: 500, epoch: 8 | loss: 0.2110446\n",
      "\tspeed: 0.0232s/iter; left time: 160.3319s\n",
      "Epoch: 8 cost time: 13.521850109100342\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2084264 Vali Loss: 0.0572500 Test Loss: 0.1541620\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1923905\n",
      "\tspeed: 0.0644s/iter; left time: 433.9296s\n",
      "\titers: 200, epoch: 9 | loss: 0.1711386\n",
      "\tspeed: 0.0232s/iter; left time: 153.7570s\n",
      "\titers: 300, epoch: 9 | loss: 0.2769994\n",
      "\tspeed: 0.0231s/iter; left time: 151.2511s\n",
      "\titers: 400, epoch: 9 | loss: 0.1681122\n",
      "\tspeed: 0.0231s/iter; left time: 148.8555s\n",
      "\titers: 500, epoch: 9 | loss: 0.1237359\n",
      "\tspeed: 0.0231s/iter; left time: 146.6113s\n",
      "Epoch: 9 cost time: 13.448994636535645\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2062991 Vali Loss: 0.0573906 Test Loss: 0.1547263\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1550510972738266, mae:0.25246042013168335\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1968861\n",
      "\tspeed: 0.0223s/iter; left time: 251.5105s\n",
      "\titers: 200, epoch: 1 | loss: 0.2074055\n",
      "\tspeed: 0.0105s/iter; left time: 117.7557s\n",
      "\titers: 300, epoch: 1 | loss: 0.2668394\n",
      "\tspeed: 0.0105s/iter; left time: 116.9208s\n",
      "\titers: 400, epoch: 1 | loss: 0.2510618\n",
      "\tspeed: 0.0105s/iter; left time: 115.7013s\n",
      "\titers: 500, epoch: 1 | loss: 0.1887305\n",
      "\tspeed: 0.0106s/iter; left time: 115.2722s\n",
      "Epoch: 1 cost time: 7.230496406555176\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2436748 Vali Loss: 0.0457816 Test Loss: 0.1451397\n",
      "Validation loss decreased (inf --> 0.045782).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3275155\n",
      "\tspeed: 0.0378s/iter; left time: 405.9966s\n",
      "\titers: 200, epoch: 2 | loss: 0.1967727\n",
      "\tspeed: 0.0106s/iter; left time: 112.3138s\n",
      "\titers: 300, epoch: 2 | loss: 0.1991920\n",
      "\tspeed: 0.0106s/iter; left time: 111.2752s\n",
      "\titers: 400, epoch: 2 | loss: 0.1718873\n",
      "\tspeed: 0.0106s/iter; left time: 110.3815s\n",
      "\titers: 500, epoch: 2 | loss: 0.2023968\n",
      "\tspeed: 0.0105s/iter; left time: 108.9513s\n",
      "Epoch: 2 cost time: 6.342720031738281\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2031243 Vali Loss: 0.0430505 Test Loss: 0.1352728\n",
      "Validation loss decreased (0.045782 --> 0.043050).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1456728\n",
      "\tspeed: 0.0371s/iter; left time: 377.1707s\n",
      "\titers: 200, epoch: 3 | loss: 0.2073755\n",
      "\tspeed: 0.0106s/iter; left time: 106.3063s\n",
      "\titers: 300, epoch: 3 | loss: 0.1619648\n",
      "\tspeed: 0.0106s/iter; left time: 105.7312s\n",
      "\titers: 400, epoch: 3 | loss: 0.1799439\n",
      "\tspeed: 0.0106s/iter; left time: 104.5877s\n",
      "\titers: 500, epoch: 3 | loss: 0.2282066\n",
      "\tspeed: 0.0106s/iter; left time: 103.2700s\n",
      "Epoch: 3 cost time: 6.340687036514282\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1790444 Vali Loss: 0.0378854 Test Loss: 0.1217924\n",
      "Validation loss decreased (0.043050 --> 0.037885).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1916265\n",
      "\tspeed: 0.0395s/iter; left time: 378.7161s\n",
      "\titers: 200, epoch: 4 | loss: 0.0987980\n",
      "\tspeed: 0.0119s/iter; left time: 112.9162s\n",
      "\titers: 300, epoch: 4 | loss: 0.1622420\n",
      "\tspeed: 0.0119s/iter; left time: 111.8076s\n",
      "\titers: 400, epoch: 4 | loss: 0.1659676\n",
      "\tspeed: 0.0119s/iter; left time: 110.3716s\n",
      "\titers: 500, epoch: 4 | loss: 0.1543243\n",
      "\tspeed: 0.0119s/iter; left time: 109.1986s\n",
      "Epoch: 4 cost time: 7.094507932662964\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1644141 Vali Loss: 0.0373587 Test Loss: 0.1246603\n",
      "Validation loss decreased (0.037885 --> 0.037359).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1373155\n",
      "\tspeed: 0.0377s/iter; left time: 340.4070s\n",
      "\titers: 200, epoch: 5 | loss: 0.1544386\n",
      "\tspeed: 0.0106s/iter; left time: 94.6474s\n",
      "\titers: 300, epoch: 5 | loss: 0.1008306\n",
      "\tspeed: 0.0106s/iter; left time: 93.6482s\n",
      "\titers: 400, epoch: 5 | loss: 0.1563086\n",
      "\tspeed: 0.0106s/iter; left time: 92.5716s\n",
      "\titers: 500, epoch: 5 | loss: 0.1853543\n",
      "\tspeed: 0.0106s/iter; left time: 91.0413s\n",
      "Epoch: 5 cost time: 6.3216705322265625\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1551671 Vali Loss: 0.0357990 Test Loss: 0.1252222\n",
      "Validation loss decreased (0.037359 --> 0.035799).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1380461\n",
      "\tspeed: 0.0362s/iter; left time: 306.3121s\n",
      "\titers: 200, epoch: 6 | loss: 0.1179587\n",
      "\tspeed: 0.0106s/iter; left time: 88.1776s\n",
      "\titers: 300, epoch: 6 | loss: 0.1492406\n",
      "\tspeed: 0.0105s/iter; left time: 86.5055s\n",
      "\titers: 400, epoch: 6 | loss: 0.1771906\n",
      "\tspeed: 0.0104s/iter; left time: 84.8026s\n",
      "\titers: 500, epoch: 6 | loss: 0.1527208\n",
      "\tspeed: 0.0104s/iter; left time: 83.8862s\n",
      "Epoch: 6 cost time: 6.280951023101807\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1508814 Vali Loss: 0.0361586 Test Loss: 0.1267179\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1690600\n",
      "\tspeed: 0.0367s/iter; left time: 289.2177s\n",
      "\titers: 200, epoch: 7 | loss: 0.2181375\n",
      "\tspeed: 0.0105s/iter; left time: 81.6168s\n",
      "\titers: 300, epoch: 7 | loss: 0.1134887\n",
      "\tspeed: 0.0105s/iter; left time: 80.4568s\n",
      "\titers: 400, epoch: 7 | loss: 0.1205991\n",
      "\tspeed: 0.0105s/iter; left time: 79.8276s\n",
      "\titers: 500, epoch: 7 | loss: 0.1330346\n",
      "\tspeed: 0.0105s/iter; left time: 78.6729s\n",
      "Epoch: 7 cost time: 6.260112762451172\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1488097 Vali Loss: 0.0360066 Test Loss: 0.1266913\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1359771\n",
      "\tspeed: 0.0368s/iter; left time: 269.0481s\n",
      "\titers: 200, epoch: 8 | loss: 0.1457441\n",
      "\tspeed: 0.0105s/iter; left time: 75.9177s\n",
      "\titers: 300, epoch: 8 | loss: 0.1882059\n",
      "\tspeed: 0.0105s/iter; left time: 74.9331s\n",
      "\titers: 400, epoch: 8 | loss: 0.2122235\n",
      "\tspeed: 0.0105s/iter; left time: 73.9300s\n",
      "\titers: 500, epoch: 8 | loss: 0.0939350\n",
      "\tspeed: 0.0105s/iter; left time: 72.8608s\n",
      "Epoch: 8 cost time: 6.31174373626709\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1469399 Vali Loss: 0.0357397 Test Loss: 0.1275342\n",
      "Validation loss decreased (0.035799 --> 0.035740).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2393412\n",
      "\tspeed: 0.0363s/iter; left time: 245.0087s\n",
      "\titers: 200, epoch: 9 | loss: 0.1287752\n",
      "\tspeed: 0.0106s/iter; left time: 70.2962s\n",
      "\titers: 300, epoch: 9 | loss: 0.0813232\n",
      "\tspeed: 0.0106s/iter; left time: 69.1659s\n",
      "\titers: 400, epoch: 9 | loss: 0.1237925\n",
      "\tspeed: 0.0106s/iter; left time: 68.1192s\n",
      "\titers: 500, epoch: 9 | loss: 0.1202961\n",
      "\tspeed: 0.0106s/iter; left time: 67.0384s\n",
      "Epoch: 9 cost time: 6.348202228546143\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1460428 Vali Loss: 0.0362367 Test Loss: 0.1276402\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1430824\n",
      "\tspeed: 0.0382s/iter; left time: 235.7023s\n",
      "\titers: 200, epoch: 10 | loss: 0.1595148\n",
      "\tspeed: 0.0105s/iter; left time: 63.7387s\n",
      "\titers: 300, epoch: 10 | loss: 0.1019429\n",
      "\tspeed: 0.0105s/iter; left time: 62.8777s\n",
      "\titers: 400, epoch: 10 | loss: 0.1213916\n",
      "\tspeed: 0.0105s/iter; left time: 61.7939s\n",
      "\titers: 500, epoch: 10 | loss: 0.2282617\n",
      "\tspeed: 0.0104s/iter; left time: 60.1725s\n",
      "Epoch: 10 cost time: 6.338618755340576\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1460244 Vali Loss: 0.0359605 Test Loss: 0.1277746\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1593354\n",
      "\tspeed: 0.0356s/iter; left time: 199.4401s\n",
      "\titers: 200, epoch: 11 | loss: 0.1470495\n",
      "\tspeed: 0.0105s/iter; left time: 57.5566s\n",
      "\titers: 300, epoch: 11 | loss: 0.1202945\n",
      "\tspeed: 0.0105s/iter; left time: 56.6788s\n",
      "\titers: 400, epoch: 11 | loss: 0.2002390\n",
      "\tspeed: 0.0105s/iter; left time: 55.6845s\n",
      "\titers: 500, epoch: 11 | loss: 0.1476796\n",
      "\tspeed: 0.0105s/iter; left time: 54.6795s\n",
      "Epoch: 11 cost time: 6.259170055389404\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1469775 Vali Loss: 0.0358628 Test Loss: 0.1277538\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1277076005935669, mae:0.22084355354309082\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2401721\n",
      "\tspeed: 0.0237s/iter; left time: 267.4126s\n",
      "\titers: 200, epoch: 1 | loss: 0.2424997\n",
      "\tspeed: 0.0112s/iter; left time: 125.6385s\n",
      "\titers: 300, epoch: 1 | loss: 0.3234658\n",
      "\tspeed: 0.0118s/iter; left time: 131.2988s\n",
      "\titers: 400, epoch: 1 | loss: 0.1961393\n",
      "\tspeed: 0.0118s/iter; left time: 130.1446s\n",
      "\titers: 500, epoch: 1 | loss: 0.1873902\n",
      "\tspeed: 0.0107s/iter; left time: 116.8745s\n",
      "Epoch: 1 cost time: 7.704045057296753\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2151568 Vali Loss: 0.0361725 Test Loss: 0.1191960\n",
      "Validation loss decreased (inf --> 0.036173).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1610263\n",
      "\tspeed: 0.0373s/iter; left time: 400.4697s\n",
      "\titers: 200, epoch: 2 | loss: 0.2114646\n",
      "\tspeed: 0.0104s/iter; left time: 110.9437s\n",
      "\titers: 300, epoch: 2 | loss: 0.1568907\n",
      "\tspeed: 0.0104s/iter; left time: 109.1707s\n",
      "\titers: 400, epoch: 2 | loss: 0.1601709\n",
      "\tspeed: 0.0103s/iter; left time: 107.6690s\n",
      "\titers: 500, epoch: 2 | loss: 0.1835256\n",
      "\tspeed: 0.0103s/iter; left time: 106.0008s\n",
      "Epoch: 2 cost time: 6.210586071014404\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1871854 Vali Loss: 0.0372932 Test Loss: 0.1156548\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.0921855\n",
      "\tspeed: 0.0381s/iter; left time: 387.5690s\n",
      "\titers: 200, epoch: 3 | loss: 0.1439721\n",
      "\tspeed: 0.0110s/iter; left time: 110.1907s\n",
      "\titers: 300, epoch: 3 | loss: 0.2918147\n",
      "\tspeed: 0.0104s/iter; left time: 103.5695s\n",
      "\titers: 400, epoch: 3 | loss: 0.1909704\n",
      "\tspeed: 0.0104s/iter; left time: 102.6474s\n",
      "\titers: 500, epoch: 3 | loss: 0.1056564\n",
      "\tspeed: 0.0104s/iter; left time: 101.1697s\n",
      "Epoch: 3 cost time: 6.407537221908569\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1657281 Vali Loss: 0.0321570 Test Loss: 0.1056891\n",
      "Validation loss decreased (0.036173 --> 0.032157).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1245359\n",
      "\tspeed: 0.0368s/iter; left time: 353.3772s\n",
      "\titers: 200, epoch: 4 | loss: 0.1304310\n",
      "\tspeed: 0.0114s/iter; left time: 107.8577s\n",
      "\titers: 300, epoch: 4 | loss: 0.0928682\n",
      "\tspeed: 0.0109s/iter; left time: 102.7018s\n",
      "\titers: 400, epoch: 4 | loss: 0.1208657\n",
      "\tspeed: 0.0103s/iter; left time: 95.4802s\n",
      "\titers: 500, epoch: 4 | loss: 0.1988564\n",
      "\tspeed: 0.0103s/iter; left time: 94.3402s\n",
      "Epoch: 4 cost time: 6.369927883148193\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1537255 Vali Loss: 0.0323878 Test Loss: 0.1043980\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1135346\n",
      "\tspeed: 0.0396s/iter; left time: 357.1419s\n",
      "\titers: 200, epoch: 5 | loss: 0.1933572\n",
      "\tspeed: 0.0118s/iter; left time: 105.3130s\n",
      "\titers: 300, epoch: 5 | loss: 0.1349498\n",
      "\tspeed: 0.0118s/iter; left time: 104.0932s\n",
      "\titers: 400, epoch: 5 | loss: 0.1440819\n",
      "\tspeed: 0.0118s/iter; left time: 102.9093s\n",
      "\titers: 500, epoch: 5 | loss: 0.1472165\n",
      "\tspeed: 0.0118s/iter; left time: 101.7130s\n",
      "Epoch: 5 cost time: 7.020961046218872\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1476107 Vali Loss: 0.0316899 Test Loss: 0.1041419\n",
      "Validation loss decreased (0.032157 --> 0.031690).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1084595\n",
      "\tspeed: 0.0402s/iter; left time: 340.0537s\n",
      "\titers: 200, epoch: 6 | loss: 0.1837830\n",
      "\tspeed: 0.0118s/iter; left time: 98.8695s\n",
      "\titers: 300, epoch: 6 | loss: 0.1503467\n",
      "\tspeed: 0.0118s/iter; left time: 97.6613s\n",
      "\titers: 400, epoch: 6 | loss: 0.1284380\n",
      "\tspeed: 0.0118s/iter; left time: 96.4747s\n",
      "\titers: 500, epoch: 6 | loss: 0.1001743\n",
      "\tspeed: 0.0118s/iter; left time: 95.2110s\n",
      "Epoch: 6 cost time: 7.043618679046631\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1432669 Vali Loss: 0.0317257 Test Loss: 0.1050080\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1394527\n",
      "\tspeed: 0.0388s/iter; left time: 305.9851s\n",
      "\titers: 200, epoch: 7 | loss: 0.1708174\n",
      "\tspeed: 0.0118s/iter; left time: 91.9604s\n",
      "\titers: 300, epoch: 7 | loss: 0.2254554\n",
      "\tspeed: 0.0118s/iter; left time: 90.5479s\n",
      "\titers: 400, epoch: 7 | loss: 0.1847659\n",
      "\tspeed: 0.0118s/iter; left time: 89.4757s\n",
      "\titers: 500, epoch: 7 | loss: 0.1539829\n",
      "\tspeed: 0.0118s/iter; left time: 88.4636s\n",
      "Epoch: 7 cost time: 7.045394420623779\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1421129 Vali Loss: 0.0313872 Test Loss: 0.1043943\n",
      "Validation loss decreased (0.031690 --> 0.031387).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1260215\n",
      "\tspeed: 0.0394s/iter; left time: 288.0099s\n",
      "\titers: 200, epoch: 8 | loss: 0.0947148\n",
      "\tspeed: 0.0118s/iter; left time: 85.0812s\n",
      "\titers: 300, epoch: 8 | loss: 0.1159244\n",
      "\tspeed: 0.0118s/iter; left time: 83.8805s\n",
      "\titers: 400, epoch: 8 | loss: 0.1393307\n",
      "\tspeed: 0.0118s/iter; left time: 82.9178s\n",
      "\titers: 500, epoch: 8 | loss: 0.0789673\n",
      "\tspeed: 0.0118s/iter; left time: 81.6546s\n",
      "Epoch: 8 cost time: 7.062925100326538\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1416707 Vali Loss: 0.0313756 Test Loss: 0.1045958\n",
      "Validation loss decreased (0.031387 --> 0.031376).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1174904\n",
      "\tspeed: 0.0399s/iter; left time: 268.8533s\n",
      "\titers: 200, epoch: 9 | loss: 0.1468606\n",
      "\tspeed: 0.0119s/iter; left time: 78.7486s\n",
      "\titers: 300, epoch: 9 | loss: 0.1139230\n",
      "\tspeed: 0.0119s/iter; left time: 77.8259s\n",
      "\titers: 400, epoch: 9 | loss: 0.1485003\n",
      "\tspeed: 0.0119s/iter; left time: 76.7478s\n",
      "\titers: 500, epoch: 9 | loss: 0.1246711\n",
      "\tspeed: 0.0110s/iter; left time: 70.0216s\n",
      "Epoch: 9 cost time: 6.896507501602173\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1395523 Vali Loss: 0.0313584 Test Loss: 0.1043669\n",
      "Validation loss decreased (0.031376 --> 0.031358).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1400148\n",
      "\tspeed: 0.0389s/iter; left time: 240.3028s\n",
      "\titers: 200, epoch: 10 | loss: 0.1162626\n",
      "\tspeed: 0.0117s/iter; left time: 71.2624s\n",
      "\titers: 300, epoch: 10 | loss: 0.1198061\n",
      "\tspeed: 0.0119s/iter; left time: 70.9776s\n",
      "\titers: 400, epoch: 10 | loss: 0.1005436\n",
      "\tspeed: 0.0119s/iter; left time: 69.6409s\n",
      "\titers: 500, epoch: 10 | loss: 0.1011312\n",
      "\tspeed: 0.0119s/iter; left time: 68.7025s\n",
      "Epoch: 10 cost time: 7.020002841949463\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1411722 Vali Loss: 0.0313830 Test Loss: 0.1042795\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1380363\n",
      "\tspeed: 0.0370s/iter; left time: 207.3663s\n",
      "\titers: 200, epoch: 11 | loss: 0.1670581\n",
      "\tspeed: 0.0104s/iter; left time: 57.0864s\n",
      "\titers: 300, epoch: 11 | loss: 0.1588926\n",
      "\tspeed: 0.0104s/iter; left time: 55.9736s\n",
      "\titers: 400, epoch: 11 | loss: 0.1483600\n",
      "\tspeed: 0.0103s/iter; left time: 54.8022s\n",
      "\titers: 500, epoch: 11 | loss: 0.1227315\n",
      "\tspeed: 0.0103s/iter; left time: 53.5605s\n",
      "Epoch: 11 cost time: 6.219078779220581\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1392189 Vali Loss: 0.0312490 Test Loss: 0.1042706\n",
      "Validation loss decreased (0.031358 --> 0.031249).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1175639\n",
      "\tspeed: 0.0373s/iter; left time: 187.7327s\n",
      "\titers: 200, epoch: 12 | loss: 0.1478161\n",
      "\tspeed: 0.0116s/iter; left time: 57.2001s\n",
      "\titers: 300, epoch: 12 | loss: 0.1135787\n",
      "\tspeed: 0.0104s/iter; left time: 50.1579s\n",
      "\titers: 400, epoch: 12 | loss: 0.1800499\n",
      "\tspeed: 0.0104s/iter; left time: 49.0769s\n",
      "\titers: 500, epoch: 12 | loss: 0.1252864\n",
      "\tspeed: 0.0104s/iter; left time: 48.0171s\n",
      "Epoch: 12 cost time: 6.45947003364563\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1398091 Vali Loss: 0.0313090 Test Loss: 0.1042849\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2295697\n",
      "\tspeed: 0.0367s/iter; left time: 163.7777s\n",
      "\titers: 200, epoch: 13 | loss: 0.1836542\n",
      "\tspeed: 0.0104s/iter; left time: 45.2191s\n",
      "\titers: 300, epoch: 13 | loss: 0.1145843\n",
      "\tspeed: 0.0104s/iter; left time: 44.1456s\n",
      "\titers: 400, epoch: 13 | loss: 0.1641874\n",
      "\tspeed: 0.0103s/iter; left time: 42.8174s\n",
      "\titers: 500, epoch: 13 | loss: 0.1717600\n",
      "\tspeed: 0.0103s/iter; left time: 41.7636s\n",
      "Epoch: 13 cost time: 6.220175266265869\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1398790 Vali Loss: 0.0312312 Test Loss: 0.1042820\n",
      "Validation loss decreased (0.031249 --> 0.031231).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1311188\n",
      "\tspeed: 0.0369s/iter; left time: 143.5396s\n",
      "\titers: 200, epoch: 14 | loss: 0.1039703\n",
      "\tspeed: 0.0114s/iter; left time: 43.3602s\n",
      "\titers: 300, epoch: 14 | loss: 0.1311249\n",
      "\tspeed: 0.0118s/iter; left time: 43.6976s\n",
      "\titers: 400, epoch: 14 | loss: 0.1338442\n",
      "\tspeed: 0.0118s/iter; left time: 42.4576s\n",
      "\titers: 500, epoch: 14 | loss: 0.1419042\n",
      "\tspeed: 0.0118s/iter; left time: 41.1697s\n",
      "Epoch: 14 cost time: 6.883718729019165\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1398163 Vali Loss: 0.0312761 Test Loss: 0.1042791\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.0882273\n",
      "\tspeed: 0.0397s/iter; left time: 131.9156s\n",
      "\titers: 200, epoch: 15 | loss: 0.1478414\n",
      "\tspeed: 0.0119s/iter; left time: 38.2137s\n",
      "\titers: 300, epoch: 15 | loss: 0.1274982\n",
      "\tspeed: 0.0118s/iter; left time: 36.8663s\n",
      "\titers: 400, epoch: 15 | loss: 0.1245105\n",
      "\tspeed: 0.0118s/iter; left time: 35.7140s\n",
      "\titers: 500, epoch: 15 | loss: 0.1308725\n",
      "\tspeed: 0.0118s/iter; left time: 34.5295s\n",
      "Epoch: 15 cost time: 7.033627986907959\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1395391 Vali Loss: 0.0312022 Test Loss: 0.1042811\n",
      "Validation loss decreased (0.031231 --> 0.031202).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.2173437\n",
      "\tspeed: 0.0382s/iter; left time: 105.1200s\n",
      "\titers: 200, epoch: 16 | loss: 0.1206843\n",
      "\tspeed: 0.0102s/iter; left time: 27.1419s\n",
      "\titers: 300, epoch: 16 | loss: 0.1893770\n",
      "\tspeed: 0.0118s/iter; left time: 30.0912s\n",
      "\titers: 400, epoch: 16 | loss: 0.1291029\n",
      "\tspeed: 0.0118s/iter; left time: 28.8636s\n",
      "\titers: 500, epoch: 16 | loss: 0.2286672\n",
      "\tspeed: 0.0118s/iter; left time: 27.7066s\n",
      "Epoch: 16 cost time: 6.72833776473999\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1400929 Vali Loss: 0.0311803 Test Loss: 0.1042822\n",
      "Validation loss decreased (0.031202 --> 0.031180).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1043478\n",
      "\tspeed: 0.0388s/iter; left time: 84.5351s\n",
      "\titers: 200, epoch: 17 | loss: 0.1241303\n",
      "\tspeed: 0.0118s/iter; left time: 24.5320s\n",
      "\titers: 300, epoch: 17 | loss: 0.1408147\n",
      "\tspeed: 0.0118s/iter; left time: 23.3598s\n",
      "\titers: 400, epoch: 17 | loss: 0.1139410\n",
      "\tspeed: 0.0118s/iter; left time: 22.1875s\n",
      "\titers: 500, epoch: 17 | loss: 0.0959115\n",
      "\tspeed: 0.0111s/iter; left time: 19.7775s\n",
      "Epoch: 17 cost time: 6.854817628860474\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1397438 Vali Loss: 0.0313544 Test Loss: 0.1042828\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1547170\n",
      "\tspeed: 0.0370s/iter; left time: 59.5919s\n",
      "\titers: 200, epoch: 18 | loss: 0.1407096\n",
      "\tspeed: 0.0103s/iter; left time: 15.5392s\n",
      "\titers: 300, epoch: 18 | loss: 0.1202417\n",
      "\tspeed: 0.0111s/iter; left time: 15.6139s\n",
      "\titers: 400, epoch: 18 | loss: 0.2134871\n",
      "\tspeed: 0.0103s/iter; left time: 13.5048s\n",
      "\titers: 500, epoch: 18 | loss: 0.1240891\n",
      "\tspeed: 0.0103s/iter; left time: 12.4882s\n",
      "Epoch: 18 cost time: 6.249206304550171\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.1396246 Vali Loss: 0.0312675 Test Loss: 0.1042829\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.814697265625e-09\n",
      "\titers: 100, epoch: 19 | loss: 0.1086819\n",
      "\tspeed: 0.0373s/iter; left time: 38.8543s\n",
      "\titers: 200, epoch: 19 | loss: 0.1497446\n",
      "\tspeed: 0.0104s/iter; left time: 9.7665s\n",
      "\titers: 300, epoch: 19 | loss: 0.1158870\n",
      "\tspeed: 0.0109s/iter; left time: 9.1455s\n",
      "\titers: 400, epoch: 19 | loss: 0.1645359\n",
      "\tspeed: 0.0116s/iter; left time: 8.5938s\n",
      "\titers: 500, epoch: 19 | loss: 0.1304467\n",
      "\tspeed: 0.0116s/iter; left time: 7.4132s\n",
      "Epoch: 19 cost time: 6.631243944168091\n",
      "Epoch: 19, Steps: 570 | Train Loss: 0.1395322 Vali Loss: 0.0313423 Test Loss: 0.1042826\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1044272854924202, mae:0.19677524268627167\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2458244\n",
      "\tspeed: 0.0222s/iter; left time: 250.5426s\n",
      "\titers: 200, epoch: 1 | loss: 0.1580599\n",
      "\tspeed: 0.0103s/iter; left time: 115.7182s\n",
      "\titers: 300, epoch: 1 | loss: 0.1607421\n",
      "\tspeed: 0.0104s/iter; left time: 114.9211s\n",
      "\titers: 400, epoch: 1 | loss: 0.1947172\n",
      "\tspeed: 0.0103s/iter; left time: 113.5780s\n",
      "\titers: 500, epoch: 1 | loss: 0.2462651\n",
      "\tspeed: 0.0103s/iter; left time: 112.6955s\n",
      "Epoch: 1 cost time: 7.1285459995269775\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2496192 Vali Loss: 0.0448467 Test Loss: 0.1367395\n",
      "Validation loss decreased (inf --> 0.044847).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2090575\n",
      "\tspeed: 0.0364s/iter; left time: 390.2529s\n",
      "\titers: 200, epoch: 2 | loss: 0.2855562\n",
      "\tspeed: 0.0104s/iter; left time: 110.2118s\n",
      "\titers: 300, epoch: 2 | loss: 0.1796440\n",
      "\tspeed: 0.0103s/iter; left time: 108.6663s\n",
      "\titers: 400, epoch: 2 | loss: 0.3255115\n",
      "\tspeed: 0.0103s/iter; left time: 107.8336s\n",
      "\titers: 500, epoch: 2 | loss: 0.2540377\n",
      "\tspeed: 0.0103s/iter; left time: 106.7753s\n",
      "Epoch: 2 cost time: 6.18909215927124\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2090411 Vali Loss: 0.0425315 Test Loss: 0.1379918\n",
      "Validation loss decreased (0.044847 --> 0.042531).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1236370\n",
      "\tspeed: 0.0376s/iter; left time: 381.8572s\n",
      "\titers: 200, epoch: 3 | loss: 0.1724854\n",
      "\tspeed: 0.0103s/iter; left time: 103.9433s\n",
      "\titers: 300, epoch: 3 | loss: 0.2191168\n",
      "\tspeed: 0.0104s/iter; left time: 103.2758s\n",
      "\titers: 400, epoch: 3 | loss: 0.1433497\n",
      "\tspeed: 0.0103s/iter; left time: 101.7128s\n",
      "\titers: 500, epoch: 3 | loss: 0.1347058\n",
      "\tspeed: 0.0103s/iter; left time: 100.5048s\n",
      "Epoch: 3 cost time: 6.29404354095459\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1822572 Vali Loss: 0.0381314 Test Loss: 0.1287609\n",
      "Validation loss decreased (0.042531 --> 0.038131).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.0993757\n",
      "\tspeed: 0.0369s/iter; left time: 353.6213s\n",
      "\titers: 200, epoch: 4 | loss: 0.1030221\n",
      "\tspeed: 0.0103s/iter; left time: 97.9425s\n",
      "\titers: 300, epoch: 4 | loss: 0.1255772\n",
      "\tspeed: 0.0103s/iter; left time: 96.9451s\n",
      "\titers: 400, epoch: 4 | loss: 0.1874091\n",
      "\tspeed: 0.0103s/iter; left time: 95.8324s\n",
      "\titers: 500, epoch: 4 | loss: 0.1362787\n",
      "\tspeed: 0.0103s/iter; left time: 94.9608s\n",
      "Epoch: 4 cost time: 6.180901765823364\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1654112 Vali Loss: 0.0390523 Test Loss: 0.1291916\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1629340\n",
      "\tspeed: 0.0353s/iter; left time: 318.3818s\n",
      "\titers: 200, epoch: 5 | loss: 0.1039051\n",
      "\tspeed: 0.0102s/iter; left time: 91.3596s\n",
      "\titers: 300, epoch: 5 | loss: 0.1660782\n",
      "\tspeed: 0.0103s/iter; left time: 90.7414s\n",
      "\titers: 400, epoch: 5 | loss: 0.1498400\n",
      "\tspeed: 0.0103s/iter; left time: 89.6274s\n",
      "\titers: 500, epoch: 5 | loss: 0.2172457\n",
      "\tspeed: 0.0102s/iter; left time: 88.2993s\n",
      "Epoch: 5 cost time: 6.123126745223999\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1577477 Vali Loss: 0.0375868 Test Loss: 0.1292249\n",
      "Validation loss decreased (0.038131 --> 0.037587).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1206211\n",
      "\tspeed: 0.0376s/iter; left time: 317.9655s\n",
      "\titers: 200, epoch: 6 | loss: 0.1372995\n",
      "\tspeed: 0.0116s/iter; left time: 96.8615s\n",
      "\titers: 300, epoch: 6 | loss: 0.1432529\n",
      "\tspeed: 0.0116s/iter; left time: 95.8719s\n",
      "\titers: 400, epoch: 6 | loss: 0.1195650\n",
      "\tspeed: 0.0116s/iter; left time: 94.6313s\n",
      "\titers: 500, epoch: 6 | loss: 0.1516001\n",
      "\tspeed: 0.0107s/iter; left time: 86.0623s\n",
      "Epoch: 6 cost time: 6.7443482875823975\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1522886 Vali Loss: 0.0371695 Test Loss: 0.1265572\n",
      "Validation loss decreased (0.037587 --> 0.037169).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1134902\n",
      "\tspeed: 0.0374s/iter; left time: 294.9613s\n",
      "\titers: 200, epoch: 7 | loss: 0.1240653\n",
      "\tspeed: 0.0103s/iter; left time: 80.3330s\n",
      "\titers: 300, epoch: 7 | loss: 0.1308912\n",
      "\tspeed: 0.0103s/iter; left time: 79.4471s\n",
      "\titers: 400, epoch: 7 | loss: 0.1020128\n",
      "\tspeed: 0.0103s/iter; left time: 78.4002s\n",
      "\titers: 500, epoch: 7 | loss: 0.1568721\n",
      "\tspeed: 0.0103s/iter; left time: 77.2060s\n",
      "Epoch: 7 cost time: 6.162172555923462\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1493799 Vali Loss: 0.0368687 Test Loss: 0.1266420\n",
      "Validation loss decreased (0.037169 --> 0.036869).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1191704\n",
      "\tspeed: 0.0355s/iter; left time: 259.8620s\n",
      "\titers: 200, epoch: 8 | loss: 0.1438675\n",
      "\tspeed: 0.0103s/iter; left time: 74.5605s\n",
      "\titers: 300, epoch: 8 | loss: 0.1819857\n",
      "\tspeed: 0.0103s/iter; left time: 73.5334s\n",
      "\titers: 400, epoch: 8 | loss: 0.1481585\n",
      "\tspeed: 0.0104s/iter; left time: 72.5717s\n",
      "\titers: 500, epoch: 8 | loss: 0.1317616\n",
      "\tspeed: 0.0103s/iter; left time: 71.4538s\n",
      "Epoch: 8 cost time: 6.203297853469849\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1479293 Vali Loss: 0.0369548 Test Loss: 0.1264471\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1167659\n",
      "\tspeed: 0.0371s/iter; left time: 250.1164s\n",
      "\titers: 200, epoch: 9 | loss: 0.1701135\n",
      "\tspeed: 0.0104s/iter; left time: 69.0401s\n",
      "\titers: 300, epoch: 9 | loss: 0.1141123\n",
      "\tspeed: 0.0103s/iter; left time: 67.5538s\n",
      "\titers: 400, epoch: 9 | loss: 0.1052207\n",
      "\tspeed: 0.0103s/iter; left time: 66.4365s\n",
      "\titers: 500, epoch: 9 | loss: 0.1552950\n",
      "\tspeed: 0.0103s/iter; left time: 65.6153s\n",
      "Epoch: 9 cost time: 6.176050424575806\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1471686 Vali Loss: 0.0371080 Test Loss: 0.1267642\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1323274\n",
      "\tspeed: 0.0378s/iter; left time: 233.0438s\n",
      "\titers: 200, epoch: 10 | loss: 0.2113402\n",
      "\tspeed: 0.0104s/iter; left time: 63.1022s\n",
      "\titers: 300, epoch: 10 | loss: 0.2047139\n",
      "\tspeed: 0.0104s/iter; left time: 61.8963s\n",
      "\titers: 400, epoch: 10 | loss: 0.1622301\n",
      "\tspeed: 0.0104s/iter; left time: 60.8663s\n",
      "\titers: 500, epoch: 10 | loss: 0.1510198\n",
      "\tspeed: 0.0104s/iter; left time: 60.0541s\n",
      "Epoch: 10 cost time: 6.226138353347778\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1486454 Vali Loss: 0.0370910 Test Loss: 0.1269608\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12679612636566162, mae:0.22187824547290802\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3012963\n",
      "\tspeed: 0.0236s/iter; left time: 266.2320s\n",
      "\titers: 200, epoch: 1 | loss: 0.2677741\n",
      "\tspeed: 0.0118s/iter; left time: 132.7142s\n",
      "\titers: 300, epoch: 1 | loss: 0.1750219\n",
      "\tspeed: 0.0119s/iter; left time: 131.6591s\n",
      "\titers: 400, epoch: 1 | loss: 0.1297865\n",
      "\tspeed: 0.0118s/iter; left time: 130.3185s\n",
      "\titers: 500, epoch: 1 | loss: 0.2133226\n",
      "\tspeed: 0.0118s/iter; left time: 129.1650s\n",
      "Epoch: 1 cost time: 7.972729921340942\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2574803 Vali Loss: 0.0458579 Test Loss: 0.1352860\n",
      "Validation loss decreased (inf --> 0.045858).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1982875\n",
      "\tspeed: 0.0409s/iter; left time: 439.1126s\n",
      "\titers: 200, epoch: 2 | loss: 0.1828269\n",
      "\tspeed: 0.0118s/iter; left time: 125.3966s\n",
      "\titers: 300, epoch: 2 | loss: 0.2175472\n",
      "\tspeed: 0.0118s/iter; left time: 124.1808s\n",
      "\titers: 400, epoch: 2 | loss: 0.1286643\n",
      "\tspeed: 0.0118s/iter; left time: 123.0639s\n",
      "\titers: 500, epoch: 2 | loss: 0.2117110\n",
      "\tspeed: 0.0118s/iter; left time: 121.8574s\n",
      "Epoch: 2 cost time: 7.171096324920654\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2258754 Vali Loss: 0.0447057 Test Loss: 0.1362181\n",
      "Validation loss decreased (0.045858 --> 0.044706).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2756763\n",
      "\tspeed: 0.0400s/iter; left time: 406.8645s\n",
      "\titers: 200, epoch: 3 | loss: 0.2064056\n",
      "\tspeed: 0.0128s/iter; left time: 128.3263s\n",
      "\titers: 300, epoch: 3 | loss: 0.1095314\n",
      "\tspeed: 0.0118s/iter; left time: 117.9139s\n",
      "\titers: 400, epoch: 3 | loss: 0.2068678\n",
      "\tspeed: 0.0119s/iter; left time: 117.2749s\n",
      "\titers: 500, epoch: 3 | loss: 0.2011012\n",
      "\tspeed: 0.0118s/iter; left time: 115.3225s\n",
      "Epoch: 3 cost time: 7.277749300003052\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2037236 Vali Loss: 0.0462443 Test Loss: 0.1363997\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1503361\n",
      "\tspeed: 0.0395s/iter; left time: 378.3864s\n",
      "\titers: 200, epoch: 4 | loss: 0.2408571\n",
      "\tspeed: 0.0119s/iter; left time: 112.5396s\n",
      "\titers: 300, epoch: 4 | loss: 0.1057899\n",
      "\tspeed: 0.0119s/iter; left time: 111.3208s\n",
      "\titers: 400, epoch: 4 | loss: 0.1237857\n",
      "\tspeed: 0.0118s/iter; left time: 110.0956s\n",
      "\titers: 500, epoch: 4 | loss: 0.1619836\n",
      "\tspeed: 0.0118s/iter; left time: 108.9041s\n",
      "Epoch: 4 cost time: 7.0587286949157715\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1848698 Vali Loss: 0.0414682 Test Loss: 0.1244780\n",
      "Validation loss decreased (0.044706 --> 0.041468).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1547678\n",
      "\tspeed: 0.0397s/iter; left time: 358.3900s\n",
      "\titers: 200, epoch: 5 | loss: 0.1490418\n",
      "\tspeed: 0.0118s/iter; left time: 105.3133s\n",
      "\titers: 300, epoch: 5 | loss: 0.2137638\n",
      "\tspeed: 0.0118s/iter; left time: 104.1030s\n",
      "\titers: 400, epoch: 5 | loss: 0.1746981\n",
      "\tspeed: 0.0118s/iter; left time: 102.9757s\n",
      "\titers: 500, epoch: 5 | loss: 0.1618993\n",
      "\tspeed: 0.0118s/iter; left time: 101.7525s\n",
      "Epoch: 5 cost time: 7.062108039855957\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1739019 Vali Loss: 0.0393822 Test Loss: 0.1224011\n",
      "Validation loss decreased (0.041468 --> 0.039382).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1413294\n",
      "\tspeed: 0.0397s/iter; left time: 335.6641s\n",
      "\titers: 200, epoch: 6 | loss: 0.0935599\n",
      "\tspeed: 0.0119s/iter; left time: 99.3474s\n",
      "\titers: 300, epoch: 6 | loss: 0.2263413\n",
      "\tspeed: 0.0119s/iter; left time: 98.0174s\n",
      "\titers: 400, epoch: 6 | loss: 0.1823796\n",
      "\tspeed: 0.0119s/iter; left time: 96.7480s\n",
      "\titers: 500, epoch: 6 | loss: 0.1201744\n",
      "\tspeed: 0.0119s/iter; left time: 95.6212s\n",
      "Epoch: 6 cost time: 7.0867040157318115\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1673662 Vali Loss: 0.0386579 Test Loss: 0.1205866\n",
      "Validation loss decreased (0.039382 --> 0.038658).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1570462\n",
      "\tspeed: 0.0407s/iter; left time: 320.8636s\n",
      "\titers: 200, epoch: 7 | loss: 0.1613402\n",
      "\tspeed: 0.0119s/iter; left time: 92.2309s\n",
      "\titers: 300, epoch: 7 | loss: 0.1535271\n",
      "\tspeed: 0.0119s/iter; left time: 91.1234s\n",
      "\titers: 400, epoch: 7 | loss: 0.1166422\n",
      "\tspeed: 0.0119s/iter; left time: 90.0093s\n",
      "\titers: 500, epoch: 7 | loss: 0.1554347\n",
      "\tspeed: 0.0119s/iter; left time: 88.7571s\n",
      "Epoch: 7 cost time: 7.095490217208862\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1631525 Vali Loss: 0.0386661 Test Loss: 0.1209979\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0983596\n",
      "\tspeed: 0.0405s/iter; left time: 296.3244s\n",
      "\titers: 200, epoch: 8 | loss: 0.1509856\n",
      "\tspeed: 0.0129s/iter; left time: 93.0613s\n",
      "\titers: 300, epoch: 8 | loss: 0.1772835\n",
      "\tspeed: 0.0129s/iter; left time: 91.9275s\n",
      "\titers: 400, epoch: 8 | loss: 0.1319452\n",
      "\tspeed: 0.0130s/iter; left time: 90.9676s\n",
      "\titers: 500, epoch: 8 | loss: 0.1221954\n",
      "\tspeed: 0.0129s/iter; left time: 89.2086s\n",
      "Epoch: 8 cost time: 7.684610843658447\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1622830 Vali Loss: 0.0382901 Test Loss: 0.1199921\n",
      "Validation loss decreased (0.038658 --> 0.038290).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1114881\n",
      "\tspeed: 0.0409s/iter; left time: 275.7673s\n",
      "\titers: 200, epoch: 9 | loss: 0.1543981\n",
      "\tspeed: 0.0118s/iter; left time: 78.6380s\n",
      "\titers: 300, epoch: 9 | loss: 0.1817339\n",
      "\tspeed: 0.0119s/iter; left time: 77.6167s\n",
      "\titers: 400, epoch: 9 | loss: 0.1271408\n",
      "\tspeed: 0.0118s/iter; left time: 76.2941s\n",
      "\titers: 500, epoch: 9 | loss: 0.1836076\n",
      "\tspeed: 0.0119s/iter; left time: 75.1612s\n",
      "Epoch: 9 cost time: 7.084026098251343\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1617940 Vali Loss: 0.0380872 Test Loss: 0.1199515\n",
      "Validation loss decreased (0.038290 --> 0.038087).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1570071\n",
      "\tspeed: 0.0395s/iter; left time: 244.0621s\n",
      "\titers: 200, epoch: 10 | loss: 0.1784642\n",
      "\tspeed: 0.0118s/iter; left time: 71.7151s\n",
      "\titers: 300, epoch: 10 | loss: 0.1366245\n",
      "\tspeed: 0.0118s/iter; left time: 70.6905s\n",
      "\titers: 400, epoch: 10 | loss: 0.1409702\n",
      "\tspeed: 0.0118s/iter; left time: 69.4661s\n",
      "\titers: 500, epoch: 10 | loss: 0.1993697\n",
      "\tspeed: 0.0118s/iter; left time: 68.2208s\n",
      "Epoch: 10 cost time: 7.046705007553101\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1603146 Vali Loss: 0.0381728 Test Loss: 0.1202390\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1282211\n",
      "\tspeed: 0.0402s/iter; left time: 225.1029s\n",
      "\titers: 200, epoch: 11 | loss: 0.2455364\n",
      "\tspeed: 0.0120s/iter; left time: 65.7936s\n",
      "\titers: 300, epoch: 11 | loss: 0.1538334\n",
      "\tspeed: 0.0119s/iter; left time: 64.1864s\n",
      "\titers: 400, epoch: 11 | loss: 0.1733185\n",
      "\tspeed: 0.0119s/iter; left time: 63.0029s\n",
      "\titers: 500, epoch: 11 | loss: 0.2067696\n",
      "\tspeed: 0.0119s/iter; left time: 61.7831s\n",
      "Epoch: 11 cost time: 7.1839683055877686\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1596519 Vali Loss: 0.0382238 Test Loss: 0.1203443\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1390537\n",
      "\tspeed: 0.0413s/iter; left time: 208.0253s\n",
      "\titers: 200, epoch: 12 | loss: 0.1245567\n",
      "\tspeed: 0.0129s/iter; left time: 63.6074s\n",
      "\titers: 300, epoch: 12 | loss: 0.1152004\n",
      "\tspeed: 0.0129s/iter; left time: 62.3250s\n",
      "\titers: 400, epoch: 12 | loss: 0.1446932\n",
      "\tspeed: 0.0129s/iter; left time: 61.0293s\n",
      "\titers: 500, epoch: 12 | loss: 0.1685894\n",
      "\tspeed: 0.0122s/iter; left time: 56.6884s\n",
      "Epoch: 12 cost time: 7.500365495681763\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1602082 Vali Loss: 0.0383167 Test Loss: 0.1203712\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12011267244815826, mae:0.2147633582353592\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1519469\n",
      "\tspeed: 0.0233s/iter; left time: 263.5850s\n",
      "\titers: 200, epoch: 1 | loss: 0.3518038\n",
      "\tspeed: 0.0115s/iter; left time: 128.8856s\n",
      "\titers: 300, epoch: 1 | loss: 0.1749141\n",
      "\tspeed: 0.0115s/iter; left time: 127.8302s\n",
      "\titers: 400, epoch: 1 | loss: 0.2966926\n",
      "\tspeed: 0.0115s/iter; left time: 126.5019s\n",
      "\titers: 500, epoch: 1 | loss: 0.1596343\n",
      "\tspeed: 0.0115s/iter; left time: 125.4803s\n",
      "Epoch: 1 cost time: 7.786570072174072\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2292013 Vali Loss: 0.0412115 Test Loss: 0.1274067\n",
      "Validation loss decreased (inf --> 0.041211).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1706311\n",
      "\tspeed: 0.0414s/iter; left time: 443.8939s\n",
      "\titers: 200, epoch: 2 | loss: 0.1401508\n",
      "\tspeed: 0.0126s/iter; left time: 133.6156s\n",
      "\titers: 300, epoch: 2 | loss: 0.1847728\n",
      "\tspeed: 0.0126s/iter; left time: 132.7553s\n",
      "\titers: 400, epoch: 2 | loss: 0.2843027\n",
      "\tspeed: 0.0126s/iter; left time: 131.7191s\n",
      "\titers: 500, epoch: 2 | loss: 0.2906266\n",
      "\tspeed: 0.0117s/iter; left time: 120.8972s\n",
      "Epoch: 2 cost time: 7.355028390884399\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2022052 Vali Loss: 0.0417904 Test Loss: 0.1300673\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1933681\n",
      "\tspeed: 0.0377s/iter; left time: 383.1887s\n",
      "\titers: 200, epoch: 3 | loss: 0.0985367\n",
      "\tspeed: 0.0115s/iter; left time: 115.4813s\n",
      "\titers: 300, epoch: 3 | loss: 0.1996156\n",
      "\tspeed: 0.0115s/iter; left time: 114.3173s\n",
      "\titers: 400, epoch: 3 | loss: 0.1665078\n",
      "\tspeed: 0.0115s/iter; left time: 113.5254s\n",
      "\titers: 500, epoch: 3 | loss: 0.2089575\n",
      "\tspeed: 0.0115s/iter; left time: 112.0292s\n",
      "Epoch: 3 cost time: 6.827014923095703\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1803595 Vali Loss: 0.0353008 Test Loss: 0.1156385\n",
      "Validation loss decreased (0.041211 --> 0.035301).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1596104\n",
      "\tspeed: 0.0391s/iter; left time: 375.1551s\n",
      "\titers: 200, epoch: 4 | loss: 0.1095146\n",
      "\tspeed: 0.0115s/iter; left time: 109.0038s\n",
      "\titers: 300, epoch: 4 | loss: 0.0954783\n",
      "\tspeed: 0.0115s/iter; left time: 107.8649s\n",
      "\titers: 400, epoch: 4 | loss: 0.1333924\n",
      "\tspeed: 0.0115s/iter; left time: 106.7529s\n",
      "\titers: 500, epoch: 4 | loss: 0.1625584\n",
      "\tspeed: 0.0115s/iter; left time: 105.6813s\n",
      "Epoch: 4 cost time: 6.873661756515503\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1650652 Vali Loss: 0.0338985 Test Loss: 0.1111508\n",
      "Validation loss decreased (0.035301 --> 0.033899).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1666282\n",
      "\tspeed: 0.0388s/iter; left time: 349.7922s\n",
      "\titers: 200, epoch: 5 | loss: 0.1077879\n",
      "\tspeed: 0.0115s/iter; left time: 102.7434s\n",
      "\titers: 300, epoch: 5 | loss: 0.2390414\n",
      "\tspeed: 0.0115s/iter; left time: 101.4003s\n",
      "\titers: 400, epoch: 5 | loss: 0.1366075\n",
      "\tspeed: 0.0115s/iter; left time: 100.4077s\n",
      "\titers: 500, epoch: 5 | loss: 0.1202177\n",
      "\tspeed: 0.0115s/iter; left time: 99.2298s\n",
      "Epoch: 5 cost time: 6.872446775436401\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1558943 Vali Loss: 0.0339489 Test Loss: 0.1121132\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1796040\n",
      "\tspeed: 0.0382s/iter; left time: 323.1573s\n",
      "\titers: 200, epoch: 6 | loss: 0.1611378\n",
      "\tspeed: 0.0115s/iter; left time: 96.0036s\n",
      "\titers: 300, epoch: 6 | loss: 0.1117627\n",
      "\tspeed: 0.0115s/iter; left time: 94.6717s\n",
      "\titers: 400, epoch: 6 | loss: 0.1110432\n",
      "\tspeed: 0.0115s/iter; left time: 93.7835s\n",
      "\titers: 500, epoch: 6 | loss: 0.1269647\n",
      "\tspeed: 0.0115s/iter; left time: 92.5573s\n",
      "Epoch: 6 cost time: 6.859149217605591\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1512940 Vali Loss: 0.0335551 Test Loss: 0.1112797\n",
      "Validation loss decreased (0.033899 --> 0.033555).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1086403\n",
      "\tspeed: 0.0381s/iter; left time: 300.2245s\n",
      "\titers: 200, epoch: 7 | loss: 0.1870593\n",
      "\tspeed: 0.0115s/iter; left time: 89.3654s\n",
      "\titers: 300, epoch: 7 | loss: 0.0947685\n",
      "\tspeed: 0.0115s/iter; left time: 88.2036s\n",
      "\titers: 400, epoch: 7 | loss: 0.1145303\n",
      "\tspeed: 0.0115s/iter; left time: 87.1263s\n",
      "\titers: 500, epoch: 7 | loss: 0.1414022\n",
      "\tspeed: 0.0115s/iter; left time: 85.7781s\n",
      "Epoch: 7 cost time: 6.852989435195923\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1482953 Vali Loss: 0.0331711 Test Loss: 0.1111351\n",
      "Validation loss decreased (0.033555 --> 0.033171).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1101499\n",
      "\tspeed: 0.0399s/iter; left time: 291.5127s\n",
      "\titers: 200, epoch: 8 | loss: 0.1743105\n",
      "\tspeed: 0.0126s/iter; left time: 90.9293s\n",
      "\titers: 300, epoch: 8 | loss: 0.1020674\n",
      "\tspeed: 0.0126s/iter; left time: 89.8658s\n",
      "\titers: 400, epoch: 8 | loss: 0.1296077\n",
      "\tspeed: 0.0126s/iter; left time: 88.0840s\n",
      "\titers: 500, epoch: 8 | loss: 0.1146621\n",
      "\tspeed: 0.0120s/iter; left time: 82.9420s\n",
      "Epoch: 8 cost time: 7.365458965301514\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1471755 Vali Loss: 0.0333272 Test Loss: 0.1113495\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2292286\n",
      "\tspeed: 0.0394s/iter; left time: 265.5992s\n",
      "\titers: 200, epoch: 9 | loss: 0.1055641\n",
      "\tspeed: 0.0126s/iter; left time: 83.9967s\n",
      "\titers: 300, epoch: 9 | loss: 0.1617718\n",
      "\tspeed: 0.0126s/iter; left time: 82.4814s\n",
      "\titers: 400, epoch: 9 | loss: 0.1685493\n",
      "\tspeed: 0.0125s/iter; left time: 80.8208s\n",
      "\titers: 500, epoch: 9 | loss: 0.2229301\n",
      "\tspeed: 0.0115s/iter; left time: 72.9083s\n",
      "Epoch: 9 cost time: 7.310041666030884\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1456881 Vali Loss: 0.0330472 Test Loss: 0.1112786\n",
      "Validation loss decreased (0.033171 --> 0.033047).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1415712\n",
      "\tspeed: 0.0414s/iter; left time: 255.3575s\n",
      "\titers: 200, epoch: 10 | loss: 0.1169592\n",
      "\tspeed: 0.0126s/iter; left time: 76.6677s\n",
      "\titers: 300, epoch: 10 | loss: 0.1292818\n",
      "\tspeed: 0.0126s/iter; left time: 75.3224s\n",
      "\titers: 400, epoch: 10 | loss: 0.1493463\n",
      "\tspeed: 0.0126s/iter; left time: 73.9930s\n",
      "\titers: 500, epoch: 10 | loss: 0.1628127\n",
      "\tspeed: 0.0126s/iter; left time: 72.5650s\n",
      "Epoch: 10 cost time: 7.508484363555908\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1465767 Vali Loss: 0.0331408 Test Loss: 0.1112651\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1133842\n",
      "\tspeed: 0.0398s/iter; left time: 222.8890s\n",
      "\titers: 200, epoch: 11 | loss: 0.1636380\n",
      "\tspeed: 0.0126s/iter; left time: 69.3769s\n",
      "\titers: 300, epoch: 11 | loss: 0.1353102\n",
      "\tspeed: 0.0121s/iter; left time: 65.5575s\n",
      "\titers: 400, epoch: 11 | loss: 0.1801481\n",
      "\tspeed: 0.0115s/iter; left time: 61.1737s\n",
      "\titers: 500, epoch: 11 | loss: 0.2217366\n",
      "\tspeed: 0.0115s/iter; left time: 60.0034s\n",
      "Epoch: 11 cost time: 7.143402576446533\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1468188 Vali Loss: 0.0330274 Test Loss: 0.1112319\n",
      "Validation loss decreased (0.033047 --> 0.033027).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1327773\n",
      "\tspeed: 0.0396s/iter; left time: 199.3454s\n",
      "\titers: 200, epoch: 12 | loss: 0.1372503\n",
      "\tspeed: 0.0126s/iter; left time: 62.1709s\n",
      "\titers: 300, epoch: 12 | loss: 0.1172687\n",
      "\tspeed: 0.0126s/iter; left time: 60.9538s\n",
      "\titers: 400, epoch: 12 | loss: 0.2106243\n",
      "\tspeed: 0.0126s/iter; left time: 59.5135s\n",
      "\titers: 500, epoch: 12 | loss: 0.1307931\n",
      "\tspeed: 0.0126s/iter; left time: 58.2949s\n",
      "Epoch: 12 cost time: 7.501107215881348\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1459731 Vali Loss: 0.0332833 Test Loss: 0.1112486\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2057044\n",
      "\tspeed: 0.0404s/iter; left time: 180.2818s\n",
      "\titers: 200, epoch: 13 | loss: 0.1851881\n",
      "\tspeed: 0.0115s/iter; left time: 49.9780s\n",
      "\titers: 300, epoch: 13 | loss: 0.1428877\n",
      "\tspeed: 0.0115s/iter; left time: 48.9197s\n",
      "\titers: 400, epoch: 13 | loss: 0.1263070\n",
      "\tspeed: 0.0115s/iter; left time: 47.7411s\n",
      "\titers: 500, epoch: 13 | loss: 0.1877145\n",
      "\tspeed: 0.0115s/iter; left time: 46.6300s\n",
      "Epoch: 13 cost time: 6.820101737976074\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1447158 Vali Loss: 0.0331614 Test Loss: 0.1112201\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1153721\n",
      "\tspeed: 0.0377s/iter; left time: 146.5508s\n",
      "\titers: 200, epoch: 14 | loss: 0.2060436\n",
      "\tspeed: 0.0115s/iter; left time: 43.5477s\n",
      "\titers: 300, epoch: 14 | loss: 0.1331390\n",
      "\tspeed: 0.0115s/iter; left time: 42.4999s\n",
      "\titers: 400, epoch: 14 | loss: 0.1472175\n",
      "\tspeed: 0.0115s/iter; left time: 41.3459s\n",
      "\titers: 500, epoch: 14 | loss: 0.2155699\n",
      "\tspeed: 0.0115s/iter; left time: 40.1754s\n",
      "Epoch: 14 cost time: 6.863035440444946\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1452995 Vali Loss: 0.0329629 Test Loss: 0.1111845\n",
      "Validation loss decreased (0.033027 --> 0.032963).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1701639\n",
      "\tspeed: 0.0388s/iter; left time: 128.7652s\n",
      "\titers: 200, epoch: 15 | loss: 0.1698841\n",
      "\tspeed: 0.0115s/iter; left time: 37.0129s\n",
      "\titers: 300, epoch: 15 | loss: 0.1489871\n",
      "\tspeed: 0.0115s/iter; left time: 35.9622s\n",
      "\titers: 400, epoch: 15 | loss: 0.1076717\n",
      "\tspeed: 0.0115s/iter; left time: 34.7631s\n",
      "\titers: 500, epoch: 15 | loss: 0.1245630\n",
      "\tspeed: 0.0115s/iter; left time: 33.5168s\n",
      "Epoch: 15 cost time: 6.843875408172607\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1444354 Vali Loss: 0.0330157 Test Loss: 0.1111896\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1521598\n",
      "\tspeed: 0.0372s/iter; left time: 102.3204s\n",
      "\titers: 200, epoch: 16 | loss: 0.1556416\n",
      "\tspeed: 0.0115s/iter; left time: 30.4867s\n",
      "\titers: 300, epoch: 16 | loss: 0.1187774\n",
      "\tspeed: 0.0115s/iter; left time: 29.4222s\n",
      "\titers: 400, epoch: 16 | loss: 0.0956007\n",
      "\tspeed: 0.0115s/iter; left time: 28.1512s\n",
      "\titers: 500, epoch: 16 | loss: 0.1450910\n",
      "\tspeed: 0.0115s/iter; left time: 26.9614s\n",
      "Epoch: 16 cost time: 6.830895900726318\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1461396 Vali Loss: 0.0330571 Test Loss: 0.1111890\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.1512654\n",
      "\tspeed: 0.0379s/iter; left time: 82.5962s\n",
      "\titers: 200, epoch: 17 | loss: 0.0914336\n",
      "\tspeed: 0.0115s/iter; left time: 23.9053s\n",
      "\titers: 300, epoch: 17 | loss: 0.1645135\n",
      "\tspeed: 0.0115s/iter; left time: 22.7363s\n",
      "\titers: 400, epoch: 17 | loss: 0.0635268\n",
      "\tspeed: 0.0115s/iter; left time: 21.5541s\n",
      "\titers: 500, epoch: 17 | loss: 0.1457991\n",
      "\tspeed: 0.0115s/iter; left time: 20.4145s\n",
      "Epoch: 17 cost time: 6.840854167938232\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1452971 Vali Loss: 0.0331200 Test Loss: 0.1111891\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11135132610797882, mae:0.20238979160785675\n",
      ">>> LR=5e-4,DO=0.2,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3023773\n",
      "\tspeed: 0.0242s/iter; left time: 273.6227s\n",
      "\titers: 200, epoch: 1 | loss: 0.3520382\n",
      "\tspeed: 0.0120s/iter; left time: 134.7686s\n",
      "\titers: 300, epoch: 1 | loss: 0.3291507\n",
      "\tspeed: 0.0120s/iter; left time: 133.0418s\n",
      "\titers: 400, epoch: 1 | loss: 0.2533366\n",
      "\tspeed: 0.0120s/iter; left time: 132.1250s\n",
      "\titers: 500, epoch: 1 | loss: 0.2025783\n",
      "\tspeed: 0.0120s/iter; left time: 131.0130s\n",
      "Epoch: 1 cost time: 8.069112062454224\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2592653 Vali Loss: 0.0470340 Test Loss: 0.1491200\n",
      "Validation loss decreased (inf --> 0.047034).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2725223\n",
      "\tspeed: 0.0382s/iter; left time: 409.9496s\n",
      "\titers: 200, epoch: 2 | loss: 0.3464039\n",
      "\tspeed: 0.0109s/iter; left time: 115.6648s\n",
      "\titers: 300, epoch: 2 | loss: 0.1615402\n",
      "\tspeed: 0.0109s/iter; left time: 114.5526s\n",
      "\titers: 400, epoch: 2 | loss: 0.1890907\n",
      "\tspeed: 0.0109s/iter; left time: 113.2820s\n",
      "\titers: 500, epoch: 2 | loss: 0.1945120\n",
      "\tspeed: 0.0108s/iter; left time: 112.0606s\n",
      "Epoch: 2 cost time: 6.507251977920532\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2294851 Vali Loss: 0.0506599 Test Loss: 0.1529285\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1668201\n",
      "\tspeed: 0.0390s/iter; left time: 396.4676s\n",
      "\titers: 200, epoch: 3 | loss: 0.1748754\n",
      "\tspeed: 0.0116s/iter; left time: 117.0029s\n",
      "\titers: 300, epoch: 3 | loss: 0.2789643\n",
      "\tspeed: 0.0108s/iter; left time: 107.8836s\n",
      "\titers: 400, epoch: 3 | loss: 0.1677472\n",
      "\tspeed: 0.0108s/iter; left time: 106.7837s\n",
      "\titers: 500, epoch: 3 | loss: 0.2055227\n",
      "\tspeed: 0.0108s/iter; left time: 105.7056s\n",
      "Epoch: 3 cost time: 6.590514183044434\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2052950 Vali Loss: 0.0422072 Test Loss: 0.1373033\n",
      "Validation loss decreased (0.047034 --> 0.042207).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1818763\n",
      "\tspeed: 0.0378s/iter; left time: 362.9290s\n",
      "\titers: 200, epoch: 4 | loss: 0.1296567\n",
      "\tspeed: 0.0108s/iter; left time: 102.9313s\n",
      "\titers: 300, epoch: 4 | loss: 0.1650539\n",
      "\tspeed: 0.0109s/iter; left time: 102.0760s\n",
      "\titers: 400, epoch: 4 | loss: 0.3428080\n",
      "\tspeed: 0.0109s/iter; left time: 100.8114s\n",
      "\titers: 500, epoch: 4 | loss: 0.1801892\n",
      "\tspeed: 0.0108s/iter; left time: 99.5819s\n",
      "Epoch: 4 cost time: 6.505101680755615\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1827566 Vali Loss: 0.0414400 Test Loss: 0.1408591\n",
      "Validation loss decreased (0.042207 --> 0.041440).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1668240\n",
      "\tspeed: 0.0383s/iter; left time: 345.7220s\n",
      "\titers: 200, epoch: 5 | loss: 0.1677529\n",
      "\tspeed: 0.0120s/iter; left time: 107.3434s\n",
      "\titers: 300, epoch: 5 | loss: 0.1709006\n",
      "\tspeed: 0.0121s/iter; left time: 106.3883s\n",
      "\titers: 400, epoch: 5 | loss: 0.1128015\n",
      "\tspeed: 0.0120s/iter; left time: 105.0780s\n",
      "\titers: 500, epoch: 5 | loss: 0.2561677\n",
      "\tspeed: 0.0120s/iter; left time: 103.6698s\n",
      "Epoch: 5 cost time: 7.180497884750366\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1705200 Vali Loss: 0.0390431 Test Loss: 0.1338034\n",
      "Validation loss decreased (0.041440 --> 0.039043).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1403041\n",
      "\tspeed: 0.0399s/iter; left time: 337.1706s\n",
      "\titers: 200, epoch: 6 | loss: 0.1482296\n",
      "\tspeed: 0.0109s/iter; left time: 90.6086s\n",
      "\titers: 300, epoch: 6 | loss: 0.1887355\n",
      "\tspeed: 0.0109s/iter; left time: 89.5781s\n",
      "\titers: 400, epoch: 6 | loss: 0.1394947\n",
      "\tspeed: 0.0109s/iter; left time: 88.5602s\n",
      "\titers: 500, epoch: 6 | loss: 0.1561777\n",
      "\tspeed: 0.0108s/iter; left time: 87.3424s\n",
      "Epoch: 6 cost time: 6.529320478439331\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1623241 Vali Loss: 0.0388792 Test Loss: 0.1365796\n",
      "Validation loss decreased (0.039043 --> 0.038879).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1332874\n",
      "\tspeed: 0.0390s/iter; left time: 307.6743s\n",
      "\titers: 200, epoch: 7 | loss: 0.1300848\n",
      "\tspeed: 0.0109s/iter; left time: 84.6280s\n",
      "\titers: 300, epoch: 7 | loss: 0.1099305\n",
      "\tspeed: 0.0109s/iter; left time: 83.5531s\n",
      "\titers: 400, epoch: 7 | loss: 0.1499352\n",
      "\tspeed: 0.0108s/iter; left time: 82.1313s\n",
      "\titers: 500, epoch: 7 | loss: 0.1161368\n",
      "\tspeed: 0.0109s/iter; left time: 81.4321s\n",
      "Epoch: 7 cost time: 6.5061774253845215\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1599118 Vali Loss: 0.0385099 Test Loss: 0.1332457\n",
      "Validation loss decreased (0.038879 --> 0.038510).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1220800\n",
      "\tspeed: 0.0378s/iter; left time: 276.6024s\n",
      "\titers: 200, epoch: 8 | loss: 0.1919292\n",
      "\tspeed: 0.0109s/iter; left time: 78.7503s\n",
      "\titers: 300, epoch: 8 | loss: 0.2086399\n",
      "\tspeed: 0.0111s/iter; left time: 78.7329s\n",
      "\titers: 400, epoch: 8 | loss: 0.1443205\n",
      "\tspeed: 0.0120s/iter; left time: 84.4548s\n",
      "\titers: 500, epoch: 8 | loss: 0.0975310\n",
      "\tspeed: 0.0120s/iter; left time: 83.1488s\n",
      "Epoch: 8 cost time: 6.880667448043823\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1577346 Vali Loss: 0.0380855 Test Loss: 0.1331222\n",
      "Validation loss decreased (0.038510 --> 0.038086).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1655697\n",
      "\tspeed: 0.0386s/iter; left time: 260.4936s\n",
      "\titers: 200, epoch: 9 | loss: 0.1161146\n",
      "\tspeed: 0.0109s/iter; left time: 72.6438s\n",
      "\titers: 300, epoch: 9 | loss: 0.2196243\n",
      "\tspeed: 0.0109s/iter; left time: 71.6140s\n",
      "\titers: 400, epoch: 9 | loss: 0.2073046\n",
      "\tspeed: 0.0109s/iter; left time: 70.3972s\n",
      "\titers: 500, epoch: 9 | loss: 0.1035316\n",
      "\tspeed: 0.0109s/iter; left time: 69.2156s\n",
      "Epoch: 9 cost time: 6.567201852798462\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1557757 Vali Loss: 0.0381641 Test Loss: 0.1334061\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1572970\n",
      "\tspeed: 0.0377s/iter; left time: 232.7969s\n",
      "\titers: 200, epoch: 10 | loss: 0.0930295\n",
      "\tspeed: 0.0109s/iter; left time: 66.2792s\n",
      "\titers: 300, epoch: 10 | loss: 0.1732631\n",
      "\tspeed: 0.0109s/iter; left time: 65.2547s\n",
      "\titers: 400, epoch: 10 | loss: 0.1366965\n",
      "\tspeed: 0.0109s/iter; left time: 64.0827s\n",
      "\titers: 500, epoch: 10 | loss: 0.1793620\n",
      "\tspeed: 0.0109s/iter; left time: 62.8559s\n",
      "Epoch: 10 cost time: 6.522702932357788\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1560762 Vali Loss: 0.0381785 Test Loss: 0.1330903\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1224089\n",
      "\tspeed: 0.0381s/iter; left time: 213.4633s\n",
      "\titers: 200, epoch: 11 | loss: 0.1424767\n",
      "\tspeed: 0.0109s/iter; left time: 59.8913s\n",
      "\titers: 300, epoch: 11 | loss: 0.1435373\n",
      "\tspeed: 0.0109s/iter; left time: 58.9219s\n",
      "\titers: 400, epoch: 11 | loss: 0.1171310\n",
      "\tspeed: 0.0109s/iter; left time: 57.7617s\n",
      "\titers: 500, epoch: 11 | loss: 0.1325386\n",
      "\tspeed: 0.0109s/iter; left time: 56.5978s\n",
      "Epoch: 11 cost time: 6.541114568710327\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1562124 Vali Loss: 0.0382650 Test Loss: 0.1332655\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13330425322055817, mae:0.2314433604478836\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2088144\n",
      "\tspeed: 0.0262s/iter; left time: 296.0795s\n",
      "\titers: 200, epoch: 1 | loss: 0.1824291\n",
      "\tspeed: 0.0145s/iter; left time: 162.7267s\n",
      "\titers: 300, epoch: 1 | loss: 0.2788930\n",
      "\tspeed: 0.0145s/iter; left time: 160.8681s\n",
      "\titers: 400, epoch: 1 | loss: 0.2357036\n",
      "\tspeed: 0.0145s/iter; left time: 159.7675s\n",
      "\titers: 500, epoch: 1 | loss: 0.1783668\n",
      "\tspeed: 0.0145s/iter; left time: 158.4819s\n",
      "Epoch: 1 cost time: 9.4953932762146\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2419916 Vali Loss: 0.0462555 Test Loss: 0.1280871\n",
      "Validation loss decreased (inf --> 0.046256).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3006781\n",
      "\tspeed: 0.0467s/iter; left time: 501.2330s\n",
      "\titers: 200, epoch: 2 | loss: 0.1948502\n",
      "\tspeed: 0.0146s/iter; left time: 155.2263s\n",
      "\titers: 300, epoch: 2 | loss: 0.1645432\n",
      "\tspeed: 0.0146s/iter; left time: 153.7717s\n",
      "\titers: 400, epoch: 2 | loss: 0.1193968\n",
      "\tspeed: 0.0146s/iter; left time: 152.2712s\n",
      "\titers: 500, epoch: 2 | loss: 0.2239374\n",
      "\tspeed: 0.0146s/iter; left time: 150.7181s\n",
      "Epoch: 2 cost time: 8.649644613265991\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2050041 Vali Loss: 0.0459917 Test Loss: 0.1426752\n",
      "Validation loss decreased (0.046256 --> 0.045992).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3307565\n",
      "\tspeed: 0.0468s/iter; left time: 475.6335s\n",
      "\titers: 200, epoch: 3 | loss: 0.1560543\n",
      "\tspeed: 0.0146s/iter; left time: 146.4645s\n",
      "\titers: 300, epoch: 3 | loss: 0.0987948\n",
      "\tspeed: 0.0146s/iter; left time: 145.1016s\n",
      "\titers: 400, epoch: 3 | loss: 0.1366630\n",
      "\tspeed: 0.0146s/iter; left time: 143.5745s\n",
      "\titers: 500, epoch: 3 | loss: 0.1182564\n",
      "\tspeed: 0.0146s/iter; left time: 142.2431s\n",
      "Epoch: 3 cost time: 8.619805574417114\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1765109 Vali Loss: 0.0391808 Test Loss: 0.1271644\n",
      "Validation loss decreased (0.045992 --> 0.039181).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1397561\n",
      "\tspeed: 0.0494s/iter; left time: 474.0160s\n",
      "\titers: 200, epoch: 4 | loss: 0.1401083\n",
      "\tspeed: 0.0145s/iter; left time: 138.0070s\n",
      "\titers: 300, epoch: 4 | loss: 0.1984763\n",
      "\tspeed: 0.0145s/iter; left time: 136.5816s\n",
      "\titers: 400, epoch: 4 | loss: 0.1807085\n",
      "\tspeed: 0.0156s/iter; left time: 144.9647s\n",
      "\titers: 500, epoch: 4 | loss: 0.1222808\n",
      "\tspeed: 0.0163s/iter; left time: 149.7612s\n",
      "Epoch: 4 cost time: 8.903204917907715\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1574438 Vali Loss: 0.0374154 Test Loss: 0.1241968\n",
      "Validation loss decreased (0.039181 --> 0.037415).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1441234\n",
      "\tspeed: 0.0461s/iter; left time: 415.6659s\n",
      "\titers: 200, epoch: 5 | loss: 0.1478885\n",
      "\tspeed: 0.0147s/iter; left time: 130.7705s\n",
      "\titers: 300, epoch: 5 | loss: 0.1427533\n",
      "\tspeed: 0.0146s/iter; left time: 129.0889s\n",
      "\titers: 400, epoch: 5 | loss: 0.1568673\n",
      "\tspeed: 0.0146s/iter; left time: 127.5483s\n",
      "\titers: 500, epoch: 5 | loss: 0.1767756\n",
      "\tspeed: 0.0146s/iter; left time: 126.0386s\n",
      "Epoch: 5 cost time: 8.650763750076294\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1490328 Vali Loss: 0.0381137 Test Loss: 0.1268485\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1252706\n",
      "\tspeed: 0.0461s/iter; left time: 389.8265s\n",
      "\titers: 200, epoch: 6 | loss: 0.1413923\n",
      "\tspeed: 0.0145s/iter; left time: 121.4084s\n",
      "\titers: 300, epoch: 6 | loss: 0.1185930\n",
      "\tspeed: 0.0145s/iter; left time: 119.9456s\n",
      "\titers: 400, epoch: 6 | loss: 0.1176067\n",
      "\tspeed: 0.0145s/iter; left time: 118.4480s\n",
      "\titers: 500, epoch: 6 | loss: 0.1488429\n",
      "\tspeed: 0.0145s/iter; left time: 116.9528s\n",
      "Epoch: 6 cost time: 8.58096432685852\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1430332 Vali Loss: 0.0376692 Test Loss: 0.1223561\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1383386\n",
      "\tspeed: 0.0468s/iter; left time: 369.1600s\n",
      "\titers: 200, epoch: 7 | loss: 0.1056294\n",
      "\tspeed: 0.0146s/iter; left time: 113.9510s\n",
      "\titers: 300, epoch: 7 | loss: 0.1030869\n",
      "\tspeed: 0.0147s/iter; left time: 112.6046s\n",
      "\titers: 400, epoch: 7 | loss: 0.1361030\n",
      "\tspeed: 0.0146s/iter; left time: 110.7500s\n",
      "\titers: 500, epoch: 7 | loss: 0.2070615\n",
      "\tspeed: 0.0146s/iter; left time: 109.2576s\n",
      "Epoch: 7 cost time: 8.678980112075806\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1416667 Vali Loss: 0.0376299 Test Loss: 0.1246513\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12436190247535706, mae:0.21817733347415924\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3041493\n",
      "\tspeed: 0.0278s/iter; left time: 314.4769s\n",
      "\titers: 200, epoch: 1 | loss: 0.2585823\n",
      "\tspeed: 0.0142s/iter; left time: 159.4398s\n",
      "\titers: 300, epoch: 1 | loss: 0.1520086\n",
      "\tspeed: 0.0142s/iter; left time: 157.2123s\n",
      "\titers: 400, epoch: 1 | loss: 0.3295628\n",
      "\tspeed: 0.0151s/iter; left time: 166.1192s\n",
      "\titers: 500, epoch: 1 | loss: 0.1836459\n",
      "\tspeed: 0.0157s/iter; left time: 171.6269s\n",
      "Epoch: 1 cost time: 9.751925468444824\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2175849 Vali Loss: 0.0411615 Test Loss: 0.1243817\n",
      "Validation loss decreased (inf --> 0.041161).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1491904\n",
      "\tspeed: 0.0463s/iter; left time: 496.5073s\n",
      "\titers: 200, epoch: 2 | loss: 0.2791035\n",
      "\tspeed: 0.0143s/iter; left time: 152.0371s\n",
      "\titers: 300, epoch: 2 | loss: 0.1883979\n",
      "\tspeed: 0.0143s/iter; left time: 151.1155s\n",
      "\titers: 400, epoch: 2 | loss: 0.1699372\n",
      "\tspeed: 0.0143s/iter; left time: 149.2766s\n",
      "\titers: 500, epoch: 2 | loss: 0.1642059\n",
      "\tspeed: 0.0150s/iter; left time: 155.4252s\n",
      "Epoch: 2 cost time: 8.685136795043945\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1894586 Vali Loss: 0.0383928 Test Loss: 0.1226561\n",
      "Validation loss decreased (0.041161 --> 0.038393).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1771815\n",
      "\tspeed: 0.0524s/iter; left time: 532.5170s\n",
      "\titers: 200, epoch: 3 | loss: 0.0839687\n",
      "\tspeed: 0.0164s/iter; left time: 164.7994s\n",
      "\titers: 300, epoch: 3 | loss: 0.2152462\n",
      "\tspeed: 0.0164s/iter; left time: 163.7541s\n",
      "\titers: 400, epoch: 3 | loss: 0.1249974\n",
      "\tspeed: 0.0164s/iter; left time: 161.8623s\n",
      "\titers: 500, epoch: 3 | loss: 0.1507803\n",
      "\tspeed: 0.0164s/iter; left time: 160.0737s\n",
      "Epoch: 3 cost time: 9.664738893508911\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1661950 Vali Loss: 0.0359099 Test Loss: 0.1177011\n",
      "Validation loss decreased (0.038393 --> 0.035910).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1583731\n",
      "\tspeed: 0.0493s/iter; left time: 472.7751s\n",
      "\titers: 200, epoch: 4 | loss: 0.1549593\n",
      "\tspeed: 0.0164s/iter; left time: 155.8270s\n",
      "\titers: 300, epoch: 4 | loss: 0.1846991\n",
      "\tspeed: 0.0164s/iter; left time: 154.1214s\n",
      "\titers: 400, epoch: 4 | loss: 0.0967964\n",
      "\tspeed: 0.0164s/iter; left time: 152.5000s\n",
      "\titers: 500, epoch: 4 | loss: 0.1095245\n",
      "\tspeed: 0.0164s/iter; left time: 150.8979s\n",
      "Epoch: 4 cost time: 9.676862239837646\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1546359 Vali Loss: 0.0341666 Test Loss: 0.1127359\n",
      "Validation loss decreased (0.035910 --> 0.034167).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0815493\n",
      "\tspeed: 0.0494s/iter; left time: 445.7467s\n",
      "\titers: 200, epoch: 5 | loss: 0.1315108\n",
      "\tspeed: 0.0164s/iter; left time: 146.7295s\n",
      "\titers: 300, epoch: 5 | loss: 0.1942120\n",
      "\tspeed: 0.0165s/iter; left time: 145.3322s\n",
      "\titers: 400, epoch: 5 | loss: 0.1044161\n",
      "\tspeed: 0.0164s/iter; left time: 143.3857s\n",
      "\titers: 500, epoch: 5 | loss: 0.1637261\n",
      "\tspeed: 0.0151s/iter; left time: 130.4375s\n",
      "Epoch: 5 cost time: 9.391724586486816\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1448837 Vali Loss: 0.0341896 Test Loss: 0.1115746\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1139430\n",
      "\tspeed: 0.0454s/iter; left time: 383.7172s\n",
      "\titers: 200, epoch: 6 | loss: 0.2346027\n",
      "\tspeed: 0.0142s/iter; left time: 118.5416s\n",
      "\titers: 300, epoch: 6 | loss: 0.2354135\n",
      "\tspeed: 0.0142s/iter; left time: 117.3476s\n",
      "\titers: 400, epoch: 6 | loss: 0.1607544\n",
      "\tspeed: 0.0142s/iter; left time: 115.8542s\n",
      "\titers: 500, epoch: 6 | loss: 0.1464290\n",
      "\tspeed: 0.0142s/iter; left time: 114.5482s\n",
      "Epoch: 6 cost time: 8.380046844482422\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1417928 Vali Loss: 0.0337849 Test Loss: 0.1103700\n",
      "Validation loss decreased (0.034167 --> 0.033785).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1030021\n",
      "\tspeed: 0.0460s/iter; left time: 362.5501s\n",
      "\titers: 200, epoch: 7 | loss: 0.1366124\n",
      "\tspeed: 0.0142s/iter; left time: 110.4461s\n",
      "\titers: 300, epoch: 7 | loss: 0.2279001\n",
      "\tspeed: 0.0142s/iter; left time: 108.8823s\n",
      "\titers: 400, epoch: 7 | loss: 0.1248613\n",
      "\tspeed: 0.0142s/iter; left time: 107.4396s\n",
      "\titers: 500, epoch: 7 | loss: 0.1861413\n",
      "\tspeed: 0.0142s/iter; left time: 105.9620s\n",
      "Epoch: 7 cost time: 8.392455816268921\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1395018 Vali Loss: 0.0332101 Test Loss: 0.1089533\n",
      "Validation loss decreased (0.033785 --> 0.033210).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0943482\n",
      "\tspeed: 0.0453s/iter; left time: 331.4455s\n",
      "\titers: 200, epoch: 8 | loss: 0.1349510\n",
      "\tspeed: 0.0143s/iter; left time: 102.9677s\n",
      "\titers: 300, epoch: 8 | loss: 0.2093175\n",
      "\tspeed: 0.0143s/iter; left time: 101.6882s\n",
      "\titers: 400, epoch: 8 | loss: 0.1370911\n",
      "\tspeed: 0.0143s/iter; left time: 100.2694s\n",
      "\titers: 500, epoch: 8 | loss: 0.1521060\n",
      "\tspeed: 0.0143s/iter; left time: 98.7586s\n",
      "Epoch: 8 cost time: 8.435569047927856\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1379513 Vali Loss: 0.0333427 Test Loss: 0.1091910\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0990435\n",
      "\tspeed: 0.0450s/iter; left time: 303.5137s\n",
      "\titers: 200, epoch: 9 | loss: 0.1075702\n",
      "\tspeed: 0.0142s/iter; left time: 94.1987s\n",
      "\titers: 300, epoch: 9 | loss: 0.1758587\n",
      "\tspeed: 0.0142s/iter; left time: 92.8158s\n",
      "\titers: 400, epoch: 9 | loss: 0.1283492\n",
      "\tspeed: 0.0142s/iter; left time: 91.3041s\n",
      "\titers: 500, epoch: 9 | loss: 0.1375320\n",
      "\tspeed: 0.0142s/iter; left time: 89.8476s\n",
      "Epoch: 9 cost time: 8.337303638458252\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1371401 Vali Loss: 0.0331055 Test Loss: 0.1088744\n",
      "Validation loss decreased (0.033210 --> 0.033106).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1440584\n",
      "\tspeed: 0.0487s/iter; left time: 300.4675s\n",
      "\titers: 200, epoch: 10 | loss: 0.1124659\n",
      "\tspeed: 0.0143s/iter; left time: 86.9187s\n",
      "\titers: 300, epoch: 10 | loss: 0.1372264\n",
      "\tspeed: 0.0143s/iter; left time: 85.5947s\n",
      "\titers: 400, epoch: 10 | loss: 0.1360071\n",
      "\tspeed: 0.0158s/iter; left time: 92.7192s\n",
      "\titers: 500, epoch: 10 | loss: 0.1742094\n",
      "\tspeed: 0.0144s/iter; left time: 82.8457s\n",
      "Epoch: 10 cost time: 8.739937782287598\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1371056 Vali Loss: 0.0330677 Test Loss: 0.1088897\n",
      "Validation loss decreased (0.033106 --> 0.033068).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2021715\n",
      "\tspeed: 0.0478s/iter; left time: 267.5538s\n",
      "\titers: 200, epoch: 11 | loss: 0.1225517\n",
      "\tspeed: 0.0164s/iter; left time: 90.2935s\n",
      "\titers: 300, epoch: 11 | loss: 0.1704631\n",
      "\tspeed: 0.0164s/iter; left time: 88.6577s\n",
      "\titers: 400, epoch: 11 | loss: 0.0939213\n",
      "\tspeed: 0.0164s/iter; left time: 86.8683s\n",
      "\titers: 500, epoch: 11 | loss: 0.2140114\n",
      "\tspeed: 0.0162s/iter; left time: 84.3539s\n",
      "Epoch: 11 cost time: 9.482523202896118\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1350687 Vali Loss: 0.0329884 Test Loss: 0.1088659\n",
      "Validation loss decreased (0.033068 --> 0.032988).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1625562\n",
      "\tspeed: 0.0458s/iter; left time: 230.6408s\n",
      "\titers: 200, epoch: 12 | loss: 0.1708721\n",
      "\tspeed: 0.0142s/iter; left time: 70.2258s\n",
      "\titers: 300, epoch: 12 | loss: 0.1148917\n",
      "\tspeed: 0.0162s/iter; left time: 78.0379s\n",
      "\titers: 400, epoch: 12 | loss: 0.1476015\n",
      "\tspeed: 0.0164s/iter; left time: 77.7580s\n",
      "\titers: 500, epoch: 12 | loss: 0.1615509\n",
      "\tspeed: 0.0164s/iter; left time: 76.1156s\n",
      "Epoch: 12 cost time: 9.228839874267578\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1370205 Vali Loss: 0.0331084 Test Loss: 0.1088943\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1212869\n",
      "\tspeed: 0.0488s/iter; left time: 217.8987s\n",
      "\titers: 200, epoch: 13 | loss: 0.0968870\n",
      "\tspeed: 0.0165s/iter; left time: 71.7611s\n",
      "\titers: 300, epoch: 13 | loss: 0.1208057\n",
      "\tspeed: 0.0164s/iter; left time: 70.0833s\n",
      "\titers: 400, epoch: 13 | loss: 0.0852482\n",
      "\tspeed: 0.0165s/iter; left time: 68.4506s\n",
      "\titers: 500, epoch: 13 | loss: 0.1432980\n",
      "\tspeed: 0.0164s/iter; left time: 66.7831s\n",
      "Epoch: 13 cost time: 9.651771545410156\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1366182 Vali Loss: 0.0329717 Test Loss: 0.1089000\n",
      "Validation loss decreased (0.032988 --> 0.032972).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1207893\n",
      "\tspeed: 0.0501s/iter; left time: 194.9690s\n",
      "\titers: 200, epoch: 14 | loss: 0.2127361\n",
      "\tspeed: 0.0159s/iter; left time: 60.4396s\n",
      "\titers: 300, epoch: 14 | loss: 0.0937397\n",
      "\tspeed: 0.0160s/iter; left time: 58.9600s\n",
      "\titers: 400, epoch: 14 | loss: 0.0882648\n",
      "\tspeed: 0.0160s/iter; left time: 57.3379s\n",
      "\titers: 500, epoch: 14 | loss: 0.0805607\n",
      "\tspeed: 0.0160s/iter; left time: 55.7877s\n",
      "Epoch: 14 cost time: 9.391402244567871\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1345381 Vali Loss: 0.0330430 Test Loss: 0.1089277\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1321850\n",
      "\tspeed: 0.0464s/iter; left time: 154.0604s\n",
      "\titers: 200, epoch: 15 | loss: 0.1116377\n",
      "\tspeed: 0.0143s/iter; left time: 45.9886s\n",
      "\titers: 300, epoch: 15 | loss: 0.1358414\n",
      "\tspeed: 0.0143s/iter; left time: 44.5702s\n",
      "\titers: 400, epoch: 15 | loss: 0.1307736\n",
      "\tspeed: 0.0143s/iter; left time: 43.1715s\n",
      "\titers: 500, epoch: 15 | loss: 0.1802043\n",
      "\tspeed: 0.0143s/iter; left time: 41.7262s\n",
      "Epoch: 15 cost time: 8.393531799316406\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1356780 Vali Loss: 0.0330492 Test Loss: 0.1089440\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1402374\n",
      "\tspeed: 0.0495s/iter; left time: 136.1815s\n",
      "\titers: 200, epoch: 16 | loss: 0.1551928\n",
      "\tspeed: 0.0141s/iter; left time: 37.4684s\n",
      "\titers: 300, epoch: 16 | loss: 0.1137916\n",
      "\tspeed: 0.0141s/iter; left time: 36.0806s\n",
      "\titers: 400, epoch: 16 | loss: 0.1630017\n",
      "\tspeed: 0.0142s/iter; left time: 34.7065s\n",
      "\titers: 500, epoch: 16 | loss: 0.1067045\n",
      "\tspeed: 0.0141s/iter; left time: 33.2273s\n",
      "Epoch: 16 cost time: 8.530990600585938\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1356732 Vali Loss: 0.0331464 Test Loss: 0.1089477\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10903660207986832, mae:0.19916637241840363\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1381665\n",
      "\tspeed: 0.0260s/iter; left time: 294.3041s\n",
      "\titers: 200, epoch: 1 | loss: 0.3449165\n",
      "\tspeed: 0.0142s/iter; left time: 159.5168s\n",
      "\titers: 300, epoch: 1 | loss: 0.2914426\n",
      "\tspeed: 0.0142s/iter; left time: 157.8268s\n",
      "\titers: 400, epoch: 1 | loss: 0.2373642\n",
      "\tspeed: 0.0142s/iter; left time: 156.4851s\n",
      "\titers: 500, epoch: 1 | loss: 0.2826465\n",
      "\tspeed: 0.0143s/iter; left time: 155.7019s\n",
      "Epoch: 1 cost time: 9.355465173721313\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2493833 Vali Loss: 0.0467754 Test Loss: 0.1420881\n",
      "Validation loss decreased (inf --> 0.046775).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1900936\n",
      "\tspeed: 0.0463s/iter; left time: 497.1709s\n",
      "\titers: 200, epoch: 2 | loss: 0.2193269\n",
      "\tspeed: 0.0142s/iter; left time: 151.0666s\n",
      "\titers: 300, epoch: 2 | loss: 0.1439174\n",
      "\tspeed: 0.0142s/iter; left time: 149.7960s\n",
      "\titers: 400, epoch: 2 | loss: 0.1681918\n",
      "\tspeed: 0.0142s/iter; left time: 148.4570s\n",
      "\titers: 500, epoch: 2 | loss: 0.2402062\n",
      "\tspeed: 0.0142s/iter; left time: 146.9094s\n",
      "Epoch: 2 cost time: 8.43770956993103\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2090211 Vali Loss: 0.0434395 Test Loss: 0.1385605\n",
      "Validation loss decreased (0.046775 --> 0.043440).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2056979\n",
      "\tspeed: 0.0470s/iter; left time: 477.7511s\n",
      "\titers: 200, epoch: 3 | loss: 0.2125686\n",
      "\tspeed: 0.0142s/iter; left time: 142.7031s\n",
      "\titers: 300, epoch: 3 | loss: 0.1561041\n",
      "\tspeed: 0.0142s/iter; left time: 141.1579s\n",
      "\titers: 400, epoch: 3 | loss: 0.1571902\n",
      "\tspeed: 0.0142s/iter; left time: 139.6252s\n",
      "\titers: 500, epoch: 3 | loss: 0.2057050\n",
      "\tspeed: 0.0141s/iter; left time: 137.9327s\n",
      "Epoch: 3 cost time: 8.354428768157959\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1796085 Vali Loss: 0.0421454 Test Loss: 0.1335378\n",
      "Validation loss decreased (0.043440 --> 0.042145).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.2165372\n",
      "\tspeed: 0.0479s/iter; left time: 459.0873s\n",
      "\titers: 200, epoch: 4 | loss: 0.1942196\n",
      "\tspeed: 0.0142s/iter; left time: 134.7607s\n",
      "\titers: 300, epoch: 4 | loss: 0.1731071\n",
      "\tspeed: 0.0142s/iter; left time: 133.6173s\n",
      "\titers: 400, epoch: 4 | loss: 0.1457002\n",
      "\tspeed: 0.0142s/iter; left time: 132.2323s\n",
      "\titers: 500, epoch: 4 | loss: 0.1088841\n",
      "\tspeed: 0.0142s/iter; left time: 130.6998s\n",
      "Epoch: 4 cost time: 8.383462905883789\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1631498 Vali Loss: 0.0403996 Test Loss: 0.1263688\n",
      "Validation loss decreased (0.042145 --> 0.040400).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1325023\n",
      "\tspeed: 0.0457s/iter; left time: 412.2423s\n",
      "\titers: 200, epoch: 5 | loss: 0.1555424\n",
      "\tspeed: 0.0142s/iter; left time: 126.9137s\n",
      "\titers: 300, epoch: 5 | loss: 0.1323898\n",
      "\tspeed: 0.0143s/iter; left time: 125.7380s\n",
      "\titers: 400, epoch: 5 | loss: 0.2263586\n",
      "\tspeed: 0.0143s/iter; left time: 124.7091s\n",
      "\titers: 500, epoch: 5 | loss: 0.1418733\n",
      "\tspeed: 0.0143s/iter; left time: 123.3043s\n",
      "Epoch: 5 cost time: 8.412789106369019\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1533076 Vali Loss: 0.0393731 Test Loss: 0.1267292\n",
      "Validation loss decreased (0.040400 --> 0.039373).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1757096\n",
      "\tspeed: 0.0461s/iter; left time: 389.6917s\n",
      "\titers: 200, epoch: 6 | loss: 0.1593852\n",
      "\tspeed: 0.0143s/iter; left time: 119.1973s\n",
      "\titers: 300, epoch: 6 | loss: 0.1035415\n",
      "\tspeed: 0.0143s/iter; left time: 117.8082s\n",
      "\titers: 400, epoch: 6 | loss: 0.1843241\n",
      "\tspeed: 0.0143s/iter; left time: 116.1755s\n",
      "\titers: 500, epoch: 6 | loss: 0.1459927\n",
      "\tspeed: 0.0142s/iter; left time: 114.6804s\n",
      "Epoch: 6 cost time: 8.447590827941895\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1486886 Vali Loss: 0.0387617 Test Loss: 0.1253418\n",
      "Validation loss decreased (0.039373 --> 0.038762).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1954851\n",
      "\tspeed: 0.0480s/iter; left time: 378.0589s\n",
      "\titers: 200, epoch: 7 | loss: 0.0975353\n",
      "\tspeed: 0.0151s/iter; left time: 117.3457s\n",
      "\titers: 300, epoch: 7 | loss: 0.0966628\n",
      "\tspeed: 0.0141s/iter; left time: 108.3929s\n",
      "\titers: 400, epoch: 7 | loss: 0.2145246\n",
      "\tspeed: 0.0141s/iter; left time: 107.0146s\n",
      "\titers: 500, epoch: 7 | loss: 0.1329321\n",
      "\tspeed: 0.0141s/iter; left time: 105.6067s\n",
      "Epoch: 7 cost time: 8.718824863433838\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1470587 Vali Loss: 0.0387335 Test Loss: 0.1235380\n",
      "Validation loss decreased (0.038762 --> 0.038734).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1238119\n",
      "\tspeed: 0.0484s/iter; left time: 354.2094s\n",
      "\titers: 200, epoch: 8 | loss: 0.1432588\n",
      "\tspeed: 0.0142s/iter; left time: 102.1763s\n",
      "\titers: 300, epoch: 8 | loss: 0.1247279\n",
      "\tspeed: 0.0141s/iter; left time: 100.5692s\n",
      "\titers: 400, epoch: 8 | loss: 0.1522371\n",
      "\tspeed: 0.0141s/iter; left time: 99.1054s\n",
      "\titers: 500, epoch: 8 | loss: 0.0975019\n",
      "\tspeed: 0.0141s/iter; left time: 97.6842s\n",
      "Epoch: 8 cost time: 8.386362791061401\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1456697 Vali Loss: 0.0395853 Test Loss: 0.1261854\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.2091442\n",
      "\tspeed: 0.0491s/iter; left time: 330.9766s\n",
      "\titers: 200, epoch: 9 | loss: 0.1126706\n",
      "\tspeed: 0.0159s/iter; left time: 105.8244s\n",
      "\titers: 300, epoch: 9 | loss: 0.0992757\n",
      "\tspeed: 0.0148s/iter; left time: 96.6632s\n",
      "\titers: 400, epoch: 9 | loss: 0.1102065\n",
      "\tspeed: 0.0143s/iter; left time: 91.8412s\n",
      "\titers: 500, epoch: 9 | loss: 0.1507562\n",
      "\tspeed: 0.0143s/iter; left time: 90.3879s\n",
      "Epoch: 9 cost time: 8.81211233139038\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1442083 Vali Loss: 0.0391426 Test Loss: 0.1253360\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1900224\n",
      "\tspeed: 0.0469s/iter; left time: 289.1788s\n",
      "\titers: 200, epoch: 10 | loss: 0.1614278\n",
      "\tspeed: 0.0142s/iter; left time: 86.3078s\n",
      "\titers: 300, epoch: 10 | loss: 0.1242339\n",
      "\tspeed: 0.0142s/iter; left time: 84.8878s\n",
      "\titers: 400, epoch: 10 | loss: 0.1666936\n",
      "\tspeed: 0.0142s/iter; left time: 83.4591s\n",
      "\titers: 500, epoch: 10 | loss: 0.1359811\n",
      "\tspeed: 0.0142s/iter; left time: 82.1738s\n",
      "Epoch: 10 cost time: 8.376512289047241\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1436869 Vali Loss: 0.0391266 Test Loss: 0.1257459\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12369126081466675, mae:0.22086134552955627\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2958392\n",
      "\tspeed: 0.0303s/iter; left time: 342.2269s\n",
      "\titers: 200, epoch: 1 | loss: 0.1971780\n",
      "\tspeed: 0.0181s/iter; left time: 202.6062s\n",
      "\titers: 300, epoch: 1 | loss: 0.2727989\n",
      "\tspeed: 0.0181s/iter; left time: 200.5571s\n",
      "\titers: 400, epoch: 1 | loss: 0.2513632\n",
      "\tspeed: 0.0181s/iter; left time: 198.7780s\n",
      "\titers: 500, epoch: 1 | loss: 0.2136229\n",
      "\tspeed: 0.0181s/iter; left time: 197.3126s\n",
      "Epoch: 1 cost time: 11.584544658660889\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2550540 Vali Loss: 0.0474312 Test Loss: 0.1496193\n",
      "Validation loss decreased (inf --> 0.047431).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2432267\n",
      "\tspeed: 0.0540s/iter; left time: 579.4563s\n",
      "\titers: 200, epoch: 2 | loss: 0.2498724\n",
      "\tspeed: 0.0181s/iter; left time: 192.3322s\n",
      "\titers: 300, epoch: 2 | loss: 0.2779619\n",
      "\tspeed: 0.0180s/iter; left time: 190.0745s\n",
      "\titers: 400, epoch: 2 | loss: 0.1651098\n",
      "\tspeed: 0.0181s/iter; left time: 188.3289s\n",
      "\titers: 500, epoch: 2 | loss: 0.1844755\n",
      "\tspeed: 0.0181s/iter; left time: 186.6024s\n",
      "Epoch: 2 cost time: 10.638998031616211\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2384399 Vali Loss: 0.0586837 Test Loss: 0.1700942\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2058848\n",
      "\tspeed: 0.0555s/iter; left time: 564.1534s\n",
      "\titers: 200, epoch: 3 | loss: 0.1983583\n",
      "\tspeed: 0.0181s/iter; left time: 182.3391s\n",
      "\titers: 300, epoch: 3 | loss: 0.3410145\n",
      "\tspeed: 0.0181s/iter; left time: 180.1700s\n",
      "\titers: 400, epoch: 3 | loss: 0.1681196\n",
      "\tspeed: 0.0181s/iter; left time: 178.7537s\n",
      "\titers: 500, epoch: 3 | loss: 0.1727709\n",
      "\tspeed: 0.0175s/iter; left time: 170.8710s\n",
      "Epoch: 3 cost time: 10.462732791900635\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2248720 Vali Loss: 0.0464250 Test Loss: 0.1407486\n",
      "Validation loss decreased (0.047431 --> 0.046425).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1574557\n",
      "\tspeed: 0.0521s/iter; left time: 499.7411s\n",
      "\titers: 200, epoch: 4 | loss: 0.2051149\n",
      "\tspeed: 0.0181s/iter; left time: 171.7190s\n",
      "\titers: 300, epoch: 4 | loss: 0.2613001\n",
      "\tspeed: 0.0181s/iter; left time: 169.6992s\n",
      "\titers: 400, epoch: 4 | loss: 0.2122735\n",
      "\tspeed: 0.0181s/iter; left time: 167.8244s\n",
      "\titers: 500, epoch: 4 | loss: 0.2093787\n",
      "\tspeed: 0.0181s/iter; left time: 166.0142s\n",
      "Epoch: 4 cost time: 10.503705024719238\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1972890 Vali Loss: 0.0443675 Test Loss: 0.1331053\n",
      "Validation loss decreased (0.046425 --> 0.044367).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1237682\n",
      "\tspeed: 0.0542s/iter; left time: 488.6117s\n",
      "\titers: 200, epoch: 5 | loss: 0.2852994\n",
      "\tspeed: 0.0181s/iter; left time: 161.1551s\n",
      "\titers: 300, epoch: 5 | loss: 0.1794762\n",
      "\tspeed: 0.0181s/iter; left time: 159.2278s\n",
      "\titers: 400, epoch: 5 | loss: 0.2154945\n",
      "\tspeed: 0.0181s/iter; left time: 157.6994s\n",
      "\titers: 500, epoch: 5 | loss: 0.1389254\n",
      "\tspeed: 0.0181s/iter; left time: 155.6525s\n",
      "Epoch: 5 cost time: 10.604403495788574\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1862711 Vali Loss: 0.0417370 Test Loss: 0.1288609\n",
      "Validation loss decreased (0.044367 --> 0.041737).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2081187\n",
      "\tspeed: 0.0529s/iter; left time: 447.3634s\n",
      "\titers: 200, epoch: 6 | loss: 0.1423266\n",
      "\tspeed: 0.0180s/iter; left time: 150.6615s\n",
      "\titers: 300, epoch: 6 | loss: 0.2063590\n",
      "\tspeed: 0.0181s/iter; left time: 149.4124s\n",
      "\titers: 400, epoch: 6 | loss: 0.2357511\n",
      "\tspeed: 0.0180s/iter; left time: 146.9401s\n",
      "\titers: 500, epoch: 6 | loss: 0.1809550\n",
      "\tspeed: 0.0181s/iter; left time: 145.3646s\n",
      "Epoch: 6 cost time: 10.591111421585083\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1806376 Vali Loss: 0.0412314 Test Loss: 0.1279439\n",
      "Validation loss decreased (0.041737 --> 0.041231).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1608773\n",
      "\tspeed: 0.0534s/iter; left time: 420.7435s\n",
      "\titers: 200, epoch: 7 | loss: 0.2030581\n",
      "\tspeed: 0.0181s/iter; left time: 140.6077s\n",
      "\titers: 300, epoch: 7 | loss: 0.2310962\n",
      "\tspeed: 0.0181s/iter; left time: 138.8188s\n",
      "\titers: 400, epoch: 7 | loss: 0.1222981\n",
      "\tspeed: 0.0181s/iter; left time: 136.8526s\n",
      "\titers: 500, epoch: 7 | loss: 0.1308530\n",
      "\tspeed: 0.0181s/iter; left time: 135.5647s\n",
      "Epoch: 7 cost time: 10.624607563018799\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1756744 Vali Loss: 0.0416139 Test Loss: 0.1276427\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1781857\n",
      "\tspeed: 0.0555s/iter; left time: 406.0790s\n",
      "\titers: 200, epoch: 8 | loss: 0.1287111\n",
      "\tspeed: 0.0181s/iter; left time: 130.7216s\n",
      "\titers: 300, epoch: 8 | loss: 0.1429879\n",
      "\tspeed: 0.0181s/iter; left time: 128.6654s\n",
      "\titers: 400, epoch: 8 | loss: 0.1149336\n",
      "\tspeed: 0.0181s/iter; left time: 126.7060s\n",
      "\titers: 500, epoch: 8 | loss: 0.1402981\n",
      "\tspeed: 0.0181s/iter; left time: 125.1046s\n",
      "Epoch: 8 cost time: 10.60359811782837\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1750025 Vali Loss: 0.0413127 Test Loss: 0.1273950\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1620007\n",
      "\tspeed: 0.0537s/iter; left time: 362.2811s\n",
      "\titers: 200, epoch: 9 | loss: 0.1230296\n",
      "\tspeed: 0.0181s/iter; left time: 119.8763s\n",
      "\titers: 300, epoch: 9 | loss: 0.1626625\n",
      "\tspeed: 0.0180s/iter; left time: 117.7691s\n",
      "\titers: 400, epoch: 9 | loss: 0.2064057\n",
      "\tspeed: 0.0180s/iter; left time: 116.1454s\n",
      "\titers: 500, epoch: 9 | loss: 0.2402835\n",
      "\tspeed: 0.0180s/iter; left time: 114.2668s\n",
      "Epoch: 9 cost time: 10.606595516204834\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1738157 Vali Loss: 0.0414313 Test Loss: 0.1275823\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1281076818704605, mae:0.2205001711845398\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2068312\n",
      "\tspeed: 0.0282s/iter; left time: 319.2306s\n",
      "\titers: 200, epoch: 1 | loss: 0.1475040\n",
      "\tspeed: 0.0164s/iter; left time: 184.0855s\n",
      "\titers: 300, epoch: 1 | loss: 0.2655107\n",
      "\tspeed: 0.0165s/iter; left time: 182.6220s\n",
      "\titers: 400, epoch: 1 | loss: 0.1900390\n",
      "\tspeed: 0.0165s/iter; left time: 181.2508s\n",
      "\titers: 500, epoch: 1 | loss: 0.1433915\n",
      "\tspeed: 0.0165s/iter; left time: 179.5374s\n",
      "Epoch: 1 cost time: 10.611324548721313\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2343052 Vali Loss: 0.0426553 Test Loss: 0.1378862\n",
      "Validation loss decreased (inf --> 0.042655).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1608283\n",
      "\tspeed: 0.0522s/iter; left time: 560.0889s\n",
      "\titers: 200, epoch: 2 | loss: 0.1833935\n",
      "\tspeed: 0.0179s/iter; left time: 190.2395s\n",
      "\titers: 300, epoch: 2 | loss: 0.2451425\n",
      "\tspeed: 0.0179s/iter; left time: 188.6663s\n",
      "\titers: 400, epoch: 2 | loss: 0.3889318\n",
      "\tspeed: 0.0179s/iter; left time: 186.3894s\n",
      "\titers: 500, epoch: 2 | loss: 0.2141431\n",
      "\tspeed: 0.0179s/iter; left time: 184.5460s\n",
      "Epoch: 2 cost time: 10.501088619232178\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2236655 Vali Loss: 0.0419637 Test Loss: 0.1399358\n",
      "Validation loss decreased (0.042655 --> 0.041964).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2036266\n",
      "\tspeed: 0.0520s/iter; left time: 528.2143s\n",
      "\titers: 200, epoch: 3 | loss: 0.2122304\n",
      "\tspeed: 0.0165s/iter; left time: 165.8269s\n",
      "\titers: 300, epoch: 3 | loss: 0.1900365\n",
      "\tspeed: 0.0165s/iter; left time: 163.8899s\n",
      "\titers: 400, epoch: 3 | loss: 0.2440022\n",
      "\tspeed: 0.0165s/iter; left time: 162.4087s\n",
      "\titers: 500, epoch: 3 | loss: 0.1065650\n",
      "\tspeed: 0.0165s/iter; left time: 160.9100s\n",
      "Epoch: 3 cost time: 9.716574907302856\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1956232 Vali Loss: 0.0376617 Test Loss: 0.1246283\n",
      "Validation loss decreased (0.041964 --> 0.037662).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1733230\n",
      "\tspeed: 0.0519s/iter; left time: 497.8470s\n",
      "\titers: 200, epoch: 4 | loss: 0.3698699\n",
      "\tspeed: 0.0165s/iter; left time: 156.9292s\n",
      "\titers: 300, epoch: 4 | loss: 0.1629041\n",
      "\tspeed: 0.0165s/iter; left time: 154.7091s\n",
      "\titers: 400, epoch: 4 | loss: 0.1191096\n",
      "\tspeed: 0.0164s/iter; left time: 152.8353s\n",
      "\titers: 500, epoch: 4 | loss: 0.2356152\n",
      "\tspeed: 0.0165s/iter; left time: 151.4117s\n",
      "Epoch: 4 cost time: 9.736165046691895\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1789920 Vali Loss: 0.0367390 Test Loss: 0.1175551\n",
      "Validation loss decreased (0.037662 --> 0.036739).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1773712\n",
      "\tspeed: 0.0534s/iter; left time: 482.1252s\n",
      "\titers: 200, epoch: 5 | loss: 0.2478509\n",
      "\tspeed: 0.0179s/iter; left time: 159.7023s\n",
      "\titers: 300, epoch: 5 | loss: 0.1592406\n",
      "\tspeed: 0.0179s/iter; left time: 157.5008s\n",
      "\titers: 400, epoch: 5 | loss: 0.1151561\n",
      "\tspeed: 0.0179s/iter; left time: 155.9672s\n",
      "\titers: 500, epoch: 5 | loss: 0.2273076\n",
      "\tspeed: 0.0179s/iter; left time: 154.1498s\n",
      "Epoch: 5 cost time: 10.548929452896118\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1693784 Vali Loss: 0.0352099 Test Loss: 0.1135109\n",
      "Validation loss decreased (0.036739 --> 0.035210).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1581204\n",
      "\tspeed: 0.0526s/iter; left time: 444.7047s\n",
      "\titers: 200, epoch: 6 | loss: 0.2331288\n",
      "\tspeed: 0.0163s/iter; left time: 136.5174s\n",
      "\titers: 300, epoch: 6 | loss: 0.2776801\n",
      "\tspeed: 0.0163s/iter; left time: 134.8671s\n",
      "\titers: 400, epoch: 6 | loss: 0.1604757\n",
      "\tspeed: 0.0164s/iter; left time: 133.3745s\n",
      "\titers: 500, epoch: 6 | loss: 0.1558830\n",
      "\tspeed: 0.0164s/iter; left time: 131.6769s\n",
      "Epoch: 6 cost time: 9.665111064910889\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1640407 Vali Loss: 0.0348077 Test Loss: 0.1118976\n",
      "Validation loss decreased (0.035210 --> 0.034808).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2196183\n",
      "\tspeed: 0.0520s/iter; left time: 410.0941s\n",
      "\titers: 200, epoch: 7 | loss: 0.1099205\n",
      "\tspeed: 0.0164s/iter; left time: 127.4045s\n",
      "\titers: 300, epoch: 7 | loss: 0.1736868\n",
      "\tspeed: 0.0163s/iter; left time: 125.2769s\n",
      "\titers: 400, epoch: 7 | loss: 0.2072969\n",
      "\tspeed: 0.0163s/iter; left time: 123.5751s\n",
      "\titers: 500, epoch: 7 | loss: 0.2354021\n",
      "\tspeed: 0.0164s/iter; left time: 122.6988s\n",
      "Epoch: 7 cost time: 9.64059591293335\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1619769 Vali Loss: 0.0341742 Test Loss: 0.1114734\n",
      "Validation loss decreased (0.034808 --> 0.034174).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0833036\n",
      "\tspeed: 0.0503s/iter; left time: 367.7773s\n",
      "\titers: 200, epoch: 8 | loss: 0.2457558\n",
      "\tspeed: 0.0164s/iter; left time: 117.9258s\n",
      "\titers: 300, epoch: 8 | loss: 0.1971235\n",
      "\tspeed: 0.0163s/iter; left time: 116.2315s\n",
      "\titers: 400, epoch: 8 | loss: 0.1781801\n",
      "\tspeed: 0.0163s/iter; left time: 114.4968s\n",
      "\titers: 500, epoch: 8 | loss: 0.1443961\n",
      "\tspeed: 0.0163s/iter; left time: 112.9303s\n",
      "Epoch: 8 cost time: 9.63207745552063\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1589763 Vali Loss: 0.0339690 Test Loss: 0.1106487\n",
      "Validation loss decreased (0.034174 --> 0.033969).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0992123\n",
      "\tspeed: 0.0518s/iter; left time: 349.3187s\n",
      "\titers: 200, epoch: 9 | loss: 0.1586944\n",
      "\tspeed: 0.0163s/iter; left time: 108.5423s\n",
      "\titers: 300, epoch: 9 | loss: 0.0967666\n",
      "\tspeed: 0.0178s/iter; left time: 116.7066s\n",
      "\titers: 400, epoch: 9 | loss: 0.1272036\n",
      "\tspeed: 0.0179s/iter; left time: 115.0043s\n",
      "\titers: 500, epoch: 9 | loss: 0.0852321\n",
      "\tspeed: 0.0179s/iter; left time: 113.2078s\n",
      "Epoch: 9 cost time: 10.287617921829224\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1580761 Vali Loss: 0.0339173 Test Loss: 0.1107529\n",
      "Validation loss decreased (0.033969 --> 0.033917).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1669910\n",
      "\tspeed: 0.0540s/iter; left time: 333.0758s\n",
      "\titers: 200, epoch: 10 | loss: 0.1650951\n",
      "\tspeed: 0.0164s/iter; left time: 99.6554s\n",
      "\titers: 300, epoch: 10 | loss: 0.1311248\n",
      "\tspeed: 0.0164s/iter; left time: 97.7972s\n",
      "\titers: 400, epoch: 10 | loss: 0.1945249\n",
      "\tspeed: 0.0164s/iter; left time: 96.1639s\n",
      "\titers: 500, epoch: 10 | loss: 0.1179071\n",
      "\tspeed: 0.0164s/iter; left time: 94.5195s\n",
      "Epoch: 10 cost time: 9.678165674209595\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1578447 Vali Loss: 0.0338990 Test Loss: 0.1106659\n",
      "Validation loss decreased (0.033917 --> 0.033899).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1007985\n",
      "\tspeed: 0.0515s/iter; left time: 288.4612s\n",
      "\titers: 200, epoch: 11 | loss: 0.1268860\n",
      "\tspeed: 0.0164s/iter; left time: 89.9666s\n",
      "\titers: 300, epoch: 11 | loss: 0.1053247\n",
      "\tspeed: 0.0164s/iter; left time: 88.3579s\n",
      "\titers: 400, epoch: 11 | loss: 0.1901793\n",
      "\tspeed: 0.0163s/iter; left time: 86.5345s\n",
      "\titers: 500, epoch: 11 | loss: 0.1574086\n",
      "\tspeed: 0.0163s/iter; left time: 84.8722s\n",
      "Epoch: 11 cost time: 9.645182609558105\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1576925 Vali Loss: 0.0340112 Test Loss: 0.1106179\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1106339\n",
      "\tspeed: 0.0500s/iter; left time: 251.3436s\n",
      "\titers: 200, epoch: 12 | loss: 0.1610774\n",
      "\tspeed: 0.0164s/iter; left time: 80.7318s\n",
      "\titers: 300, epoch: 12 | loss: 0.0931303\n",
      "\tspeed: 0.0164s/iter; left time: 79.1157s\n",
      "\titers: 400, epoch: 12 | loss: 0.1648269\n",
      "\tspeed: 0.0164s/iter; left time: 77.4771s\n",
      "\titers: 500, epoch: 12 | loss: 0.2387092\n",
      "\tspeed: 0.0164s/iter; left time: 75.8411s\n",
      "Epoch: 12 cost time: 9.649921417236328\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1574193 Vali Loss: 0.0339695 Test Loss: 0.1105909\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2163615\n",
      "\tspeed: 0.0512s/iter; left time: 228.3882s\n",
      "\titers: 200, epoch: 13 | loss: 0.2399940\n",
      "\tspeed: 0.0164s/iter; left time: 71.3858s\n",
      "\titers: 300, epoch: 13 | loss: 0.2056655\n",
      "\tspeed: 0.0164s/iter; left time: 69.7993s\n",
      "\titers: 400, epoch: 13 | loss: 0.1324357\n",
      "\tspeed: 0.0165s/iter; left time: 68.7965s\n",
      "\titers: 500, epoch: 13 | loss: 0.1442983\n",
      "\tspeed: 0.0179s/iter; left time: 72.4908s\n",
      "Epoch: 13 cost time: 10.049834728240967\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1575063 Vali Loss: 0.0340451 Test Loss: 0.1105785\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11083490401506424, mae:0.20302008092403412\n",
      ">>> LR=5e-4,DO=0.2,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2738749\n",
      "\tspeed: 0.0292s/iter; left time: 330.4245s\n",
      "\titers: 200, epoch: 1 | loss: 0.2410658\n",
      "\tspeed: 0.0170s/iter; left time: 190.5798s\n",
      "\titers: 300, epoch: 1 | loss: 0.1838185\n",
      "\tspeed: 0.0171s/iter; left time: 189.5716s\n",
      "\titers: 400, epoch: 1 | loss: 0.3203909\n",
      "\tspeed: 0.0171s/iter; left time: 188.1951s\n",
      "\titers: 500, epoch: 1 | loss: 0.3899314\n",
      "\tspeed: 0.0158s/iter; left time: 172.5631s\n",
      "Epoch: 1 cost time: 10.762296676635742\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2626514 Vali Loss: 0.0484837 Test Loss: 0.1565852\n",
      "Validation loss decreased (inf --> 0.048484).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2507294\n",
      "\tspeed: 0.0508s/iter; left time: 545.0885s\n",
      "\titers: 200, epoch: 2 | loss: 0.1554500\n",
      "\tspeed: 0.0155s/iter; left time: 164.3190s\n",
      "\titers: 300, epoch: 2 | loss: 0.2832112\n",
      "\tspeed: 0.0154s/iter; left time: 162.0583s\n",
      "\titers: 400, epoch: 2 | loss: 0.2342743\n",
      "\tspeed: 0.0154s/iter; left time: 160.6650s\n",
      "\titers: 500, epoch: 2 | loss: 0.3202891\n",
      "\tspeed: 0.0154s/iter; left time: 158.8916s\n",
      "Epoch: 2 cost time: 9.18026065826416\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2514978 Vali Loss: 0.0754417 Test Loss: 0.2028981\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.3472426\n",
      "\tspeed: 0.0476s/iter; left time: 483.9405s\n",
      "\titers: 200, epoch: 3 | loss: 0.1513417\n",
      "\tspeed: 0.0154s/iter; left time: 155.2889s\n",
      "\titers: 300, epoch: 3 | loss: 0.3537053\n",
      "\tspeed: 0.0154s/iter; left time: 153.8164s\n",
      "\titers: 400, epoch: 3 | loss: 0.2166460\n",
      "\tspeed: 0.0154s/iter; left time: 151.9139s\n",
      "\titers: 500, epoch: 3 | loss: 0.2753240\n",
      "\tspeed: 0.0154s/iter; left time: 150.4456s\n",
      "Epoch: 3 cost time: 9.07128357887268\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2626431 Vali Loss: 0.0480382 Test Loss: 0.1509399\n",
      "Validation loss decreased (0.048484 --> 0.048038).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1961316\n",
      "\tspeed: 0.0492s/iter; left time: 471.6767s\n",
      "\titers: 200, epoch: 4 | loss: 0.2935966\n",
      "\tspeed: 0.0155s/iter; left time: 147.0779s\n",
      "\titers: 300, epoch: 4 | loss: 0.3480114\n",
      "\tspeed: 0.0155s/iter; left time: 145.7484s\n",
      "\titers: 400, epoch: 4 | loss: 0.2189986\n",
      "\tspeed: 0.0155s/iter; left time: 144.0504s\n",
      "\titers: 500, epoch: 4 | loss: 0.2833834\n",
      "\tspeed: 0.0155s/iter; left time: 142.2614s\n",
      "Epoch: 4 cost time: 9.147878170013428\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2311800 Vali Loss: 0.0482368 Test Loss: 0.1470153\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1678876\n",
      "\tspeed: 0.0477s/iter; left time: 430.4962s\n",
      "\titers: 200, epoch: 5 | loss: 0.1962808\n",
      "\tspeed: 0.0154s/iter; left time: 137.2758s\n",
      "\titers: 300, epoch: 5 | loss: 0.1936736\n",
      "\tspeed: 0.0154s/iter; left time: 135.6552s\n",
      "\titers: 400, epoch: 5 | loss: 0.1584071\n",
      "\tspeed: 0.0154s/iter; left time: 134.2444s\n",
      "\titers: 500, epoch: 5 | loss: 0.2072518\n",
      "\tspeed: 0.0154s/iter; left time: 132.8328s\n",
      "Epoch: 5 cost time: 9.102423667907715\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2162158 Vali Loss: 0.0466264 Test Loss: 0.1401968\n",
      "Validation loss decreased (0.048038 --> 0.046626).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1975028\n",
      "\tspeed: 0.0521s/iter; left time: 440.1814s\n",
      "\titers: 200, epoch: 6 | loss: 0.1984609\n",
      "\tspeed: 0.0171s/iter; left time: 142.5164s\n",
      "\titers: 300, epoch: 6 | loss: 0.1290805\n",
      "\tspeed: 0.0170s/iter; left time: 140.6034s\n",
      "\titers: 400, epoch: 6 | loss: 0.1410978\n",
      "\tspeed: 0.0167s/iter; left time: 135.8300s\n",
      "\titers: 500, epoch: 6 | loss: 0.1558844\n",
      "\tspeed: 0.0155s/iter; left time: 124.7038s\n",
      "Epoch: 6 cost time: 9.744792222976685\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2068572 Vali Loss: 0.0456033 Test Loss: 0.1415516\n",
      "Validation loss decreased (0.046626 --> 0.045603).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2529387\n",
      "\tspeed: 0.0491s/iter; left time: 387.3464s\n",
      "\titers: 200, epoch: 7 | loss: 0.1949639\n",
      "\tspeed: 0.0154s/iter; left time: 120.0030s\n",
      "\titers: 300, epoch: 7 | loss: 0.1584079\n",
      "\tspeed: 0.0154s/iter; left time: 118.4326s\n",
      "\titers: 400, epoch: 7 | loss: 0.2333105\n",
      "\tspeed: 0.0154s/iter; left time: 116.5349s\n",
      "\titers: 500, epoch: 7 | loss: 0.2131604\n",
      "\tspeed: 0.0153s/iter; left time: 114.5144s\n",
      "Epoch: 7 cost time: 9.073714971542358\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2030053 Vali Loss: 0.0455402 Test Loss: 0.1392490\n",
      "Validation loss decreased (0.045603 --> 0.045540).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1815846\n",
      "\tspeed: 0.0506s/iter; left time: 369.8982s\n",
      "\titers: 200, epoch: 8 | loss: 0.2485581\n",
      "\tspeed: 0.0154s/iter; left time: 110.7186s\n",
      "\titers: 300, epoch: 8 | loss: 0.1278086\n",
      "\tspeed: 0.0154s/iter; left time: 109.2119s\n",
      "\titers: 400, epoch: 8 | loss: 0.1952574\n",
      "\tspeed: 0.0153s/iter; left time: 107.4408s\n",
      "\titers: 500, epoch: 8 | loss: 0.1229836\n",
      "\tspeed: 0.0153s/iter; left time: 105.9393s\n",
      "Epoch: 8 cost time: 9.06553864479065\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2005512 Vali Loss: 0.0445065 Test Loss: 0.1393705\n",
      "Validation loss decreased (0.045540 --> 0.044507).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1886239\n",
      "\tspeed: 0.0504s/iter; left time: 339.9592s\n",
      "\titers: 200, epoch: 9 | loss: 0.1721874\n",
      "\tspeed: 0.0155s/iter; left time: 102.7430s\n",
      "\titers: 300, epoch: 9 | loss: 0.2327887\n",
      "\tspeed: 0.0155s/iter; left time: 101.1658s\n",
      "\titers: 400, epoch: 9 | loss: 0.1441329\n",
      "\tspeed: 0.0155s/iter; left time: 99.6591s\n",
      "\titers: 500, epoch: 9 | loss: 0.1836031\n",
      "\tspeed: 0.0155s/iter; left time: 98.0881s\n",
      "Epoch: 9 cost time: 9.128331899642944\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1996132 Vali Loss: 0.0446082 Test Loss: 0.1392387\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2538546\n",
      "\tspeed: 0.0478s/iter; left time: 294.9375s\n",
      "\titers: 200, epoch: 10 | loss: 0.2266675\n",
      "\tspeed: 0.0153s/iter; left time: 93.1360s\n",
      "\titers: 300, epoch: 10 | loss: 0.1425828\n",
      "\tspeed: 0.0153s/iter; left time: 91.5887s\n",
      "\titers: 400, epoch: 10 | loss: 0.2254750\n",
      "\tspeed: 0.0153s/iter; left time: 90.0371s\n",
      "\titers: 500, epoch: 10 | loss: 0.3943844\n",
      "\tspeed: 0.0153s/iter; left time: 88.5095s\n",
      "Epoch: 10 cost time: 9.062186241149902\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2003246 Vali Loss: 0.0444149 Test Loss: 0.1389218\n",
      "Validation loss decreased (0.044507 --> 0.044415).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2145433\n",
      "\tspeed: 0.0518s/iter; left time: 289.9967s\n",
      "\titers: 200, epoch: 11 | loss: 0.1607526\n",
      "\tspeed: 0.0154s/iter; left time: 84.7926s\n",
      "\titers: 300, epoch: 11 | loss: 0.1815906\n",
      "\tspeed: 0.0154s/iter; left time: 83.3375s\n",
      "\titers: 400, epoch: 11 | loss: 0.2530977\n",
      "\tspeed: 0.0154s/iter; left time: 81.5207s\n",
      "\titers: 500, epoch: 11 | loss: 0.2577681\n",
      "\tspeed: 0.0153s/iter; left time: 79.7163s\n",
      "Epoch: 11 cost time: 9.098636627197266\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1997284 Vali Loss: 0.0445360 Test Loss: 0.1388688\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1345384\n",
      "\tspeed: 0.0474s/iter; left time: 238.4263s\n",
      "\titers: 200, epoch: 12 | loss: 0.3895433\n",
      "\tspeed: 0.0153s/iter; left time: 75.6527s\n",
      "\titers: 300, epoch: 12 | loss: 0.2682382\n",
      "\tspeed: 0.0154s/iter; left time: 74.2959s\n",
      "\titers: 400, epoch: 12 | loss: 0.1882094\n",
      "\tspeed: 0.0154s/iter; left time: 72.8893s\n",
      "\titers: 500, epoch: 12 | loss: 0.1769363\n",
      "\tspeed: 0.0154s/iter; left time: 71.2368s\n",
      "Epoch: 12 cost time: 9.029338598251343\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2000720 Vali Loss: 0.0445321 Test Loss: 0.1389161\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2760415\n",
      "\tspeed: 0.0469s/iter; left time: 209.1814s\n",
      "\titers: 200, epoch: 13 | loss: 0.1822870\n",
      "\tspeed: 0.0153s/iter; left time: 66.8248s\n",
      "\titers: 300, epoch: 13 | loss: 0.1951600\n",
      "\tspeed: 0.0153s/iter; left time: 65.3628s\n",
      "\titers: 400, epoch: 13 | loss: 0.2494859\n",
      "\tspeed: 0.0154s/iter; left time: 63.8787s\n",
      "\titers: 500, epoch: 13 | loss: 0.1471906\n",
      "\tspeed: 0.0153s/iter; left time: 62.1921s\n",
      "Epoch: 13 cost time: 9.011364221572876\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2007352 Vali Loss: 0.0444837 Test Loss: 0.1388606\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1390725076198578, mae:0.2316804677248001\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1979800\n",
      "\tspeed: 0.0304s/iter; left time: 343.3097s\n",
      "\titers: 200, epoch: 1 | loss: 0.2329361\n",
      "\tspeed: 0.0187s/iter; left time: 209.3887s\n",
      "\titers: 300, epoch: 1 | loss: 0.3890593\n",
      "\tspeed: 0.0187s/iter; left time: 207.7155s\n",
      "\titers: 400, epoch: 1 | loss: 0.1947859\n",
      "\tspeed: 0.0187s/iter; left time: 206.0058s\n",
      "\titers: 500, epoch: 1 | loss: 0.1655631\n",
      "\tspeed: 0.0187s/iter; left time: 203.8076s\n",
      "Epoch: 1 cost time: 11.883400678634644\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2447270 Vali Loss: 0.0476489 Test Loss: 0.1459083\n",
      "Validation loss decreased (inf --> 0.047649).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3815386\n",
      "\tspeed: 0.0566s/iter; left time: 607.1312s\n",
      "\titers: 200, epoch: 2 | loss: 0.1765963\n",
      "\tspeed: 0.0186s/iter; left time: 197.4937s\n",
      "\titers: 300, epoch: 2 | loss: 0.1362110\n",
      "\tspeed: 0.0186s/iter; left time: 195.5372s\n",
      "\titers: 400, epoch: 2 | loss: 0.1704368\n",
      "\tspeed: 0.0186s/iter; left time: 193.8894s\n",
      "\titers: 500, epoch: 2 | loss: 0.1821329\n",
      "\tspeed: 0.0186s/iter; left time: 192.3857s\n",
      "Epoch: 2 cost time: 10.908458471298218\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2095919 Vali Loss: 0.0445460 Test Loss: 0.1347510\n",
      "Validation loss decreased (0.047649 --> 0.044546).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1477922\n",
      "\tspeed: 0.0582s/iter; left time: 591.0012s\n",
      "\titers: 200, epoch: 3 | loss: 0.1658632\n",
      "\tspeed: 0.0186s/iter; left time: 186.9425s\n",
      "\titers: 300, epoch: 3 | loss: 0.2155154\n",
      "\tspeed: 0.0186s/iter; left time: 185.4586s\n",
      "\titers: 400, epoch: 3 | loss: 0.1262809\n",
      "\tspeed: 0.0186s/iter; left time: 183.3533s\n",
      "\titers: 500, epoch: 3 | loss: 0.1303732\n",
      "\tspeed: 0.0187s/iter; left time: 182.0518s\n",
      "Epoch: 3 cost time: 10.90052342414856\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1854190 Vali Loss: 0.0436595 Test Loss: 0.1318966\n",
      "Validation loss decreased (0.044546 --> 0.043659).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1870910\n",
      "\tspeed: 0.0560s/iter; left time: 537.0370s\n",
      "\titers: 200, epoch: 4 | loss: 0.2351542\n",
      "\tspeed: 0.0186s/iter; left time: 176.3182s\n",
      "\titers: 300, epoch: 4 | loss: 0.1840209\n",
      "\tspeed: 0.0185s/iter; left time: 173.9994s\n",
      "\titers: 400, epoch: 4 | loss: 0.0944235\n",
      "\tspeed: 0.0186s/iter; left time: 172.3667s\n",
      "\titers: 500, epoch: 4 | loss: 0.1628318\n",
      "\tspeed: 0.0185s/iter; left time: 170.1814s\n",
      "Epoch: 4 cost time: 10.848828554153442\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1660591 Vali Loss: 0.0401228 Test Loss: 0.1307222\n",
      "Validation loss decreased (0.043659 --> 0.040123).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2354830\n",
      "\tspeed: 0.0597s/iter; left time: 538.7158s\n",
      "\titers: 200, epoch: 5 | loss: 0.1616431\n",
      "\tspeed: 0.0186s/iter; left time: 166.2975s\n",
      "\titers: 300, epoch: 5 | loss: 0.1415432\n",
      "\tspeed: 0.0187s/iter; left time: 164.5683s\n",
      "\titers: 400, epoch: 5 | loss: 0.1302704\n",
      "\tspeed: 0.0186s/iter; left time: 162.5723s\n",
      "\titers: 500, epoch: 5 | loss: 0.1366715\n",
      "\tspeed: 0.0186s/iter; left time: 160.4705s\n",
      "Epoch: 5 cost time: 10.894646406173706\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1549483 Vali Loss: 0.0393205 Test Loss: 0.1318874\n",
      "Validation loss decreased (0.040123 --> 0.039320).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0862106\n",
      "\tspeed: 0.0562s/iter; left time: 475.2517s\n",
      "\titers: 200, epoch: 6 | loss: 0.1426957\n",
      "\tspeed: 0.0186s/iter; left time: 155.7222s\n",
      "\titers: 300, epoch: 6 | loss: 0.1217947\n",
      "\tspeed: 0.0186s/iter; left time: 153.8347s\n",
      "\titers: 400, epoch: 6 | loss: 0.1642987\n",
      "\tspeed: 0.0186s/iter; left time: 151.2122s\n",
      "\titers: 500, epoch: 6 | loss: 0.1378561\n",
      "\tspeed: 0.0185s/iter; left time: 149.1290s\n",
      "Epoch: 6 cost time: 10.929916143417358\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1476896 Vali Loss: 0.0385074 Test Loss: 0.1282430\n",
      "Validation loss decreased (0.039320 --> 0.038507).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1076406\n",
      "\tspeed: 0.0598s/iter; left time: 470.9100s\n",
      "\titers: 200, epoch: 7 | loss: 0.1230809\n",
      "\tspeed: 0.0186s/iter; left time: 144.9959s\n",
      "\titers: 300, epoch: 7 | loss: 0.1243722\n",
      "\tspeed: 0.0186s/iter; left time: 142.8975s\n",
      "\titers: 400, epoch: 7 | loss: 0.1548696\n",
      "\tspeed: 0.0186s/iter; left time: 141.0774s\n",
      "\titers: 500, epoch: 7 | loss: 0.2566475\n",
      "\tspeed: 0.0186s/iter; left time: 139.1013s\n",
      "Epoch: 7 cost time: 10.90614128112793\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1445968 Vali Loss: 0.0385820 Test Loss: 0.1284373\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1278794\n",
      "\tspeed: 0.0562s/iter; left time: 410.5942s\n",
      "\titers: 200, epoch: 8 | loss: 0.1256059\n",
      "\tspeed: 0.0186s/iter; left time: 134.0446s\n",
      "\titers: 300, epoch: 8 | loss: 0.1016735\n",
      "\tspeed: 0.0186s/iter; left time: 132.1373s\n",
      "\titers: 400, epoch: 8 | loss: 0.1710809\n",
      "\tspeed: 0.0186s/iter; left time: 130.2310s\n",
      "\titers: 500, epoch: 8 | loss: 0.1221571\n",
      "\tspeed: 0.0186s/iter; left time: 128.3341s\n",
      "Epoch: 8 cost time: 10.889902591705322\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1430321 Vali Loss: 0.0382561 Test Loss: 0.1283470\n",
      "Validation loss decreased (0.038507 --> 0.038256).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0890513\n",
      "\tspeed: 0.0586s/iter; left time: 395.0435s\n",
      "\titers: 200, epoch: 9 | loss: 0.1911742\n",
      "\tspeed: 0.0186s/iter; left time: 123.5529s\n",
      "\titers: 300, epoch: 9 | loss: 0.0837665\n",
      "\tspeed: 0.0186s/iter; left time: 121.7650s\n",
      "\titers: 400, epoch: 9 | loss: 0.1713939\n",
      "\tspeed: 0.0186s/iter; left time: 119.9300s\n",
      "\titers: 500, epoch: 9 | loss: 0.0705790\n",
      "\tspeed: 0.0186s/iter; left time: 117.8947s\n",
      "Epoch: 9 cost time: 10.894598484039307\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1432252 Vali Loss: 0.0386749 Test Loss: 0.1292707\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1382120\n",
      "\tspeed: 0.0584s/iter; left time: 360.6818s\n",
      "\titers: 200, epoch: 10 | loss: 0.1594550\n",
      "\tspeed: 0.0190s/iter; left time: 115.2172s\n",
      "\titers: 300, epoch: 10 | loss: 0.1034489\n",
      "\tspeed: 0.0187s/iter; left time: 111.9469s\n",
      "\titers: 400, epoch: 10 | loss: 0.1186969\n",
      "\tspeed: 0.0187s/iter; left time: 109.8453s\n",
      "\titers: 500, epoch: 10 | loss: 0.1209665\n",
      "\tspeed: 0.0187s/iter; left time: 107.9039s\n",
      "Epoch: 10 cost time: 11.22367787361145\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1419138 Vali Loss: 0.0382800 Test Loss: 0.1293405\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1135975\n",
      "\tspeed: 0.0555s/iter; left time: 310.8138s\n",
      "\titers: 200, epoch: 11 | loss: 0.2162677\n",
      "\tspeed: 0.0185s/iter; left time: 101.9567s\n",
      "\titers: 300, epoch: 11 | loss: 0.1270639\n",
      "\tspeed: 0.0186s/iter; left time: 100.2542s\n",
      "\titers: 400, epoch: 11 | loss: 0.0953551\n",
      "\tspeed: 0.0185s/iter; left time: 98.1621s\n",
      "\titers: 500, epoch: 11 | loss: 0.2087691\n",
      "\tspeed: 0.0185s/iter; left time: 96.2225s\n",
      "Epoch: 11 cost time: 10.83960509300232\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1425096 Vali Loss: 0.0383815 Test Loss: 0.1293128\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12851981818675995, mae:0.22300393879413605\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.4062195\n",
      "\tspeed: 0.0304s/iter; left time: 343.0527s\n",
      "\titers: 200, epoch: 1 | loss: 0.2367173\n",
      "\tspeed: 0.0183s/iter; left time: 205.4180s\n",
      "\titers: 300, epoch: 1 | loss: 0.1880140\n",
      "\tspeed: 0.0184s/iter; left time: 204.1766s\n",
      "\titers: 400, epoch: 1 | loss: 0.1825474\n",
      "\tspeed: 0.0183s/iter; left time: 201.8166s\n",
      "\titers: 500, epoch: 1 | loss: 0.2249993\n",
      "\tspeed: 0.0184s/iter; left time: 200.6227s\n",
      "Epoch: 1 cost time: 11.728650569915771\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2210236 Vali Loss: 0.0382343 Test Loss: 0.1270257\n",
      "Validation loss decreased (inf --> 0.038234).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1443575\n",
      "\tspeed: 0.0563s/iter; left time: 603.8906s\n",
      "\titers: 200, epoch: 2 | loss: 0.2346351\n",
      "\tspeed: 0.0184s/iter; left time: 196.0722s\n",
      "\titers: 300, epoch: 2 | loss: 0.3214695\n",
      "\tspeed: 0.0184s/iter; left time: 193.6431s\n",
      "\titers: 400, epoch: 2 | loss: 0.1769039\n",
      "\tspeed: 0.0184s/iter; left time: 192.3485s\n",
      "\titers: 500, epoch: 2 | loss: 0.1559059\n",
      "\tspeed: 0.0184s/iter; left time: 190.3425s\n",
      "Epoch: 2 cost time: 10.815415143966675\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1931597 Vali Loss: 0.0417072 Test Loss: 0.1257905\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1181805\n",
      "\tspeed: 0.0568s/iter; left time: 577.0389s\n",
      "\titers: 200, epoch: 3 | loss: 0.1790282\n",
      "\tspeed: 0.0185s/iter; left time: 185.6643s\n",
      "\titers: 300, epoch: 3 | loss: 0.1843037\n",
      "\tspeed: 0.0184s/iter; left time: 183.2296s\n",
      "\titers: 400, epoch: 3 | loss: 0.1559345\n",
      "\tspeed: 0.0184s/iter; left time: 181.6900s\n",
      "\titers: 500, epoch: 3 | loss: 0.1333293\n",
      "\tspeed: 0.0184s/iter; left time: 179.4039s\n",
      "Epoch: 3 cost time: 10.77991509437561\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1680607 Vali Loss: 0.0361680 Test Loss: 0.1142172\n",
      "Validation loss decreased (0.038234 --> 0.036168).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1780870\n",
      "\tspeed: 0.0555s/iter; left time: 532.1133s\n",
      "\titers: 200, epoch: 4 | loss: 0.1038720\n",
      "\tspeed: 0.0184s/iter; left time: 174.6098s\n",
      "\titers: 300, epoch: 4 | loss: 0.2237743\n",
      "\tspeed: 0.0184s/iter; left time: 172.3771s\n",
      "\titers: 400, epoch: 4 | loss: 0.1229403\n",
      "\tspeed: 0.0183s/iter; left time: 169.8617s\n",
      "\titers: 500, epoch: 4 | loss: 0.1177608\n",
      "\tspeed: 0.0182s/iter; left time: 167.2656s\n",
      "Epoch: 4 cost time: 10.752959728240967\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1538203 Vali Loss: 0.0342794 Test Loss: 0.1126993\n",
      "Validation loss decreased (0.036168 --> 0.034279).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1363425\n",
      "\tspeed: 0.0572s/iter; left time: 516.4073s\n",
      "\titers: 200, epoch: 5 | loss: 0.1396369\n",
      "\tspeed: 0.0183s/iter; left time: 163.3413s\n",
      "\titers: 300, epoch: 5 | loss: 0.1429909\n",
      "\tspeed: 0.0184s/iter; left time: 161.8782s\n",
      "\titers: 400, epoch: 5 | loss: 0.1354701\n",
      "\tspeed: 0.0184s/iter; left time: 160.1659s\n",
      "\titers: 500, epoch: 5 | loss: 0.1390233\n",
      "\tspeed: 0.0184s/iter; left time: 158.3864s\n",
      "Epoch: 5 cost time: 10.771042346954346\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1457334 Vali Loss: 0.0337416 Test Loss: 0.1117296\n",
      "Validation loss decreased (0.034279 --> 0.033742).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2053926\n",
      "\tspeed: 0.0576s/iter; left time: 486.4908s\n",
      "\titers: 200, epoch: 6 | loss: 0.1320635\n",
      "\tspeed: 0.0182s/iter; left time: 152.2783s\n",
      "\titers: 300, epoch: 6 | loss: 0.1648138\n",
      "\tspeed: 0.0182s/iter; left time: 150.1161s\n",
      "\titers: 400, epoch: 6 | loss: 0.1140018\n",
      "\tspeed: 0.0182s/iter; left time: 148.2676s\n",
      "\titers: 500, epoch: 6 | loss: 0.0900856\n",
      "\tspeed: 0.0182s/iter; left time: 146.3664s\n",
      "Epoch: 6 cost time: 10.65932846069336\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1408665 Vali Loss: 0.0336674 Test Loss: 0.1124479\n",
      "Validation loss decreased (0.033742 --> 0.033667).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1324584\n",
      "\tspeed: 0.0584s/iter; left time: 460.2924s\n",
      "\titers: 200, epoch: 7 | loss: 0.1815987\n",
      "\tspeed: 0.0182s/iter; left time: 141.4782s\n",
      "\titers: 300, epoch: 7 | loss: 0.1007532\n",
      "\tspeed: 0.0182s/iter; left time: 139.6235s\n",
      "\titers: 400, epoch: 7 | loss: 0.1524494\n",
      "\tspeed: 0.0182s/iter; left time: 137.8103s\n",
      "\titers: 500, epoch: 7 | loss: 0.1171991\n",
      "\tspeed: 0.0182s/iter; left time: 136.0599s\n",
      "Epoch: 7 cost time: 10.656153678894043\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1371023 Vali Loss: 0.0332240 Test Loss: 0.1123765\n",
      "Validation loss decreased (0.033667 --> 0.033224).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1047971\n",
      "\tspeed: 0.0552s/iter; left time: 403.6625s\n",
      "\titers: 200, epoch: 8 | loss: 0.1174079\n",
      "\tspeed: 0.0183s/iter; left time: 131.8025s\n",
      "\titers: 300, epoch: 8 | loss: 0.1243928\n",
      "\tspeed: 0.0183s/iter; left time: 130.0273s\n",
      "\titers: 400, epoch: 8 | loss: 0.1190089\n",
      "\tspeed: 0.0183s/iter; left time: 128.1797s\n",
      "\titers: 500, epoch: 8 | loss: 0.1612852\n",
      "\tspeed: 0.0183s/iter; left time: 126.3364s\n",
      "Epoch: 8 cost time: 10.737685203552246\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1349000 Vali Loss: 0.0330598 Test Loss: 0.1125979\n",
      "Validation loss decreased (0.033224 --> 0.033060).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0788062\n",
      "\tspeed: 0.0584s/iter; left time: 393.6135s\n",
      "\titers: 200, epoch: 9 | loss: 0.1062236\n",
      "\tspeed: 0.0192s/iter; left time: 127.6488s\n",
      "\titers: 300, epoch: 9 | loss: 0.1499604\n",
      "\tspeed: 0.0184s/iter; left time: 120.3126s\n",
      "\titers: 400, epoch: 9 | loss: 0.1308397\n",
      "\tspeed: 0.0184s/iter; left time: 118.5087s\n",
      "\titers: 500, epoch: 9 | loss: 0.1163688\n",
      "\tspeed: 0.0184s/iter; left time: 116.7561s\n",
      "Epoch: 9 cost time: 11.10419249534607\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1347633 Vali Loss: 0.0328941 Test Loss: 0.1120978\n",
      "Validation loss decreased (0.033060 --> 0.032894).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1081880\n",
      "\tspeed: 0.0556s/iter; left time: 342.9346s\n",
      "\titers: 200, epoch: 10 | loss: 0.0949696\n",
      "\tspeed: 0.0183s/iter; left time: 110.9965s\n",
      "\titers: 300, epoch: 10 | loss: 0.1786988\n",
      "\tspeed: 0.0183s/iter; left time: 109.1668s\n",
      "\titers: 400, epoch: 10 | loss: 0.1464879\n",
      "\tspeed: 0.0183s/iter; left time: 107.4663s\n",
      "\titers: 500, epoch: 10 | loss: 0.1142340\n",
      "\tspeed: 0.0183s/iter; left time: 105.5238s\n",
      "Epoch: 10 cost time: 10.707236051559448\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1351930 Vali Loss: 0.0327680 Test Loss: 0.1121678\n",
      "Validation loss decreased (0.032894 --> 0.032768).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1656882\n",
      "\tspeed: 0.0567s/iter; left time: 317.5606s\n",
      "\titers: 200, epoch: 11 | loss: 0.1380123\n",
      "\tspeed: 0.0183s/iter; left time: 100.5420s\n",
      "\titers: 300, epoch: 11 | loss: 0.0956098\n",
      "\tspeed: 0.0183s/iter; left time: 98.7369s\n",
      "\titers: 400, epoch: 11 | loss: 0.1846106\n",
      "\tspeed: 0.0182s/iter; left time: 96.6628s\n",
      "\titers: 500, epoch: 11 | loss: 0.1490075\n",
      "\tspeed: 0.0182s/iter; left time: 94.8412s\n",
      "Epoch: 11 cost time: 10.719211339950562\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1345975 Vali Loss: 0.0328031 Test Loss: 0.1123543\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.1867373\n",
      "\tspeed: 0.0554s/iter; left time: 278.8406s\n",
      "\titers: 200, epoch: 12 | loss: 0.1077482\n",
      "\tspeed: 0.0183s/iter; left time: 90.2543s\n",
      "\titers: 300, epoch: 12 | loss: 0.0930673\n",
      "\tspeed: 0.0183s/iter; left time: 88.3143s\n",
      "\titers: 400, epoch: 12 | loss: 0.1318797\n",
      "\tspeed: 0.0183s/iter; left time: 86.4201s\n",
      "\titers: 500, epoch: 12 | loss: 0.1422749\n",
      "\tspeed: 0.0183s/iter; left time: 84.7086s\n",
      "Epoch: 12 cost time: 10.71445083618164\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1345187 Vali Loss: 0.0327841 Test Loss: 0.1122793\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1140422\n",
      "\tspeed: 0.0552s/iter; left time: 246.0310s\n",
      "\titers: 200, epoch: 13 | loss: 0.1710508\n",
      "\tspeed: 0.0182s/iter; left time: 79.5244s\n",
      "\titers: 300, epoch: 13 | loss: 0.1083229\n",
      "\tspeed: 0.0182s/iter; left time: 77.6160s\n",
      "\titers: 400, epoch: 13 | loss: 0.1506232\n",
      "\tspeed: 0.0182s/iter; left time: 75.7479s\n",
      "\titers: 500, epoch: 13 | loss: 0.2035553\n",
      "\tspeed: 0.0182s/iter; left time: 73.9074s\n",
      "Epoch: 13 cost time: 10.69846510887146\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1355682 Vali Loss: 0.0328358 Test Loss: 0.1122939\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1123042106628418, mae:0.20286034047603607\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2797449\n",
      "\tspeed: 0.0325s/iter; left time: 366.8529s\n",
      "\titers: 200, epoch: 1 | loss: 0.1862150\n",
      "\tspeed: 0.0205s/iter; left time: 229.3223s\n",
      "\titers: 300, epoch: 1 | loss: 0.3039144\n",
      "\tspeed: 0.0205s/iter; left time: 227.3624s\n",
      "\titers: 400, epoch: 1 | loss: 0.2908368\n",
      "\tspeed: 0.0205s/iter; left time: 225.9623s\n",
      "\titers: 500, epoch: 1 | loss: 0.3222593\n",
      "\tspeed: 0.0205s/iter; left time: 223.6544s\n",
      "Epoch: 1 cost time: 12.94074010848999\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2490867 Vali Loss: 0.0448612 Test Loss: 0.1411850\n",
      "Validation loss decreased (inf --> 0.044861).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3115781\n",
      "\tspeed: 0.0597s/iter; left time: 640.5384s\n",
      "\titers: 200, epoch: 2 | loss: 0.3009275\n",
      "\tspeed: 0.0183s/iter; left time: 194.9364s\n",
      "\titers: 300, epoch: 2 | loss: 0.4145051\n",
      "\tspeed: 0.0183s/iter; left time: 193.1870s\n",
      "\titers: 400, epoch: 2 | loss: 0.1729305\n",
      "\tspeed: 0.0184s/iter; left time: 191.6454s\n",
      "\titers: 500, epoch: 2 | loss: 0.1927388\n",
      "\tspeed: 0.0184s/iter; left time: 189.6867s\n",
      "Epoch: 2 cost time: 10.770537853240967\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2132458 Vali Loss: 0.0427954 Test Loss: 0.1374245\n",
      "Validation loss decreased (0.044861 --> 0.042795).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.1280790\n",
      "\tspeed: 0.0559s/iter; left time: 568.3807s\n",
      "\titers: 200, epoch: 3 | loss: 0.2055959\n",
      "\tspeed: 0.0183s/iter; left time: 184.2526s\n",
      "\titers: 300, epoch: 3 | loss: 0.2088492\n",
      "\tspeed: 0.0183s/iter; left time: 182.2767s\n",
      "\titers: 400, epoch: 3 | loss: 0.2187045\n",
      "\tspeed: 0.0190s/iter; left time: 187.1080s\n",
      "\titers: 500, epoch: 3 | loss: 0.1717113\n",
      "\tspeed: 0.0211s/iter; left time: 205.7830s\n",
      "Epoch: 3 cost time: 11.275119066238403\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1804804 Vali Loss: 0.0406638 Test Loss: 0.1281530\n",
      "Validation loss decreased (0.042795 --> 0.040664).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1274240\n",
      "\tspeed: 0.0602s/iter; left time: 576.9717s\n",
      "\titers: 200, epoch: 4 | loss: 0.1697613\n",
      "\tspeed: 0.0211s/iter; left time: 200.5047s\n",
      "\titers: 300, epoch: 4 | loss: 0.1532433\n",
      "\tspeed: 0.0211s/iter; left time: 198.3432s\n",
      "\titers: 400, epoch: 4 | loss: 0.1963158\n",
      "\tspeed: 0.0211s/iter; left time: 196.2255s\n",
      "\titers: 500, epoch: 4 | loss: 0.1010367\n",
      "\tspeed: 0.0211s/iter; left time: 194.0158s\n",
      "Epoch: 4 cost time: 12.33368468284607\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1598551 Vali Loss: 0.0383151 Test Loss: 0.1256488\n",
      "Validation loss decreased (0.040664 --> 0.038315).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1172608\n",
      "\tspeed: 0.0619s/iter; left time: 558.7867s\n",
      "\titers: 200, epoch: 5 | loss: 0.1494514\n",
      "\tspeed: 0.0210s/iter; left time: 187.6397s\n",
      "\titers: 300, epoch: 5 | loss: 0.1080816\n",
      "\tspeed: 0.0210s/iter; left time: 185.5095s\n",
      "\titers: 400, epoch: 5 | loss: 0.1401497\n",
      "\tspeed: 0.0210s/iter; left time: 183.5318s\n",
      "\titers: 500, epoch: 5 | loss: 0.0738565\n",
      "\tspeed: 0.0210s/iter; left time: 181.3693s\n",
      "Epoch: 5 cost time: 12.307719469070435\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1475491 Vali Loss: 0.0380924 Test Loss: 0.1269571\n",
      "Validation loss decreased (0.038315 --> 0.038092).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1919727\n",
      "\tspeed: 0.0610s/iter; left time: 515.7824s\n",
      "\titers: 200, epoch: 6 | loss: 0.0946734\n",
      "\tspeed: 0.0210s/iter; left time: 175.6424s\n",
      "\titers: 300, epoch: 6 | loss: 0.2285027\n",
      "\tspeed: 0.0210s/iter; left time: 173.4857s\n",
      "\titers: 400, epoch: 6 | loss: 0.1166387\n",
      "\tspeed: 0.0211s/iter; left time: 171.6239s\n",
      "\titers: 500, epoch: 6 | loss: 0.1483490\n",
      "\tspeed: 0.0210s/iter; left time: 169.2288s\n",
      "Epoch: 6 cost time: 12.101080894470215\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1427690 Vali Loss: 0.0374106 Test Loss: 0.1276500\n",
      "Validation loss decreased (0.038092 --> 0.037411).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1298775\n",
      "\tspeed: 0.0564s/iter; left time: 444.8562s\n",
      "\titers: 200, epoch: 7 | loss: 0.1101149\n",
      "\tspeed: 0.0183s/iter; left time: 142.2841s\n",
      "\titers: 300, epoch: 7 | loss: 0.1458274\n",
      "\tspeed: 0.0183s/iter; left time: 140.5574s\n",
      "\titers: 400, epoch: 7 | loss: 0.1348525\n",
      "\tspeed: 0.0183s/iter; left time: 138.5433s\n",
      "\titers: 500, epoch: 7 | loss: 0.1037479\n",
      "\tspeed: 0.0183s/iter; left time: 136.6010s\n",
      "Epoch: 7 cost time: 10.701912641525269\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1406734 Vali Loss: 0.0375035 Test Loss: 0.1276350\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1153444\n",
      "\tspeed: 0.0558s/iter; left time: 408.1235s\n",
      "\titers: 200, epoch: 8 | loss: 0.0958842\n",
      "\tspeed: 0.0182s/iter; left time: 131.5591s\n",
      "\titers: 300, epoch: 8 | loss: 0.1312107\n",
      "\tspeed: 0.0182s/iter; left time: 129.7584s\n",
      "\titers: 400, epoch: 8 | loss: 0.1608980\n",
      "\tspeed: 0.0182s/iter; left time: 127.8687s\n",
      "\titers: 500, epoch: 8 | loss: 0.1197002\n",
      "\tspeed: 0.0182s/iter; left time: 126.0362s\n",
      "Epoch: 8 cost time: 10.706734895706177\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1382457 Vali Loss: 0.0379177 Test Loss: 0.1280280\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1367284\n",
      "\tspeed: 0.0569s/iter; left time: 383.6618s\n",
      "\titers: 200, epoch: 9 | loss: 0.1761189\n",
      "\tspeed: 0.0182s/iter; left time: 121.0432s\n",
      "\titers: 300, epoch: 9 | loss: 0.1626104\n",
      "\tspeed: 0.0182s/iter; left time: 119.1044s\n",
      "\titers: 400, epoch: 9 | loss: 0.2542892\n",
      "\tspeed: 0.0182s/iter; left time: 117.0429s\n",
      "\titers: 500, epoch: 9 | loss: 0.1088795\n",
      "\tspeed: 0.0182s/iter; left time: 115.2074s\n",
      "Epoch: 9 cost time: 10.634943962097168\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1378798 Vali Loss: 0.0379509 Test Loss: 0.1280553\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12781256437301636, mae:0.22418509423732758\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2265389\n",
      "\tspeed: 0.0351s/iter; left time: 397.1666s\n",
      "\titers: 200, epoch: 1 | loss: 0.2037170\n",
      "\tspeed: 0.0220s/iter; left time: 246.0457s\n",
      "\titers: 300, epoch: 1 | loss: 0.2611356\n",
      "\tspeed: 0.0220s/iter; left time: 243.8453s\n",
      "\titers: 400, epoch: 1 | loss: 0.3736100\n",
      "\tspeed: 0.0220s/iter; left time: 241.7193s\n",
      "\titers: 500, epoch: 1 | loss: 0.3269736\n",
      "\tspeed: 0.0220s/iter; left time: 239.4175s\n",
      "Epoch: 1 cost time: 13.895604133605957\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2639419 Vali Loss: 0.0465951 Test Loss: 0.1468236\n",
      "Validation loss decreased (inf --> 0.046595).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3247042\n",
      "\tspeed: 0.0634s/iter; left time: 679.8546s\n",
      "\titers: 200, epoch: 2 | loss: 0.2474189\n",
      "\tspeed: 0.0220s/iter; left time: 233.4122s\n",
      "\titers: 300, epoch: 2 | loss: 0.2419414\n",
      "\tspeed: 0.0219s/iter; left time: 231.1049s\n",
      "\titers: 400, epoch: 2 | loss: 0.3111796\n",
      "\tspeed: 0.0219s/iter; left time: 228.9530s\n",
      "\titers: 500, epoch: 2 | loss: 0.2558926\n",
      "\tspeed: 0.0220s/iter; left time: 226.8309s\n",
      "Epoch: 2 cost time: 12.83036494255066\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2886750 Vali Loss: 0.0646475 Test Loss: 0.1807764\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4931170\n",
      "\tspeed: 0.0619s/iter; left time: 628.8194s\n",
      "\titers: 200, epoch: 3 | loss: 0.2203465\n",
      "\tspeed: 0.0221s/iter; left time: 221.9305s\n",
      "\titers: 300, epoch: 3 | loss: 0.3041163\n",
      "\tspeed: 0.0220s/iter; left time: 219.5328s\n",
      "\titers: 400, epoch: 3 | loss: 0.2258451\n",
      "\tspeed: 0.0220s/iter; left time: 217.3383s\n",
      "\titers: 500, epoch: 3 | loss: 0.1609135\n",
      "\tspeed: 0.0220s/iter; left time: 215.0009s\n",
      "Epoch: 3 cost time: 12.845449686050415\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2678624 Vali Loss: 0.0592624 Test Loss: 0.1633920\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1885575\n",
      "\tspeed: 0.0639s/iter; left time: 613.1158s\n",
      "\titers: 200, epoch: 4 | loss: 0.2064136\n",
      "\tspeed: 0.0222s/iter; left time: 210.4561s\n",
      "\titers: 300, epoch: 4 | loss: 0.2522214\n",
      "\tspeed: 0.0221s/iter; left time: 207.3696s\n",
      "\titers: 400, epoch: 4 | loss: 0.2098793\n",
      "\tspeed: 0.0221s/iter; left time: 205.1063s\n",
      "\titers: 500, epoch: 4 | loss: 0.3203610\n",
      "\tspeed: 0.0221s/iter; left time: 202.9010s\n",
      "Epoch: 4 cost time: 13.10605502128601\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2464373 Vali Loss: 0.0537332 Test Loss: 0.1535708\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14701522886753082, mae:0.238888680934906\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2473196\n",
      "\tspeed: 0.0353s/iter; left time: 398.6536s\n",
      "\titers: 200, epoch: 1 | loss: 0.1257081\n",
      "\tspeed: 0.0232s/iter; left time: 259.8141s\n",
      "\titers: 300, epoch: 1 | loss: 0.1898378\n",
      "\tspeed: 0.0233s/iter; left time: 258.2384s\n",
      "\titers: 400, epoch: 1 | loss: 0.2509260\n",
      "\tspeed: 0.0232s/iter; left time: 254.9653s\n",
      "\titers: 500, epoch: 1 | loss: 0.2319994\n",
      "\tspeed: 0.0232s/iter; left time: 252.6140s\n",
      "Epoch: 1 cost time: 14.490164995193481\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2497239 Vali Loss: 0.0487074 Test Loss: 0.1458330\n",
      "Validation loss decreased (inf --> 0.048707).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.2340444\n",
      "\tspeed: 0.0646s/iter; left time: 692.8995s\n",
      "\titers: 200, epoch: 2 | loss: 0.1911051\n",
      "\tspeed: 0.0214s/iter; left time: 227.7004s\n",
      "\titers: 300, epoch: 2 | loss: 0.3099531\n",
      "\tspeed: 0.0214s/iter; left time: 225.6109s\n",
      "\titers: 400, epoch: 2 | loss: 0.2114587\n",
      "\tspeed: 0.0214s/iter; left time: 223.1910s\n",
      "\titers: 500, epoch: 2 | loss: 0.2178767\n",
      "\tspeed: 0.0214s/iter; left time: 221.2444s\n",
      "Epoch: 2 cost time: 12.524475574493408\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2483829 Vali Loss: 0.0637570 Test Loss: 0.1865307\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2303837\n",
      "\tspeed: 0.0627s/iter; left time: 637.0914s\n",
      "\titers: 200, epoch: 3 | loss: 0.2040206\n",
      "\tspeed: 0.0233s/iter; left time: 234.4661s\n",
      "\titers: 300, epoch: 3 | loss: 0.2155994\n",
      "\tspeed: 0.0233s/iter; left time: 232.2606s\n",
      "\titers: 400, epoch: 3 | loss: 0.1691144\n",
      "\tspeed: 0.0233s/iter; left time: 229.4769s\n",
      "\titers: 500, epoch: 3 | loss: 0.2280182\n",
      "\tspeed: 0.0233s/iter; left time: 227.2830s\n",
      "Epoch: 3 cost time: 13.536677598953247\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2552357 Vali Loss: 0.0537550 Test Loss: 0.1570746\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1636872\n",
      "\tspeed: 0.0638s/iter; left time: 612.1449s\n",
      "\titers: 200, epoch: 4 | loss: 0.2324255\n",
      "\tspeed: 0.0232s/iter; left time: 220.3252s\n",
      "\titers: 300, epoch: 4 | loss: 0.2311193\n",
      "\tspeed: 0.0232s/iter; left time: 217.9078s\n",
      "\titers: 400, epoch: 4 | loss: 0.2508463\n",
      "\tspeed: 0.0232s/iter; left time: 215.6394s\n",
      "\titers: 500, epoch: 4 | loss: 0.1465907\n",
      "\tspeed: 0.0232s/iter; left time: 213.1369s\n",
      "Epoch: 4 cost time: 13.496400117874146\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2251661 Vali Loss: 0.0478529 Test Loss: 0.1428172\n",
      "Validation loss decreased (0.048707 --> 0.047853).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3381991\n",
      "\tspeed: 0.0662s/iter; left time: 597.3159s\n",
      "\titers: 200, epoch: 5 | loss: 0.1688681\n",
      "\tspeed: 0.0233s/iter; left time: 208.1886s\n",
      "\titers: 300, epoch: 5 | loss: 0.2440124\n",
      "\tspeed: 0.0233s/iter; left time: 205.4333s\n",
      "\titers: 400, epoch: 5 | loss: 0.2265646\n",
      "\tspeed: 0.0233s/iter; left time: 202.9997s\n",
      "\titers: 500, epoch: 5 | loss: 0.1937955\n",
      "\tspeed: 0.0233s/iter; left time: 200.7527s\n",
      "Epoch: 5 cost time: 13.576146841049194\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2147581 Vali Loss: 0.0463137 Test Loss: 0.1410170\n",
      "Validation loss decreased (0.047853 --> 0.046314).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1619216\n",
      "\tspeed: 0.0653s/iter; left time: 551.7464s\n",
      "\titers: 200, epoch: 6 | loss: 0.2720591\n",
      "\tspeed: 0.0233s/iter; left time: 194.7488s\n",
      "\titers: 300, epoch: 6 | loss: 0.2588261\n",
      "\tspeed: 0.0233s/iter; left time: 191.8974s\n",
      "\titers: 400, epoch: 6 | loss: 0.1421844\n",
      "\tspeed: 0.0233s/iter; left time: 189.5194s\n",
      "\titers: 500, epoch: 6 | loss: 0.1271585\n",
      "\tspeed: 0.0233s/iter; left time: 187.2925s\n",
      "Epoch: 6 cost time: 13.571237087249756\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2085781 Vali Loss: 0.0456530 Test Loss: 0.1389347\n",
      "Validation loss decreased (0.046314 --> 0.045653).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1957026\n",
      "\tspeed: 0.0654s/iter; left time: 515.2826s\n",
      "\titers: 200, epoch: 7 | loss: 0.2440834\n",
      "\tspeed: 0.0233s/iter; left time: 181.2803s\n",
      "\titers: 300, epoch: 7 | loss: 0.2426528\n",
      "\tspeed: 0.0233s/iter; left time: 178.8299s\n",
      "\titers: 400, epoch: 7 | loss: 0.1396694\n",
      "\tspeed: 0.0233s/iter; left time: 176.5877s\n",
      "\titers: 500, epoch: 7 | loss: 0.2076509\n",
      "\tspeed: 0.0233s/iter; left time: 174.3031s\n",
      "Epoch: 7 cost time: 13.60701584815979\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2040462 Vali Loss: 0.0450921 Test Loss: 0.1383753\n",
      "Validation loss decreased (0.045653 --> 0.045092).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1679175\n",
      "\tspeed: 0.0653s/iter; left time: 477.6291s\n",
      "\titers: 200, epoch: 8 | loss: 0.2228707\n",
      "\tspeed: 0.0232s/iter; left time: 167.4423s\n",
      "\titers: 300, epoch: 8 | loss: 0.2266141\n",
      "\tspeed: 0.0231s/iter; left time: 164.2244s\n",
      "\titers: 400, epoch: 8 | loss: 0.1868207\n",
      "\tspeed: 0.0213s/iter; left time: 149.5927s\n",
      "\titers: 500, epoch: 8 | loss: 0.2489204\n",
      "\tspeed: 0.0213s/iter; left time: 147.5495s\n",
      "Epoch: 8 cost time: 13.00919222831726\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2040983 Vali Loss: 0.0444818 Test Loss: 0.1387045\n",
      "Validation loss decreased (0.045092 --> 0.044482).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.1514863\n",
      "\tspeed: 0.0633s/iter; left time: 426.6607s\n",
      "\titers: 200, epoch: 9 | loss: 0.3282959\n",
      "\tspeed: 0.0232s/iter; left time: 154.3100s\n",
      "\titers: 300, epoch: 9 | loss: 0.2165562\n",
      "\tspeed: 0.0232s/iter; left time: 151.9408s\n",
      "\titers: 400, epoch: 9 | loss: 0.1657842\n",
      "\tspeed: 0.0232s/iter; left time: 149.4859s\n",
      "\titers: 500, epoch: 9 | loss: 0.1373940\n",
      "\tspeed: 0.0232s/iter; left time: 147.0737s\n",
      "Epoch: 9 cost time: 13.532051801681519\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.2025875 Vali Loss: 0.0441560 Test Loss: 0.1378595\n",
      "Validation loss decreased (0.044482 --> 0.044156).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.1225090\n",
      "\tspeed: 0.0670s/iter; left time: 413.1492s\n",
      "\titers: 200, epoch: 10 | loss: 0.2219221\n",
      "\tspeed: 0.0233s/iter; left time: 141.5912s\n",
      "\titers: 300, epoch: 10 | loss: 0.1584260\n",
      "\tspeed: 0.0233s/iter; left time: 139.1048s\n",
      "\titers: 400, epoch: 10 | loss: 0.1926998\n",
      "\tspeed: 0.0233s/iter; left time: 136.6486s\n",
      "\titers: 500, epoch: 10 | loss: 0.2200271\n",
      "\tspeed: 0.0233s/iter; left time: 134.6092s\n",
      "Epoch: 10 cost time: 13.593413591384888\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.2016098 Vali Loss: 0.0441128 Test Loss: 0.1378013\n",
      "Validation loss decreased (0.044156 --> 0.044113).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.3134599\n",
      "\tspeed: 0.0643s/iter; left time: 360.1511s\n",
      "\titers: 200, epoch: 11 | loss: 0.2411440\n",
      "\tspeed: 0.0213s/iter; left time: 116.9405s\n",
      "\titers: 300, epoch: 11 | loss: 0.1653250\n",
      "\tspeed: 0.0213s/iter; left time: 114.8080s\n",
      "\titers: 400, epoch: 11 | loss: 0.1539716\n",
      "\tspeed: 0.0213s/iter; left time: 112.6514s\n",
      "\titers: 500, epoch: 11 | loss: 0.2373441\n",
      "\tspeed: 0.0232s/iter; left time: 120.5475s\n",
      "Epoch: 11 cost time: 12.899716854095459\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.2021595 Vali Loss: 0.0440318 Test Loss: 0.1377459\n",
      "Validation loss decreased (0.044113 --> 0.044032).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2353914\n",
      "\tspeed: 0.0648s/iter; left time: 326.1478s\n",
      "\titers: 200, epoch: 12 | loss: 0.3114094\n",
      "\tspeed: 0.0233s/iter; left time: 114.9770s\n",
      "\titers: 300, epoch: 12 | loss: 0.1593771\n",
      "\tspeed: 0.0233s/iter; left time: 112.6726s\n",
      "\titers: 400, epoch: 12 | loss: 0.3602030\n",
      "\tspeed: 0.0233s/iter; left time: 110.1724s\n",
      "\titers: 500, epoch: 12 | loss: 0.1722682\n",
      "\tspeed: 0.0233s/iter; left time: 108.0088s\n",
      "Epoch: 12 cost time: 13.593066692352295\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.2014231 Vali Loss: 0.0440433 Test Loss: 0.1377519\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.1846561\n",
      "\tspeed: 0.0638s/iter; left time: 284.4261s\n",
      "\titers: 200, epoch: 13 | loss: 0.2406921\n",
      "\tspeed: 0.0223s/iter; left time: 97.1484s\n",
      "\titers: 300, epoch: 13 | loss: 0.2706851\n",
      "\tspeed: 0.0212s/iter; left time: 90.4857s\n",
      "\titers: 400, epoch: 13 | loss: 0.1599784\n",
      "\tspeed: 0.0212s/iter; left time: 88.3751s\n",
      "\titers: 500, epoch: 13 | loss: 0.2028231\n",
      "\tspeed: 0.0212s/iter; left time: 86.2093s\n",
      "Epoch: 13 cost time: 12.68300986289978\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.2015567 Vali Loss: 0.0441264 Test Loss: 0.1377645\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.1936617\n",
      "\tspeed: 0.0608s/iter; left time: 236.6772s\n",
      "\titers: 200, epoch: 14 | loss: 0.2265234\n",
      "\tspeed: 0.0214s/iter; left time: 81.1330s\n",
      "\titers: 300, epoch: 14 | loss: 0.2004135\n",
      "\tspeed: 0.0214s/iter; left time: 78.8454s\n",
      "\titers: 400, epoch: 14 | loss: 0.1715649\n",
      "\tspeed: 0.0214s/iter; left time: 76.7130s\n",
      "\titers: 500, epoch: 14 | loss: 0.2285334\n",
      "\tspeed: 0.0214s/iter; left time: 74.5824s\n",
      "Epoch: 14 cost time: 12.462833642959595\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.2025883 Vali Loss: 0.0440201 Test Loss: 0.1377260\n",
      "Validation loss decreased (0.044032 --> 0.044020).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1840862\n",
      "\tspeed: 0.0636s/iter; left time: 211.1392s\n",
      "\titers: 200, epoch: 15 | loss: 0.1368307\n",
      "\tspeed: 0.0232s/iter; left time: 74.6847s\n",
      "\titers: 300, epoch: 15 | loss: 0.1548997\n",
      "\tspeed: 0.0232s/iter; left time: 72.3874s\n",
      "\titers: 400, epoch: 15 | loss: 0.1958587\n",
      "\tspeed: 0.0232s/iter; left time: 70.1014s\n",
      "\titers: 500, epoch: 15 | loss: 0.1892700\n",
      "\tspeed: 0.0232s/iter; left time: 67.6616s\n",
      "Epoch: 15 cost time: 13.542206048965454\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.2019070 Vali Loss: 0.0439447 Test Loss: 0.1377172\n",
      "Validation loss decreased (0.044020 --> 0.043945).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.1813580\n",
      "\tspeed: 0.0649s/iter; left time: 178.4514s\n",
      "\titers: 200, epoch: 16 | loss: 0.1363460\n",
      "\tspeed: 0.0214s/iter; left time: 56.6539s\n",
      "\titers: 300, epoch: 16 | loss: 0.2831162\n",
      "\tspeed: 0.0213s/iter; left time: 54.3488s\n",
      "\titers: 400, epoch: 16 | loss: 0.2373928\n",
      "\tspeed: 0.0213s/iter; left time: 52.1967s\n",
      "\titers: 500, epoch: 16 | loss: 0.2242551\n",
      "\tspeed: 0.0213s/iter; left time: 50.1016s\n",
      "Epoch: 16 cost time: 12.528944730758667\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.2016798 Vali Loss: 0.0439457 Test Loss: 0.1377160\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.2514225\n",
      "\tspeed: 0.0632s/iter; left time: 137.9263s\n",
      "\titers: 200, epoch: 17 | loss: 0.3110075\n",
      "\tspeed: 0.0214s/iter; left time: 44.4794s\n",
      "\titers: 300, epoch: 17 | loss: 0.2045411\n",
      "\tspeed: 0.0213s/iter; left time: 42.2202s\n",
      "\titers: 400, epoch: 17 | loss: 0.2309391\n",
      "\tspeed: 0.0213s/iter; left time: 40.0405s\n",
      "\titers: 500, epoch: 17 | loss: 0.1832193\n",
      "\tspeed: 0.0213s/iter; left time: 37.9171s\n",
      "Epoch: 17 cost time: 12.470415353775024\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.2015760 Vali Loss: 0.0441191 Test Loss: 0.1377131\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1943845\n",
      "\tspeed: 0.0629s/iter; left time: 101.2517s\n",
      "\titers: 200, epoch: 18 | loss: 0.3082238\n",
      "\tspeed: 0.0214s/iter; left time: 32.3039s\n",
      "\titers: 300, epoch: 18 | loss: 0.2240575\n",
      "\tspeed: 0.0214s/iter; left time: 30.1362s\n",
      "\titers: 400, epoch: 18 | loss: 0.2067386\n",
      "\tspeed: 0.0214s/iter; left time: 28.0041s\n",
      "\titers: 500, epoch: 18 | loss: 0.2080798\n",
      "\tspeed: 0.0214s/iter; left time: 25.8699s\n",
      "Epoch: 18 cost time: 12.660406827926636\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.2016109 Vali Loss: 0.0440172 Test Loss: 0.1377118\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13791650533676147, mae:0.23043178021907806\n",
      ">>> LR=5e-4,DO=0.2,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.2                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0005              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3004980\n",
      "\tspeed: 0.0318s/iter; left time: 359.3059s\n",
      "\titers: 200, epoch: 1 | loss: 0.1989134\n",
      "\tspeed: 0.0200s/iter; left time: 224.4763s\n",
      "\titers: 300, epoch: 1 | loss: 0.3044634\n",
      "\tspeed: 0.0200s/iter; left time: 222.1053s\n",
      "\titers: 400, epoch: 1 | loss: 0.2734650\n",
      "\tspeed: 0.0200s/iter; left time: 220.4115s\n",
      "\titers: 500, epoch: 1 | loss: 0.2556673\n",
      "\tspeed: 0.0200s/iter; left time: 218.0886s\n",
      "Epoch: 1 cost time: 12.643127202987671\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2702008 Vali Loss: 0.0588236 Test Loss: 0.1728681\n",
      "Validation loss decreased (inf --> 0.058824).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.1507782\n",
      "\tspeed: 0.0591s/iter; left time: 633.8855s\n",
      "\titers: 200, epoch: 2 | loss: 0.4606629\n",
      "\tspeed: 0.0199s/iter; left time: 211.9307s\n",
      "\titers: 300, epoch: 2 | loss: 0.3003632\n",
      "\tspeed: 0.0199s/iter; left time: 209.8366s\n",
      "\titers: 400, epoch: 2 | loss: 0.2307945\n",
      "\tspeed: 0.0199s/iter; left time: 207.8509s\n",
      "\titers: 500, epoch: 2 | loss: 0.1799846\n",
      "\tspeed: 0.0199s/iter; left time: 205.7875s\n",
      "Epoch: 2 cost time: 11.673454523086548\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.2892339 Vali Loss: 0.0639202 Test Loss: 0.1825890\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2069496\n",
      "\tspeed: 0.0578s/iter; left time: 587.3237s\n",
      "\titers: 200, epoch: 3 | loss: 0.2117680\n",
      "\tspeed: 0.0199s/iter; left time: 200.4126s\n",
      "\titers: 300, epoch: 3 | loss: 0.2944541\n",
      "\tspeed: 0.0199s/iter; left time: 198.4977s\n",
      "\titers: 400, epoch: 3 | loss: 0.2844866\n",
      "\tspeed: 0.0199s/iter; left time: 196.4444s\n",
      "\titers: 500, epoch: 3 | loss: 0.2531351\n",
      "\tspeed: 0.0199s/iter; left time: 194.4123s\n",
      "Epoch: 3 cost time: 11.632629156112671\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.2709073 Vali Loss: 0.0599584 Test Loss: 0.1668101\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.1952371\n",
      "\tspeed: 0.0590s/iter; left time: 566.1379s\n",
      "\titers: 200, epoch: 4 | loss: 0.3761272\n",
      "\tspeed: 0.0200s/iter; left time: 189.4557s\n",
      "\titers: 300, epoch: 4 | loss: 0.1631347\n",
      "\tspeed: 0.0199s/iter; left time: 186.9598s\n",
      "\titers: 400, epoch: 4 | loss: 0.2734665\n",
      "\tspeed: 0.0199s/iter; left time: 184.9284s\n",
      "\titers: 500, epoch: 4 | loss: 0.1820910\n",
      "\tspeed: 0.0199s/iter; left time: 182.8826s\n",
      "Epoch: 4 cost time: 11.612287998199463\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.2504997 Vali Loss: 0.0569234 Test Loss: 0.1600959\n",
      "Validation loss decreased (0.058824 --> 0.056923).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2042698\n",
      "\tspeed: 0.0590s/iter; left time: 531.9859s\n",
      "\titers: 200, epoch: 5 | loss: 0.3836574\n",
      "\tspeed: 0.0200s/iter; left time: 178.2830s\n",
      "\titers: 300, epoch: 5 | loss: 0.2233628\n",
      "\tspeed: 0.0200s/iter; left time: 176.2430s\n",
      "\titers: 400, epoch: 5 | loss: 0.2100992\n",
      "\tspeed: 0.0200s/iter; left time: 174.2359s\n",
      "\titers: 500, epoch: 5 | loss: 0.2138872\n",
      "\tspeed: 0.0203s/iter; left time: 174.9917s\n",
      "Epoch: 5 cost time: 11.74637484550476\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.2398774 Vali Loss: 0.0535849 Test Loss: 0.1552137\n",
      "Validation loss decreased (0.056923 --> 0.053585).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2638490\n",
      "\tspeed: 0.0608s/iter; left time: 514.1663s\n",
      "\titers: 200, epoch: 6 | loss: 0.2066480\n",
      "\tspeed: 0.0208s/iter; left time: 173.9817s\n",
      "\titers: 300, epoch: 6 | loss: 0.2113417\n",
      "\tspeed: 0.0221s/iter; left time: 182.4678s\n",
      "\titers: 400, epoch: 6 | loss: 0.1573086\n",
      "\tspeed: 0.0221s/iter; left time: 180.2088s\n",
      "\titers: 500, epoch: 6 | loss: 0.3463879\n",
      "\tspeed: 0.0221s/iter; left time: 177.9045s\n",
      "Epoch: 6 cost time: 12.567992925643921\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.2346667 Vali Loss: 0.0539009 Test Loss: 0.1536500\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2611862\n",
      "\tspeed: 0.0605s/iter; left time: 476.4527s\n",
      "\titers: 200, epoch: 7 | loss: 0.2718537\n",
      "\tspeed: 0.0221s/iter; left time: 171.9939s\n",
      "\titers: 300, epoch: 7 | loss: 0.2823459\n",
      "\tspeed: 0.0213s/iter; left time: 163.9701s\n",
      "\titers: 400, epoch: 7 | loss: 0.3576207\n",
      "\tspeed: 0.0199s/iter; left time: 150.9061s\n",
      "\titers: 500, epoch: 7 | loss: 0.1743409\n",
      "\tspeed: 0.0199s/iter; left time: 148.8838s\n",
      "Epoch: 7 cost time: 12.082859992980957\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.2333656 Vali Loss: 0.0536834 Test Loss: 0.1523338\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2495489\n",
      "\tspeed: 0.0623s/iter; left time: 455.4128s\n",
      "\titers: 200, epoch: 8 | loss: 0.4250316\n",
      "\tspeed: 0.0200s/iter; left time: 144.1982s\n",
      "\titers: 300, epoch: 8 | loss: 0.2799234\n",
      "\tspeed: 0.0200s/iter; left time: 142.4702s\n",
      "\titers: 400, epoch: 8 | loss: 0.1764279\n",
      "\tspeed: 0.0200s/iter; left time: 140.5093s\n",
      "\titers: 500, epoch: 8 | loss: 0.2514960\n",
      "\tspeed: 0.0200s/iter; left time: 138.3653s\n",
      "Epoch: 8 cost time: 11.695039749145508\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.2304643 Vali Loss: 0.0535885 Test Loss: 0.1519138\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.15540441870689392, mae:0.24983768165111542\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1513899\n",
      "\tspeed: 0.0218s/iter; left time: 246.3004s\n",
      "\titers: 200, epoch: 1 | loss: 0.1708261\n",
      "\tspeed: 0.0100s/iter; left time: 111.7105s\n",
      "\titers: 300, epoch: 1 | loss: 0.2072538\n",
      "\tspeed: 0.0100s/iter; left time: 110.8285s\n",
      "\titers: 400, epoch: 1 | loss: 0.1874213\n",
      "\tspeed: 0.0099s/iter; left time: 109.3852s\n",
      "\titers: 500, epoch: 1 | loss: 0.1462569\n",
      "\tspeed: 0.0100s/iter; left time: 108.7865s\n",
      "Epoch: 1 cost time: 6.911050319671631\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1782682 Vali Loss: 0.0488664 Test Loss: 0.1440767\n",
      "Validation loss decreased (inf --> 0.048866).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1783890\n",
      "\tspeed: 0.0356s/iter; left time: 381.9431s\n",
      "\titers: 200, epoch: 2 | loss: 0.2236717\n",
      "\tspeed: 0.0100s/iter; left time: 106.5063s\n",
      "\titers: 300, epoch: 2 | loss: 0.1199877\n",
      "\tspeed: 0.0100s/iter; left time: 105.2077s\n",
      "\titers: 400, epoch: 2 | loss: 0.1373761\n",
      "\tspeed: 0.0100s/iter; left time: 104.7091s\n",
      "\titers: 500, epoch: 2 | loss: 0.1278298\n",
      "\tspeed: 0.0100s/iter; left time: 103.7587s\n",
      "Epoch: 2 cost time: 6.003725528717041\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1318140 Vali Loss: 0.0394691 Test Loss: 0.1216232\n",
      "Validation loss decreased (0.048866 --> 0.039469).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0813655\n",
      "\tspeed: 0.0370s/iter; left time: 376.0158s\n",
      "\titers: 200, epoch: 3 | loss: 0.1225430\n",
      "\tspeed: 0.0099s/iter; left time: 99.9350s\n",
      "\titers: 300, epoch: 3 | loss: 0.0964295\n",
      "\tspeed: 0.0100s/iter; left time: 99.1670s\n",
      "\titers: 400, epoch: 3 | loss: 0.1345346\n",
      "\tspeed: 0.0100s/iter; left time: 98.1471s\n",
      "\titers: 500, epoch: 3 | loss: 0.1609011\n",
      "\tspeed: 0.0099s/iter; left time: 96.9188s\n",
      "Epoch: 3 cost time: 6.021456956863403\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1044723 Vali Loss: 0.0375850 Test Loss: 0.1230151\n",
      "Validation loss decreased (0.039469 --> 0.037585).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1030105\n",
      "\tspeed: 0.0364s/iter; left time: 348.7036s\n",
      "\titers: 200, epoch: 4 | loss: 0.0546012\n",
      "\tspeed: 0.0099s/iter; left time: 94.3217s\n",
      "\titers: 300, epoch: 4 | loss: 0.0743815\n",
      "\tspeed: 0.0100s/iter; left time: 93.6556s\n",
      "\titers: 400, epoch: 4 | loss: 0.0863305\n",
      "\tspeed: 0.0100s/iter; left time: 92.5803s\n",
      "\titers: 500, epoch: 4 | loss: 0.0859571\n",
      "\tspeed: 0.0100s/iter; left time: 91.4956s\n",
      "Epoch: 4 cost time: 5.967909812927246\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0912220 Vali Loss: 0.0352333 Test Loss: 0.1292616\n",
      "Validation loss decreased (0.037585 --> 0.035233).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0888323\n",
      "\tspeed: 0.0352s/iter; left time: 317.4891s\n",
      "\titers: 200, epoch: 5 | loss: 0.0870868\n",
      "\tspeed: 0.0099s/iter; left time: 88.2498s\n",
      "\titers: 300, epoch: 5 | loss: 0.0659903\n",
      "\tspeed: 0.0099s/iter; left time: 87.1189s\n",
      "\titers: 400, epoch: 5 | loss: 0.0912737\n",
      "\tspeed: 0.0099s/iter; left time: 86.4218s\n",
      "\titers: 500, epoch: 5 | loss: 0.0844885\n",
      "\tspeed: 0.0099s/iter; left time: 85.4188s\n",
      "Epoch: 5 cost time: 5.996942520141602\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0837980 Vali Loss: 0.0351836 Test Loss: 0.1282767\n",
      "Validation loss decreased (0.035233 --> 0.035184).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0707374\n",
      "\tspeed: 0.0361s/iter; left time: 305.2414s\n",
      "\titers: 200, epoch: 6 | loss: 0.0545981\n",
      "\tspeed: 0.0100s/iter; left time: 83.5742s\n",
      "\titers: 300, epoch: 6 | loss: 0.0826763\n",
      "\tspeed: 0.0100s/iter; left time: 82.6133s\n",
      "\titers: 400, epoch: 6 | loss: 0.0825454\n",
      "\tspeed: 0.0100s/iter; left time: 81.5377s\n",
      "\titers: 500, epoch: 6 | loss: 0.0937315\n",
      "\tspeed: 0.0100s/iter; left time: 80.2988s\n",
      "Epoch: 6 cost time: 5.992112874984741\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0797587 Vali Loss: 0.0353576 Test Loss: 0.1341149\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0572066\n",
      "\tspeed: 0.0360s/iter; left time: 283.3976s\n",
      "\titers: 200, epoch: 7 | loss: 0.0860813\n",
      "\tspeed: 0.0099s/iter; left time: 76.9901s\n",
      "\titers: 300, epoch: 7 | loss: 0.0820051\n",
      "\tspeed: 0.0099s/iter; left time: 76.0053s\n",
      "\titers: 400, epoch: 7 | loss: 0.0914026\n",
      "\tspeed: 0.0099s/iter; left time: 75.0007s\n",
      "\titers: 500, epoch: 7 | loss: 0.0608955\n",
      "\tspeed: 0.0099s/iter; left time: 73.9583s\n",
      "Epoch: 7 cost time: 5.895593881607056\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0778621 Vali Loss: 0.0352114 Test Loss: 0.1327515\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0954885\n",
      "\tspeed: 0.0363s/iter; left time: 265.1624s\n",
      "\titers: 200, epoch: 8 | loss: 0.0881027\n",
      "\tspeed: 0.0112s/iter; left time: 80.7317s\n",
      "\titers: 300, epoch: 8 | loss: 0.0847088\n",
      "\tspeed: 0.0112s/iter; left time: 79.6022s\n",
      "\titers: 400, epoch: 8 | loss: 0.1198116\n",
      "\tspeed: 0.0112s/iter; left time: 78.4532s\n",
      "\titers: 500, epoch: 8 | loss: 0.0666076\n",
      "\tspeed: 0.0112s/iter; left time: 77.3143s\n",
      "Epoch: 8 cost time: 6.6564366817474365\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0767807 Vali Loss: 0.0349456 Test Loss: 0.1335279\n",
      "Validation loss decreased (0.035184 --> 0.034946).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0757837\n",
      "\tspeed: 0.0375s/iter; left time: 253.1225s\n",
      "\titers: 200, epoch: 9 | loss: 0.0716201\n",
      "\tspeed: 0.0099s/iter; left time: 65.9872s\n",
      "\titers: 300, epoch: 9 | loss: 0.0531166\n",
      "\tspeed: 0.0099s/iter; left time: 64.9821s\n",
      "\titers: 400, epoch: 9 | loss: 0.0605998\n",
      "\tspeed: 0.0099s/iter; left time: 63.8739s\n",
      "\titers: 500, epoch: 9 | loss: 0.0825795\n",
      "\tspeed: 0.0099s/iter; left time: 62.8846s\n",
      "Epoch: 9 cost time: 5.938507795333862\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0762669 Vali Loss: 0.0351849 Test Loss: 0.1326815\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0987884\n",
      "\tspeed: 0.0364s/iter; left time: 224.7884s\n",
      "\titers: 200, epoch: 10 | loss: 0.0958489\n",
      "\tspeed: 0.0100s/iter; left time: 60.4522s\n",
      "\titers: 300, epoch: 10 | loss: 0.0612050\n",
      "\tspeed: 0.0100s/iter; left time: 59.4677s\n",
      "\titers: 400, epoch: 10 | loss: 0.0695738\n",
      "\tspeed: 0.0100s/iter; left time: 58.7287s\n",
      "\titers: 500, epoch: 10 | loss: 0.0914397\n",
      "\tspeed: 0.0100s/iter; left time: 57.6907s\n",
      "Epoch: 10 cost time: 5.978084087371826\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0759110 Vali Loss: 0.0349752 Test Loss: 0.1329615\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0891797\n",
      "\tspeed: 0.0359s/iter; left time: 201.2044s\n",
      "\titers: 200, epoch: 11 | loss: 0.0654194\n",
      "\tspeed: 0.0099s/iter; left time: 54.3065s\n",
      "\titers: 300, epoch: 11 | loss: 0.0499686\n",
      "\tspeed: 0.0099s/iter; left time: 53.5169s\n",
      "\titers: 400, epoch: 11 | loss: 0.0801086\n",
      "\tspeed: 0.0099s/iter; left time: 52.5448s\n",
      "\titers: 500, epoch: 11 | loss: 0.0725849\n",
      "\tspeed: 0.0099s/iter; left time: 51.2588s\n",
      "Epoch: 11 cost time: 5.887052059173584\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.0758053 Vali Loss: 0.0349229 Test Loss: 0.1328993\n",
      "Validation loss decreased (0.034946 --> 0.034923).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.0766716\n",
      "\tspeed: 0.0349s/iter; left time: 175.4425s\n",
      "\titers: 200, epoch: 12 | loss: 0.1050293\n",
      "\tspeed: 0.0100s/iter; left time: 49.5113s\n",
      "\titers: 300, epoch: 12 | loss: 0.0558165\n",
      "\tspeed: 0.0101s/iter; left time: 48.6738s\n",
      "\titers: 400, epoch: 12 | loss: 0.0781581\n",
      "\tspeed: 0.0101s/iter; left time: 47.6298s\n",
      "\titers: 500, epoch: 12 | loss: 0.1023269\n",
      "\tspeed: 0.0100s/iter; left time: 46.2475s\n",
      "Epoch: 12 cost time: 6.007117748260498\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.0757633 Vali Loss: 0.0349610 Test Loss: 0.1328122\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.0476927\n",
      "\tspeed: 0.0370s/iter; left time: 165.1497s\n",
      "\titers: 200, epoch: 13 | loss: 0.0836869\n",
      "\tspeed: 0.0106s/iter; left time: 46.0298s\n",
      "\titers: 300, epoch: 13 | loss: 0.0739400\n",
      "\tspeed: 0.0103s/iter; left time: 44.0769s\n",
      "\titers: 400, epoch: 13 | loss: 0.0708614\n",
      "\tspeed: 0.0112s/iter; left time: 46.6802s\n",
      "\titers: 500, epoch: 13 | loss: 0.0761347\n",
      "\tspeed: 0.0112s/iter; left time: 45.6423s\n",
      "Epoch: 13 cost time: 6.530152082443237\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.0757305 Vali Loss: 0.0349727 Test Loss: 0.1329227\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.0647223\n",
      "\tspeed: 0.0371s/iter; left time: 144.2329s\n",
      "\titers: 200, epoch: 14 | loss: 0.0526018\n",
      "\tspeed: 0.0100s/iter; left time: 37.8160s\n",
      "\titers: 300, epoch: 14 | loss: 0.0859268\n",
      "\tspeed: 0.0099s/iter; left time: 36.6997s\n",
      "\titers: 400, epoch: 14 | loss: 0.0826752\n",
      "\tspeed: 0.0099s/iter; left time: 35.7123s\n",
      "\titers: 500, epoch: 14 | loss: 0.0700437\n",
      "\tspeed: 0.0100s/iter; left time: 34.7480s\n",
      "Epoch: 14 cost time: 5.960535764694214\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.0756925 Vali Loss: 0.0349790 Test Loss: 0.1329065\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13309873640537262, mae:0.23310883343219757\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1296794\n",
      "\tspeed: 0.0214s/iter; left time: 241.8310s\n",
      "\titers: 200, epoch: 1 | loss: 0.1537673\n",
      "\tspeed: 0.0098s/iter; left time: 110.0783s\n",
      "\titers: 300, epoch: 1 | loss: 0.1868851\n",
      "\tspeed: 0.0098s/iter; left time: 108.6446s\n",
      "\titers: 400, epoch: 1 | loss: 0.1413876\n",
      "\tspeed: 0.0098s/iter; left time: 107.8086s\n",
      "\titers: 500, epoch: 1 | loss: 0.1430622\n",
      "\tspeed: 0.0098s/iter; left time: 107.0308s\n",
      "Epoch: 1 cost time: 6.793713569641113\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1467088 Vali Loss: 0.0350296 Test Loss: 0.1152290\n",
      "Validation loss decreased (inf --> 0.035030).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1114759\n",
      "\tspeed: 0.0357s/iter; left time: 382.9954s\n",
      "\titers: 200, epoch: 2 | loss: 0.1284230\n",
      "\tspeed: 0.0097s/iter; left time: 103.1264s\n",
      "\titers: 300, epoch: 2 | loss: 0.0688358\n",
      "\tspeed: 0.0097s/iter; left time: 101.9539s\n",
      "\titers: 400, epoch: 2 | loss: 0.1128854\n",
      "\tspeed: 0.0097s/iter; left time: 101.1371s\n",
      "\titers: 500, epoch: 2 | loss: 0.1115896\n",
      "\tspeed: 0.0097s/iter; left time: 100.0091s\n",
      "Epoch: 2 cost time: 5.836026191711426\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1114781 Vali Loss: 0.0329555 Test Loss: 0.1099542\n",
      "Validation loss decreased (0.035030 --> 0.032956).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0609866\n",
      "\tspeed: 0.0354s/iter; left time: 359.8601s\n",
      "\titers: 200, epoch: 3 | loss: 0.0976384\n",
      "\tspeed: 0.0098s/iter; left time: 98.2899s\n",
      "\titers: 300, epoch: 3 | loss: 0.1724140\n",
      "\tspeed: 0.0098s/iter; left time: 97.3679s\n",
      "\titers: 400, epoch: 3 | loss: 0.0866911\n",
      "\tspeed: 0.0097s/iter; left time: 96.0133s\n",
      "\titers: 500, epoch: 3 | loss: 0.0692270\n",
      "\tspeed: 0.0098s/iter; left time: 95.2227s\n",
      "Epoch: 3 cost time: 5.846827030181885\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0921401 Vali Loss: 0.0308499 Test Loss: 0.1111345\n",
      "Validation loss decreased (0.032956 --> 0.030850).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0585588\n",
      "\tspeed: 0.0363s/iter; left time: 347.9494s\n",
      "\titers: 200, epoch: 4 | loss: 0.0662341\n",
      "\tspeed: 0.0097s/iter; left time: 92.3542s\n",
      "\titers: 300, epoch: 4 | loss: 0.0571444\n",
      "\tspeed: 0.0097s/iter; left time: 91.5263s\n",
      "\titers: 400, epoch: 4 | loss: 0.0533146\n",
      "\tspeed: 0.0098s/iter; left time: 90.7453s\n",
      "\titers: 500, epoch: 4 | loss: 0.0809954\n",
      "\tspeed: 0.0098s/iter; left time: 89.6768s\n",
      "Epoch: 4 cost time: 5.854820013046265\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0807265 Vali Loss: 0.0310337 Test Loss: 0.1128686\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0621617\n",
      "\tspeed: 0.0359s/iter; left time: 323.8459s\n",
      "\titers: 200, epoch: 5 | loss: 0.1101965\n",
      "\tspeed: 0.0098s/iter; left time: 87.0880s\n",
      "\titers: 300, epoch: 5 | loss: 0.0706058\n",
      "\tspeed: 0.0098s/iter; left time: 86.2239s\n",
      "\titers: 400, epoch: 5 | loss: 0.0821538\n",
      "\tspeed: 0.0097s/iter; left time: 85.0063s\n",
      "\titers: 500, epoch: 5 | loss: 0.0852641\n",
      "\tspeed: 0.0097s/iter; left time: 83.9870s\n",
      "Epoch: 5 cost time: 5.88787841796875\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0744010 Vali Loss: 0.0319490 Test Loss: 0.1164137\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0432204\n",
      "\tspeed: 0.0347s/iter; left time: 293.1205s\n",
      "\titers: 200, epoch: 6 | loss: 0.1052784\n",
      "\tspeed: 0.0097s/iter; left time: 80.9573s\n",
      "\titers: 300, epoch: 6 | loss: 0.1071064\n",
      "\tspeed: 0.0097s/iter; left time: 80.1610s\n",
      "\titers: 400, epoch: 6 | loss: 0.0691435\n",
      "\tspeed: 0.0097s/iter; left time: 79.1394s\n",
      "\titers: 500, epoch: 6 | loss: 0.0560922\n",
      "\tspeed: 0.0097s/iter; left time: 77.9755s\n",
      "Epoch: 6 cost time: 5.808897972106934\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0713012 Vali Loss: 0.0315096 Test Loss: 0.1154929\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11129950731992722, mae:0.20474335551261902\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1780639\n",
      "\tspeed: 0.0228s/iter; left time: 258.0103s\n",
      "\titers: 200, epoch: 1 | loss: 0.1372520\n",
      "\tspeed: 0.0109s/iter; left time: 122.1899s\n",
      "\titers: 300, epoch: 1 | loss: 0.1191107\n",
      "\tspeed: 0.0109s/iter; left time: 121.3336s\n",
      "\titers: 400, epoch: 1 | loss: 0.0929553\n",
      "\tspeed: 0.0109s/iter; left time: 119.9661s\n",
      "\titers: 500, epoch: 1 | loss: 0.1663685\n",
      "\tspeed: 0.0109s/iter; left time: 119.1672s\n",
      "Epoch: 1 cost time: 7.4652252197265625\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1820953 Vali Loss: 0.0429141 Test Loss: 0.1349617\n",
      "Validation loss decreased (inf --> 0.042914).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1548477\n",
      "\tspeed: 0.0383s/iter; left time: 410.8134s\n",
      "\titers: 200, epoch: 2 | loss: 0.2209670\n",
      "\tspeed: 0.0111s/iter; left time: 117.8986s\n",
      "\titers: 300, epoch: 2 | loss: 0.1052781\n",
      "\tspeed: 0.0111s/iter; left time: 116.6775s\n",
      "\titers: 400, epoch: 2 | loss: 0.1976978\n",
      "\tspeed: 0.0110s/iter; left time: 115.2247s\n",
      "\titers: 500, epoch: 2 | loss: 0.1454997\n",
      "\tspeed: 0.0106s/iter; left time: 109.7139s\n",
      "Epoch: 2 cost time: 6.472599744796753\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1344605 Vali Loss: 0.0395189 Test Loss: 0.1307461\n",
      "Validation loss decreased (0.042914 --> 0.039519).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0897920\n",
      "\tspeed: 0.0371s/iter; left time: 377.4194s\n",
      "\titers: 200, epoch: 3 | loss: 0.0834154\n",
      "\tspeed: 0.0109s/iter; left time: 109.7199s\n",
      "\titers: 300, epoch: 3 | loss: 0.1152317\n",
      "\tspeed: 0.0109s/iter; left time: 108.7243s\n",
      "\titers: 400, epoch: 3 | loss: 0.0778258\n",
      "\tspeed: 0.0109s/iter; left time: 107.6156s\n",
      "\titers: 500, epoch: 3 | loss: 0.0889585\n",
      "\tspeed: 0.0109s/iter; left time: 106.4009s\n",
      "Epoch: 3 cost time: 6.5677101612091064\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1071869 Vali Loss: 0.0380230 Test Loss: 0.1316162\n",
      "Validation loss decreased (0.039519 --> 0.038023).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0710069\n",
      "\tspeed: 0.0367s/iter; left time: 351.8428s\n",
      "\titers: 200, epoch: 4 | loss: 0.0664880\n",
      "\tspeed: 0.0097s/iter; left time: 92.5239s\n",
      "\titers: 300, epoch: 4 | loss: 0.0639720\n",
      "\tspeed: 0.0097s/iter; left time: 91.2697s\n",
      "\titers: 400, epoch: 4 | loss: 0.1217632\n",
      "\tspeed: 0.0097s/iter; left time: 90.2531s\n",
      "\titers: 500, epoch: 4 | loss: 0.0955939\n",
      "\tspeed: 0.0097s/iter; left time: 89.2904s\n",
      "Epoch: 4 cost time: 5.844275236129761\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0925274 Vali Loss: 0.0394200 Test Loss: 0.1415058\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0904921\n",
      "\tspeed: 0.0356s/iter; left time: 320.9369s\n",
      "\titers: 200, epoch: 5 | loss: 0.0606137\n",
      "\tspeed: 0.0097s/iter; left time: 86.6527s\n",
      "\titers: 300, epoch: 5 | loss: 0.0757486\n",
      "\tspeed: 0.0097s/iter; left time: 85.6652s\n",
      "\titers: 400, epoch: 5 | loss: 0.0715954\n",
      "\tspeed: 0.0097s/iter; left time: 84.6651s\n",
      "\titers: 500, epoch: 5 | loss: 0.0930780\n",
      "\tspeed: 0.0097s/iter; left time: 83.6766s\n",
      "Epoch: 5 cost time: 5.850264072418213\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0848515 Vali Loss: 0.0375565 Test Loss: 0.1370987\n",
      "Validation loss decreased (0.038023 --> 0.037556).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0669339\n",
      "\tspeed: 0.0353s/iter; left time: 298.4126s\n",
      "\titers: 200, epoch: 6 | loss: 0.0817979\n",
      "\tspeed: 0.0097s/iter; left time: 80.8213s\n",
      "\titers: 300, epoch: 6 | loss: 0.0868388\n",
      "\tspeed: 0.0097s/iter; left time: 79.8259s\n",
      "\titers: 400, epoch: 6 | loss: 0.0847087\n",
      "\tspeed: 0.0097s/iter; left time: 79.0305s\n",
      "\titers: 500, epoch: 6 | loss: 0.0854470\n",
      "\tspeed: 0.0097s/iter; left time: 78.1901s\n",
      "Epoch: 6 cost time: 5.845348119735718\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0807174 Vali Loss: 0.0377012 Test Loss: 0.1391779\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0496188\n",
      "\tspeed: 0.0359s/iter; left time: 283.0343s\n",
      "\titers: 200, epoch: 7 | loss: 0.0840604\n",
      "\tspeed: 0.0098s/iter; left time: 76.1433s\n",
      "\titers: 300, epoch: 7 | loss: 0.0612264\n",
      "\tspeed: 0.0098s/iter; left time: 75.1802s\n",
      "\titers: 400, epoch: 7 | loss: 0.0641417\n",
      "\tspeed: 0.0098s/iter; left time: 74.1172s\n",
      "\titers: 500, epoch: 7 | loss: 0.0803898\n",
      "\tspeed: 0.0098s/iter; left time: 73.1293s\n",
      "Epoch: 7 cost time: 5.878122329711914\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0786098 Vali Loss: 0.0379949 Test Loss: 0.1402828\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0726003\n",
      "\tspeed: 0.0357s/iter; left time: 261.1568s\n",
      "\titers: 200, epoch: 8 | loss: 0.0959347\n",
      "\tspeed: 0.0097s/iter; left time: 69.7637s\n",
      "\titers: 300, epoch: 8 | loss: 0.0586260\n",
      "\tspeed: 0.0097s/iter; left time: 68.6807s\n",
      "\titers: 400, epoch: 8 | loss: 0.0814930\n",
      "\tspeed: 0.0097s/iter; left time: 67.8754s\n",
      "\titers: 500, epoch: 8 | loss: 0.0738687\n",
      "\tspeed: 0.0097s/iter; left time: 67.0096s\n",
      "Epoch: 8 cost time: 5.820334434509277\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0774923 Vali Loss: 0.0378750 Test Loss: 0.1392616\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13732154667377472, mae:0.2374982088804245\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2075656\n",
      "\tspeed: 0.0231s/iter; left time: 261.1090s\n",
      "\titers: 200, epoch: 1 | loss: 0.1891302\n",
      "\tspeed: 0.0114s/iter; left time: 127.4100s\n",
      "\titers: 300, epoch: 1 | loss: 0.1438425\n",
      "\tspeed: 0.0114s/iter; left time: 126.0559s\n",
      "\titers: 400, epoch: 1 | loss: 0.0960854\n",
      "\tspeed: 0.0114s/iter; left time: 125.0615s\n",
      "\titers: 500, epoch: 1 | loss: 0.1460082\n",
      "\tspeed: 0.0114s/iter; left time: 124.6780s\n",
      "Epoch: 1 cost time: 7.719037771224976\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1726389 Vali Loss: 0.0416023 Test Loss: 0.1333482\n",
      "Validation loss decreased (inf --> 0.041602).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1125999\n",
      "\tspeed: 0.0390s/iter; left time: 418.2467s\n",
      "\titers: 200, epoch: 2 | loss: 0.1601015\n",
      "\tspeed: 0.0115s/iter; left time: 121.9135s\n",
      "\titers: 300, epoch: 2 | loss: 0.0863702\n",
      "\tspeed: 0.0115s/iter; left time: 120.7772s\n",
      "\titers: 400, epoch: 2 | loss: 0.0722242\n",
      "\tspeed: 0.0115s/iter; left time: 119.5703s\n",
      "\titers: 500, epoch: 2 | loss: 0.1252543\n",
      "\tspeed: 0.0115s/iter; left time: 118.5673s\n",
      "Epoch: 2 cost time: 6.856490135192871\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1206118 Vali Loss: 0.0371007 Test Loss: 0.1300596\n",
      "Validation loss decreased (0.041602 --> 0.037101).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1112814\n",
      "\tspeed: 0.0399s/iter; left time: 405.1475s\n",
      "\titers: 200, epoch: 3 | loss: 0.1019169\n",
      "\tspeed: 0.0115s/iter; left time: 115.2426s\n",
      "\titers: 300, epoch: 3 | loss: 0.0533755\n",
      "\tspeed: 0.0115s/iter; left time: 114.2295s\n",
      "\titers: 400, epoch: 3 | loss: 0.0745492\n",
      "\tspeed: 0.0115s/iter; left time: 112.9506s\n",
      "\titers: 500, epoch: 3 | loss: 0.0664679\n",
      "\tspeed: 0.0115s/iter; left time: 111.7697s\n",
      "Epoch: 3 cost time: 6.845390796661377\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0850507 Vali Loss: 0.0382536 Test Loss: 0.1343134\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0650218\n",
      "\tspeed: 0.0397s/iter; left time: 380.5708s\n",
      "\titers: 200, epoch: 4 | loss: 0.0675176\n",
      "\tspeed: 0.0114s/iter; left time: 107.8480s\n",
      "\titers: 300, epoch: 4 | loss: 0.0475311\n",
      "\tspeed: 0.0114s/iter; left time: 106.8736s\n",
      "\titers: 400, epoch: 4 | loss: 0.0494996\n",
      "\tspeed: 0.0114s/iter; left time: 105.6451s\n",
      "\titers: 500, epoch: 4 | loss: 0.0569383\n",
      "\tspeed: 0.0114s/iter; left time: 104.4804s\n",
      "Epoch: 4 cost time: 6.790072917938232\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0661452 Vali Loss: 0.0368891 Test Loss: 0.1419170\n",
      "Validation loss decreased (0.037101 --> 0.036889).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0638862\n",
      "\tspeed: 0.0384s/iter; left time: 346.0693s\n",
      "\titers: 200, epoch: 5 | loss: 0.0473970\n",
      "\tspeed: 0.0114s/iter; left time: 101.6917s\n",
      "\titers: 300, epoch: 5 | loss: 0.0737297\n",
      "\tspeed: 0.0114s/iter; left time: 100.5639s\n",
      "\titers: 400, epoch: 5 | loss: 0.0438493\n",
      "\tspeed: 0.0114s/iter; left time: 99.3957s\n",
      "\titers: 500, epoch: 5 | loss: 0.0572657\n",
      "\tspeed: 0.0114s/iter; left time: 98.2505s\n",
      "Epoch: 5 cost time: 6.831918001174927\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0570209 Vali Loss: 0.0372287 Test Loss: 0.1375001\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0528088\n",
      "\tspeed: 0.0380s/iter; left time: 320.8675s\n",
      "\titers: 200, epoch: 6 | loss: 0.0441225\n",
      "\tspeed: 0.0114s/iter; left time: 95.3897s\n",
      "\titers: 300, epoch: 6 | loss: 0.0589389\n",
      "\tspeed: 0.0114s/iter; left time: 94.4198s\n",
      "\titers: 400, epoch: 6 | loss: 0.0440373\n",
      "\tspeed: 0.0114s/iter; left time: 92.9605s\n",
      "\titers: 500, epoch: 6 | loss: 0.0396450\n",
      "\tspeed: 0.0114s/iter; left time: 91.7200s\n",
      "Epoch: 6 cost time: 6.808444023132324\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0524589 Vali Loss: 0.0373383 Test Loss: 0.1398008\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0418494\n",
      "\tspeed: 0.0371s/iter; left time: 292.3338s\n",
      "\titers: 200, epoch: 7 | loss: 0.0407792\n",
      "\tspeed: 0.0114s/iter; left time: 88.9878s\n",
      "\titers: 300, epoch: 7 | loss: 0.0543232\n",
      "\tspeed: 0.0114s/iter; left time: 87.9391s\n",
      "\titers: 400, epoch: 7 | loss: 0.0451226\n",
      "\tspeed: 0.0114s/iter; left time: 86.6740s\n",
      "\titers: 500, epoch: 7 | loss: 0.0532091\n",
      "\tspeed: 0.0114s/iter; left time: 85.6423s\n",
      "Epoch: 7 cost time: 6.7970662117004395\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0501880 Vali Loss: 0.0371221 Test Loss: 0.1395622\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1421547830104828, mae:0.2445380836725235\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.0821424\n",
      "\tspeed: 0.0244s/iter; left time: 276.0178s\n",
      "\titers: 200, epoch: 1 | loss: 0.2055272\n",
      "\tspeed: 0.0121s/iter; left time: 135.4935s\n",
      "\titers: 300, epoch: 1 | loss: 0.1187363\n",
      "\tspeed: 0.0121s/iter; left time: 133.9655s\n",
      "\titers: 400, epoch: 1 | loss: 0.1494484\n",
      "\tspeed: 0.0121s/iter; left time: 132.8790s\n",
      "\titers: 500, epoch: 1 | loss: 0.1010472\n",
      "\tspeed: 0.0121s/iter; left time: 132.0276s\n",
      "Epoch: 1 cost time: 8.176197290420532\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1439106 Vali Loss: 0.0382256 Test Loss: 0.1186200\n",
      "Validation loss decreased (inf --> 0.038226).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1050378\n",
      "\tspeed: 0.0394s/iter; left time: 422.2665s\n",
      "\titers: 200, epoch: 2 | loss: 0.0721160\n",
      "\tspeed: 0.0110s/iter; left time: 117.2625s\n",
      "\titers: 300, epoch: 2 | loss: 0.1025319\n",
      "\tspeed: 0.0110s/iter; left time: 116.3265s\n",
      "\titers: 400, epoch: 2 | loss: 0.1451430\n",
      "\tspeed: 0.0111s/iter; left time: 115.2968s\n",
      "\titers: 500, epoch: 2 | loss: 0.1316334\n",
      "\tspeed: 0.0110s/iter; left time: 113.8296s\n",
      "Epoch: 2 cost time: 6.623372793197632\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1032850 Vali Loss: 0.0366824 Test Loss: 0.1188014\n",
      "Validation loss decreased (0.038226 --> 0.036682).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0892414\n",
      "\tspeed: 0.0391s/iter; left time: 397.4793s\n",
      "\titers: 200, epoch: 3 | loss: 0.0581202\n",
      "\tspeed: 0.0111s/iter; left time: 111.7640s\n",
      "\titers: 300, epoch: 3 | loss: 0.0749623\n",
      "\tspeed: 0.0111s/iter; left time: 110.4746s\n",
      "\titers: 400, epoch: 3 | loss: 0.0981017\n",
      "\tspeed: 0.0111s/iter; left time: 109.1906s\n",
      "\titers: 500, epoch: 3 | loss: 0.0787319\n",
      "\tspeed: 0.0111s/iter; left time: 108.2012s\n",
      "Epoch: 3 cost time: 6.660517692565918\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0738748 Vali Loss: 0.0344610 Test Loss: 0.1209547\n",
      "Validation loss decreased (0.036682 --> 0.034461).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0760859\n",
      "\tspeed: 0.0385s/iter; left time: 369.6381s\n",
      "\titers: 200, epoch: 4 | loss: 0.0513707\n",
      "\tspeed: 0.0111s/iter; left time: 105.5488s\n",
      "\titers: 300, epoch: 4 | loss: 0.0340439\n",
      "\tspeed: 0.0111s/iter; left time: 104.2960s\n",
      "\titers: 400, epoch: 4 | loss: 0.0534885\n",
      "\tspeed: 0.0111s/iter; left time: 103.2467s\n",
      "\titers: 500, epoch: 4 | loss: 0.0699914\n",
      "\tspeed: 0.0111s/iter; left time: 102.0638s\n",
      "Epoch: 4 cost time: 6.665702819824219\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0581782 Vali Loss: 0.0338879 Test Loss: 0.1185862\n",
      "Validation loss decreased (0.034461 --> 0.033888).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0432031\n",
      "\tspeed: 0.0395s/iter; left time: 356.1895s\n",
      "\titers: 200, epoch: 5 | loss: 0.0489614\n",
      "\tspeed: 0.0121s/iter; left time: 107.8506s\n",
      "\titers: 300, epoch: 5 | loss: 0.0570353\n",
      "\tspeed: 0.0121s/iter; left time: 106.9057s\n",
      "\titers: 400, epoch: 5 | loss: 0.0587425\n",
      "\tspeed: 0.0121s/iter; left time: 105.4398s\n",
      "\titers: 500, epoch: 5 | loss: 0.0486464\n",
      "\tspeed: 0.0121s/iter; left time: 104.3042s\n",
      "Epoch: 5 cost time: 7.242405414581299\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0501565 Vali Loss: 0.0345894 Test Loss: 0.1200429\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0685314\n",
      "\tspeed: 0.0378s/iter; left time: 319.7258s\n",
      "\titers: 200, epoch: 6 | loss: 0.0445221\n",
      "\tspeed: 0.0110s/iter; left time: 91.9871s\n",
      "\titers: 300, epoch: 6 | loss: 0.0449592\n",
      "\tspeed: 0.0110s/iter; left time: 90.9270s\n",
      "\titers: 400, epoch: 6 | loss: 0.0366026\n",
      "\tspeed: 0.0110s/iter; left time: 89.8215s\n",
      "\titers: 500, epoch: 6 | loss: 0.0413571\n",
      "\tspeed: 0.0110s/iter; left time: 88.7078s\n",
      "Epoch: 6 cost time: 6.565810680389404\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0462577 Vali Loss: 0.0350961 Test Loss: 0.1203088\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0393076\n",
      "\tspeed: 0.0382s/iter; left time: 301.2521s\n",
      "\titers: 200, epoch: 7 | loss: 0.0463046\n",
      "\tspeed: 0.0111s/iter; left time: 86.0247s\n",
      "\titers: 300, epoch: 7 | loss: 0.0442514\n",
      "\tspeed: 0.0111s/iter; left time: 84.9276s\n",
      "\titers: 400, epoch: 7 | loss: 0.0389046\n",
      "\tspeed: 0.0111s/iter; left time: 83.8447s\n",
      "\titers: 500, epoch: 7 | loss: 0.0392465\n",
      "\tspeed: 0.0111s/iter; left time: 82.7298s\n",
      "Epoch: 7 cost time: 6.645265579223633\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0443031 Vali Loss: 0.0349080 Test Loss: 0.1201539\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11877221614122391, mae:0.21709507703781128\n",
      ">>> LR=1e-4,DO=0.0,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1887072\n",
      "\tspeed: 0.0219s/iter; left time: 247.8981s\n",
      "\titers: 200, epoch: 1 | loss: 0.1759307\n",
      "\tspeed: 0.0103s/iter; left time: 114.8240s\n",
      "\titers: 300, epoch: 1 | loss: 0.2258705\n",
      "\tspeed: 0.0103s/iter; left time: 114.0519s\n",
      "\titers: 400, epoch: 1 | loss: 0.1309080\n",
      "\tspeed: 0.0103s/iter; left time: 112.9353s\n",
      "\titers: 500, epoch: 1 | loss: 0.1294963\n",
      "\tspeed: 0.0103s/iter; left time: 111.9383s\n",
      "Epoch: 1 cost time: 7.0697784423828125\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1765686 Vali Loss: 0.0423134 Test Loss: 0.1332751\n",
      "Validation loss decreased (inf --> 0.042313).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1715980\n",
      "\tspeed: 0.0375s/iter; left time: 402.7249s\n",
      "\titers: 200, epoch: 2 | loss: 0.1917507\n",
      "\tspeed: 0.0103s/iter; left time: 109.8028s\n",
      "\titers: 300, epoch: 2 | loss: 0.0993218\n",
      "\tspeed: 0.0103s/iter; left time: 108.3572s\n",
      "\titers: 400, epoch: 2 | loss: 0.0857142\n",
      "\tspeed: 0.0103s/iter; left time: 107.6253s\n",
      "\titers: 500, epoch: 2 | loss: 0.1163648\n",
      "\tspeed: 0.0103s/iter; left time: 106.7131s\n",
      "Epoch: 2 cost time: 6.2165491580963135\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1256958 Vali Loss: 0.0456945 Test Loss: 0.1378805\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1145572\n",
      "\tspeed: 0.0377s/iter; left time: 383.3426s\n",
      "\titers: 200, epoch: 3 | loss: 0.0797853\n",
      "\tspeed: 0.0103s/iter; left time: 103.3550s\n",
      "\titers: 300, epoch: 3 | loss: 0.0870536\n",
      "\tspeed: 0.0103s/iter; left time: 102.5083s\n",
      "\titers: 400, epoch: 3 | loss: 0.0757016\n",
      "\tspeed: 0.0103s/iter; left time: 101.7502s\n",
      "\titers: 500, epoch: 3 | loss: 0.1084943\n",
      "\tspeed: 0.0103s/iter; left time: 100.4450s\n",
      "Epoch: 3 cost time: 6.179216146469116\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0880999 Vali Loss: 0.0434391 Test Loss: 0.1412951\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0574450\n",
      "\tspeed: 0.0356s/iter; left time: 341.2204s\n",
      "\titers: 200, epoch: 4 | loss: 0.0721468\n",
      "\tspeed: 0.0103s/iter; left time: 97.7148s\n",
      "\titers: 300, epoch: 4 | loss: 0.0654138\n",
      "\tspeed: 0.0103s/iter; left time: 96.5665s\n",
      "\titers: 400, epoch: 4 | loss: 0.0990636\n",
      "\tspeed: 0.0103s/iter; left time: 95.7119s\n",
      "\titers: 500, epoch: 4 | loss: 0.0713094\n",
      "\tspeed: 0.0103s/iter; left time: 94.4837s\n",
      "Epoch: 4 cost time: 6.124274730682373\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0684439 Vali Loss: 0.0446042 Test Loss: 0.1375321\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1334594041109085, mae:0.23299375176429749\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1846062\n",
      "\tspeed: 0.0261s/iter; left time: 295.4808s\n",
      "\titers: 200, epoch: 1 | loss: 0.1460764\n",
      "\tspeed: 0.0138s/iter; left time: 154.7184s\n",
      "\titers: 300, epoch: 1 | loss: 0.2441318\n",
      "\tspeed: 0.0153s/iter; left time: 169.6166s\n",
      "\titers: 400, epoch: 1 | loss: 0.1463033\n",
      "\tspeed: 0.0154s/iter; left time: 169.8305s\n",
      "\titers: 500, epoch: 1 | loss: 0.1389970\n",
      "\tspeed: 0.0154s/iter; left time: 168.1610s\n",
      "Epoch: 1 cost time: 9.733242750167847\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1763569 Vali Loss: 0.0408216 Test Loss: 0.1298443\n",
      "Validation loss decreased (inf --> 0.040822).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1671756\n",
      "\tspeed: 0.0469s/iter; left time: 502.8866s\n",
      "\titers: 200, epoch: 2 | loss: 0.1170338\n",
      "\tspeed: 0.0138s/iter; left time: 146.2960s\n",
      "\titers: 300, epoch: 2 | loss: 0.1050745\n",
      "\tspeed: 0.0137s/iter; left time: 144.6303s\n",
      "\titers: 400, epoch: 2 | loss: 0.1136662\n",
      "\tspeed: 0.0137s/iter; left time: 142.6892s\n",
      "\titers: 500, epoch: 2 | loss: 0.1486110\n",
      "\tspeed: 0.0137s/iter; left time: 141.2727s\n",
      "Epoch: 2 cost time: 8.143746137619019\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1301342 Vali Loss: 0.0375515 Test Loss: 0.1284046\n",
      "Validation loss decreased (0.040822 --> 0.037552).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1702181\n",
      "\tspeed: 0.0470s/iter; left time: 477.3676s\n",
      "\titers: 200, epoch: 3 | loss: 0.0794896\n",
      "\tspeed: 0.0137s/iter; left time: 138.0545s\n",
      "\titers: 300, epoch: 3 | loss: 0.0642059\n",
      "\tspeed: 0.0137s/iter; left time: 136.6395s\n",
      "\titers: 400, epoch: 3 | loss: 0.0825356\n",
      "\tspeed: 0.0137s/iter; left time: 135.0571s\n",
      "\titers: 500, epoch: 3 | loss: 0.0720266\n",
      "\tspeed: 0.0137s/iter; left time: 133.7010s\n",
      "Epoch: 3 cost time: 8.108181953430176\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0985180 Vali Loss: 0.0344121 Test Loss: 0.1251863\n",
      "Validation loss decreased (0.037552 --> 0.034412).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0837777\n",
      "\tspeed: 0.0490s/iter; left time: 469.5353s\n",
      "\titers: 200, epoch: 4 | loss: 0.0715140\n",
      "\tspeed: 0.0154s/iter; left time: 146.4279s\n",
      "\titers: 300, epoch: 4 | loss: 0.1077494\n",
      "\tspeed: 0.0143s/iter; left time: 134.6733s\n",
      "\titers: 400, epoch: 4 | loss: 0.0905709\n",
      "\tspeed: 0.0137s/iter; left time: 127.3721s\n",
      "\titers: 500, epoch: 4 | loss: 0.0786342\n",
      "\tspeed: 0.0137s/iter; left time: 126.3451s\n",
      "Epoch: 4 cost time: 8.539471626281738\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0826425 Vali Loss: 0.0340419 Test Loss: 0.1338193\n",
      "Validation loss decreased (0.034412 --> 0.034042).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0822442\n",
      "\tspeed: 0.0489s/iter; left time: 441.1925s\n",
      "\titers: 200, epoch: 5 | loss: 0.0708540\n",
      "\tspeed: 0.0154s/iter; left time: 137.7757s\n",
      "\titers: 300, epoch: 5 | loss: 0.0620825\n",
      "\tspeed: 0.0155s/iter; left time: 136.4932s\n",
      "\titers: 400, epoch: 5 | loss: 0.0708510\n",
      "\tspeed: 0.0154s/iter; left time: 134.2401s\n",
      "\titers: 500, epoch: 5 | loss: 0.0573040\n",
      "\tspeed: 0.0154s/iter; left time: 132.3609s\n",
      "Epoch: 5 cost time: 9.076831817626953\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0745537 Vali Loss: 0.0338613 Test Loss: 0.1355065\n",
      "Validation loss decreased (0.034042 --> 0.033861).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0625130\n",
      "\tspeed: 0.0458s/iter; left time: 387.3798s\n",
      "\titers: 200, epoch: 6 | loss: 0.1003545\n",
      "\tspeed: 0.0137s/iter; left time: 114.1892s\n",
      "\titers: 300, epoch: 6 | loss: 0.0615088\n",
      "\tspeed: 0.0137s/iter; left time: 113.0349s\n",
      "\titers: 400, epoch: 6 | loss: 0.0667660\n",
      "\tspeed: 0.0137s/iter; left time: 111.7361s\n",
      "\titers: 500, epoch: 6 | loss: 0.0611838\n",
      "\tspeed: 0.0137s/iter; left time: 110.4292s\n",
      "Epoch: 6 cost time: 8.11998701095581\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0702705 Vali Loss: 0.0333649 Test Loss: 0.1425450\n",
      "Validation loss decreased (0.033861 --> 0.033365).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0654657\n",
      "\tspeed: 0.0461s/iter; left time: 363.4200s\n",
      "\titers: 200, epoch: 7 | loss: 0.0705935\n",
      "\tspeed: 0.0138s/iter; left time: 107.2216s\n",
      "\titers: 300, epoch: 7 | loss: 0.0509100\n",
      "\tspeed: 0.0138s/iter; left time: 106.0751s\n",
      "\titers: 400, epoch: 7 | loss: 0.0633409\n",
      "\tspeed: 0.0137s/iter; left time: 104.1857s\n",
      "\titers: 500, epoch: 7 | loss: 0.0975587\n",
      "\tspeed: 0.0138s/iter; left time: 102.9758s\n",
      "Epoch: 7 cost time: 8.155791282653809\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0681824 Vali Loss: 0.0336655 Test Loss: 0.1403572\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0956806\n",
      "\tspeed: 0.0445s/iter; left time: 325.0548s\n",
      "\titers: 200, epoch: 8 | loss: 0.0627450\n",
      "\tspeed: 0.0137s/iter; left time: 98.9477s\n",
      "\titers: 300, epoch: 8 | loss: 0.0663303\n",
      "\tspeed: 0.0137s/iter; left time: 97.6096s\n",
      "\titers: 400, epoch: 8 | loss: 0.0545766\n",
      "\tspeed: 0.0137s/iter; left time: 96.2621s\n",
      "\titers: 500, epoch: 8 | loss: 0.0640488\n",
      "\tspeed: 0.0137s/iter; left time: 94.9963s\n",
      "Epoch: 8 cost time: 8.114282608032227\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0669320 Vali Loss: 0.0336026 Test Loss: 0.1410938\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0882823\n",
      "\tspeed: 0.0461s/iter; left time: 310.4282s\n",
      "\titers: 200, epoch: 9 | loss: 0.0625098\n",
      "\tspeed: 0.0147s/iter; left time: 97.5299s\n",
      "\titers: 300, epoch: 9 | loss: 0.0696202\n",
      "\tspeed: 0.0137s/iter; left time: 89.4622s\n",
      "\titers: 400, epoch: 9 | loss: 0.0970610\n",
      "\tspeed: 0.0137s/iter; left time: 88.1161s\n",
      "\titers: 500, epoch: 9 | loss: 0.0535186\n",
      "\tspeed: 0.0137s/iter; left time: 86.7883s\n",
      "Epoch: 9 cost time: 8.346998691558838\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0663572 Vali Loss: 0.0335445 Test Loss: 0.1410328\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14278028905391693, mae:0.2392059564590454\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2086009\n",
      "\tspeed: 0.0269s/iter; left time: 303.5026s\n",
      "\titers: 200, epoch: 1 | loss: 0.1268833\n",
      "\tspeed: 0.0151s/iter; left time: 169.4263s\n",
      "\titers: 300, epoch: 1 | loss: 0.0825019\n",
      "\tspeed: 0.0151s/iter; left time: 167.5031s\n",
      "\titers: 400, epoch: 1 | loss: 0.1786777\n",
      "\tspeed: 0.0152s/iter; left time: 166.8682s\n",
      "\titers: 500, epoch: 1 | loss: 0.1494922\n",
      "\tspeed: 0.0151s/iter; left time: 165.0966s\n",
      "Epoch: 1 cost time: 9.847372770309448\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1467718 Vali Loss: 0.0342594 Test Loss: 0.1109782\n",
      "Validation loss decreased (inf --> 0.034259).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0795360\n",
      "\tspeed: 0.0459s/iter; left time: 492.4158s\n",
      "\titers: 200, epoch: 2 | loss: 0.1822288\n",
      "\tspeed: 0.0135s/iter; left time: 142.9983s\n",
      "\titers: 300, epoch: 2 | loss: 0.1266771\n",
      "\tspeed: 0.0135s/iter; left time: 142.1528s\n",
      "\titers: 400, epoch: 2 | loss: 0.0741273\n",
      "\tspeed: 0.0135s/iter; left time: 140.4425s\n",
      "\titers: 500, epoch: 2 | loss: 0.0941638\n",
      "\tspeed: 0.0134s/iter; left time: 138.7434s\n",
      "Epoch: 2 cost time: 7.985386848449707\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1086508 Vali Loss: 0.0347206 Test Loss: 0.1152831\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1089271\n",
      "\tspeed: 0.0438s/iter; left time: 444.7230s\n",
      "\titers: 200, epoch: 3 | loss: 0.0491227\n",
      "\tspeed: 0.0135s/iter; left time: 135.5102s\n",
      "\titers: 300, epoch: 3 | loss: 0.0805668\n",
      "\tspeed: 0.0135s/iter; left time: 134.2271s\n",
      "\titers: 400, epoch: 3 | loss: 0.0633435\n",
      "\tspeed: 0.0135s/iter; left time: 132.7364s\n",
      "\titers: 500, epoch: 3 | loss: 0.0806450\n",
      "\tspeed: 0.0134s/iter; left time: 131.1357s\n",
      "Epoch: 3 cost time: 7.9351561069488525\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0849287 Vali Loss: 0.0339566 Test Loss: 0.1128228\n",
      "Validation loss decreased (0.034259 --> 0.033957).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0430873\n",
      "\tspeed: 0.0442s/iter; left time: 423.7460s\n",
      "\titers: 200, epoch: 4 | loss: 0.0846040\n",
      "\tspeed: 0.0136s/iter; left time: 128.8513s\n",
      "\titers: 300, epoch: 4 | loss: 0.0854110\n",
      "\tspeed: 0.0136s/iter; left time: 127.6128s\n",
      "\titers: 400, epoch: 4 | loss: 0.0510526\n",
      "\tspeed: 0.0136s/iter; left time: 126.4209s\n",
      "\titers: 500, epoch: 4 | loss: 0.0482591\n",
      "\tspeed: 0.0136s/iter; left time: 125.0089s\n",
      "Epoch: 4 cost time: 8.070059776306152\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0707148 Vali Loss: 0.0325064 Test Loss: 0.1138103\n",
      "Validation loss decreased (0.033957 --> 0.032506).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0418888\n",
      "\tspeed: 0.0455s/iter; left time: 410.6865s\n",
      "\titers: 200, epoch: 5 | loss: 0.0535754\n",
      "\tspeed: 0.0134s/iter; left time: 119.5364s\n",
      "\titers: 300, epoch: 5 | loss: 0.0558173\n",
      "\tspeed: 0.0134s/iter; left time: 118.6132s\n",
      "\titers: 400, epoch: 5 | loss: 0.0423305\n",
      "\tspeed: 0.0134s/iter; left time: 117.1849s\n",
      "\titers: 500, epoch: 5 | loss: 0.0710006\n",
      "\tspeed: 0.0134s/iter; left time: 115.5864s\n",
      "Epoch: 5 cost time: 7.992184162139893\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0630975 Vali Loss: 0.0331896 Test Loss: 0.1157545\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0559983\n",
      "\tspeed: 0.0440s/iter; left time: 371.9094s\n",
      "\titers: 200, epoch: 6 | loss: 0.0775681\n",
      "\tspeed: 0.0135s/iter; left time: 112.8285s\n",
      "\titers: 300, epoch: 6 | loss: 0.0614349\n",
      "\tspeed: 0.0135s/iter; left time: 111.4589s\n",
      "\titers: 400, epoch: 6 | loss: 0.0877111\n",
      "\tspeed: 0.0135s/iter; left time: 110.1107s\n",
      "\titers: 500, epoch: 6 | loss: 0.0537601\n",
      "\tspeed: 0.0135s/iter; left time: 108.6056s\n",
      "Epoch: 6 cost time: 7.969736337661743\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0592694 Vali Loss: 0.0337582 Test Loss: 0.1174152\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0422037\n",
      "\tspeed: 0.0468s/iter; left time: 368.8233s\n",
      "\titers: 200, epoch: 7 | loss: 0.0650079\n",
      "\tspeed: 0.0156s/iter; left time: 121.0199s\n",
      "\titers: 300, epoch: 7 | loss: 0.0573400\n",
      "\tspeed: 0.0155s/iter; left time: 119.3513s\n",
      "\titers: 400, epoch: 7 | loss: 0.0773056\n",
      "\tspeed: 0.0156s/iter; left time: 117.9019s\n",
      "\titers: 500, epoch: 7 | loss: 0.0634231\n",
      "\tspeed: 0.0155s/iter; left time: 116.2757s\n",
      "Epoch: 7 cost time: 9.143394470214844\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0573789 Vali Loss: 0.0333042 Test Loss: 0.1173780\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1139611005783081, mae:0.2082395851612091\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1144149\n",
      "\tspeed: 0.0254s/iter; left time: 287.1521s\n",
      "\titers: 200, epoch: 1 | loss: 0.2406504\n",
      "\tspeed: 0.0136s/iter; left time: 152.1221s\n",
      "\titers: 300, epoch: 1 | loss: 0.2033081\n",
      "\tspeed: 0.0136s/iter; left time: 150.4241s\n",
      "\titers: 400, epoch: 1 | loss: 0.1833813\n",
      "\tspeed: 0.0136s/iter; left time: 149.2219s\n",
      "\titers: 500, epoch: 1 | loss: 0.2188929\n",
      "\tspeed: 0.0136s/iter; left time: 147.9470s\n",
      "Epoch: 1 cost time: 8.966982364654541\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1830724 Vali Loss: 0.0439534 Test Loss: 0.1360427\n",
      "Validation loss decreased (inf --> 0.043953).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1161361\n",
      "\tspeed: 0.0452s/iter; left time: 484.7631s\n",
      "\titers: 200, epoch: 2 | loss: 0.1070751\n",
      "\tspeed: 0.0135s/iter; left time: 143.6299s\n",
      "\titers: 300, epoch: 2 | loss: 0.0788092\n",
      "\tspeed: 0.0135s/iter; left time: 141.9702s\n",
      "\titers: 400, epoch: 2 | loss: 0.1083257\n",
      "\tspeed: 0.0135s/iter; left time: 140.5868s\n",
      "\titers: 500, epoch: 2 | loss: 0.1613746\n",
      "\tspeed: 0.0135s/iter; left time: 139.4492s\n",
      "Epoch: 2 cost time: 8.012458562850952\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1330781 Vali Loss: 0.0425666 Test Loss: 0.1355622\n",
      "Validation loss decreased (0.043953 --> 0.042567).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1262835\n",
      "\tspeed: 0.0476s/iter; left time: 484.0278s\n",
      "\titers: 200, epoch: 3 | loss: 0.1231543\n",
      "\tspeed: 0.0135s/iter; left time: 135.8384s\n",
      "\titers: 300, epoch: 3 | loss: 0.1130273\n",
      "\tspeed: 0.0135s/iter; left time: 134.5116s\n",
      "\titers: 400, epoch: 3 | loss: 0.0933835\n",
      "\tspeed: 0.0135s/iter; left time: 133.1970s\n",
      "\titers: 500, epoch: 3 | loss: 0.0828833\n",
      "\tspeed: 0.0135s/iter; left time: 131.4686s\n",
      "Epoch: 3 cost time: 7.970752000808716\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1018775 Vali Loss: 0.0402669 Test Loss: 0.1315529\n",
      "Validation loss decreased (0.042567 --> 0.040267).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0914271\n",
      "\tspeed: 0.0450s/iter; left time: 431.4423s\n",
      "\titers: 200, epoch: 4 | loss: 0.0881497\n",
      "\tspeed: 0.0135s/iter; left time: 128.3817s\n",
      "\titers: 300, epoch: 4 | loss: 0.1122045\n",
      "\tspeed: 0.0135s/iter; left time: 126.7946s\n",
      "\titers: 400, epoch: 4 | loss: 0.0924591\n",
      "\tspeed: 0.0135s/iter; left time: 125.3890s\n",
      "\titers: 500, epoch: 4 | loss: 0.0600994\n",
      "\tspeed: 0.0135s/iter; left time: 124.1717s\n",
      "Epoch: 4 cost time: 8.00985050201416\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0849475 Vali Loss: 0.0398159 Test Loss: 0.1358591\n",
      "Validation loss decreased (0.040267 --> 0.039816).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0953159\n",
      "\tspeed: 0.0458s/iter; left time: 413.2375s\n",
      "\titers: 200, epoch: 5 | loss: 0.0844836\n",
      "\tspeed: 0.0135s/iter; left time: 120.8500s\n",
      "\titers: 300, epoch: 5 | loss: 0.0806522\n",
      "\tspeed: 0.0136s/iter; left time: 119.6739s\n",
      "\titers: 400, epoch: 5 | loss: 0.1095421\n",
      "\tspeed: 0.0136s/iter; left time: 118.3585s\n",
      "\titers: 500, epoch: 5 | loss: 0.0639920\n",
      "\tspeed: 0.0136s/iter; left time: 117.0391s\n",
      "Epoch: 5 cost time: 8.033741235733032\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0758155 Vali Loss: 0.0397634 Test Loss: 0.1374840\n",
      "Validation loss decreased (0.039816 --> 0.039763).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0751844\n",
      "\tspeed: 0.0450s/iter; left time: 380.2091s\n",
      "\titers: 200, epoch: 6 | loss: 0.0710672\n",
      "\tspeed: 0.0136s/iter; left time: 113.1653s\n",
      "\titers: 300, epoch: 6 | loss: 0.0579034\n",
      "\tspeed: 0.0136s/iter; left time: 111.8779s\n",
      "\titers: 400, epoch: 6 | loss: 0.1137909\n",
      "\tspeed: 0.0135s/iter; left time: 110.4330s\n",
      "\titers: 500, epoch: 6 | loss: 0.0788763\n",
      "\tspeed: 0.0135s/iter; left time: 109.0721s\n",
      "Epoch: 6 cost time: 8.00044870376587\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0710732 Vali Loss: 0.0392962 Test Loss: 0.1376793\n",
      "Validation loss decreased (0.039763 --> 0.039296).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0663552\n",
      "\tspeed: 0.0468s/iter; left time: 368.6887s\n",
      "\titers: 200, epoch: 7 | loss: 0.0596826\n",
      "\tspeed: 0.0136s/iter; left time: 106.1222s\n",
      "\titers: 300, epoch: 7 | loss: 0.0695392\n",
      "\tspeed: 0.0137s/iter; left time: 104.8674s\n",
      "\titers: 400, epoch: 7 | loss: 0.0918856\n",
      "\tspeed: 0.0136s/iter; left time: 103.3925s\n",
      "\titers: 500, epoch: 7 | loss: 0.0480557\n",
      "\tspeed: 0.0136s/iter; left time: 101.8550s\n",
      "Epoch: 7 cost time: 8.188356399536133\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0685185 Vali Loss: 0.0394485 Test Loss: 0.1380076\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0535319\n",
      "\tspeed: 0.0444s/iter; left time: 324.9272s\n",
      "\titers: 200, epoch: 8 | loss: 0.0492976\n",
      "\tspeed: 0.0136s/iter; left time: 98.0654s\n",
      "\titers: 300, epoch: 8 | loss: 0.0699289\n",
      "\tspeed: 0.0136s/iter; left time: 96.7324s\n",
      "\titers: 400, epoch: 8 | loss: 0.0725985\n",
      "\tspeed: 0.0136s/iter; left time: 95.3926s\n",
      "\titers: 500, epoch: 8 | loss: 0.0543380\n",
      "\tspeed: 0.0136s/iter; left time: 93.8839s\n",
      "Epoch: 8 cost time: 8.032116651535034\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0673541 Vali Loss: 0.0400491 Test Loss: 0.1392234\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0756388\n",
      "\tspeed: 0.0462s/iter; left time: 311.7130s\n",
      "\titers: 200, epoch: 9 | loss: 0.0615586\n",
      "\tspeed: 0.0135s/iter; left time: 89.4975s\n",
      "\titers: 300, epoch: 9 | loss: 0.0448848\n",
      "\tspeed: 0.0135s/iter; left time: 88.1384s\n",
      "\titers: 400, epoch: 9 | loss: 0.0649287\n",
      "\tspeed: 0.0135s/iter; left time: 86.7789s\n",
      "\titers: 500, epoch: 9 | loss: 0.0648801\n",
      "\tspeed: 0.0135s/iter; left time: 85.3952s\n",
      "Epoch: 9 cost time: 7.943592309951782\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0667382 Vali Loss: 0.0394727 Test Loss: 0.1387659\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13790731132030487, mae:0.24107909202575684\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2078299\n",
      "\tspeed: 0.0299s/iter; left time: 337.6181s\n",
      "\titers: 200, epoch: 1 | loss: 0.1424055\n",
      "\tspeed: 0.0168s/iter; left time: 188.5422s\n",
      "\titers: 300, epoch: 1 | loss: 0.1897625\n",
      "\tspeed: 0.0163s/iter; left time: 181.0576s\n",
      "\titers: 400, epoch: 1 | loss: 0.1409163\n",
      "\tspeed: 0.0163s/iter; left time: 179.6436s\n",
      "\titers: 500, epoch: 1 | loss: 0.1303341\n",
      "\tspeed: 0.0163s/iter; left time: 178.1641s\n",
      "Epoch: 1 cost time: 10.756354570388794\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1713149 Vali Loss: 0.0450306 Test Loss: 0.1379519\n",
      "Validation loss decreased (inf --> 0.045031).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1361812\n",
      "\tspeed: 0.0518s/iter; left time: 556.0741s\n",
      "\titers: 200, epoch: 2 | loss: 0.1315437\n",
      "\tspeed: 0.0174s/iter; left time: 184.7382s\n",
      "\titers: 300, epoch: 2 | loss: 0.1162446\n",
      "\tspeed: 0.0163s/iter; left time: 171.7494s\n",
      "\titers: 400, epoch: 2 | loss: 0.0781730\n",
      "\tspeed: 0.0163s/iter; left time: 170.1253s\n",
      "\titers: 500, epoch: 2 | loss: 0.0761840\n",
      "\tspeed: 0.0163s/iter; left time: 168.1779s\n",
      "Epoch: 2 cost time: 9.858685731887817\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1173301 Vali Loss: 0.0351438 Test Loss: 0.1238170\n",
      "Validation loss decreased (0.045031 --> 0.035144).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0778482\n",
      "\tspeed: 0.0500s/iter; left time: 507.8258s\n",
      "\titers: 200, epoch: 3 | loss: 0.0825490\n",
      "\tspeed: 0.0163s/iter; left time: 164.4866s\n",
      "\titers: 300, epoch: 3 | loss: 0.1426406\n",
      "\tspeed: 0.0164s/iter; left time: 162.9999s\n",
      "\titers: 400, epoch: 3 | loss: 0.0660927\n",
      "\tspeed: 0.0163s/iter; left time: 161.1585s\n",
      "\titers: 500, epoch: 3 | loss: 0.0541380\n",
      "\tspeed: 0.0163s/iter; left time: 159.5571s\n",
      "Epoch: 3 cost time: 9.611938953399658\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0766610 Vali Loss: 0.0350214 Test Loss: 0.1284641\n",
      "Validation loss decreased (0.035144 --> 0.035021).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0515998\n",
      "\tspeed: 0.0502s/iter; left time: 481.3074s\n",
      "\titers: 200, epoch: 4 | loss: 0.0599067\n",
      "\tspeed: 0.0163s/iter; left time: 154.6950s\n",
      "\titers: 300, epoch: 4 | loss: 0.0413344\n",
      "\tspeed: 0.0163s/iter; left time: 153.1628s\n",
      "\titers: 400, epoch: 4 | loss: 0.0718858\n",
      "\tspeed: 0.0163s/iter; left time: 151.0935s\n",
      "\titers: 500, epoch: 4 | loss: 0.0584151\n",
      "\tspeed: 0.0163s/iter; left time: 149.5000s\n",
      "Epoch: 4 cost time: 9.617743015289307\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0558121 Vali Loss: 0.0368052 Test Loss: 0.1358294\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0419310\n",
      "\tspeed: 0.0500s/iter; left time: 450.7432s\n",
      "\titers: 200, epoch: 5 | loss: 0.0496591\n",
      "\tspeed: 0.0163s/iter; left time: 145.6540s\n",
      "\titers: 300, epoch: 5 | loss: 0.0489003\n",
      "\tspeed: 0.0170s/iter; left time: 149.9195s\n",
      "\titers: 400, epoch: 5 | loss: 0.0590560\n",
      "\tspeed: 0.0170s/iter; left time: 148.3170s\n",
      "\titers: 500, epoch: 5 | loss: 0.0397884\n",
      "\tspeed: 0.0164s/iter; left time: 141.5396s\n",
      "Epoch: 5 cost time: 9.779428243637085\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0466488 Vali Loss: 0.0354055 Test Loss: 0.1407815\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0395191\n",
      "\tspeed: 0.0492s/iter; left time: 415.7760s\n",
      "\titers: 200, epoch: 6 | loss: 0.0326661\n",
      "\tspeed: 0.0163s/iter; left time: 135.7793s\n",
      "\titers: 300, epoch: 6 | loss: 0.0438034\n",
      "\tspeed: 0.0163s/iter; left time: 134.0866s\n",
      "\titers: 400, epoch: 6 | loss: 0.0483762\n",
      "\tspeed: 0.0163s/iter; left time: 132.4723s\n",
      "\titers: 500, epoch: 6 | loss: 0.0372350\n",
      "\tspeed: 0.0163s/iter; left time: 130.8755s\n",
      "Epoch: 6 cost time: 9.553225040435791\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0420321 Vali Loss: 0.0355620 Test Loss: 0.1407559\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12866666913032532, mae:0.23136582970619202\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1477773\n",
      "\tspeed: 0.0276s/iter; left time: 311.4221s\n",
      "\titers: 200, epoch: 1 | loss: 0.0753202\n",
      "\tspeed: 0.0158s/iter; left time: 176.6321s\n",
      "\titers: 300, epoch: 1 | loss: 0.1597915\n",
      "\tspeed: 0.0158s/iter; left time: 175.0127s\n",
      "\titers: 400, epoch: 1 | loss: 0.1282118\n",
      "\tspeed: 0.0158s/iter; left time: 173.3450s\n",
      "\titers: 500, epoch: 1 | loss: 0.0819353\n",
      "\tspeed: 0.0158s/iter; left time: 171.9961s\n",
      "Epoch: 1 cost time: 10.220461368560791\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1416919 Vali Loss: 0.0340319 Test Loss: 0.1118738\n",
      "Validation loss decreased (inf --> 0.034032).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1035396\n",
      "\tspeed: 0.0499s/iter; left time: 535.5446s\n",
      "\titers: 200, epoch: 2 | loss: 0.0897047\n",
      "\tspeed: 0.0158s/iter; left time: 168.1354s\n",
      "\titers: 300, epoch: 2 | loss: 0.1571794\n",
      "\tspeed: 0.0158s/iter; left time: 166.9000s\n",
      "\titers: 400, epoch: 2 | loss: 0.0875617\n",
      "\tspeed: 0.0158s/iter; left time: 164.9477s\n",
      "\titers: 500, epoch: 2 | loss: 0.0733796\n",
      "\tspeed: 0.0158s/iter; left time: 163.6464s\n",
      "Epoch: 2 cost time: 9.347056865692139\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.0999425 Vali Loss: 0.0362646 Test Loss: 0.1160469\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0676384\n",
      "\tspeed: 0.0492s/iter; left time: 499.4163s\n",
      "\titers: 200, epoch: 3 | loss: 0.0603915\n",
      "\tspeed: 0.0170s/iter; left time: 171.2039s\n",
      "\titers: 300, epoch: 3 | loss: 0.0641372\n",
      "\tspeed: 0.0158s/iter; left time: 157.8411s\n",
      "\titers: 400, epoch: 3 | loss: 0.0787638\n",
      "\tspeed: 0.0159s/iter; left time: 156.3721s\n",
      "\titers: 500, epoch: 3 | loss: 0.0494916\n",
      "\tspeed: 0.0158s/iter; left time: 154.6424s\n",
      "Epoch: 3 cost time: 9.570987462997437\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0663488 Vali Loss: 0.0348005 Test Loss: 0.1191276\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0568060\n",
      "\tspeed: 0.0492s/iter; left time: 471.6696s\n",
      "\titers: 200, epoch: 4 | loss: 0.0655669\n",
      "\tspeed: 0.0158s/iter; left time: 150.0566s\n",
      "\titers: 300, epoch: 4 | loss: 0.0402272\n",
      "\tspeed: 0.0157s/iter; left time: 147.8605s\n",
      "\titers: 400, epoch: 4 | loss: 0.0446116\n",
      "\tspeed: 0.0157s/iter; left time: 146.0819s\n",
      "\titers: 500, epoch: 4 | loss: 0.0549300\n",
      "\tspeed: 0.0157s/iter; left time: 144.4756s\n",
      "Epoch: 4 cost time: 9.302895545959473\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0491865 Vali Loss: 0.0349448 Test Loss: 0.1214562\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11205563694238663, mae:0.20530520379543304\n",
      ">>> LR=1e-4,DO=0.0,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1708764\n",
      "\tspeed: 0.0264s/iter; left time: 298.8679s\n",
      "\titers: 200, epoch: 1 | loss: 0.1193258\n",
      "\tspeed: 0.0147s/iter; left time: 164.2314s\n",
      "\titers: 300, epoch: 1 | loss: 0.1656508\n",
      "\tspeed: 0.0147s/iter; left time: 162.8241s\n",
      "\titers: 400, epoch: 1 | loss: 0.1511722\n",
      "\tspeed: 0.0147s/iter; left time: 161.7395s\n",
      "\titers: 500, epoch: 1 | loss: 0.2388563\n",
      "\tspeed: 0.0147s/iter; left time: 160.0216s\n",
      "Epoch: 1 cost time: 9.596433639526367\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1793625 Vali Loss: 0.0441069 Test Loss: 0.1392629\n",
      "Validation loss decreased (inf --> 0.044107).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1864589\n",
      "\tspeed: 0.0477s/iter; left time: 511.6191s\n",
      "\titers: 200, epoch: 2 | loss: 0.0733957\n",
      "\tspeed: 0.0148s/iter; left time: 156.9551s\n",
      "\titers: 300, epoch: 2 | loss: 0.1169123\n",
      "\tspeed: 0.0148s/iter; left time: 155.5349s\n",
      "\titers: 400, epoch: 2 | loss: 0.1325214\n",
      "\tspeed: 0.0148s/iter; left time: 153.8665s\n",
      "\titers: 500, epoch: 2 | loss: 0.1146732\n",
      "\tspeed: 0.0147s/iter; left time: 152.3125s\n",
      "Epoch: 2 cost time: 8.725805282592773\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1177451 Vali Loss: 0.0400731 Test Loss: 0.1421569\n",
      "Validation loss decreased (0.044107 --> 0.040073).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0823320\n",
      "\tspeed: 0.0475s/iter; left time: 482.9730s\n",
      "\titers: 200, epoch: 3 | loss: 0.0564177\n",
      "\tspeed: 0.0147s/iter; left time: 147.9563s\n",
      "\titers: 300, epoch: 3 | loss: 0.0987329\n",
      "\tspeed: 0.0147s/iter; left time: 146.4744s\n",
      "\titers: 400, epoch: 3 | loss: 0.0602564\n",
      "\tspeed: 0.0147s/iter; left time: 145.0209s\n",
      "\titers: 500, epoch: 3 | loss: 0.0829141\n",
      "\tspeed: 0.0147s/iter; left time: 143.5454s\n",
      "Epoch: 3 cost time: 8.67487096786499\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0726184 Vali Loss: 0.0375189 Test Loss: 0.1363660\n",
      "Validation loss decreased (0.040073 --> 0.037519).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0425757\n",
      "\tspeed: 0.0480s/iter; left time: 460.4176s\n",
      "\titers: 200, epoch: 4 | loss: 0.0856920\n",
      "\tspeed: 0.0147s/iter; left time: 139.9568s\n",
      "\titers: 300, epoch: 4 | loss: 0.0566378\n",
      "\tspeed: 0.0147s/iter; left time: 137.8108s\n",
      "\titers: 400, epoch: 4 | loss: 0.0437738\n",
      "\tspeed: 0.0147s/iter; left time: 136.5454s\n",
      "\titers: 500, epoch: 4 | loss: 0.0620285\n",
      "\tspeed: 0.0147s/iter; left time: 135.0241s\n",
      "Epoch: 4 cost time: 8.667943239212036\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0526851 Vali Loss: 0.0370105 Test Loss: 0.1403879\n",
      "Validation loss decreased (0.037519 --> 0.037011).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0406114\n",
      "\tspeed: 0.0468s/iter; left time: 422.1289s\n",
      "\titers: 200, epoch: 5 | loss: 0.0422068\n",
      "\tspeed: 0.0147s/iter; left time: 131.4009s\n",
      "\titers: 300, epoch: 5 | loss: 0.0453009\n",
      "\tspeed: 0.0147s/iter; left time: 129.7250s\n",
      "\titers: 400, epoch: 5 | loss: 0.0435832\n",
      "\tspeed: 0.0147s/iter; left time: 128.2108s\n",
      "\titers: 500, epoch: 5 | loss: 0.0368674\n",
      "\tspeed: 0.0147s/iter; left time: 126.8584s\n",
      "Epoch: 5 cost time: 8.666273832321167\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0436435 Vali Loss: 0.0378411 Test Loss: 0.1423287\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0305940\n",
      "\tspeed: 0.0478s/iter; left time: 403.7260s\n",
      "\titers: 200, epoch: 6 | loss: 0.0309110\n",
      "\tspeed: 0.0146s/iter; left time: 122.2153s\n",
      "\titers: 300, epoch: 6 | loss: 0.0290665\n",
      "\tspeed: 0.0146s/iter; left time: 120.7654s\n",
      "\titers: 400, epoch: 6 | loss: 0.0337679\n",
      "\tspeed: 0.0146s/iter; left time: 119.1185s\n",
      "\titers: 500, epoch: 6 | loss: 0.0298539\n",
      "\tspeed: 0.0146s/iter; left time: 117.6469s\n",
      "Epoch: 6 cost time: 8.618802309036255\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0393141 Vali Loss: 0.0380284 Test Loss: 0.1433180\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0388735\n",
      "\tspeed: 0.0459s/iter; left time: 361.8422s\n",
      "\titers: 200, epoch: 7 | loss: 0.0317292\n",
      "\tspeed: 0.0146s/iter; left time: 113.8060s\n",
      "\titers: 300, epoch: 7 | loss: 0.0503877\n",
      "\tspeed: 0.0146s/iter; left time: 112.3527s\n",
      "\titers: 400, epoch: 7 | loss: 0.0319478\n",
      "\tspeed: 0.0146s/iter; left time: 110.7869s\n",
      "\titers: 500, epoch: 7 | loss: 0.0518978\n",
      "\tspeed: 0.0146s/iter; left time: 109.3582s\n",
      "Epoch: 7 cost time: 8.615338563919067\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0371862 Vali Loss: 0.0383767 Test Loss: 0.1444984\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14058546721935272, mae:0.2441914826631546\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1207708\n",
      "\tspeed: 0.0315s/iter; left time: 355.8846s\n",
      "\titers: 200, epoch: 1 | loss: 0.1658366\n",
      "\tspeed: 0.0182s/iter; left time: 203.6576s\n",
      "\titers: 300, epoch: 1 | loss: 0.2600677\n",
      "\tspeed: 0.0176s/iter; left time: 194.8982s\n",
      "\titers: 400, epoch: 1 | loss: 0.1324711\n",
      "\tspeed: 0.0176s/iter; left time: 193.6093s\n",
      "\titers: 500, epoch: 1 | loss: 0.1258223\n",
      "\tspeed: 0.0176s/iter; left time: 192.1152s\n",
      "Epoch: 1 cost time: 11.536066055297852\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1767459 Vali Loss: 0.0456234 Test Loss: 0.1338982\n",
      "Validation loss decreased (inf --> 0.045623).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1645802\n",
      "\tspeed: 0.0552s/iter; left time: 592.4459s\n",
      "\titers: 200, epoch: 2 | loss: 0.1161484\n",
      "\tspeed: 0.0176s/iter; left time: 187.2089s\n",
      "\titers: 300, epoch: 2 | loss: 0.1286412\n",
      "\tspeed: 0.0176s/iter; left time: 185.1962s\n",
      "\titers: 400, epoch: 2 | loss: 0.1164942\n",
      "\tspeed: 0.0176s/iter; left time: 183.8046s\n",
      "\titers: 500, epoch: 2 | loss: 0.1442048\n",
      "\tspeed: 0.0176s/iter; left time: 181.4330s\n",
      "Epoch: 2 cost time: 10.399192571640015\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1249090 Vali Loss: 0.0433283 Test Loss: 0.1317271\n",
      "Validation loss decreased (0.045623 --> 0.043328).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0977896\n",
      "\tspeed: 0.0577s/iter; left time: 586.5457s\n",
      "\titers: 200, epoch: 3 | loss: 0.0681730\n",
      "\tspeed: 0.0201s/iter; left time: 202.3260s\n",
      "\titers: 300, epoch: 3 | loss: 0.0849765\n",
      "\tspeed: 0.0201s/iter; left time: 200.1293s\n",
      "\titers: 400, epoch: 3 | loss: 0.0804550\n",
      "\tspeed: 0.0201s/iter; left time: 198.6785s\n",
      "\titers: 500, epoch: 3 | loss: 0.0725421\n",
      "\tspeed: 0.0201s/iter; left time: 196.4736s\n",
      "Epoch: 3 cost time: 11.788912534713745\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0941606 Vali Loss: 0.0353566 Test Loss: 0.1254870\n",
      "Validation loss decreased (0.043328 --> 0.035357).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0793846\n",
      "\tspeed: 0.0606s/iter; left time: 581.3496s\n",
      "\titers: 200, epoch: 4 | loss: 0.0919872\n",
      "\tspeed: 0.0202s/iter; left time: 192.1408s\n",
      "\titers: 300, epoch: 4 | loss: 0.0671227\n",
      "\tspeed: 0.0203s/iter; left time: 190.2016s\n",
      "\titers: 400, epoch: 4 | loss: 0.0602730\n",
      "\tspeed: 0.0203s/iter; left time: 188.3894s\n",
      "\titers: 500, epoch: 4 | loss: 0.0878569\n",
      "\tspeed: 0.0202s/iter; left time: 186.0652s\n",
      "Epoch: 4 cost time: 11.852471828460693\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0772238 Vali Loss: 0.0342536 Test Loss: 0.1399447\n",
      "Validation loss decreased (0.035357 --> 0.034254).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0656800\n",
      "\tspeed: 0.0590s/iter; left time: 532.5329s\n",
      "\titers: 200, epoch: 5 | loss: 0.0561583\n",
      "\tspeed: 0.0201s/iter; left time: 179.5940s\n",
      "\titers: 300, epoch: 5 | loss: 0.0804801\n",
      "\tspeed: 0.0201s/iter; left time: 177.5356s\n",
      "\titers: 400, epoch: 5 | loss: 0.0545313\n",
      "\tspeed: 0.0201s/iter; left time: 175.3201s\n",
      "\titers: 500, epoch: 5 | loss: 0.0810072\n",
      "\tspeed: 0.0201s/iter; left time: 173.3436s\n",
      "Epoch: 5 cost time: 11.775858163833618\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0680126 Vali Loss: 0.0335107 Test Loss: 0.1322938\n",
      "Validation loss decreased (0.034254 --> 0.033511).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0404243\n",
      "\tspeed: 0.0590s/iter; left time: 498.4312s\n",
      "\titers: 200, epoch: 6 | loss: 0.0678751\n",
      "\tspeed: 0.0202s/iter; left time: 168.6742s\n",
      "\titers: 300, epoch: 6 | loss: 0.0676996\n",
      "\tspeed: 0.0202s/iter; left time: 166.6787s\n",
      "\titers: 400, epoch: 6 | loss: 0.0885342\n",
      "\tspeed: 0.0202s/iter; left time: 164.5565s\n",
      "\titers: 500, epoch: 6 | loss: 0.0566523\n",
      "\tspeed: 0.0202s/iter; left time: 162.5612s\n",
      "Epoch: 6 cost time: 11.806547403335571\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0635844 Vali Loss: 0.0332794 Test Loss: 0.1336828\n",
      "Validation loss decreased (0.033511 --> 0.033279).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0589511\n",
      "\tspeed: 0.0572s/iter; left time: 450.6698s\n",
      "\titers: 200, epoch: 7 | loss: 0.0571943\n",
      "\tspeed: 0.0174s/iter; left time: 135.0704s\n",
      "\titers: 300, epoch: 7 | loss: 0.0717927\n",
      "\tspeed: 0.0173s/iter; left time: 133.2322s\n",
      "\titers: 400, epoch: 7 | loss: 0.0719622\n",
      "\tspeed: 0.0173s/iter; left time: 131.4966s\n",
      "\titers: 500, epoch: 7 | loss: 0.0520488\n",
      "\tspeed: 0.0174s/iter; left time: 129.9712s\n",
      "Epoch: 7 cost time: 10.21242356300354\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0611406 Vali Loss: 0.0330539 Test Loss: 0.1343281\n",
      "Validation loss decreased (0.033279 --> 0.033054).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0647300\n",
      "\tspeed: 0.0563s/iter; left time: 411.9439s\n",
      "\titers: 200, epoch: 8 | loss: 0.0595467\n",
      "\tspeed: 0.0175s/iter; left time: 125.9396s\n",
      "\titers: 300, epoch: 8 | loss: 0.0460011\n",
      "\tspeed: 0.0175s/iter; left time: 124.1192s\n",
      "\titers: 400, epoch: 8 | loss: 0.0597768\n",
      "\tspeed: 0.0174s/iter; left time: 122.3091s\n",
      "\titers: 500, epoch: 8 | loss: 0.0478682\n",
      "\tspeed: 0.0174s/iter; left time: 120.5109s\n",
      "Epoch: 8 cost time: 10.27616572380066\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0599008 Vali Loss: 0.0333627 Test Loss: 0.1343927\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0450619\n",
      "\tspeed: 0.0538s/iter; left time: 362.3760s\n",
      "\titers: 200, epoch: 9 | loss: 0.0991942\n",
      "\tspeed: 0.0174s/iter; left time: 115.3352s\n",
      "\titers: 300, epoch: 9 | loss: 0.0457475\n",
      "\tspeed: 0.0174s/iter; left time: 113.5359s\n",
      "\titers: 400, epoch: 9 | loss: 0.0412745\n",
      "\tspeed: 0.0174s/iter; left time: 111.8607s\n",
      "\titers: 500, epoch: 9 | loss: 0.0411304\n",
      "\tspeed: 0.0174s/iter; left time: 110.1161s\n",
      "Epoch: 9 cost time: 10.205106258392334\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.0593183 Vali Loss: 0.0332272 Test Loss: 0.1348051\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0575047\n",
      "\tspeed: 0.0546s/iter; left time: 336.9763s\n",
      "\titers: 200, epoch: 10 | loss: 0.0695797\n",
      "\tspeed: 0.0174s/iter; left time: 105.8516s\n",
      "\titers: 300, epoch: 10 | loss: 0.0439022\n",
      "\tspeed: 0.0174s/iter; left time: 103.8646s\n",
      "\titers: 400, epoch: 10 | loss: 0.0652481\n",
      "\tspeed: 0.0174s/iter; left time: 102.1745s\n",
      "\titers: 500, epoch: 10 | loss: 0.0322872\n",
      "\tspeed: 0.0174s/iter; left time: 100.3637s\n",
      "Epoch: 10 cost time: 10.224437713623047\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.0589843 Vali Loss: 0.0331905 Test Loss: 0.1361058\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13452480733394623, mae:0.23368285596370697\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.3226862\n",
      "\tspeed: 0.0287s/iter; left time: 324.8456s\n",
      "\titers: 200, epoch: 1 | loss: 0.1668440\n",
      "\tspeed: 0.0171s/iter; left time: 192.0673s\n",
      "\titers: 300, epoch: 1 | loss: 0.1776613\n",
      "\tspeed: 0.0171s/iter; left time: 190.2011s\n",
      "\titers: 400, epoch: 1 | loss: 0.1115717\n",
      "\tspeed: 0.0172s/iter; left time: 188.6897s\n",
      "\titers: 500, epoch: 1 | loss: 0.1671815\n",
      "\tspeed: 0.0171s/iter; left time: 186.8331s\n",
      "Epoch: 1 cost time: 10.990800857543945\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1464359 Vali Loss: 0.0346369 Test Loss: 0.1122974\n",
      "Validation loss decreased (inf --> 0.034637).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0989168\n",
      "\tspeed: 0.0539s/iter; left time: 578.7761s\n",
      "\titers: 200, epoch: 2 | loss: 0.1485835\n",
      "\tspeed: 0.0171s/iter; left time: 182.0885s\n",
      "\titers: 300, epoch: 2 | loss: 0.1692001\n",
      "\tspeed: 0.0172s/iter; left time: 180.9079s\n",
      "\titers: 400, epoch: 2 | loss: 0.0990008\n",
      "\tspeed: 0.0171s/iter; left time: 178.6821s\n",
      "\titers: 500, epoch: 2 | loss: 0.0971779\n",
      "\tspeed: 0.0171s/iter; left time: 177.0269s\n",
      "Epoch: 2 cost time: 10.109720230102539\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1085422 Vali Loss: 0.0356164 Test Loss: 0.1155539\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0768530\n",
      "\tspeed: 0.0557s/iter; left time: 565.9163s\n",
      "\titers: 200, epoch: 3 | loss: 0.0811924\n",
      "\tspeed: 0.0172s/iter; left time: 172.8669s\n",
      "\titers: 300, epoch: 3 | loss: 0.0978360\n",
      "\tspeed: 0.0172s/iter; left time: 171.1942s\n",
      "\titers: 400, epoch: 3 | loss: 0.0615743\n",
      "\tspeed: 0.0172s/iter; left time: 169.1427s\n",
      "\titers: 500, epoch: 3 | loss: 0.0687462\n",
      "\tspeed: 0.0172s/iter; left time: 167.5609s\n",
      "Epoch: 3 cost time: 10.098459482192993\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0823146 Vali Loss: 0.0338312 Test Loss: 0.1130264\n",
      "Validation loss decreased (0.034637 --> 0.033831).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0664183\n",
      "\tspeed: 0.0534s/iter; left time: 512.3058s\n",
      "\titers: 200, epoch: 4 | loss: 0.0526979\n",
      "\tspeed: 0.0172s/iter; left time: 162.8415s\n",
      "\titers: 300, epoch: 4 | loss: 0.0824772\n",
      "\tspeed: 0.0172s/iter; left time: 161.2442s\n",
      "\titers: 400, epoch: 4 | loss: 0.0836798\n",
      "\tspeed: 0.0171s/iter; left time: 159.1569s\n",
      "\titers: 500, epoch: 4 | loss: 0.0645510\n",
      "\tspeed: 0.0171s/iter; left time: 157.3931s\n",
      "Epoch: 4 cost time: 10.071844339370728\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0654951 Vali Loss: 0.0329143 Test Loss: 0.1138199\n",
      "Validation loss decreased (0.033831 --> 0.032914).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0522060\n",
      "\tspeed: 0.0557s/iter; left time: 502.2308s\n",
      "\titers: 200, epoch: 5 | loss: 0.0592664\n",
      "\tspeed: 0.0192s/iter; left time: 171.3831s\n",
      "\titers: 300, epoch: 5 | loss: 0.0512895\n",
      "\tspeed: 0.0192s/iter; left time: 169.4563s\n",
      "\titers: 400, epoch: 5 | loss: 0.0640452\n",
      "\tspeed: 0.0192s/iter; left time: 167.5009s\n",
      "\titers: 500, epoch: 5 | loss: 0.0600721\n",
      "\tspeed: 0.0192s/iter; left time: 165.4613s\n",
      "Epoch: 5 cost time: 11.255624771118164\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0567611 Vali Loss: 0.0334743 Test Loss: 0.1158179\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0593614\n",
      "\tspeed: 0.0554s/iter; left time: 468.4867s\n",
      "\titers: 200, epoch: 6 | loss: 0.0394505\n",
      "\tspeed: 0.0173s/iter; left time: 144.2485s\n",
      "\titers: 300, epoch: 6 | loss: 0.0592579\n",
      "\tspeed: 0.0173s/iter; left time: 142.4552s\n",
      "\titers: 400, epoch: 6 | loss: 0.0466528\n",
      "\tspeed: 0.0173s/iter; left time: 140.7197s\n",
      "\titers: 500, epoch: 6 | loss: 0.0352588\n",
      "\tspeed: 0.0172s/iter; left time: 138.6235s\n",
      "Epoch: 6 cost time: 10.150465250015259\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0526633 Vali Loss: 0.0331158 Test Loss: 0.1148413\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0470458\n",
      "\tspeed: 0.0565s/iter; left time: 445.5645s\n",
      "\titers: 200, epoch: 7 | loss: 0.0658160\n",
      "\tspeed: 0.0192s/iter; left time: 149.4241s\n",
      "\titers: 300, epoch: 7 | loss: 0.0436879\n",
      "\tspeed: 0.0181s/iter; left time: 139.2962s\n",
      "\titers: 400, epoch: 7 | loss: 0.0596417\n",
      "\tspeed: 0.0172s/iter; left time: 130.7616s\n",
      "\titers: 500, epoch: 7 | loss: 0.0392330\n",
      "\tspeed: 0.0173s/iter; left time: 129.0794s\n",
      "Epoch: 7 cost time: 10.635610342025757\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0504884 Vali Loss: 0.0333423 Test Loss: 0.1157486\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11395978182554245, mae:0.20874567329883575\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2526689\n",
      "\tspeed: 0.0313s/iter; left time: 353.7906s\n",
      "\titers: 200, epoch: 1 | loss: 0.1506011\n",
      "\tspeed: 0.0194s/iter; left time: 217.1471s\n",
      "\titers: 300, epoch: 1 | loss: 0.1895043\n",
      "\tspeed: 0.0194s/iter; left time: 215.6675s\n",
      "\titers: 400, epoch: 1 | loss: 0.1955848\n",
      "\tspeed: 0.0194s/iter; left time: 213.8754s\n",
      "\titers: 500, epoch: 1 | loss: 0.2660136\n",
      "\tspeed: 0.0194s/iter; left time: 211.2190s\n",
      "Epoch: 1 cost time: 12.301667928695679\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1832555 Vali Loss: 0.0414752 Test Loss: 0.1353214\n",
      "Validation loss decreased (inf --> 0.041475).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1894857\n",
      "\tspeed: 0.0588s/iter; left time: 630.7406s\n",
      "\titers: 200, epoch: 2 | loss: 0.2140321\n",
      "\tspeed: 0.0173s/iter; left time: 183.6757s\n",
      "\titers: 300, epoch: 2 | loss: 0.2231693\n",
      "\tspeed: 0.0173s/iter; left time: 181.8062s\n",
      "\titers: 400, epoch: 2 | loss: 0.1023709\n",
      "\tspeed: 0.0173s/iter; left time: 180.4059s\n",
      "\titers: 500, epoch: 2 | loss: 0.0706140\n",
      "\tspeed: 0.0173s/iter; left time: 178.5975s\n",
      "Epoch: 2 cost time: 10.270910739898682\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1284721 Vali Loss: 0.0430929 Test Loss: 0.1399398\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1038830\n",
      "\tspeed: 0.0549s/iter; left time: 557.7296s\n",
      "\titers: 200, epoch: 3 | loss: 0.1072234\n",
      "\tspeed: 0.0194s/iter; left time: 195.5055s\n",
      "\titers: 300, epoch: 3 | loss: 0.0985372\n",
      "\tspeed: 0.0200s/iter; left time: 199.4483s\n",
      "\titers: 400, epoch: 3 | loss: 0.0693403\n",
      "\tspeed: 0.0200s/iter; left time: 197.6306s\n",
      "\titers: 500, epoch: 3 | loss: 0.0678159\n",
      "\tspeed: 0.0189s/iter; left time: 184.8847s\n",
      "Epoch: 3 cost time: 11.058108568191528\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0921640 Vali Loss: 0.0411940 Test Loss: 0.1413373\n",
      "Validation loss decreased (0.041475 --> 0.041194).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0639332\n",
      "\tspeed: 0.0569s/iter; left time: 546.0422s\n",
      "\titers: 200, epoch: 4 | loss: 0.0903152\n",
      "\tspeed: 0.0194s/iter; left time: 184.2020s\n",
      "\titers: 300, epoch: 4 | loss: 0.0720685\n",
      "\tspeed: 0.0194s/iter; left time: 182.2950s\n",
      "\titers: 400, epoch: 4 | loss: 0.0802401\n",
      "\tspeed: 0.0194s/iter; left time: 180.3670s\n",
      "\titers: 500, epoch: 4 | loss: 0.0611074\n",
      "\tspeed: 0.0194s/iter; left time: 178.4050s\n",
      "Epoch: 4 cost time: 11.358631610870361\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0717763 Vali Loss: 0.0385932 Test Loss: 0.1380353\n",
      "Validation loss decreased (0.041194 --> 0.038593).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0552789\n",
      "\tspeed: 0.0565s/iter; left time: 509.8717s\n",
      "\titers: 200, epoch: 5 | loss: 0.0573784\n",
      "\tspeed: 0.0173s/iter; left time: 154.7055s\n",
      "\titers: 300, epoch: 5 | loss: 0.0559159\n",
      "\tspeed: 0.0173s/iter; left time: 152.7007s\n",
      "\titers: 400, epoch: 5 | loss: 0.0563369\n",
      "\tspeed: 0.0173s/iter; left time: 150.8240s\n",
      "\titers: 500, epoch: 5 | loss: 0.0454566\n",
      "\tspeed: 0.0173s/iter; left time: 149.2895s\n",
      "Epoch: 5 cost time: 10.19694209098816\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0615526 Vali Loss: 0.0402327 Test Loss: 0.1463692\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0563857\n",
      "\tspeed: 0.0565s/iter; left time: 477.2357s\n",
      "\titers: 200, epoch: 6 | loss: 0.0544896\n",
      "\tspeed: 0.0193s/iter; left time: 161.3685s\n",
      "\titers: 300, epoch: 6 | loss: 0.0795186\n",
      "\tspeed: 0.0192s/iter; left time: 158.6411s\n",
      "\titers: 400, epoch: 6 | loss: 0.0644698\n",
      "\tspeed: 0.0192s/iter; left time: 156.3005s\n",
      "\titers: 500, epoch: 6 | loss: 0.0572942\n",
      "\tspeed: 0.0193s/iter; left time: 155.5157s\n",
      "Epoch: 6 cost time: 11.306349515914917\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0564596 Vali Loss: 0.0390845 Test Loss: 0.1458327\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0485623\n",
      "\tspeed: 0.0584s/iter; left time: 460.0897s\n",
      "\titers: 200, epoch: 7 | loss: 0.0468063\n",
      "\tspeed: 0.0201s/iter; left time: 156.0798s\n",
      "\titers: 300, epoch: 7 | loss: 0.0684707\n",
      "\tspeed: 0.0200s/iter; left time: 153.6194s\n",
      "\titers: 400, epoch: 7 | loss: 0.0643950\n",
      "\tspeed: 0.0200s/iter; left time: 151.7115s\n",
      "\titers: 500, epoch: 7 | loss: 0.0415587\n",
      "\tspeed: 0.0200s/iter; left time: 149.6281s\n",
      "Epoch: 7 cost time: 11.60454797744751\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0539953 Vali Loss: 0.0394990 Test Loss: 0.1465579\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13822001218795776, mae:0.23914697766304016\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1888228\n",
      "\tspeed: 0.0330s/iter; left time: 373.4069s\n",
      "\titers: 200, epoch: 1 | loss: 0.1582943\n",
      "\tspeed: 0.0213s/iter; left time: 238.1746s\n",
      "\titers: 300, epoch: 1 | loss: 0.1563228\n",
      "\tspeed: 0.0212s/iter; left time: 235.7607s\n",
      "\titers: 400, epoch: 1 | loss: 0.2523674\n",
      "\tspeed: 0.0212s/iter; left time: 233.7124s\n",
      "\titers: 500, epoch: 1 | loss: 0.2365478\n",
      "\tspeed: 0.0213s/iter; left time: 231.8212s\n",
      "Epoch: 1 cost time: 13.352354049682617\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1755242 Vali Loss: 0.0442618 Test Loss: 0.1377467\n",
      "Validation loss decreased (inf --> 0.044262).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1871693\n",
      "\tspeed: 0.0634s/iter; left time: 680.4544s\n",
      "\titers: 200, epoch: 2 | loss: 0.0924348\n",
      "\tspeed: 0.0213s/iter; left time: 226.2418s\n",
      "\titers: 300, epoch: 2 | loss: 0.1252180\n",
      "\tspeed: 0.0213s/iter; left time: 224.0350s\n",
      "\titers: 400, epoch: 2 | loss: 0.1210898\n",
      "\tspeed: 0.0213s/iter; left time: 221.8571s\n",
      "\titers: 500, epoch: 2 | loss: 0.0729622\n",
      "\tspeed: 0.0213s/iter; left time: 219.7002s\n",
      "Epoch: 2 cost time: 12.45240592956543\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1170748 Vali Loss: 0.0391907 Test Loss: 0.1344381\n",
      "Validation loss decreased (0.044262 --> 0.039191).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0959562\n",
      "\tspeed: 0.0627s/iter; left time: 636.9416s\n",
      "\titers: 200, epoch: 3 | loss: 0.0675417\n",
      "\tspeed: 0.0213s/iter; left time: 214.5693s\n",
      "\titers: 300, epoch: 3 | loss: 0.0984169\n",
      "\tspeed: 0.0213s/iter; left time: 212.6383s\n",
      "\titers: 400, epoch: 3 | loss: 0.0519896\n",
      "\tspeed: 0.0213s/iter; left time: 210.1327s\n",
      "\titers: 500, epoch: 3 | loss: 0.0565442\n",
      "\tspeed: 0.0213s/iter; left time: 208.0683s\n",
      "Epoch: 3 cost time: 12.452890157699585\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0725646 Vali Loss: 0.0358213 Test Loss: 0.1385621\n",
      "Validation loss decreased (0.039191 --> 0.035821).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0500028\n",
      "\tspeed: 0.0638s/iter; left time: 612.3572s\n",
      "\titers: 200, epoch: 4 | loss: 0.0390338\n",
      "\tspeed: 0.0213s/iter; left time: 202.1930s\n",
      "\titers: 300, epoch: 4 | loss: 0.0459121\n",
      "\tspeed: 0.0214s/iter; left time: 201.2188s\n",
      "\titers: 400, epoch: 4 | loss: 0.0498361\n",
      "\tspeed: 0.0215s/iter; left time: 199.3294s\n",
      "\titers: 500, epoch: 4 | loss: 0.0820427\n",
      "\tspeed: 0.0214s/iter; left time: 197.1320s\n",
      "Epoch: 4 cost time: 12.546135425567627\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0506902 Vali Loss: 0.0382494 Test Loss: 0.1395986\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0575483\n",
      "\tspeed: 0.0605s/iter; left time: 545.7770s\n",
      "\titers: 200, epoch: 5 | loss: 0.0359831\n",
      "\tspeed: 0.0214s/iter; left time: 190.8560s\n",
      "\titers: 300, epoch: 5 | loss: 0.0293592\n",
      "\tspeed: 0.0214s/iter; left time: 188.8994s\n",
      "\titers: 400, epoch: 5 | loss: 0.0327550\n",
      "\tspeed: 0.0214s/iter; left time: 186.5905s\n",
      "\titers: 500, epoch: 5 | loss: 0.0358799\n",
      "\tspeed: 0.0214s/iter; left time: 184.4448s\n",
      "Epoch: 5 cost time: 12.495253562927246\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0410975 Vali Loss: 0.0380475 Test Loss: 0.1372982\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0283897\n",
      "\tspeed: 0.0632s/iter; left time: 534.4561s\n",
      "\titers: 200, epoch: 6 | loss: 0.0320395\n",
      "\tspeed: 0.0232s/iter; left time: 193.6637s\n",
      "\titers: 300, epoch: 6 | loss: 0.0325276\n",
      "\tspeed: 0.0232s/iter; left time: 191.0877s\n",
      "\titers: 400, epoch: 6 | loss: 0.0307756\n",
      "\tspeed: 0.0232s/iter; left time: 188.8699s\n",
      "\titers: 500, epoch: 6 | loss: 0.0497869\n",
      "\tspeed: 0.0232s/iter; left time: 186.6592s\n",
      "Epoch: 6 cost time: 13.485600233078003\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0364597 Vali Loss: 0.0385614 Test Loss: 0.1396468\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13873861730098724, mae:0.24120382964611053\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1323162\n",
      "\tspeed: 0.0324s/iter; left time: 365.9606s\n",
      "\titers: 200, epoch: 1 | loss: 0.0824137\n",
      "\tspeed: 0.0205s/iter; left time: 229.9565s\n",
      "\titers: 300, epoch: 1 | loss: 0.1541070\n",
      "\tspeed: 0.0206s/iter; left time: 228.2453s\n",
      "\titers: 400, epoch: 1 | loss: 0.1056086\n",
      "\tspeed: 0.0207s/iter; left time: 227.9306s\n",
      "\titers: 500, epoch: 1 | loss: 0.1541315\n",
      "\tspeed: 0.0207s/iter; left time: 225.5029s\n",
      "Epoch: 1 cost time: 12.992482662200928\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1440308 Vali Loss: 0.0353116 Test Loss: 0.1119556\n",
      "Validation loss decreased (inf --> 0.035312).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1237014\n",
      "\tspeed: 0.0629s/iter; left time: 674.7764s\n",
      "\titers: 200, epoch: 2 | loss: 0.0813918\n",
      "\tspeed: 0.0205s/iter; left time: 218.2935s\n",
      "\titers: 300, epoch: 2 | loss: 0.1027531\n",
      "\tspeed: 0.0205s/iter; left time: 216.0846s\n",
      "\titers: 400, epoch: 2 | loss: 0.0933123\n",
      "\tspeed: 0.0205s/iter; left time: 214.1870s\n",
      "\titers: 500, epoch: 2 | loss: 0.0625068\n",
      "\tspeed: 0.0205s/iter; left time: 212.0182s\n",
      "Epoch: 2 cost time: 12.043232202529907\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.0988597 Vali Loss: 0.0343631 Test Loss: 0.1173671\n",
      "Validation loss decreased (0.035312 --> 0.034363).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0641420\n",
      "\tspeed: 0.0610s/iter; left time: 619.9910s\n",
      "\titers: 200, epoch: 3 | loss: 0.0536526\n",
      "\tspeed: 0.0206s/iter; left time: 207.4061s\n",
      "\titers: 300, epoch: 3 | loss: 0.0694883\n",
      "\tspeed: 0.0206s/iter; left time: 205.4276s\n",
      "\titers: 400, epoch: 3 | loss: 0.0559392\n",
      "\tspeed: 0.0206s/iter; left time: 203.2567s\n",
      "\titers: 500, epoch: 3 | loss: 0.0553465\n",
      "\tspeed: 0.0206s/iter; left time: 201.1398s\n",
      "Epoch: 3 cost time: 12.063308000564575\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0621782 Vali Loss: 0.0342525 Test Loss: 0.1219186\n",
      "Validation loss decreased (0.034363 --> 0.034253).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0366462\n",
      "\tspeed: 0.0624s/iter; left time: 598.9145s\n",
      "\titers: 200, epoch: 4 | loss: 0.0427919\n",
      "\tspeed: 0.0211s/iter; left time: 200.1421s\n",
      "\titers: 300, epoch: 4 | loss: 0.0457355\n",
      "\tspeed: 0.0206s/iter; left time: 193.1645s\n",
      "\titers: 400, epoch: 4 | loss: 0.0432367\n",
      "\tspeed: 0.0206s/iter; left time: 191.3034s\n",
      "\titers: 500, epoch: 4 | loss: 0.0349271\n",
      "\tspeed: 0.0206s/iter; left time: 189.0720s\n",
      "Epoch: 4 cost time: 12.267435550689697\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0436198 Vali Loss: 0.0343519 Test Loss: 0.1205175\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0357755\n",
      "\tspeed: 0.0618s/iter; left time: 557.1993s\n",
      "\titers: 200, epoch: 5 | loss: 0.0355270\n",
      "\tspeed: 0.0207s/iter; left time: 184.3108s\n",
      "\titers: 300, epoch: 5 | loss: 0.0306037\n",
      "\tspeed: 0.0207s/iter; left time: 182.3336s\n",
      "\titers: 400, epoch: 5 | loss: 0.0420264\n",
      "\tspeed: 0.0207s/iter; left time: 180.2034s\n",
      "\titers: 500, epoch: 5 | loss: 0.0385151\n",
      "\tspeed: 0.0207s/iter; left time: 178.3067s\n",
      "Epoch: 5 cost time: 12.058452129364014\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0357689 Vali Loss: 0.0346102 Test Loss: 0.1223844\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0234594\n",
      "\tspeed: 0.0595s/iter; left time: 502.6456s\n",
      "\titers: 200, epoch: 6 | loss: 0.0269406\n",
      "\tspeed: 0.0207s/iter; left time: 172.6149s\n",
      "\titers: 300, epoch: 6 | loss: 0.0281651\n",
      "\tspeed: 0.0206s/iter; left time: 170.2992s\n",
      "\titers: 400, epoch: 6 | loss: 0.0390316\n",
      "\tspeed: 0.0207s/iter; left time: 168.4123s\n",
      "\titers: 500, epoch: 6 | loss: 0.0268994\n",
      "\tspeed: 0.0207s/iter; left time: 166.3358s\n",
      "Epoch: 6 cost time: 12.044403791427612\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0320021 Vali Loss: 0.0347968 Test Loss: 0.1230432\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12209206074476242, mae:0.2197163850069046\n",
      ">>> LR=1e-4,DO=0.0,EL=4,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1993479\n",
      "\tspeed: 0.0313s/iter; left time: 353.9002s\n",
      "\titers: 200, epoch: 1 | loss: 0.1782197\n",
      "\tspeed: 0.0191s/iter; left time: 214.2549s\n",
      "\titers: 300, epoch: 1 | loss: 0.1975592\n",
      "\tspeed: 0.0191s/iter; left time: 212.4062s\n",
      "\titers: 400, epoch: 1 | loss: 0.2005119\n",
      "\tspeed: 0.0192s/iter; left time: 211.2888s\n",
      "\titers: 500, epoch: 1 | loss: 0.1762355\n",
      "\tspeed: 0.0191s/iter; left time: 208.7330s\n",
      "Epoch: 1 cost time: 12.183826208114624\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1741566 Vali Loss: 0.0434220 Test Loss: 0.1350538\n",
      "Validation loss decreased (inf --> 0.043422).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0638883\n",
      "\tspeed: 0.0599s/iter; left time: 642.2771s\n",
      "\titers: 200, epoch: 2 | loss: 0.1771397\n",
      "\tspeed: 0.0192s/iter; left time: 203.6211s\n",
      "\titers: 300, epoch: 2 | loss: 0.0777541\n",
      "\tspeed: 0.0191s/iter; left time: 201.1651s\n",
      "\titers: 400, epoch: 2 | loss: 0.0867392\n",
      "\tspeed: 0.0191s/iter; left time: 199.4075s\n",
      "\titers: 500, epoch: 2 | loss: 0.0822950\n",
      "\tspeed: 0.0191s/iter; left time: 197.1210s\n",
      "Epoch: 2 cost time: 11.251380681991577\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1135138 Vali Loss: 0.0410773 Test Loss: 0.1439481\n",
      "Validation loss decreased (0.043422 --> 0.041077).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0571769\n",
      "\tspeed: 0.0656s/iter; left time: 666.7367s\n",
      "\titers: 200, epoch: 3 | loss: 0.0578771\n",
      "\tspeed: 0.0211s/iter; left time: 212.3458s\n",
      "\titers: 300, epoch: 3 | loss: 0.0776076\n",
      "\tspeed: 0.0211s/iter; left time: 209.8417s\n",
      "\titers: 400, epoch: 3 | loss: 0.0569135\n",
      "\tspeed: 0.0198s/iter; left time: 194.9057s\n",
      "\titers: 500, epoch: 3 | loss: 0.0758753\n",
      "\tspeed: 0.0199s/iter; left time: 194.6192s\n",
      "Epoch: 3 cost time: 12.129183769226074\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.0699668 Vali Loss: 0.0388080 Test Loss: 0.1375320\n",
      "Validation loss decreased (0.041077 --> 0.038808).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0319529\n",
      "\tspeed: 0.0622s/iter; left time: 596.9635s\n",
      "\titers: 200, epoch: 4 | loss: 0.0382339\n",
      "\tspeed: 0.0192s/iter; left time: 181.8529s\n",
      "\titers: 300, epoch: 4 | loss: 0.0422708\n",
      "\tspeed: 0.0192s/iter; left time: 179.8590s\n",
      "\titers: 400, epoch: 4 | loss: 0.0494687\n",
      "\tspeed: 0.0192s/iter; left time: 177.9689s\n",
      "\titers: 500, epoch: 4 | loss: 0.0360663\n",
      "\tspeed: 0.0192s/iter; left time: 176.4502s\n",
      "Epoch: 4 cost time: 11.298121690750122\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0483091 Vali Loss: 0.0379723 Test Loss: 0.1417125\n",
      "Validation loss decreased (0.038808 --> 0.037972).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0414709\n",
      "\tspeed: 0.0618s/iter; left time: 557.2423s\n",
      "\titers: 200, epoch: 5 | loss: 0.0288012\n",
      "\tspeed: 0.0192s/iter; left time: 171.6335s\n",
      "\titers: 300, epoch: 5 | loss: 0.0336405\n",
      "\tspeed: 0.0192s/iter; left time: 169.6984s\n",
      "\titers: 400, epoch: 5 | loss: 0.0298387\n",
      "\tspeed: 0.0192s/iter; left time: 167.4228s\n",
      "\titers: 500, epoch: 5 | loss: 0.0345208\n",
      "\tspeed: 0.0192s/iter; left time: 165.4227s\n",
      "Epoch: 5 cost time: 11.44356894493103\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0384476 Vali Loss: 0.0387255 Test Loss: 0.1445214\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0388057\n",
      "\tspeed: 0.0599s/iter; left time: 506.0071s\n",
      "\titers: 200, epoch: 6 | loss: 0.0374171\n",
      "\tspeed: 0.0192s/iter; left time: 160.3272s\n",
      "\titers: 300, epoch: 6 | loss: 0.0292357\n",
      "\tspeed: 0.0192s/iter; left time: 158.1825s\n",
      "\titers: 400, epoch: 6 | loss: 0.0309061\n",
      "\tspeed: 0.0191s/iter; left time: 155.9263s\n",
      "\titers: 500, epoch: 6 | loss: 0.0376697\n",
      "\tspeed: 0.0191s/iter; left time: 154.0442s\n",
      "Epoch: 6 cost time: 11.293319702148438\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0338836 Vali Loss: 0.0390634 Test Loss: 0.1482659\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0233778\n",
      "\tspeed: 0.0589s/iter; left time: 464.0918s\n",
      "\titers: 200, epoch: 7 | loss: 0.0288248\n",
      "\tspeed: 0.0193s/iter; left time: 149.8265s\n",
      "\titers: 300, epoch: 7 | loss: 0.0329367\n",
      "\tspeed: 0.0193s/iter; left time: 147.8818s\n",
      "\titers: 400, epoch: 7 | loss: 0.0305000\n",
      "\tspeed: 0.0192s/iter; left time: 145.7541s\n",
      "\titers: 500, epoch: 7 | loss: 0.0248829\n",
      "\tspeed: 0.0192s/iter; left time: 143.6645s\n",
      "Epoch: 7 cost time: 11.314306735992432\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0316444 Vali Loss: 0.0388381 Test Loss: 0.1495141\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.14190757274627686, mae:0.24510309100151062\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1661820\n",
      "\tspeed: 0.0222s/iter; left time: 250.6142s\n",
      "\titers: 200, epoch: 1 | loss: 0.1882960\n",
      "\tspeed: 0.0106s/iter; left time: 118.3839s\n",
      "\titers: 300, epoch: 1 | loss: 0.2432883\n",
      "\tspeed: 0.0106s/iter; left time: 117.4909s\n",
      "\titers: 400, epoch: 1 | loss: 0.2106672\n",
      "\tspeed: 0.0106s/iter; left time: 116.2514s\n",
      "\titers: 500, epoch: 1 | loss: 0.1615628\n",
      "\tspeed: 0.0106s/iter; left time: 115.7923s\n",
      "Epoch: 1 cost time: 7.239778995513916\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2132680 Vali Loss: 0.0489154 Test Loss: 0.1486931\n",
      "Validation loss decreased (inf --> 0.048915).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2554823\n",
      "\tspeed: 0.0380s/iter; left time: 407.6740s\n",
      "\titers: 200, epoch: 2 | loss: 0.2682980\n",
      "\tspeed: 0.0119s/iter; left time: 126.4507s\n",
      "\titers: 300, epoch: 2 | loss: 0.1647600\n",
      "\tspeed: 0.0118s/iter; left time: 124.6689s\n",
      "\titers: 400, epoch: 2 | loss: 0.1572925\n",
      "\tspeed: 0.0119s/iter; left time: 123.8443s\n",
      "\titers: 500, epoch: 2 | loss: 0.1673287\n",
      "\tspeed: 0.0118s/iter; left time: 122.2583s\n",
      "Epoch: 2 cost time: 7.06139349937439\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1726924 Vali Loss: 0.0418939 Test Loss: 0.1265778\n",
      "Validation loss decreased (0.048915 --> 0.041894).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1181338\n",
      "\tspeed: 0.0377s/iter; left time: 383.5674s\n",
      "\titers: 200, epoch: 3 | loss: 0.2001222\n",
      "\tspeed: 0.0106s/iter; left time: 106.4028s\n",
      "\titers: 300, epoch: 3 | loss: 0.1377586\n",
      "\tspeed: 0.0105s/iter; left time: 104.8992s\n",
      "\titers: 400, epoch: 3 | loss: 0.1699878\n",
      "\tspeed: 0.0105s/iter; left time: 103.9559s\n",
      "\titers: 500, epoch: 3 | loss: 0.2055827\n",
      "\tspeed: 0.0105s/iter; left time: 102.8905s\n",
      "Epoch: 3 cost time: 6.316495180130005\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1537922 Vali Loss: 0.0383821 Test Loss: 0.1203843\n",
      "Validation loss decreased (0.041894 --> 0.038382).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1481126\n",
      "\tspeed: 0.0376s/iter; left time: 360.3226s\n",
      "\titers: 200, epoch: 4 | loss: 0.0932456\n",
      "\tspeed: 0.0105s/iter; left time: 99.7941s\n",
      "\titers: 300, epoch: 4 | loss: 0.1380324\n",
      "\tspeed: 0.0105s/iter; left time: 98.3991s\n",
      "\titers: 400, epoch: 4 | loss: 0.1560184\n",
      "\tspeed: 0.0105s/iter; left time: 97.8371s\n",
      "\titers: 500, epoch: 4 | loss: 0.1368372\n",
      "\tspeed: 0.0105s/iter; left time: 96.8480s\n",
      "Epoch: 4 cost time: 6.35309910774231\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1447483 Vali Loss: 0.0380562 Test Loss: 0.1202246\n",
      "Validation loss decreased (0.038382 --> 0.038056).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1547439\n",
      "\tspeed: 0.0378s/iter; left time: 340.8727s\n",
      "\titers: 200, epoch: 5 | loss: 0.1538793\n",
      "\tspeed: 0.0106s/iter; left time: 94.2498s\n",
      "\titers: 300, epoch: 5 | loss: 0.1062656\n",
      "\tspeed: 0.0105s/iter; left time: 92.9775s\n",
      "\titers: 400, epoch: 5 | loss: 0.1278121\n",
      "\tspeed: 0.0105s/iter; left time: 91.7616s\n",
      "\titers: 500, epoch: 5 | loss: 0.1534337\n",
      "\tspeed: 0.0105s/iter; left time: 90.7040s\n",
      "Epoch: 5 cost time: 6.323312759399414\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1410183 Vali Loss: 0.0372186 Test Loss: 0.1193272\n",
      "Validation loss decreased (0.038056 --> 0.037219).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1385755\n",
      "\tspeed: 0.0368s/iter; left time: 310.6411s\n",
      "\titers: 200, epoch: 6 | loss: 0.0998583\n",
      "\tspeed: 0.0106s/iter; left time: 88.4317s\n",
      "\titers: 300, epoch: 6 | loss: 0.1453860\n",
      "\tspeed: 0.0106s/iter; left time: 87.3720s\n",
      "\titers: 400, epoch: 6 | loss: 0.1546631\n",
      "\tspeed: 0.0106s/iter; left time: 86.1603s\n",
      "\titers: 500, epoch: 6 | loss: 0.1294211\n",
      "\tspeed: 0.0106s/iter; left time: 85.1284s\n",
      "Epoch: 6 cost time: 6.296482801437378\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1377903 Vali Loss: 0.0368099 Test Loss: 0.1188920\n",
      "Validation loss decreased (0.037219 --> 0.036810).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1413189\n",
      "\tspeed: 0.0359s/iter; left time: 283.1596s\n",
      "\titers: 200, epoch: 7 | loss: 0.2008557\n",
      "\tspeed: 0.0105s/iter; left time: 81.7073s\n",
      "\titers: 300, epoch: 7 | loss: 0.1179606\n",
      "\tspeed: 0.0105s/iter; left time: 80.8587s\n",
      "\titers: 400, epoch: 7 | loss: 0.1264820\n",
      "\tspeed: 0.0105s/iter; left time: 79.9358s\n",
      "\titers: 500, epoch: 7 | loss: 0.1174680\n",
      "\tspeed: 0.0105s/iter; left time: 78.8463s\n",
      "Epoch: 7 cost time: 6.268507242202759\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1374366 Vali Loss: 0.0371287 Test Loss: 0.1191760\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1442029\n",
      "\tspeed: 0.0368s/iter; left time: 269.3503s\n",
      "\titers: 200, epoch: 8 | loss: 0.1226638\n",
      "\tspeed: 0.0106s/iter; left time: 76.4617s\n",
      "\titers: 300, epoch: 8 | loss: 0.1857536\n",
      "\tspeed: 0.0106s/iter; left time: 75.6486s\n",
      "\titers: 400, epoch: 8 | loss: 0.1947010\n",
      "\tspeed: 0.0106s/iter; left time: 74.3704s\n",
      "\titers: 500, epoch: 8 | loss: 0.1027075\n",
      "\tspeed: 0.0106s/iter; left time: 72.9932s\n",
      "Epoch: 8 cost time: 6.318368911743164\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1363312 Vali Loss: 0.0368068 Test Loss: 0.1189497\n",
      "Validation loss decreased (0.036810 --> 0.036807).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1744780\n",
      "\tspeed: 0.0358s/iter; left time: 241.2950s\n",
      "\titers: 200, epoch: 9 | loss: 0.1239212\n",
      "\tspeed: 0.0105s/iter; left time: 70.0058s\n",
      "\titers: 300, epoch: 9 | loss: 0.0659825\n",
      "\tspeed: 0.0105s/iter; left time: 68.9620s\n",
      "\titers: 400, epoch: 9 | loss: 0.1558087\n",
      "\tspeed: 0.0105s/iter; left time: 67.9012s\n",
      "\titers: 500, epoch: 9 | loss: 0.1267672\n",
      "\tspeed: 0.0105s/iter; left time: 66.8203s\n",
      "Epoch: 9 cost time: 6.302349805831909\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1359967 Vali Loss: 0.0371288 Test Loss: 0.1191462\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1579017\n",
      "\tspeed: 0.0366s/iter; left time: 225.5972s\n",
      "\titers: 200, epoch: 10 | loss: 0.1459462\n",
      "\tspeed: 0.0105s/iter; left time: 63.5154s\n",
      "\titers: 300, epoch: 10 | loss: 0.1129273\n",
      "\tspeed: 0.0105s/iter; left time: 62.4484s\n",
      "\titers: 400, epoch: 10 | loss: 0.1299830\n",
      "\tspeed: 0.0105s/iter; left time: 61.4162s\n",
      "\titers: 500, epoch: 10 | loss: 0.1774755\n",
      "\tspeed: 0.0105s/iter; left time: 60.3617s\n",
      "Epoch: 10 cost time: 6.251264572143555\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1356069 Vali Loss: 0.0369601 Test Loss: 0.1190087\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1467692\n",
      "\tspeed: 0.0379s/iter; left time: 212.1115s\n",
      "\titers: 200, epoch: 11 | loss: 0.1327035\n",
      "\tspeed: 0.0105s/iter; left time: 57.8825s\n",
      "\titers: 300, epoch: 11 | loss: 0.1024538\n",
      "\tspeed: 0.0105s/iter; left time: 56.8103s\n",
      "\titers: 400, epoch: 11 | loss: 0.1856325\n",
      "\tspeed: 0.0105s/iter; left time: 55.7238s\n",
      "\titers: 500, epoch: 11 | loss: 0.1188164\n",
      "\tspeed: 0.0105s/iter; left time: 54.7720s\n",
      "Epoch: 11 cost time: 6.27304220199585\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1367319 Vali Loss: 0.0368543 Test Loss: 0.1190960\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1191156804561615, mae:0.21371115744113922\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1632898\n",
      "\tspeed: 0.0218s/iter; left time: 246.5830s\n",
      "\titers: 200, epoch: 1 | loss: 0.1962866\n",
      "\tspeed: 0.0102s/iter; left time: 114.3054s\n",
      "\titers: 300, epoch: 1 | loss: 0.2366278\n",
      "\tspeed: 0.0102s/iter; left time: 113.4205s\n",
      "\titers: 400, epoch: 1 | loss: 0.1839257\n",
      "\tspeed: 0.0102s/iter; left time: 111.7654s\n",
      "\titers: 500, epoch: 1 | loss: 0.1764508\n",
      "\tspeed: 0.0102s/iter; left time: 110.9079s\n",
      "Epoch: 1 cost time: 7.018388271331787\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1842088 Vali Loss: 0.0355790 Test Loss: 0.1113996\n",
      "Validation loss decreased (inf --> 0.035579).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1228776\n",
      "\tspeed: 0.0367s/iter; left time: 394.2276s\n",
      "\titers: 200, epoch: 2 | loss: 0.1663849\n",
      "\tspeed: 0.0102s/iter; left time: 108.1300s\n",
      "\titers: 300, epoch: 2 | loss: 0.1062605\n",
      "\tspeed: 0.0102s/iter; left time: 107.2741s\n",
      "\titers: 400, epoch: 2 | loss: 0.1292071\n",
      "\tspeed: 0.0102s/iter; left time: 106.0506s\n",
      "\titers: 500, epoch: 2 | loss: 0.1493803\n",
      "\tspeed: 0.0102s/iter; left time: 105.0874s\n",
      "Epoch: 2 cost time: 6.114845514297485\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1527847 Vali Loss: 0.0330815 Test Loss: 0.1088629\n",
      "Validation loss decreased (0.035579 --> 0.033082).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0841182\n",
      "\tspeed: 0.0391s/iter; left time: 397.7245s\n",
      "\titers: 200, epoch: 3 | loss: 0.1208962\n",
      "\tspeed: 0.0115s/iter; left time: 115.8434s\n",
      "\titers: 300, epoch: 3 | loss: 0.2659534\n",
      "\tspeed: 0.0116s/iter; left time: 115.3574s\n",
      "\titers: 400, epoch: 3 | loss: 0.1304747\n",
      "\tspeed: 0.0109s/iter; left time: 107.6394s\n",
      "\titers: 500, epoch: 3 | loss: 0.1050824\n",
      "\tspeed: 0.0102s/iter; left time: 99.1294s\n",
      "Epoch: 3 cost time: 6.601623296737671\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1381241 Vali Loss: 0.0310028 Test Loss: 0.1073266\n",
      "Validation loss decreased (0.033082 --> 0.031003).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1128397\n",
      "\tspeed: 0.0357s/iter; left time: 342.4858s\n",
      "\titers: 200, epoch: 4 | loss: 0.0994237\n",
      "\tspeed: 0.0102s/iter; left time: 97.0915s\n",
      "\titers: 300, epoch: 4 | loss: 0.1038814\n",
      "\tspeed: 0.0102s/iter; left time: 96.2277s\n",
      "\titers: 400, epoch: 4 | loss: 0.0991919\n",
      "\tspeed: 0.0102s/iter; left time: 95.2095s\n",
      "\titers: 500, epoch: 4 | loss: 0.1753294\n",
      "\tspeed: 0.0102s/iter; left time: 93.8611s\n",
      "Epoch: 4 cost time: 6.121987342834473\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1310582 Vali Loss: 0.0308950 Test Loss: 0.1073900\n",
      "Validation loss decreased (0.031003 --> 0.030895).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0961585\n",
      "\tspeed: 0.0362s/iter; left time: 327.0038s\n",
      "\titers: 200, epoch: 5 | loss: 0.1698866\n",
      "\tspeed: 0.0102s/iter; left time: 91.4142s\n",
      "\titers: 300, epoch: 5 | loss: 0.1103002\n",
      "\tspeed: 0.0104s/iter; left time: 92.0804s\n",
      "\titers: 400, epoch: 5 | loss: 0.1169631\n",
      "\tspeed: 0.0115s/iter; left time: 100.4042s\n",
      "\titers: 500, epoch: 5 | loss: 0.1244035\n",
      "\tspeed: 0.0104s/iter; left time: 89.6826s\n",
      "Epoch: 5 cost time: 6.326802492141724\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1278323 Vali Loss: 0.0307792 Test Loss: 0.1089362\n",
      "Validation loss decreased (0.030895 --> 0.030779).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0868208\n",
      "\tspeed: 0.0354s/iter; left time: 299.5051s\n",
      "\titers: 200, epoch: 6 | loss: 0.1824658\n",
      "\tspeed: 0.0102s/iter; left time: 85.5613s\n",
      "\titers: 300, epoch: 6 | loss: 0.1520076\n",
      "\tspeed: 0.0103s/iter; left time: 84.6214s\n",
      "\titers: 400, epoch: 6 | loss: 0.1010766\n",
      "\tspeed: 0.0103s/iter; left time: 83.5593s\n",
      "\titers: 500, epoch: 6 | loss: 0.0868167\n",
      "\tspeed: 0.0103s/iter; left time: 82.6184s\n",
      "Epoch: 6 cost time: 6.124037265777588\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1265988 Vali Loss: 0.0305154 Test Loss: 0.1082917\n",
      "Validation loss decreased (0.030779 --> 0.030515).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1266780\n",
      "\tspeed: 0.0364s/iter; left time: 287.2099s\n",
      "\titers: 200, epoch: 7 | loss: 0.1439131\n",
      "\tspeed: 0.0102s/iter; left time: 79.5050s\n",
      "\titers: 300, epoch: 7 | loss: 0.2094483\n",
      "\tspeed: 0.0102s/iter; left time: 78.6671s\n",
      "\titers: 400, epoch: 7 | loss: 0.1722923\n",
      "\tspeed: 0.0102s/iter; left time: 77.6778s\n",
      "\titers: 500, epoch: 7 | loss: 0.1462710\n",
      "\tspeed: 0.0102s/iter; left time: 76.6145s\n",
      "Epoch: 7 cost time: 6.154587030410767\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1256750 Vali Loss: 0.0302904 Test Loss: 0.1083232\n",
      "Validation loss decreased (0.030515 --> 0.030290).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1091781\n",
      "\tspeed: 0.0361s/iter; left time: 263.6772s\n",
      "\titers: 200, epoch: 8 | loss: 0.0930290\n",
      "\tspeed: 0.0102s/iter; left time: 73.3987s\n",
      "\titers: 300, epoch: 8 | loss: 0.1188539\n",
      "\tspeed: 0.0102s/iter; left time: 72.3639s\n",
      "\titers: 400, epoch: 8 | loss: 0.1192022\n",
      "\tspeed: 0.0102s/iter; left time: 71.3404s\n",
      "\titers: 500, epoch: 8 | loss: 0.0936297\n",
      "\tspeed: 0.0102s/iter; left time: 70.3139s\n",
      "Epoch: 8 cost time: 6.100367784500122\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1259217 Vali Loss: 0.0301730 Test Loss: 0.1081155\n",
      "Validation loss decreased (0.030290 --> 0.030173).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1100343\n",
      "\tspeed: 0.0385s/iter; left time: 259.3094s\n",
      "\titers: 200, epoch: 9 | loss: 0.1464002\n",
      "\tspeed: 0.0115s/iter; left time: 76.4908s\n",
      "\titers: 300, epoch: 9 | loss: 0.1102298\n",
      "\tspeed: 0.0116s/iter; left time: 75.7863s\n",
      "\titers: 400, epoch: 9 | loss: 0.1361244\n",
      "\tspeed: 0.0115s/iter; left time: 74.3426s\n",
      "\titers: 500, epoch: 9 | loss: 0.1263459\n",
      "\tspeed: 0.0115s/iter; left time: 73.1347s\n",
      "Epoch: 9 cost time: 6.807799339294434\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1245330 Vali Loss: 0.0302652 Test Loss: 0.1083011\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1255611\n",
      "\tspeed: 0.0364s/iter; left time: 224.3205s\n",
      "\titers: 200, epoch: 10 | loss: 0.1017917\n",
      "\tspeed: 0.0102s/iter; left time: 61.8935s\n",
      "\titers: 300, epoch: 10 | loss: 0.1145379\n",
      "\tspeed: 0.0102s/iter; left time: 60.9543s\n",
      "\titers: 400, epoch: 10 | loss: 0.0913101\n",
      "\tspeed: 0.0102s/iter; left time: 60.0854s\n",
      "\titers: 500, epoch: 10 | loss: 0.0766238\n",
      "\tspeed: 0.0102s/iter; left time: 58.9830s\n",
      "Epoch: 10 cost time: 6.1081013679504395\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1257018 Vali Loss: 0.0302718 Test Loss: 0.1083023\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1139698\n",
      "\tspeed: 0.0358s/iter; left time: 200.6467s\n",
      "\titers: 200, epoch: 11 | loss: 0.1388310\n",
      "\tspeed: 0.0102s/iter; left time: 56.0307s\n",
      "\titers: 300, epoch: 11 | loss: 0.1469189\n",
      "\tspeed: 0.0102s/iter; left time: 55.3534s\n",
      "\titers: 400, epoch: 11 | loss: 0.1414951\n",
      "\tspeed: 0.0102s/iter; left time: 54.3014s\n",
      "\titers: 500, epoch: 11 | loss: 0.1077557\n",
      "\tspeed: 0.0102s/iter; left time: 53.1065s\n",
      "Epoch: 11 cost time: 6.127962827682495\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1238729 Vali Loss: 0.0301528 Test Loss: 0.1083141\n",
      "Validation loss decreased (0.030173 --> 0.030153).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1152398\n",
      "\tspeed: 0.0352s/iter; left time: 177.0054s\n",
      "\titers: 200, epoch: 12 | loss: 0.1277250\n",
      "\tspeed: 0.0102s/iter; left time: 50.2516s\n",
      "\titers: 300, epoch: 12 | loss: 0.1104521\n",
      "\tspeed: 0.0102s/iter; left time: 49.2282s\n",
      "\titers: 400, epoch: 12 | loss: 0.1891102\n",
      "\tspeed: 0.0102s/iter; left time: 48.2184s\n",
      "\titers: 500, epoch: 12 | loss: 0.1129850\n",
      "\tspeed: 0.0102s/iter; left time: 47.1954s\n",
      "Epoch: 12 cost time: 6.116507291793823\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1244618 Vali Loss: 0.0302571 Test Loss: 0.1083152\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1319189\n",
      "\tspeed: 0.0359s/iter; left time: 160.0878s\n",
      "\titers: 200, epoch: 13 | loss: 0.1576452\n",
      "\tspeed: 0.0102s/iter; left time: 44.6931s\n",
      "\titers: 300, epoch: 13 | loss: 0.0837472\n",
      "\tspeed: 0.0102s/iter; left time: 43.6319s\n",
      "\titers: 400, epoch: 13 | loss: 0.1625548\n",
      "\tspeed: 0.0102s/iter; left time: 42.6177s\n",
      "\titers: 500, epoch: 13 | loss: 0.1285427\n",
      "\tspeed: 0.0102s/iter; left time: 41.5922s\n",
      "Epoch: 13 cost time: 6.156998872756958\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1248096 Vali Loss: 0.0301347 Test Loss: 0.1083239\n",
      "Validation loss decreased (0.030153 --> 0.030135).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1196261\n",
      "\tspeed: 0.0377s/iter; left time: 146.5662s\n",
      "\titers: 200, epoch: 14 | loss: 0.0963889\n",
      "\tspeed: 0.0102s/iter; left time: 38.4896s\n",
      "\titers: 300, epoch: 14 | loss: 0.1159598\n",
      "\tspeed: 0.0101s/iter; left time: 37.4628s\n",
      "\titers: 400, epoch: 14 | loss: 0.1377869\n",
      "\tspeed: 0.0102s/iter; left time: 36.5019s\n",
      "\titers: 500, epoch: 14 | loss: 0.1410586\n",
      "\tspeed: 0.0101s/iter; left time: 35.3945s\n",
      "Epoch: 14 cost time: 6.097911357879639\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1254393 Vali Loss: 0.0301758 Test Loss: 0.1083240\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.0806293\n",
      "\tspeed: 0.0353s/iter; left time: 117.2353s\n",
      "\titers: 200, epoch: 15 | loss: 0.1490056\n",
      "\tspeed: 0.0102s/iter; left time: 32.8127s\n",
      "\titers: 300, epoch: 15 | loss: 0.1265745\n",
      "\tspeed: 0.0102s/iter; left time: 31.8812s\n",
      "\titers: 400, epoch: 15 | loss: 0.1200736\n",
      "\tspeed: 0.0102s/iter; left time: 30.8237s\n",
      "\titers: 500, epoch: 15 | loss: 0.0979574\n",
      "\tspeed: 0.0102s/iter; left time: 29.7878s\n",
      "Epoch: 15 cost time: 6.124320983886719\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1243336 Vali Loss: 0.0301346 Test Loss: 0.1083250\n",
      "Validation loss decreased (0.030135 --> 0.030135).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.1814100\n",
      "\tspeed: 0.0385s/iter; left time: 105.9496s\n",
      "\titers: 200, epoch: 16 | loss: 0.1212711\n",
      "\tspeed: 0.0115s/iter; left time: 30.4974s\n",
      "\titers: 300, epoch: 16 | loss: 0.1605145\n",
      "\tspeed: 0.0115s/iter; left time: 29.4578s\n",
      "\titers: 400, epoch: 16 | loss: 0.1443062\n",
      "\tspeed: 0.0116s/iter; left time: 28.3126s\n",
      "\titers: 500, epoch: 16 | loss: 0.2023915\n",
      "\tspeed: 0.0115s/iter; left time: 27.0615s\n",
      "Epoch: 16 cost time: 6.846858978271484\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1248819 Vali Loss: 0.0301262 Test Loss: 0.1083253\n",
      "Validation loss decreased (0.030135 --> 0.030126).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "\titers: 100, epoch: 17 | loss: 0.0958868\n",
      "\tspeed: 0.0369s/iter; left time: 80.5308s\n",
      "\titers: 200, epoch: 17 | loss: 0.1025478\n",
      "\tspeed: 0.0102s/iter; left time: 21.2907s\n",
      "\titers: 300, epoch: 17 | loss: 0.1280416\n",
      "\tspeed: 0.0103s/iter; left time: 20.3369s\n",
      "\titers: 400, epoch: 17 | loss: 0.1144716\n",
      "\tspeed: 0.0102s/iter; left time: 19.1870s\n",
      "\titers: 500, epoch: 17 | loss: 0.0766145\n",
      "\tspeed: 0.0101s/iter; left time: 18.0249s\n",
      "Epoch: 17 cost time: 6.1271374225616455\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1247482 Vali Loss: 0.0302355 Test Loss: 0.1083252\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1366455\n",
      "\tspeed: 0.0362s/iter; left time: 58.3660s\n",
      "\titers: 200, epoch: 18 | loss: 0.1215379\n",
      "\tspeed: 0.0102s/iter; left time: 15.4759s\n",
      "\titers: 300, epoch: 18 | loss: 0.0830668\n",
      "\tspeed: 0.0102s/iter; left time: 14.4382s\n",
      "\titers: 400, epoch: 18 | loss: 0.2063132\n",
      "\tspeed: 0.0102s/iter; left time: 13.4203s\n",
      "\titers: 500, epoch: 18 | loss: 0.1021243\n",
      "\tspeed: 0.0102s/iter; left time: 12.3855s\n",
      "Epoch: 18 cost time: 6.133134126663208\n",
      "Epoch: 18, Steps: 570 | Train Loss: 0.1247364 Vali Loss: 0.0302222 Test Loss: 0.1083251\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "\titers: 100, epoch: 19 | loss: 0.1282438\n",
      "\tspeed: 0.0368s/iter; left time: 38.3337s\n",
      "\titers: 200, epoch: 19 | loss: 0.1352768\n",
      "\tspeed: 0.0103s/iter; left time: 9.6458s\n",
      "\titers: 300, epoch: 19 | loss: 0.1141587\n",
      "\tspeed: 0.0103s/iter; left time: 8.6281s\n",
      "\titers: 400, epoch: 19 | loss: 0.1481847\n",
      "\tspeed: 0.0103s/iter; left time: 7.6227s\n",
      "\titers: 500, epoch: 19 | loss: 0.1201239\n",
      "\tspeed: 0.0102s/iter; left time: 6.5539s\n",
      "Epoch: 19 cost time: 6.160877704620361\n",
      "Epoch: 19, Steps: 570 | Train Loss: 0.1244544 Vali Loss: 0.0302172 Test Loss: 0.1083252\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10847773402929306, mae:0.19956164062023163\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2209077\n",
      "\tspeed: 0.0233s/iter; left time: 262.9458s\n",
      "\titers: 200, epoch: 1 | loss: 0.1631805\n",
      "\tspeed: 0.0116s/iter; left time: 130.3035s\n",
      "\titers: 300, epoch: 1 | loss: 0.1428613\n",
      "\tspeed: 0.0116s/iter; left time: 128.7246s\n",
      "\titers: 400, epoch: 1 | loss: 0.1150904\n",
      "\tspeed: 0.0106s/iter; left time: 116.5211s\n",
      "\titers: 500, epoch: 1 | loss: 0.2018290\n",
      "\tspeed: 0.0105s/iter; left time: 114.4966s\n",
      "Epoch: 1 cost time: 7.63770055770874\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2200478 Vali Loss: 0.0456244 Test Loss: 0.1333508\n",
      "Validation loss decreased (inf --> 0.045624).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1866460\n",
      "\tspeed: 0.0390s/iter; left time: 418.6787s\n",
      "\titers: 200, epoch: 2 | loss: 0.2389505\n",
      "\tspeed: 0.0118s/iter; left time: 124.9717s\n",
      "\titers: 300, epoch: 2 | loss: 0.1493279\n",
      "\tspeed: 0.0117s/iter; left time: 123.4916s\n",
      "\titers: 400, epoch: 2 | loss: 0.2726944\n",
      "\tspeed: 0.0117s/iter; left time: 122.0296s\n",
      "\titers: 500, epoch: 2 | loss: 0.2013200\n",
      "\tspeed: 0.0117s/iter; left time: 121.1357s\n",
      "Epoch: 2 cost time: 7.014589786529541\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1774382 Vali Loss: 0.0395741 Test Loss: 0.1253181\n",
      "Validation loss decreased (0.045624 --> 0.039574).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1207781\n",
      "\tspeed: 0.0389s/iter; left time: 395.1414s\n",
      "\titers: 200, epoch: 3 | loss: 0.1570719\n",
      "\tspeed: 0.0118s/iter; left time: 118.7299s\n",
      "\titers: 300, epoch: 3 | loss: 0.1950693\n",
      "\tspeed: 0.0118s/iter; left time: 117.4065s\n",
      "\titers: 400, epoch: 3 | loss: 0.1283651\n",
      "\tspeed: 0.0118s/iter; left time: 116.1808s\n",
      "\titers: 500, epoch: 3 | loss: 0.1199954\n",
      "\tspeed: 0.0118s/iter; left time: 114.9246s\n",
      "Epoch: 3 cost time: 7.022442102432251\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1601210 Vali Loss: 0.0392600 Test Loss: 0.1258912\n",
      "Validation loss decreased (0.039574 --> 0.039260).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0857576\n",
      "\tspeed: 0.0388s/iter; left time: 372.5799s\n",
      "\titers: 200, epoch: 4 | loss: 0.1435241\n",
      "\tspeed: 0.0116s/iter; left time: 110.3919s\n",
      "\titers: 300, epoch: 4 | loss: 0.1204739\n",
      "\tspeed: 0.0117s/iter; left time: 109.7021s\n",
      "\titers: 400, epoch: 4 | loss: 0.1990416\n",
      "\tspeed: 0.0117s/iter; left time: 108.4840s\n",
      "\titers: 500, epoch: 4 | loss: 0.1360075\n",
      "\tspeed: 0.0117s/iter; left time: 107.0969s\n",
      "Epoch: 4 cost time: 6.952067136764526\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1509492 Vali Loss: 0.0388266 Test Loss: 0.1260145\n",
      "Validation loss decreased (0.039260 --> 0.038827).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1325593\n",
      "\tspeed: 0.0385s/iter; left time: 347.0304s\n",
      "\titers: 200, epoch: 5 | loss: 0.1007131\n",
      "\tspeed: 0.0104s/iter; left time: 93.0331s\n",
      "\titers: 300, epoch: 5 | loss: 0.1569553\n",
      "\tspeed: 0.0104s/iter; left time: 91.9492s\n",
      "\titers: 400, epoch: 5 | loss: 0.1501538\n",
      "\tspeed: 0.0104s/iter; left time: 90.8563s\n",
      "\titers: 500, epoch: 5 | loss: 0.1873112\n",
      "\tspeed: 0.0104s/iter; left time: 89.5338s\n",
      "Epoch: 5 cost time: 6.2336156368255615\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1477954 Vali Loss: 0.0387457 Test Loss: 0.1248546\n",
      "Validation loss decreased (0.038827 --> 0.038746).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1031383\n",
      "\tspeed: 0.0381s/iter; left time: 321.7265s\n",
      "\titers: 200, epoch: 6 | loss: 0.1437926\n",
      "\tspeed: 0.0103s/iter; left time: 86.2350s\n",
      "\titers: 300, epoch: 6 | loss: 0.1388285\n",
      "\tspeed: 0.0104s/iter; left time: 85.4679s\n",
      "\titers: 400, epoch: 6 | loss: 0.1307970\n",
      "\tspeed: 0.0104s/iter; left time: 84.4752s\n",
      "\titers: 500, epoch: 6 | loss: 0.1496309\n",
      "\tspeed: 0.0103s/iter; left time: 83.3176s\n",
      "Epoch: 6 cost time: 6.3297693729400635\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1451480 Vali Loss: 0.0380487 Test Loss: 0.1235802\n",
      "Validation loss decreased (0.038746 --> 0.038049).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1069827\n",
      "\tspeed: 0.0370s/iter; left time: 291.9795s\n",
      "\titers: 200, epoch: 7 | loss: 0.1461647\n",
      "\tspeed: 0.0103s/iter; left time: 80.2985s\n",
      "\titers: 300, epoch: 7 | loss: 0.1125339\n",
      "\tspeed: 0.0103s/iter; left time: 79.0755s\n",
      "\titers: 400, epoch: 7 | loss: 0.0938165\n",
      "\tspeed: 0.0103s/iter; left time: 78.1718s\n",
      "\titers: 500, epoch: 7 | loss: 0.1562604\n",
      "\tspeed: 0.0103s/iter; left time: 77.2716s\n",
      "Epoch: 7 cost time: 6.203816175460815\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1428303 Vali Loss: 0.0379655 Test Loss: 0.1234711\n",
      "Validation loss decreased (0.038049 --> 0.037965).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1077710\n",
      "\tspeed: 0.0365s/iter; left time: 266.6859s\n",
      "\titers: 200, epoch: 8 | loss: 0.1454666\n",
      "\tspeed: 0.0104s/iter; left time: 74.8010s\n",
      "\titers: 300, epoch: 8 | loss: 0.1180467\n",
      "\tspeed: 0.0104s/iter; left time: 73.9666s\n",
      "\titers: 400, epoch: 8 | loss: 0.1400131\n",
      "\tspeed: 0.0104s/iter; left time: 72.9599s\n",
      "\titers: 500, epoch: 8 | loss: 0.1238756\n",
      "\tspeed: 0.0104s/iter; left time: 71.7593s\n",
      "Epoch: 8 cost time: 6.227437973022461\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1415453 Vali Loss: 0.0379648 Test Loss: 0.1236288\n",
      "Validation loss decreased (0.037965 --> 0.037965).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1197457\n",
      "\tspeed: 0.0380s/iter; left time: 255.8434s\n",
      "\titers: 200, epoch: 9 | loss: 0.1663950\n",
      "\tspeed: 0.0104s/iter; left time: 68.7376s\n",
      "\titers: 300, epoch: 9 | loss: 0.1219669\n",
      "\tspeed: 0.0103s/iter; left time: 67.6192s\n",
      "\titers: 400, epoch: 9 | loss: 0.1165582\n",
      "\tspeed: 0.0103s/iter; left time: 66.5851s\n",
      "\titers: 500, epoch: 9 | loss: 0.1473963\n",
      "\tspeed: 0.0103s/iter; left time: 65.5674s\n",
      "Epoch: 9 cost time: 6.219622373580933\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1408470 Vali Loss: 0.0381282 Test Loss: 0.1237652\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1218221\n",
      "\tspeed: 0.0356s/iter; left time: 219.6921s\n",
      "\titers: 200, epoch: 10 | loss: 0.1730519\n",
      "\tspeed: 0.0103s/iter; left time: 62.7181s\n",
      "\titers: 300, epoch: 10 | loss: 0.1865845\n",
      "\tspeed: 0.0103s/iter; left time: 61.6895s\n",
      "\titers: 400, epoch: 10 | loss: 0.1654164\n",
      "\tspeed: 0.0103s/iter; left time: 60.6897s\n",
      "\titers: 500, epoch: 10 | loss: 0.1427130\n",
      "\tspeed: 0.0103s/iter; left time: 59.6842s\n",
      "Epoch: 10 cost time: 6.18736720085144\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1427504 Vali Loss: 0.0381587 Test Loss: 0.1238057\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1540829\n",
      "\tspeed: 0.0382s/iter; left time: 214.0318s\n",
      "\titers: 200, epoch: 11 | loss: 0.1129410\n",
      "\tspeed: 0.0103s/iter; left time: 56.5746s\n",
      "\titers: 300, epoch: 11 | loss: 0.1937230\n",
      "\tspeed: 0.0103s/iter; left time: 55.5420s\n",
      "\titers: 400, epoch: 11 | loss: 0.1644548\n",
      "\tspeed: 0.0103s/iter; left time: 54.4800s\n",
      "\titers: 500, epoch: 11 | loss: 0.1550523\n",
      "\tspeed: 0.0103s/iter; left time: 53.4269s\n",
      "Epoch: 11 cost time: 6.195549726486206\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1425434 Vali Loss: 0.0379050 Test Loss: 0.1238274\n",
      "Validation loss decreased (0.037965 --> 0.037905).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1438372\n",
      "\tspeed: 0.0362s/iter; left time: 182.3562s\n",
      "\titers: 200, epoch: 12 | loss: 0.1561584\n",
      "\tspeed: 0.0104s/iter; left time: 51.1878s\n",
      "\titers: 300, epoch: 12 | loss: 0.1043206\n",
      "\tspeed: 0.0104s/iter; left time: 50.1231s\n",
      "\titers: 400, epoch: 12 | loss: 0.1268635\n",
      "\tspeed: 0.0104s/iter; left time: 49.0069s\n",
      "\titers: 500, epoch: 12 | loss: 0.0814305\n",
      "\tspeed: 0.0103s/iter; left time: 47.7420s\n",
      "Epoch: 12 cost time: 6.220138788223267\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1417405 Vali Loss: 0.0379074 Test Loss: 0.1238480\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.1408719\n",
      "\tspeed: 0.0363s/iter; left time: 162.0342s\n",
      "\titers: 200, epoch: 13 | loss: 0.1521952\n",
      "\tspeed: 0.0104s/iter; left time: 45.2060s\n",
      "\titers: 300, epoch: 13 | loss: 0.1613741\n",
      "\tspeed: 0.0104s/iter; left time: 44.1122s\n",
      "\titers: 400, epoch: 13 | loss: 0.1620815\n",
      "\tspeed: 0.0104s/iter; left time: 43.1349s\n",
      "\titers: 500, epoch: 13 | loss: 0.1417292\n",
      "\tspeed: 0.0104s/iter; left time: 42.0515s\n",
      "Epoch: 13 cost time: 6.22285008430481\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1424282 Vali Loss: 0.0379697 Test Loss: 0.1238488\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1171110\n",
      "\tspeed: 0.0365s/iter; left time: 142.1003s\n",
      "\titers: 200, epoch: 14 | loss: 0.1668206\n",
      "\tspeed: 0.0103s/iter; left time: 39.1014s\n",
      "\titers: 300, epoch: 14 | loss: 0.1107595\n",
      "\tspeed: 0.0103s/iter; left time: 38.1605s\n",
      "\titers: 400, epoch: 14 | loss: 0.0863452\n",
      "\tspeed: 0.0103s/iter; left time: 37.1254s\n",
      "\titers: 500, epoch: 14 | loss: 0.1830739\n",
      "\tspeed: 0.0103s/iter; left time: 36.1073s\n",
      "Epoch: 14 cost time: 6.163467884063721\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1414670 Vali Loss: 0.0378636 Test Loss: 0.1238594\n",
      "Validation loss decreased (0.037905 --> 0.037864).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1206651\n",
      "\tspeed: 0.0372s/iter; left time: 123.6651s\n",
      "\titers: 200, epoch: 15 | loss: 0.1962299\n",
      "\tspeed: 0.0104s/iter; left time: 33.5395s\n",
      "\titers: 300, epoch: 15 | loss: 0.0880813\n",
      "\tspeed: 0.0104s/iter; left time: 32.5267s\n",
      "\titers: 400, epoch: 15 | loss: 0.0686535\n",
      "\tspeed: 0.0104s/iter; left time: 31.4705s\n",
      "\titers: 500, epoch: 15 | loss: 0.2198229\n",
      "\tspeed: 0.0104s/iter; left time: 30.3425s\n",
      "Epoch: 15 cost time: 6.245046854019165\n",
      "Epoch: 15, Steps: 570 | Train Loss: 0.1417189 Vali Loss: 0.0379597 Test Loss: 0.1238618\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.1515440\n",
      "\tspeed: 0.0368s/iter; left time: 101.3102s\n",
      "\titers: 200, epoch: 16 | loss: 0.1062499\n",
      "\tspeed: 0.0104s/iter; left time: 27.4517s\n",
      "\titers: 300, epoch: 16 | loss: 0.1378633\n",
      "\tspeed: 0.0104s/iter; left time: 26.5151s\n",
      "\titers: 400, epoch: 16 | loss: 0.1257110\n",
      "\tspeed: 0.0104s/iter; left time: 25.5303s\n",
      "\titers: 500, epoch: 16 | loss: 0.1545956\n",
      "\tspeed: 0.0104s/iter; left time: 24.5022s\n",
      "Epoch: 16 cost time: 6.189252138137817\n",
      "Epoch: 16, Steps: 570 | Train Loss: 0.1420061 Vali Loss: 0.0381102 Test Loss: 0.1238612\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "\titers: 100, epoch: 17 | loss: 0.1119800\n",
      "\tspeed: 0.0354s/iter; left time: 77.2569s\n",
      "\titers: 200, epoch: 17 | loss: 0.0862425\n",
      "\tspeed: 0.0103s/iter; left time: 21.5177s\n",
      "\titers: 300, epoch: 17 | loss: 0.0969094\n",
      "\tspeed: 0.0104s/iter; left time: 20.5051s\n",
      "\titers: 400, epoch: 17 | loss: 0.1524913\n",
      "\tspeed: 0.0104s/iter; left time: 19.4999s\n",
      "\titers: 500, epoch: 17 | loss: 0.0960862\n",
      "\tspeed: 0.0104s/iter; left time: 18.4637s\n",
      "Epoch: 17 cost time: 6.191120862960815\n",
      "Epoch: 17, Steps: 570 | Train Loss: 0.1422323 Vali Loss: 0.0381015 Test Loss: 0.1238618\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1240406259894371, mae:0.2200387865304947\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2494522\n",
      "\tspeed: 0.0235s/iter; left time: 265.0232s\n",
      "\titers: 200, epoch: 1 | loss: 0.2129330\n",
      "\tspeed: 0.0118s/iter; left time: 132.5749s\n",
      "\titers: 300, epoch: 1 | loss: 0.1523527\n",
      "\tspeed: 0.0119s/iter; left time: 131.5559s\n",
      "\titers: 400, epoch: 1 | loss: 0.1196123\n",
      "\tspeed: 0.0118s/iter; left time: 130.1602s\n",
      "\titers: 500, epoch: 1 | loss: 0.1970649\n",
      "\tspeed: 0.0118s/iter; left time: 129.0262s\n",
      "Epoch: 1 cost time: 7.964946746826172\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2074562 Vali Loss: 0.0426341 Test Loss: 0.1295917\n",
      "Validation loss decreased (inf --> 0.042634).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1755087\n",
      "\tspeed: 0.0390s/iter; left time: 418.3246s\n",
      "\titers: 200, epoch: 2 | loss: 0.1553559\n",
      "\tspeed: 0.0118s/iter; left time: 125.8841s\n",
      "\titers: 300, epoch: 2 | loss: 0.1324689\n",
      "\tspeed: 0.0118s/iter; left time: 124.5494s\n",
      "\titers: 400, epoch: 2 | loss: 0.0915983\n",
      "\tspeed: 0.0118s/iter; left time: 123.4613s\n",
      "\titers: 500, epoch: 2 | loss: 0.1688437\n",
      "\tspeed: 0.0118s/iter; left time: 122.2389s\n",
      "Epoch: 2 cost time: 7.054393768310547\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1654598 Vali Loss: 0.0396928 Test Loss: 0.1245952\n",
      "Validation loss decreased (0.042634 --> 0.039693).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1581594\n",
      "\tspeed: 0.0401s/iter; left time: 407.0415s\n",
      "\titers: 200, epoch: 3 | loss: 0.1721560\n",
      "\tspeed: 0.0119s/iter; left time: 120.0727s\n",
      "\titers: 300, epoch: 3 | loss: 0.1140315\n",
      "\tspeed: 0.0119s/iter; left time: 118.6418s\n",
      "\titers: 400, epoch: 3 | loss: 0.1244937\n",
      "\tspeed: 0.0119s/iter; left time: 117.2887s\n",
      "\titers: 500, epoch: 3 | loss: 0.1060665\n",
      "\tspeed: 0.0119s/iter; left time: 115.6736s\n",
      "Epoch: 3 cost time: 7.101539373397827\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1411879 Vali Loss: 0.0367619 Test Loss: 0.1210644\n",
      "Validation loss decreased (0.039693 --> 0.036762).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1151561\n",
      "\tspeed: 0.0401s/iter; left time: 384.6031s\n",
      "\titers: 200, epoch: 4 | loss: 0.1654308\n",
      "\tspeed: 0.0119s/iter; left time: 112.4916s\n",
      "\titers: 300, epoch: 4 | loss: 0.0820960\n",
      "\tspeed: 0.0118s/iter; left time: 111.1506s\n",
      "\titers: 400, epoch: 4 | loss: 0.0956668\n",
      "\tspeed: 0.0118s/iter; left time: 110.0428s\n",
      "\titers: 500, epoch: 4 | loss: 0.1009726\n",
      "\tspeed: 0.0118s/iter; left time: 108.7345s\n",
      "Epoch: 4 cost time: 7.0425145626068115\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1289499 Vali Loss: 0.0352489 Test Loss: 0.1231693\n",
      "Validation loss decreased (0.036762 --> 0.035249).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1123193\n",
      "\tspeed: 0.0408s/iter; left time: 368.0452s\n",
      "\titers: 200, epoch: 5 | loss: 0.1100611\n",
      "\tspeed: 0.0119s/iter; left time: 105.7735s\n",
      "\titers: 300, epoch: 5 | loss: 0.1749112\n",
      "\tspeed: 0.0118s/iter; left time: 104.5052s\n",
      "\titers: 400, epoch: 5 | loss: 0.1129525\n",
      "\tspeed: 0.0118s/iter; left time: 103.0954s\n",
      "\titers: 500, epoch: 5 | loss: 0.1089170\n",
      "\tspeed: 0.0118s/iter; left time: 102.1049s\n",
      "Epoch: 5 cost time: 7.089262962341309\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1233984 Vali Loss: 0.0358403 Test Loss: 0.1185238\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1055258\n",
      "\tspeed: 0.0402s/iter; left time: 340.1395s\n",
      "\titers: 200, epoch: 6 | loss: 0.0744367\n",
      "\tspeed: 0.0130s/iter; left time: 108.3538s\n",
      "\titers: 300, epoch: 6 | loss: 0.1253520\n",
      "\tspeed: 0.0120s/iter; left time: 99.4066s\n",
      "\titers: 400, epoch: 6 | loss: 0.1410496\n",
      "\tspeed: 0.0119s/iter; left time: 96.8267s\n",
      "\titers: 500, epoch: 6 | loss: 0.0853137\n",
      "\tspeed: 0.0119s/iter; left time: 95.6787s\n",
      "Epoch: 6 cost time: 7.299660682678223\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1182106 Vali Loss: 0.0345302 Test Loss: 0.1180393\n",
      "Validation loss decreased (0.035249 --> 0.034530).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1296884\n",
      "\tspeed: 0.0399s/iter; left time: 314.0823s\n",
      "\titers: 200, epoch: 7 | loss: 0.1308485\n",
      "\tspeed: 0.0119s/iter; left time: 92.7475s\n",
      "\titers: 300, epoch: 7 | loss: 0.1279680\n",
      "\tspeed: 0.0119s/iter; left time: 91.6730s\n",
      "\titers: 400, epoch: 7 | loss: 0.1000333\n",
      "\tspeed: 0.0119s/iter; left time: 89.8781s\n",
      "\titers: 500, epoch: 7 | loss: 0.1005313\n",
      "\tspeed: 0.0118s/iter; left time: 88.2229s\n",
      "Epoch: 7 cost time: 7.055084943771362\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1169174 Vali Loss: 0.0343042 Test Loss: 0.1196582\n",
      "Validation loss decreased (0.034530 --> 0.034304).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0725371\n",
      "\tspeed: 0.0387s/iter; left time: 282.6404s\n",
      "\titers: 200, epoch: 8 | loss: 0.1225613\n",
      "\tspeed: 0.0119s/iter; left time: 85.5749s\n",
      "\titers: 300, epoch: 8 | loss: 0.0996163\n",
      "\tspeed: 0.0119s/iter; left time: 84.5497s\n",
      "\titers: 400, epoch: 8 | loss: 0.0855558\n",
      "\tspeed: 0.0119s/iter; left time: 83.2841s\n",
      "\titers: 500, epoch: 8 | loss: 0.0986222\n",
      "\tspeed: 0.0119s/iter; left time: 82.1517s\n",
      "Epoch: 8 cost time: 7.076983690261841\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1168448 Vali Loss: 0.0341010 Test Loss: 0.1192758\n",
      "Validation loss decreased (0.034304 --> 0.034101).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0960807\n",
      "\tspeed: 0.0405s/iter; left time: 272.8916s\n",
      "\titers: 200, epoch: 9 | loss: 0.1103233\n",
      "\tspeed: 0.0119s/iter; left time: 78.9928s\n",
      "\titers: 300, epoch: 9 | loss: 0.1214779\n",
      "\tspeed: 0.0119s/iter; left time: 77.6149s\n",
      "\titers: 400, epoch: 9 | loss: 0.0919171\n",
      "\tspeed: 0.0119s/iter; left time: 76.3894s\n",
      "\titers: 500, epoch: 9 | loss: 0.1461041\n",
      "\tspeed: 0.0119s/iter; left time: 75.2630s\n",
      "Epoch: 9 cost time: 7.056760549545288\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1163970 Vali Loss: 0.0340609 Test Loss: 0.1190358\n",
      "Validation loss decreased (0.034101 --> 0.034061).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1156739\n",
      "\tspeed: 0.0388s/iter; left time: 239.7428s\n",
      "\titers: 200, epoch: 10 | loss: 0.1168119\n",
      "\tspeed: 0.0119s/iter; left time: 72.1330s\n",
      "\titers: 300, epoch: 10 | loss: 0.0976095\n",
      "\tspeed: 0.0119s/iter; left time: 70.8071s\n",
      "\titers: 400, epoch: 10 | loss: 0.0884077\n",
      "\tspeed: 0.0118s/iter; left time: 69.3088s\n",
      "\titers: 500, epoch: 10 | loss: 0.1521445\n",
      "\tspeed: 0.0118s/iter; left time: 68.1344s\n",
      "Epoch: 10 cost time: 7.050570249557495\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1171683 Vali Loss: 0.0342109 Test Loss: 0.1189131\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0946117\n",
      "\tspeed: 0.0383s/iter; left time: 214.5942s\n",
      "\titers: 200, epoch: 11 | loss: 0.1715285\n",
      "\tspeed: 0.0119s/iter; left time: 65.2749s\n",
      "\titers: 300, epoch: 11 | loss: 0.1023363\n",
      "\tspeed: 0.0119s/iter; left time: 64.0904s\n",
      "\titers: 400, epoch: 11 | loss: 0.1114139\n",
      "\tspeed: 0.0119s/iter; left time: 62.8651s\n",
      "\titers: 500, epoch: 11 | loss: 0.1463158\n",
      "\tspeed: 0.0119s/iter; left time: 61.7512s\n",
      "Epoch: 11 cost time: 7.052458047866821\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1154145 Vali Loss: 0.0342090 Test Loss: 0.1189196\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1003364\n",
      "\tspeed: 0.0389s/iter; left time: 195.7564s\n",
      "\titers: 200, epoch: 12 | loss: 0.0884850\n",
      "\tspeed: 0.0118s/iter; left time: 58.2721s\n",
      "\titers: 300, epoch: 12 | loss: 0.0798764\n",
      "\tspeed: 0.0118s/iter; left time: 57.2158s\n",
      "\titers: 400, epoch: 12 | loss: 0.0917849\n",
      "\tspeed: 0.0118s/iter; left time: 56.0417s\n",
      "\titers: 500, epoch: 12 | loss: 0.1094390\n",
      "\tspeed: 0.0118s/iter; left time: 54.7479s\n",
      "Epoch: 12 cost time: 7.01141357421875\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1151760 Vali Loss: 0.0342856 Test Loss: 0.1188759\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11919538676738739, mae:0.2178240269422531\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1061446\n",
      "\tspeed: 0.0234s/iter; left time: 264.8686s\n",
      "\titers: 200, epoch: 1 | loss: 0.2938389\n",
      "\tspeed: 0.0115s/iter; left time: 128.5274s\n",
      "\titers: 300, epoch: 1 | loss: 0.1393132\n",
      "\tspeed: 0.0114s/iter; left time: 127.0863s\n",
      "\titers: 400, epoch: 1 | loss: 0.1742748\n",
      "\tspeed: 0.0115s/iter; left time: 126.0869s\n",
      "\titers: 500, epoch: 1 | loss: 0.1219428\n",
      "\tspeed: 0.0115s/iter; left time: 125.0517s\n",
      "Epoch: 1 cost time: 7.786781549453735\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1809858 Vali Loss: 0.0358772 Test Loss: 0.1120169\n",
      "Validation loss decreased (inf --> 0.035877).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1285634\n",
      "\tspeed: 0.0384s/iter; left time: 412.2877s\n",
      "\titers: 200, epoch: 2 | loss: 0.1006706\n",
      "\tspeed: 0.0114s/iter; left time: 121.4759s\n",
      "\titers: 300, epoch: 2 | loss: 0.1497626\n",
      "\tspeed: 0.0114s/iter; left time: 120.1753s\n",
      "\titers: 400, epoch: 2 | loss: 0.2290181\n",
      "\tspeed: 0.0114s/iter; left time: 119.3082s\n",
      "\titers: 500, epoch: 2 | loss: 0.2421417\n",
      "\tspeed: 0.0114s/iter; left time: 118.0904s\n",
      "Epoch: 2 cost time: 6.823119401931763\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1469245 Vali Loss: 0.0362476 Test Loss: 0.1125096\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1461935\n",
      "\tspeed: 0.0388s/iter; left time: 394.1486s\n",
      "\titers: 200, epoch: 3 | loss: 0.0874312\n",
      "\tspeed: 0.0115s/iter; left time: 115.8116s\n",
      "\titers: 300, epoch: 3 | loss: 0.1629128\n",
      "\tspeed: 0.0115s/iter; left time: 114.7367s\n",
      "\titers: 400, epoch: 3 | loss: 0.1303105\n",
      "\tspeed: 0.0115s/iter; left time: 113.3063s\n",
      "\titers: 500, epoch: 3 | loss: 0.1554490\n",
      "\tspeed: 0.0115s/iter; left time: 112.4466s\n",
      "Epoch: 3 cost time: 6.865642070770264\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1299274 Vali Loss: 0.0332274 Test Loss: 0.1086616\n",
      "Validation loss decreased (0.035877 --> 0.033227).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1100514\n",
      "\tspeed: 0.0392s/iter; left time: 375.7712s\n",
      "\titers: 200, epoch: 4 | loss: 0.0898602\n",
      "\tspeed: 0.0115s/iter; left time: 109.0299s\n",
      "\titers: 300, epoch: 4 | loss: 0.0719668\n",
      "\tspeed: 0.0115s/iter; left time: 107.8533s\n",
      "\titers: 400, epoch: 4 | loss: 0.1100080\n",
      "\tspeed: 0.0115s/iter; left time: 106.7717s\n",
      "\titers: 500, epoch: 4 | loss: 0.1275932\n",
      "\tspeed: 0.0115s/iter; left time: 105.3988s\n",
      "Epoch: 4 cost time: 6.853309869766235\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1192766 Vali Loss: 0.0334128 Test Loss: 0.1083727\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0998100\n",
      "\tspeed: 0.0381s/iter; left time: 343.9719s\n",
      "\titers: 200, epoch: 5 | loss: 0.0951665\n",
      "\tspeed: 0.0114s/iter; left time: 102.0941s\n",
      "\titers: 300, epoch: 5 | loss: 0.1616226\n",
      "\tspeed: 0.0115s/iter; left time: 101.0238s\n",
      "\titers: 400, epoch: 5 | loss: 0.0922074\n",
      "\tspeed: 0.0114s/iter; left time: 99.8290s\n",
      "\titers: 500, epoch: 5 | loss: 0.1030626\n",
      "\tspeed: 0.0115s/iter; left time: 98.7641s\n",
      "Epoch: 5 cost time: 6.813181638717651\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1137036 Vali Loss: 0.0332378 Test Loss: 0.1093978\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1444074\n",
      "\tspeed: 0.0390s/iter; left time: 329.4091s\n",
      "\titers: 200, epoch: 6 | loss: 0.1259873\n",
      "\tspeed: 0.0114s/iter; left time: 95.3829s\n",
      "\titers: 300, epoch: 6 | loss: 0.0823032\n",
      "\tspeed: 0.0114s/iter; left time: 94.2001s\n",
      "\titers: 400, epoch: 6 | loss: 0.0880875\n",
      "\tspeed: 0.0114s/iter; left time: 93.0735s\n",
      "\titers: 500, epoch: 6 | loss: 0.0839217\n",
      "\tspeed: 0.0114s/iter; left time: 91.9036s\n",
      "Epoch: 6 cost time: 6.773877859115601\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1108379 Vali Loss: 0.0329414 Test Loss: 0.1084211\n",
      "Validation loss decreased (0.033227 --> 0.032941).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0801231\n",
      "\tspeed: 0.0386s/iter; left time: 304.3710s\n",
      "\titers: 200, epoch: 7 | loss: 0.1289176\n",
      "\tspeed: 0.0115s/iter; left time: 89.1119s\n",
      "\titers: 300, epoch: 7 | loss: 0.0806299\n",
      "\tspeed: 0.0114s/iter; left time: 87.8314s\n",
      "\titers: 400, epoch: 7 | loss: 0.0882797\n",
      "\tspeed: 0.0114s/iter; left time: 86.7611s\n",
      "\titers: 500, epoch: 7 | loss: 0.0941452\n",
      "\tspeed: 0.0115s/iter; left time: 85.6820s\n",
      "Epoch: 7 cost time: 6.853348255157471\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1095375 Vali Loss: 0.0327797 Test Loss: 0.1082623\n",
      "Validation loss decreased (0.032941 --> 0.032780).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0955910\n",
      "\tspeed: 0.0410s/iter; left time: 299.4672s\n",
      "\titers: 200, epoch: 8 | loss: 0.1279755\n",
      "\tspeed: 0.0126s/iter; left time: 90.6148s\n",
      "\titers: 300, epoch: 8 | loss: 0.0893521\n",
      "\tspeed: 0.0126s/iter; left time: 89.3987s\n",
      "\titers: 400, epoch: 8 | loss: 0.1013135\n",
      "\tspeed: 0.0125s/iter; left time: 87.9513s\n",
      "\titers: 500, epoch: 8 | loss: 0.1093070\n",
      "\tspeed: 0.0126s/iter; left time: 86.7835s\n",
      "Epoch: 8 cost time: 7.4016640186309814\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1093392 Vali Loss: 0.0331353 Test Loss: 0.1088808\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1575224\n",
      "\tspeed: 0.0382s/iter; left time: 257.5765s\n",
      "\titers: 200, epoch: 9 | loss: 0.0802651\n",
      "\tspeed: 0.0114s/iter; left time: 75.8035s\n",
      "\titers: 300, epoch: 9 | loss: 0.1117401\n",
      "\tspeed: 0.0114s/iter; left time: 74.7031s\n",
      "\titers: 400, epoch: 9 | loss: 0.1396321\n",
      "\tspeed: 0.0114s/iter; left time: 73.5536s\n",
      "\titers: 500, epoch: 9 | loss: 0.1535444\n",
      "\tspeed: 0.0114s/iter; left time: 72.3558s\n",
      "Epoch: 9 cost time: 6.796843528747559\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1096551 Vali Loss: 0.0328618 Test Loss: 0.1087080\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1076999\n",
      "\tspeed: 0.0385s/iter; left time: 237.6055s\n",
      "\titers: 200, epoch: 10 | loss: 0.0950411\n",
      "\tspeed: 0.0115s/iter; left time: 69.7585s\n",
      "\titers: 300, epoch: 10 | loss: 0.0886016\n",
      "\tspeed: 0.0115s/iter; left time: 68.5387s\n",
      "\titers: 400, epoch: 10 | loss: 0.1124933\n",
      "\tspeed: 0.0115s/iter; left time: 67.4635s\n",
      "\titers: 500, epoch: 10 | loss: 0.1248781\n",
      "\tspeed: 0.0115s/iter; left time: 66.3052s\n",
      "Epoch: 10 cost time: 6.925210237503052\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1088378 Vali Loss: 0.0329134 Test Loss: 0.1087530\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10842026770114899, mae:0.19971726834774017\n",
      ">>> LR=1e-4,DO=0.1,EL=2,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2666608\n",
      "\tspeed: 0.0239s/iter; left time: 270.6068s\n",
      "\titers: 200, epoch: 1 | loss: 0.2344479\n",
      "\tspeed: 0.0121s/iter; left time: 135.1357s\n",
      "\titers: 300, epoch: 1 | loss: 0.2681766\n",
      "\tspeed: 0.0120s/iter; left time: 133.5235s\n",
      "\titers: 400, epoch: 1 | loss: 0.1758684\n",
      "\tspeed: 0.0120s/iter; left time: 132.2285s\n",
      "\titers: 500, epoch: 1 | loss: 0.1921344\n",
      "\tspeed: 0.0120s/iter; left time: 130.9836s\n",
      "Epoch: 1 cost time: 8.111766338348389\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2114521 Vali Loss: 0.0422289 Test Loss: 0.1334401\n",
      "Validation loss decreased (inf --> 0.042229).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2222213\n",
      "\tspeed: 0.0402s/iter; left time: 431.4816s\n",
      "\titers: 200, epoch: 2 | loss: 0.2736513\n",
      "\tspeed: 0.0109s/iter; left time: 115.7575s\n",
      "\titers: 300, epoch: 2 | loss: 0.1277289\n",
      "\tspeed: 0.0109s/iter; left time: 114.8668s\n",
      "\titers: 400, epoch: 2 | loss: 0.1396603\n",
      "\tspeed: 0.0109s/iter; left time: 113.8811s\n",
      "\titers: 500, epoch: 2 | loss: 0.1350700\n",
      "\tspeed: 0.0109s/iter; left time: 112.6880s\n",
      "Epoch: 2 cost time: 6.547900676727295\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1713502 Vali Loss: 0.0425691 Test Loss: 0.1256566\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1520723\n",
      "\tspeed: 0.0382s/iter; left time: 388.4952s\n",
      "\titers: 200, epoch: 3 | loss: 0.1291768\n",
      "\tspeed: 0.0109s/iter; left time: 109.5412s\n",
      "\titers: 300, epoch: 3 | loss: 0.1434106\n",
      "\tspeed: 0.0109s/iter; left time: 108.4263s\n",
      "\titers: 400, epoch: 3 | loss: 0.1218131\n",
      "\tspeed: 0.0109s/iter; left time: 107.4908s\n",
      "\titers: 500, epoch: 3 | loss: 0.1572614\n",
      "\tspeed: 0.0109s/iter; left time: 106.0547s\n",
      "Epoch: 3 cost time: 6.531020879745483\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1472495 Vali Loss: 0.0413527 Test Loss: 0.1271744\n",
      "Validation loss decreased (0.042229 --> 0.041353).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1284194\n",
      "\tspeed: 0.0373s/iter; left time: 358.1311s\n",
      "\titers: 200, epoch: 4 | loss: 0.1104371\n",
      "\tspeed: 0.0108s/iter; left time: 102.8916s\n",
      "\titers: 300, epoch: 4 | loss: 0.1248381\n",
      "\tspeed: 0.0108s/iter; left time: 101.8034s\n",
      "\titers: 400, epoch: 4 | loss: 0.2190025\n",
      "\tspeed: 0.0108s/iter; left time: 100.7281s\n",
      "\titers: 500, epoch: 4 | loss: 0.1397098\n",
      "\tspeed: 0.0109s/iter; left time: 99.7418s\n",
      "Epoch: 4 cost time: 6.483416557312012\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1364210 Vali Loss: 0.0399190 Test Loss: 0.1251965\n",
      "Validation loss decreased (0.041353 --> 0.039919).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1347206\n",
      "\tspeed: 0.0376s/iter; left time: 339.4459s\n",
      "\titers: 200, epoch: 5 | loss: 0.1331736\n",
      "\tspeed: 0.0109s/iter; left time: 97.4326s\n",
      "\titers: 300, epoch: 5 | loss: 0.1527700\n",
      "\tspeed: 0.0109s/iter; left time: 96.2058s\n",
      "\titers: 400, epoch: 5 | loss: 0.1086815\n",
      "\tspeed: 0.0109s/iter; left time: 95.3130s\n",
      "\titers: 500, epoch: 5 | loss: 0.2206286\n",
      "\tspeed: 0.0109s/iter; left time: 94.3051s\n",
      "Epoch: 5 cost time: 6.539482831954956\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1303347 Vali Loss: 0.0386281 Test Loss: 0.1224689\n",
      "Validation loss decreased (0.039919 --> 0.038628).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1115298\n",
      "\tspeed: 0.0376s/iter; left time: 317.5938s\n",
      "\titers: 200, epoch: 6 | loss: 0.1223001\n",
      "\tspeed: 0.0109s/iter; left time: 91.4313s\n",
      "\titers: 300, epoch: 6 | loss: 0.1253359\n",
      "\tspeed: 0.0109s/iter; left time: 90.1247s\n",
      "\titers: 400, epoch: 6 | loss: 0.1120289\n",
      "\tspeed: 0.0109s/iter; left time: 89.0627s\n",
      "\titers: 500, epoch: 6 | loss: 0.1296152\n",
      "\tspeed: 0.0109s/iter; left time: 87.8042s\n",
      "Epoch: 6 cost time: 6.572933197021484\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1268731 Vali Loss: 0.0391719 Test Loss: 0.1254759\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1168155\n",
      "\tspeed: 0.0385s/iter; left time: 303.1453s\n",
      "\titers: 200, epoch: 7 | loss: 0.1033156\n",
      "\tspeed: 0.0109s/iter; left time: 85.1624s\n",
      "\titers: 300, epoch: 7 | loss: 0.0881221\n",
      "\tspeed: 0.0109s/iter; left time: 83.7601s\n",
      "\titers: 400, epoch: 7 | loss: 0.1157171\n",
      "\tspeed: 0.0109s/iter; left time: 82.5541s\n",
      "\titers: 500, epoch: 7 | loss: 0.0893376\n",
      "\tspeed: 0.0109s/iter; left time: 81.4827s\n",
      "Epoch: 7 cost time: 6.508885145187378\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1247528 Vali Loss: 0.0385924 Test Loss: 0.1243976\n",
      "Validation loss decreased (0.038628 --> 0.038592).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0976155\n",
      "\tspeed: 0.0392s/iter; left time: 286.9121s\n",
      "\titers: 200, epoch: 8 | loss: 0.1807430\n",
      "\tspeed: 0.0109s/iter; left time: 78.7378s\n",
      "\titers: 300, epoch: 8 | loss: 0.1563657\n",
      "\tspeed: 0.0109s/iter; left time: 77.7113s\n",
      "\titers: 400, epoch: 8 | loss: 0.1276036\n",
      "\tspeed: 0.0109s/iter; left time: 76.5717s\n",
      "\titers: 500, epoch: 8 | loss: 0.0864860\n",
      "\tspeed: 0.0109s/iter; left time: 75.3454s\n",
      "Epoch: 8 cost time: 6.530410051345825\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1243899 Vali Loss: 0.0384668 Test Loss: 0.1241978\n",
      "Validation loss decreased (0.038592 --> 0.038467).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1099533\n",
      "\tspeed: 0.0396s/iter; left time: 266.6070s\n",
      "\titers: 200, epoch: 9 | loss: 0.0833538\n",
      "\tspeed: 0.0109s/iter; left time: 72.3045s\n",
      "\titers: 300, epoch: 9 | loss: 0.1339110\n",
      "\tspeed: 0.0109s/iter; left time: 71.2093s\n",
      "\titers: 400, epoch: 9 | loss: 0.1569254\n",
      "\tspeed: 0.0109s/iter; left time: 70.0874s\n",
      "\titers: 500, epoch: 9 | loss: 0.0781038\n",
      "\tspeed: 0.0109s/iter; left time: 69.1190s\n",
      "Epoch: 9 cost time: 6.561692237854004\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1230334 Vali Loss: 0.0385081 Test Loss: 0.1237361\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1158035\n",
      "\tspeed: 0.0379s/iter; left time: 233.6552s\n",
      "\titers: 200, epoch: 10 | loss: 0.0832513\n",
      "\tspeed: 0.0109s/iter; left time: 65.8914s\n",
      "\titers: 300, epoch: 10 | loss: 0.1323909\n",
      "\tspeed: 0.0109s/iter; left time: 64.8823s\n",
      "\titers: 400, epoch: 10 | loss: 0.0938356\n",
      "\tspeed: 0.0109s/iter; left time: 63.9206s\n",
      "\titers: 500, epoch: 10 | loss: 0.1200961\n",
      "\tspeed: 0.0108s/iter; left time: 62.5934s\n",
      "Epoch: 10 cost time: 6.487656831741333\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1232119 Vali Loss: 0.0384464 Test Loss: 0.1238844\n",
      "Validation loss decreased (0.038467 --> 0.038446).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.0880294\n",
      "\tspeed: 0.0375s/iter; left time: 209.9328s\n",
      "\titers: 200, epoch: 11 | loss: 0.1333495\n",
      "\tspeed: 0.0109s/iter; left time: 59.7905s\n",
      "\titers: 300, epoch: 11 | loss: 0.1132784\n",
      "\tspeed: 0.0108s/iter; left time: 58.5201s\n",
      "\titers: 400, epoch: 11 | loss: 0.0909003\n",
      "\tspeed: 0.0108s/iter; left time: 57.3680s\n",
      "\titers: 500, epoch: 11 | loss: 0.0962447\n",
      "\tspeed: 0.0108s/iter; left time: 56.2935s\n",
      "Epoch: 11 cost time: 6.49623441696167\n",
      "Epoch: 11, Steps: 570 | Train Loss: 0.1235934 Vali Loss: 0.0384384 Test Loss: 0.1238350\n",
      "Validation loss decreased (0.038446 --> 0.038438).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1400836\n",
      "\tspeed: 0.0377s/iter; left time: 189.6402s\n",
      "\titers: 200, epoch: 12 | loss: 0.1498244\n",
      "\tspeed: 0.0109s/iter; left time: 53.8549s\n",
      "\titers: 300, epoch: 12 | loss: 0.1078468\n",
      "\tspeed: 0.0109s/iter; left time: 52.7431s\n",
      "\titers: 400, epoch: 12 | loss: 0.1485666\n",
      "\tspeed: 0.0109s/iter; left time: 51.6569s\n",
      "\titers: 500, epoch: 12 | loss: 0.1239595\n",
      "\tspeed: 0.0109s/iter; left time: 50.5666s\n",
      "Epoch: 12 cost time: 6.543392896652222\n",
      "Epoch: 12, Steps: 570 | Train Loss: 0.1226309 Vali Loss: 0.0384481 Test Loss: 0.1238305\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.0807109\n",
      "\tspeed: 0.0379s/iter; left time: 168.8785s\n",
      "\titers: 200, epoch: 13 | loss: 0.1681882\n",
      "\tspeed: 0.0108s/iter; left time: 47.1445s\n",
      "\titers: 300, epoch: 13 | loss: 0.0886898\n",
      "\tspeed: 0.0108s/iter; left time: 46.0921s\n",
      "\titers: 400, epoch: 13 | loss: 0.0915739\n",
      "\tspeed: 0.0108s/iter; left time: 45.0377s\n",
      "\titers: 500, epoch: 13 | loss: 0.1158995\n",
      "\tspeed: 0.0108s/iter; left time: 44.0430s\n",
      "Epoch: 13 cost time: 6.503815174102783\n",
      "Epoch: 13, Steps: 570 | Train Loss: 0.1236351 Vali Loss: 0.0384767 Test Loss: 0.1238470\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1881920\n",
      "\tspeed: 0.0390s/iter; left time: 151.8588s\n",
      "\titers: 200, epoch: 14 | loss: 0.1902320\n",
      "\tspeed: 0.0110s/iter; left time: 41.5252s\n",
      "\titers: 300, epoch: 14 | loss: 0.1145264\n",
      "\tspeed: 0.0110s/iter; left time: 40.5510s\n",
      "\titers: 400, epoch: 14 | loss: 0.1251795\n",
      "\tspeed: 0.0110s/iter; left time: 39.4848s\n",
      "\titers: 500, epoch: 14 | loss: 0.1174224\n",
      "\tspeed: 0.0110s/iter; left time: 38.2404s\n",
      "Epoch: 14 cost time: 6.58145546913147\n",
      "Epoch: 14, Steps: 570 | Train Loss: 0.1234293 Vali Loss: 0.0384395 Test Loss: 0.1238526\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1240076944231987, mae:0.22189994156360626\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2047569\n",
      "\tspeed: 0.0260s/iter; left time: 293.4117s\n",
      "\titers: 200, epoch: 1 | loss: 0.1721311\n",
      "\tspeed: 0.0145s/iter; left time: 162.1858s\n",
      "\titers: 300, epoch: 1 | loss: 0.2918872\n",
      "\tspeed: 0.0145s/iter; left time: 160.4213s\n",
      "\titers: 400, epoch: 1 | loss: 0.1840911\n",
      "\tspeed: 0.0145s/iter; left time: 159.0711s\n",
      "\titers: 500, epoch: 1 | loss: 0.1514521\n",
      "\tspeed: 0.0145s/iter; left time: 157.9750s\n",
      "Epoch: 1 cost time: 9.451337575912476\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2111790 Vali Loss: 0.0435493 Test Loss: 0.1358985\n",
      "Validation loss decreased (inf --> 0.043549).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2041141\n",
      "\tspeed: 0.0475s/iter; left time: 509.7456s\n",
      "\titers: 200, epoch: 2 | loss: 0.1462274\n",
      "\tspeed: 0.0145s/iter; left time: 154.3992s\n",
      "\titers: 300, epoch: 2 | loss: 0.1372207\n",
      "\tspeed: 0.0145s/iter; left time: 153.0773s\n",
      "\titers: 400, epoch: 2 | loss: 0.1192545\n",
      "\tspeed: 0.0145s/iter; left time: 151.3126s\n",
      "\titers: 500, epoch: 2 | loss: 0.1763059\n",
      "\tspeed: 0.0145s/iter; left time: 150.0586s\n",
      "Epoch: 2 cost time: 8.590759515762329\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1698193 Vali Loss: 0.0397341 Test Loss: 0.1301434\n",
      "Validation loss decreased (0.043549 --> 0.039734).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2607693\n",
      "\tspeed: 0.0501s/iter; left time: 509.1628s\n",
      "\titers: 200, epoch: 3 | loss: 0.1628471\n",
      "\tspeed: 0.0163s/iter; left time: 163.5324s\n",
      "\titers: 300, epoch: 3 | loss: 0.1069581\n",
      "\tspeed: 0.0162s/iter; left time: 161.6998s\n",
      "\titers: 400, epoch: 3 | loss: 0.1171424\n",
      "\tspeed: 0.0162s/iter; left time: 160.2218s\n",
      "\titers: 500, epoch: 3 | loss: 0.1060702\n",
      "\tspeed: 0.0163s/iter; left time: 158.6615s\n",
      "Epoch: 3 cost time: 9.5415780544281\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1490578 Vali Loss: 0.0359175 Test Loss: 0.1201296\n",
      "Validation loss decreased (0.039734 --> 0.035917).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1515058\n",
      "\tspeed: 0.0476s/iter; left time: 456.5381s\n",
      "\titers: 200, epoch: 4 | loss: 0.1164116\n",
      "\tspeed: 0.0145s/iter; left time: 137.9265s\n",
      "\titers: 300, epoch: 4 | loss: 0.1514658\n",
      "\tspeed: 0.0145s/iter; left time: 136.4556s\n",
      "\titers: 400, epoch: 4 | loss: 0.1478230\n",
      "\tspeed: 0.0145s/iter; left time: 135.0452s\n",
      "\titers: 500, epoch: 4 | loss: 0.1313840\n",
      "\tspeed: 0.0145s/iter; left time: 133.5186s\n",
      "Epoch: 4 cost time: 8.595931053161621\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1398823 Vali Loss: 0.0353378 Test Loss: 0.1230004\n",
      "Validation loss decreased (0.035917 --> 0.035338).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1263737\n",
      "\tspeed: 0.0477s/iter; left time: 430.7195s\n",
      "\titers: 200, epoch: 5 | loss: 0.1141983\n",
      "\tspeed: 0.0145s/iter; left time: 128.9838s\n",
      "\titers: 300, epoch: 5 | loss: 0.1274197\n",
      "\tspeed: 0.0145s/iter; left time: 127.5850s\n",
      "\titers: 400, epoch: 5 | loss: 0.1358247\n",
      "\tspeed: 0.0145s/iter; left time: 126.1657s\n",
      "\titers: 500, epoch: 5 | loss: 0.1271198\n",
      "\tspeed: 0.0145s/iter; left time: 124.6313s\n",
      "Epoch: 5 cost time: 8.553182125091553\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1352223 Vali Loss: 0.0348664 Test Loss: 0.1199729\n",
      "Validation loss decreased (0.035338 --> 0.034866).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0976155\n",
      "\tspeed: 0.0473s/iter; left time: 400.1051s\n",
      "\titers: 200, epoch: 6 | loss: 0.1496920\n",
      "\tspeed: 0.0145s/iter; left time: 121.2702s\n",
      "\titers: 300, epoch: 6 | loss: 0.1000441\n",
      "\tspeed: 0.0145s/iter; left time: 119.8589s\n",
      "\titers: 400, epoch: 6 | loss: 0.0984927\n",
      "\tspeed: 0.0145s/iter; left time: 118.3952s\n",
      "\titers: 500, epoch: 6 | loss: 0.1439775\n",
      "\tspeed: 0.0145s/iter; left time: 116.9126s\n",
      "Epoch: 6 cost time: 8.625679969787598\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1312461 Vali Loss: 0.0346095 Test Loss: 0.1215637\n",
      "Validation loss decreased (0.034866 --> 0.034610).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1307121\n",
      "\tspeed: 0.0488s/iter; left time: 384.2000s\n",
      "\titers: 200, epoch: 7 | loss: 0.1020568\n",
      "\tspeed: 0.0162s/iter; left time: 126.3758s\n",
      "\titers: 300, epoch: 7 | loss: 0.1043422\n",
      "\tspeed: 0.0162s/iter; left time: 124.7740s\n",
      "\titers: 400, epoch: 7 | loss: 0.1314693\n",
      "\tspeed: 0.0162s/iter; left time: 123.1365s\n",
      "\titers: 500, epoch: 7 | loss: 0.2119968\n",
      "\tspeed: 0.0163s/iter; left time: 122.1376s\n",
      "Epoch: 7 cost time: 9.65189504623413\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1305236 Vali Loss: 0.0346760 Test Loss: 0.1211119\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1867866\n",
      "\tspeed: 0.0465s/iter; left time: 340.2264s\n",
      "\titers: 200, epoch: 8 | loss: 0.1295784\n",
      "\tspeed: 0.0144s/iter; left time: 103.7992s\n",
      "\titers: 300, epoch: 8 | loss: 0.1420640\n",
      "\tspeed: 0.0144s/iter; left time: 102.5519s\n",
      "\titers: 400, epoch: 8 | loss: 0.0948894\n",
      "\tspeed: 0.0144s/iter; left time: 101.1137s\n",
      "\titers: 500, epoch: 8 | loss: 0.1524995\n",
      "\tspeed: 0.0144s/iter; left time: 99.5629s\n",
      "Epoch: 8 cost time: 8.485867023468018\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1304179 Vali Loss: 0.0348385 Test Loss: 0.1214825\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1644307\n",
      "\tspeed: 0.0502s/iter; left time: 338.2019s\n",
      "\titers: 200, epoch: 9 | loss: 0.1492928\n",
      "\tspeed: 0.0162s/iter; left time: 107.7062s\n",
      "\titers: 300, epoch: 9 | loss: 0.1900343\n",
      "\tspeed: 0.0157s/iter; left time: 102.8160s\n",
      "\titers: 400, epoch: 9 | loss: 0.1507646\n",
      "\tspeed: 0.0143s/iter; left time: 92.3838s\n",
      "\titers: 500, epoch: 9 | loss: 0.1052462\n",
      "\tspeed: 0.0144s/iter; left time: 91.1764s\n",
      "Epoch: 9 cost time: 8.986774444580078\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1295061 Vali Loss: 0.0347421 Test Loss: 0.1210433\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12174582481384277, mae:0.21724450588226318\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=256,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2714466\n",
      "\tspeed: 0.0282s/iter; left time: 318.7579s\n",
      "\titers: 200, epoch: 1 | loss: 0.1833450\n",
      "\tspeed: 0.0161s/iter; left time: 180.6746s\n",
      "\titers: 300, epoch: 1 | loss: 0.1116743\n",
      "\tspeed: 0.0161s/iter; left time: 179.0634s\n",
      "\titers: 400, epoch: 1 | loss: 0.2457210\n",
      "\tspeed: 0.0161s/iter; left time: 177.4160s\n",
      "\titers: 500, epoch: 1 | loss: 0.1758959\n",
      "\tspeed: 0.0161s/iter; left time: 175.6195s\n",
      "Epoch: 1 cost time: 10.447259187698364\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1832130 Vali Loss: 0.0354777 Test Loss: 0.1102525\n",
      "Validation loss decreased (inf --> 0.035478).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1255521\n",
      "\tspeed: 0.0489s/iter; left time: 525.1447s\n",
      "\titers: 200, epoch: 2 | loss: 0.2327390\n",
      "\tspeed: 0.0154s/iter; left time: 164.0590s\n",
      "\titers: 300, epoch: 2 | loss: 0.1772514\n",
      "\tspeed: 0.0161s/iter; left time: 170.0402s\n",
      "\titers: 400, epoch: 2 | loss: 0.1205304\n",
      "\tspeed: 0.0161s/iter; left time: 167.7539s\n",
      "\titers: 500, epoch: 2 | loss: 0.1233689\n",
      "\tspeed: 0.0161s/iter; left time: 166.4049s\n",
      "Epoch: 2 cost time: 9.429375886917114\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1506715 Vali Loss: 0.0339201 Test Loss: 0.1126614\n",
      "Validation loss decreased (0.035478 --> 0.033920).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1473355\n",
      "\tspeed: 0.0470s/iter; left time: 477.8686s\n",
      "\titers: 200, epoch: 3 | loss: 0.0864547\n",
      "\tspeed: 0.0143s/iter; left time: 143.9958s\n",
      "\titers: 300, epoch: 3 | loss: 0.1745018\n",
      "\tspeed: 0.0143s/iter; left time: 142.2864s\n",
      "\titers: 400, epoch: 3 | loss: 0.1090745\n",
      "\tspeed: 0.0143s/iter; left time: 140.5964s\n",
      "\titers: 500, epoch: 3 | loss: 0.1212042\n",
      "\tspeed: 0.0142s/iter; left time: 139.0582s\n",
      "Epoch: 3 cost time: 8.439759731292725\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1344462 Vali Loss: 0.0329960 Test Loss: 0.1109708\n",
      "Validation loss decreased (0.033920 --> 0.032996).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1262608\n",
      "\tspeed: 0.0474s/iter; left time: 454.4113s\n",
      "\titers: 200, epoch: 4 | loss: 0.1213391\n",
      "\tspeed: 0.0143s/iter; left time: 135.3385s\n",
      "\titers: 300, epoch: 4 | loss: 0.1373242\n",
      "\tspeed: 0.0143s/iter; left time: 134.2154s\n",
      "\titers: 400, epoch: 4 | loss: 0.0776137\n",
      "\tspeed: 0.0143s/iter; left time: 132.8094s\n",
      "\titers: 500, epoch: 4 | loss: 0.0916152\n",
      "\tspeed: 0.0143s/iter; left time: 131.4924s\n",
      "Epoch: 4 cost time: 8.468201875686646\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1285917 Vali Loss: 0.0319500 Test Loss: 0.1105838\n",
      "Validation loss decreased (0.032996 --> 0.031950).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0646266\n",
      "\tspeed: 0.0453s/iter; left time: 408.4127s\n",
      "\titers: 200, epoch: 5 | loss: 0.1230586\n",
      "\tspeed: 0.0143s/iter; left time: 127.7471s\n",
      "\titers: 300, epoch: 5 | loss: 0.1696747\n",
      "\tspeed: 0.0143s/iter; left time: 126.3821s\n",
      "\titers: 400, epoch: 5 | loss: 0.1049808\n",
      "\tspeed: 0.0143s/iter; left time: 124.9197s\n",
      "\titers: 500, epoch: 5 | loss: 0.1418797\n",
      "\tspeed: 0.0143s/iter; left time: 123.3858s\n",
      "Epoch: 5 cost time: 8.43315052986145\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1240557 Vali Loss: 0.0317498 Test Loss: 0.1098400\n",
      "Validation loss decreased (0.031950 --> 0.031750).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1115341\n",
      "\tspeed: 0.0466s/iter; left time: 393.9493s\n",
      "\titers: 200, epoch: 6 | loss: 0.2230012\n",
      "\tspeed: 0.0143s/iter; left time: 119.4400s\n",
      "\titers: 300, epoch: 6 | loss: 0.1876464\n",
      "\tspeed: 0.0143s/iter; left time: 118.0569s\n",
      "\titers: 400, epoch: 6 | loss: 0.1356148\n",
      "\tspeed: 0.0143s/iter; left time: 116.3375s\n",
      "\titers: 500, epoch: 6 | loss: 0.1370379\n",
      "\tspeed: 0.0143s/iter; left time: 114.9026s\n",
      "Epoch: 6 cost time: 8.418618440628052\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1222151 Vali Loss: 0.0315075 Test Loss: 0.1099989\n",
      "Validation loss decreased (0.031750 --> 0.031507).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0917359\n",
      "\tspeed: 0.0474s/iter; left time: 373.4032s\n",
      "\titers: 200, epoch: 7 | loss: 0.1327952\n",
      "\tspeed: 0.0142s/iter; left time: 110.8711s\n",
      "\titers: 300, epoch: 7 | loss: 0.1693450\n",
      "\tspeed: 0.0143s/iter; left time: 109.5829s\n",
      "\titers: 400, epoch: 7 | loss: 0.1225072\n",
      "\tspeed: 0.0142s/iter; left time: 107.8454s\n",
      "\titers: 500, epoch: 7 | loss: 0.1464634\n",
      "\tspeed: 0.0143s/iter; left time: 106.6227s\n",
      "Epoch: 7 cost time: 8.394713163375854\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1204937 Vali Loss: 0.0312481 Test Loss: 0.1096683\n",
      "Validation loss decreased (0.031507 --> 0.031248).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0855149\n",
      "\tspeed: 0.0450s/iter; left time: 329.2351s\n",
      "\titers: 200, epoch: 8 | loss: 0.1063012\n",
      "\tspeed: 0.0143s/iter; left time: 103.0597s\n",
      "\titers: 300, epoch: 8 | loss: 0.1573913\n",
      "\tspeed: 0.0143s/iter; left time: 101.6298s\n",
      "\titers: 400, epoch: 8 | loss: 0.1176097\n",
      "\tspeed: 0.0143s/iter; left time: 100.2037s\n",
      "\titers: 500, epoch: 8 | loss: 0.1097314\n",
      "\tspeed: 0.0143s/iter; left time: 98.7342s\n",
      "Epoch: 8 cost time: 8.43489694595337\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1196738 Vali Loss: 0.0313694 Test Loss: 0.1097663\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0974496\n",
      "\tspeed: 0.0476s/iter; left time: 320.5712s\n",
      "\titers: 200, epoch: 9 | loss: 0.1121854\n",
      "\tspeed: 0.0143s/iter; left time: 94.9717s\n",
      "\titers: 300, epoch: 9 | loss: 0.1125480\n",
      "\tspeed: 0.0143s/iter; left time: 93.6465s\n",
      "\titers: 400, epoch: 9 | loss: 0.0968294\n",
      "\tspeed: 0.0143s/iter; left time: 91.8920s\n",
      "\titers: 500, epoch: 9 | loss: 0.1291035\n",
      "\tspeed: 0.0143s/iter; left time: 90.4439s\n",
      "Epoch: 9 cost time: 8.423711061477661\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1191862 Vali Loss: 0.0312849 Test Loss: 0.1098480\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1136008\n",
      "\tspeed: 0.0468s/iter; left time: 288.7689s\n",
      "\titers: 200, epoch: 10 | loss: 0.1118148\n",
      "\tspeed: 0.0143s/iter; left time: 86.6982s\n",
      "\titers: 300, epoch: 10 | loss: 0.1124306\n",
      "\tspeed: 0.0143s/iter; left time: 85.1577s\n",
      "\titers: 400, epoch: 10 | loss: 0.1297452\n",
      "\tspeed: 0.0142s/iter; left time: 83.5469s\n",
      "\titers: 500, epoch: 10 | loss: 0.1475537\n",
      "\tspeed: 0.0143s/iter; left time: 82.2688s\n",
      "Epoch: 10 cost time: 8.417852640151978\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1196228 Vali Loss: 0.0313251 Test Loss: 0.1098910\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.10982311517000198, mae:0.20000316202640533\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=256,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1340132\n",
      "\tspeed: 0.0279s/iter; left time: 315.6928s\n",
      "\titers: 200, epoch: 1 | loss: 0.2731330\n",
      "\tspeed: 0.0161s/iter; left time: 179.9575s\n",
      "\titers: 300, epoch: 1 | loss: 0.2700988\n",
      "\tspeed: 0.0160s/iter; left time: 178.1332s\n",
      "\titers: 400, epoch: 1 | loss: 0.2357663\n",
      "\tspeed: 0.0161s/iter; left time: 176.6694s\n",
      "\titers: 500, epoch: 1 | loss: 0.2441259\n",
      "\tspeed: 0.0160s/iter; left time: 174.9165s\n",
      "Epoch: 1 cost time: 10.319015741348267\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2175356 Vali Loss: 0.0434821 Test Loss: 0.1324463\n",
      "Validation loss decreased (inf --> 0.043482).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1540688\n",
      "\tspeed: 0.0474s/iter; left time: 509.0747s\n",
      "\titers: 200, epoch: 2 | loss: 0.1154824\n",
      "\tspeed: 0.0142s/iter; left time: 151.4273s\n",
      "\titers: 300, epoch: 2 | loss: 0.1036720\n",
      "\tspeed: 0.0149s/iter; left time: 157.2018s\n",
      "\titers: 400, epoch: 2 | loss: 0.1394471\n",
      "\tspeed: 0.0164s/iter; left time: 171.1589s\n",
      "\titers: 500, epoch: 2 | loss: 0.2109354\n",
      "\tspeed: 0.0164s/iter; left time: 169.1343s\n",
      "Epoch: 2 cost time: 9.077941179275513\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1765220 Vali Loss: 0.0419425 Test Loss: 0.1278307\n",
      "Validation loss decreased (0.043482 --> 0.041942).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1866744\n",
      "\tspeed: 0.0519s/iter; left time: 527.2743s\n",
      "\titers: 200, epoch: 3 | loss: 0.1807503\n",
      "\tspeed: 0.0164s/iter; left time: 165.0974s\n",
      "\titers: 300, epoch: 3 | loss: 0.1508223\n",
      "\tspeed: 0.0164s/iter; left time: 163.6765s\n",
      "\titers: 400, epoch: 3 | loss: 0.1283116\n",
      "\tspeed: 0.0164s/iter; left time: 161.7852s\n",
      "\titers: 500, epoch: 3 | loss: 0.1517522\n",
      "\tspeed: 0.0164s/iter; left time: 160.3386s\n",
      "Epoch: 3 cost time: 9.642561197280884\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1552826 Vali Loss: 0.0393618 Test Loss: 0.1237901\n",
      "Validation loss decreased (0.041942 --> 0.039362).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1693730\n",
      "\tspeed: 0.0510s/iter; left time: 489.0691s\n",
      "\titers: 200, epoch: 4 | loss: 0.1441906\n",
      "\tspeed: 0.0166s/iter; left time: 157.4760s\n",
      "\titers: 300, epoch: 4 | loss: 0.1830863\n",
      "\tspeed: 0.0165s/iter; left time: 155.2422s\n",
      "\titers: 400, epoch: 4 | loss: 0.1477285\n",
      "\tspeed: 0.0165s/iter; left time: 153.5429s\n",
      "\titers: 500, epoch: 4 | loss: 0.1046303\n",
      "\tspeed: 0.0165s/iter; left time: 151.8226s\n",
      "Epoch: 4 cost time: 9.749010562896729\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1463910 Vali Loss: 0.0377049 Test Loss: 0.1226720\n",
      "Validation loss decreased (0.039362 --> 0.037705).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1406279\n",
      "\tspeed: 0.0496s/iter; left time: 447.0917s\n",
      "\titers: 200, epoch: 5 | loss: 0.1610852\n",
      "\tspeed: 0.0143s/iter; left time: 127.7083s\n",
      "\titers: 300, epoch: 5 | loss: 0.1367887\n",
      "\tspeed: 0.0143s/iter; left time: 126.2480s\n",
      "\titers: 400, epoch: 5 | loss: 0.1876882\n",
      "\tspeed: 0.0143s/iter; left time: 124.9413s\n",
      "\titers: 500, epoch: 5 | loss: 0.1052222\n",
      "\tspeed: 0.0143s/iter; left time: 123.6347s\n",
      "Epoch: 5 cost time: 8.63890266418457\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1414276 Vali Loss: 0.0380426 Test Loss: 0.1222891\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2116309\n",
      "\tspeed: 0.0462s/iter; left time: 390.4115s\n",
      "\titers: 200, epoch: 6 | loss: 0.1298434\n",
      "\tspeed: 0.0163s/iter; left time: 136.3929s\n",
      "\titers: 300, epoch: 6 | loss: 0.0935109\n",
      "\tspeed: 0.0164s/iter; left time: 135.1040s\n",
      "\titers: 400, epoch: 6 | loss: 0.1730927\n",
      "\tspeed: 0.0163s/iter; left time: 133.1878s\n",
      "\titers: 500, epoch: 6 | loss: 0.1374914\n",
      "\tspeed: 0.0164s/iter; left time: 131.7060s\n",
      "Epoch: 6 cost time: 9.425790309906006\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1391261 Vali Loss: 0.0377288 Test Loss: 0.1226217\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1686033\n",
      "\tspeed: 0.0486s/iter; left time: 383.3123s\n",
      "\titers: 200, epoch: 7 | loss: 0.0903399\n",
      "\tspeed: 0.0161s/iter; left time: 125.0860s\n",
      "\titers: 300, epoch: 7 | loss: 0.1061012\n",
      "\tspeed: 0.0154s/iter; left time: 118.1236s\n",
      "\titers: 400, epoch: 7 | loss: 0.1717959\n",
      "\tspeed: 0.0143s/iter; left time: 108.1807s\n",
      "\titers: 500, epoch: 7 | loss: 0.1215039\n",
      "\tspeed: 0.0143s/iter; left time: 106.9074s\n",
      "Epoch: 7 cost time: 8.920162916183472\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1389024 Vali Loss: 0.0375392 Test Loss: 0.1220629\n",
      "Validation loss decreased (0.037705 --> 0.037539).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1050478\n",
      "\tspeed: 0.0464s/iter; left time: 339.4073s\n",
      "\titers: 200, epoch: 8 | loss: 0.1073643\n",
      "\tspeed: 0.0143s/iter; left time: 103.2432s\n",
      "\titers: 300, epoch: 8 | loss: 0.1274797\n",
      "\tspeed: 0.0143s/iter; left time: 101.7857s\n",
      "\titers: 400, epoch: 8 | loss: 0.1463042\n",
      "\tspeed: 0.0143s/iter; left time: 100.3137s\n",
      "\titers: 500, epoch: 8 | loss: 0.1025157\n",
      "\tspeed: 0.0143s/iter; left time: 98.8370s\n",
      "Epoch: 8 cost time: 8.488332986831665\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1376869 Vali Loss: 0.0380538 Test Loss: 0.1229385\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1774512\n",
      "\tspeed: 0.0480s/iter; left time: 323.5725s\n",
      "\titers: 200, epoch: 9 | loss: 0.1372954\n",
      "\tspeed: 0.0164s/iter; left time: 108.7852s\n",
      "\titers: 300, epoch: 9 | loss: 0.0735341\n",
      "\tspeed: 0.0165s/iter; left time: 107.7589s\n",
      "\titers: 400, epoch: 9 | loss: 0.1188553\n",
      "\tspeed: 0.0165s/iter; left time: 106.5614s\n",
      "\titers: 500, epoch: 9 | loss: 0.1156157\n",
      "\tspeed: 0.0165s/iter; left time: 104.4837s\n",
      "Epoch: 9 cost time: 9.6681649684906\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1363195 Vali Loss: 0.0376753 Test Loss: 0.1227020\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1548845\n",
      "\tspeed: 0.0473s/iter; left time: 292.0595s\n",
      "\titers: 200, epoch: 10 | loss: 0.1635989\n",
      "\tspeed: 0.0144s/iter; left time: 87.4624s\n",
      "\titers: 300, epoch: 10 | loss: 0.1354944\n",
      "\tspeed: 0.0144s/iter; left time: 86.0314s\n",
      "\titers: 400, epoch: 10 | loss: 0.1608072\n",
      "\tspeed: 0.0144s/iter; left time: 84.5541s\n",
      "\titers: 500, epoch: 10 | loss: 0.1307496\n",
      "\tspeed: 0.0144s/iter; left time: 83.3068s\n",
      "Epoch: 10 cost time: 8.510529518127441\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1368901 Vali Loss: 0.0376425 Test Loss: 0.1224218\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.12225611507892609, mae:0.21893946826457977\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=512,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2642496\n",
      "\tspeed: 0.0302s/iter; left time: 341.5997s\n",
      "\titers: 200, epoch: 1 | loss: 0.1672373\n",
      "\tspeed: 0.0168s/iter; left time: 188.4413s\n",
      "\titers: 300, epoch: 1 | loss: 0.2110314\n",
      "\tspeed: 0.0168s/iter; left time: 186.7510s\n",
      "\titers: 400, epoch: 1 | loss: 0.1752806\n",
      "\tspeed: 0.0168s/iter; left time: 185.1820s\n",
      "\titers: 500, epoch: 1 | loss: 0.1534884\n",
      "\tspeed: 0.0169s/iter; left time: 183.7136s\n",
      "Epoch: 1 cost time: 10.980354309082031\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2048573 Vali Loss: 0.0442407 Test Loss: 0.1334456\n",
      "Validation loss decreased (inf --> 0.044241).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1922308\n",
      "\tspeed: 0.0523s/iter; left time: 561.4676s\n",
      "\titers: 200, epoch: 2 | loss: 0.1989005\n",
      "\tspeed: 0.0170s/iter; left time: 180.2659s\n",
      "\titers: 300, epoch: 2 | loss: 0.1904143\n",
      "\tspeed: 0.0169s/iter; left time: 177.9666s\n",
      "\titers: 400, epoch: 2 | loss: 0.1112681\n",
      "\tspeed: 0.0169s/iter; left time: 176.2745s\n",
      "\titers: 500, epoch: 2 | loss: 0.1280198\n",
      "\tspeed: 0.0169s/iter; left time: 174.7552s\n",
      "Epoch: 2 cost time: 9.96340560913086\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1616140 Vali Loss: 0.0378118 Test Loss: 0.1226735\n",
      "Validation loss decreased (0.044241 --> 0.037812).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1237671\n",
      "\tspeed: 0.0508s/iter; left time: 515.9404s\n",
      "\titers: 200, epoch: 3 | loss: 0.1354339\n",
      "\tspeed: 0.0169s/iter; left time: 169.5458s\n",
      "\titers: 300, epoch: 3 | loss: 0.2374647\n",
      "\tspeed: 0.0168s/iter; left time: 167.8121s\n",
      "\titers: 400, epoch: 3 | loss: 0.1247521\n",
      "\tspeed: 0.0169s/iter; left time: 166.3438s\n",
      "\titers: 500, epoch: 3 | loss: 0.0987477\n",
      "\tspeed: 0.0169s/iter; left time: 164.5028s\n",
      "Epoch: 3 cost time: 9.898221492767334\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1346405 Vali Loss: 0.0368421 Test Loss: 0.1211564\n",
      "Validation loss decreased (0.037812 --> 0.036842).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0937603\n",
      "\tspeed: 0.0518s/iter; left time: 497.1729s\n",
      "\titers: 200, epoch: 4 | loss: 0.1194085\n",
      "\tspeed: 0.0171s/iter; left time: 162.0807s\n",
      "\titers: 300, epoch: 4 | loss: 0.1261603\n",
      "\tspeed: 0.0169s/iter; left time: 159.0545s\n",
      "\titers: 400, epoch: 4 | loss: 0.1470387\n",
      "\tspeed: 0.0169s/iter; left time: 157.3541s\n",
      "\titers: 500, epoch: 4 | loss: 0.1260980\n",
      "\tspeed: 0.0180s/iter; left time: 165.4955s\n",
      "Epoch: 4 cost time: 10.345334529876709\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1197446 Vali Loss: 0.0364672 Test Loss: 0.1205641\n",
      "Validation loss decreased (0.036842 --> 0.036467).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0868400\n",
      "\tspeed: 0.0518s/iter; left time: 467.4152s\n",
      "\titers: 200, epoch: 5 | loss: 0.1205349\n",
      "\tspeed: 0.0169s/iter; left time: 151.1517s\n",
      "\titers: 300, epoch: 5 | loss: 0.1141921\n",
      "\tspeed: 0.0169s/iter; left time: 149.4298s\n",
      "\titers: 400, epoch: 5 | loss: 0.1284914\n",
      "\tspeed: 0.0169s/iter; left time: 147.6132s\n",
      "\titers: 500, epoch: 5 | loss: 0.0882356\n",
      "\tspeed: 0.0169s/iter; left time: 145.8724s\n",
      "Epoch: 5 cost time: 9.9378342628479\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1131571 Vali Loss: 0.0348938 Test Loss: 0.1195340\n",
      "Validation loss decreased (0.036467 --> 0.034894).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1147439\n",
      "\tspeed: 0.0509s/iter; left time: 429.9195s\n",
      "\titers: 200, epoch: 6 | loss: 0.0958291\n",
      "\tspeed: 0.0169s/iter; left time: 141.0050s\n",
      "\titers: 300, epoch: 6 | loss: 0.1061931\n",
      "\tspeed: 0.0169s/iter; left time: 139.1984s\n",
      "\titers: 400, epoch: 6 | loss: 0.1339055\n",
      "\tspeed: 0.0169s/iter; left time: 137.6319s\n",
      "\titers: 500, epoch: 6 | loss: 0.1064150\n",
      "\tspeed: 0.0169s/iter; left time: 135.8044s\n",
      "Epoch: 6 cost time: 9.927862167358398\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1091125 Vali Loss: 0.0349584 Test Loss: 0.1170439\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1292955\n",
      "\tspeed: 0.0503s/iter; left time: 396.2937s\n",
      "\titers: 200, epoch: 7 | loss: 0.1387319\n",
      "\tspeed: 0.0169s/iter; left time: 131.4405s\n",
      "\titers: 300, epoch: 7 | loss: 0.1260076\n",
      "\tspeed: 0.0169s/iter; left time: 129.6388s\n",
      "\titers: 400, epoch: 7 | loss: 0.0798721\n",
      "\tspeed: 0.0169s/iter; left time: 127.9866s\n",
      "\titers: 500, epoch: 7 | loss: 0.0855768\n",
      "\tspeed: 0.0169s/iter; left time: 126.3074s\n",
      "Epoch: 7 cost time: 9.940653562545776\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1072664 Vali Loss: 0.0340650 Test Loss: 0.1183059\n",
      "Validation loss decreased (0.034894 --> 0.034065).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0993857\n",
      "\tspeed: 0.0518s/iter; left time: 378.9113s\n",
      "\titers: 200, epoch: 8 | loss: 0.0821224\n",
      "\tspeed: 0.0169s/iter; left time: 121.9947s\n",
      "\titers: 300, epoch: 8 | loss: 0.1028370\n",
      "\tspeed: 0.0169s/iter; left time: 120.3873s\n",
      "\titers: 400, epoch: 8 | loss: 0.0694930\n",
      "\tspeed: 0.0169s/iter; left time: 118.6302s\n",
      "\titers: 500, epoch: 8 | loss: 0.0738068\n",
      "\tspeed: 0.0169s/iter; left time: 116.8483s\n",
      "Epoch: 8 cost time: 9.95321798324585\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1070141 Vali Loss: 0.0343289 Test Loss: 0.1180926\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0767808\n",
      "\tspeed: 0.0498s/iter; left time: 336.0337s\n",
      "\titers: 200, epoch: 9 | loss: 0.0842962\n",
      "\tspeed: 0.0169s/iter; left time: 112.0541s\n",
      "\titers: 300, epoch: 9 | loss: 0.0946802\n",
      "\tspeed: 0.0169s/iter; left time: 110.3749s\n",
      "\titers: 400, epoch: 9 | loss: 0.0880783\n",
      "\tspeed: 0.0169s/iter; left time: 108.5552s\n",
      "\titers: 500, epoch: 9 | loss: 0.1501595\n",
      "\tspeed: 0.0169s/iter; left time: 106.9095s\n",
      "Epoch: 9 cost time: 9.906736612319946\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1058018 Vali Loss: 0.0342370 Test Loss: 0.1178512\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1148590\n",
      "\tspeed: 0.0507s/iter; left time: 313.1653s\n",
      "\titers: 200, epoch: 10 | loss: 0.0611055\n",
      "\tspeed: 0.0169s/iter; left time: 102.7823s\n",
      "\titers: 300, epoch: 10 | loss: 0.1586422\n",
      "\tspeed: 0.0169s/iter; left time: 101.0901s\n",
      "\titers: 400, epoch: 10 | loss: 0.0999018\n",
      "\tspeed: 0.0169s/iter; left time: 99.3769s\n",
      "\titers: 500, epoch: 10 | loss: 0.0855434\n",
      "\tspeed: 0.0169s/iter; left time: 97.7803s\n",
      "Epoch: 10 cost time: 9.921478748321533\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1060700 Vali Loss: 0.0345414 Test Loss: 0.1175926\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.1184559091925621, mae:0.21707218885421753\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=512,PL=24\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1555285\n",
      "\tspeed: 0.0301s/iter; left time: 339.9116s\n",
      "\titers: 200, epoch: 1 | loss: 0.1058938\n",
      "\tspeed: 0.0179s/iter; left time: 200.5435s\n",
      "\titers: 300, epoch: 1 | loss: 0.1976846\n",
      "\tspeed: 0.0179s/iter; left time: 198.7343s\n",
      "\titers: 400, epoch: 1 | loss: 0.1717134\n",
      "\tspeed: 0.0179s/iter; left time: 197.4091s\n",
      "\titers: 500, epoch: 1 | loss: 0.1015578\n",
      "\tspeed: 0.0179s/iter; left time: 195.5389s\n",
      "Epoch: 1 cost time: 11.491021156311035\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1783853 Vali Loss: 0.0342684 Test Loss: 0.1115713\n",
      "Validation loss decreased (inf --> 0.034268).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1322298\n",
      "\tspeed: 0.0543s/iter; left time: 582.1629s\n",
      "\titers: 200, epoch: 2 | loss: 0.1116886\n",
      "\tspeed: 0.0179s/iter; left time: 190.5898s\n",
      "\titers: 300, epoch: 2 | loss: 0.2077729\n",
      "\tspeed: 0.0177s/iter; left time: 185.9355s\n",
      "\titers: 400, epoch: 2 | loss: 0.1784001\n",
      "\tspeed: 0.0163s/iter; left time: 170.1066s\n",
      "\titers: 500, epoch: 2 | loss: 0.1166745\n",
      "\tspeed: 0.0163s/iter; left time: 168.4049s\n",
      "Epoch: 2 cost time: 10.06808614730835\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1438919 Vali Loss: 0.0351572 Test Loss: 0.1138219\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1210392\n",
      "\tspeed: 0.0494s/iter; left time: 501.7495s\n",
      "\titers: 200, epoch: 3 | loss: 0.1237201\n",
      "\tspeed: 0.0164s/iter; left time: 165.2665s\n",
      "\titers: 300, epoch: 3 | loss: 0.1078374\n",
      "\tspeed: 0.0164s/iter; left time: 163.6087s\n",
      "\titers: 400, epoch: 3 | loss: 0.1568393\n",
      "\tspeed: 0.0164s/iter; left time: 161.9879s\n",
      "\titers: 500, epoch: 3 | loss: 0.0859967\n",
      "\tspeed: 0.0164s/iter; left time: 160.4602s\n",
      "Epoch: 3 cost time: 9.665194988250732\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1227649 Vali Loss: 0.0325438 Test Loss: 0.1098657\n",
      "Validation loss decreased (0.034268 --> 0.032544).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1230057\n",
      "\tspeed: 0.0524s/iter; left time: 502.3770s\n",
      "\titers: 200, epoch: 4 | loss: 0.1713229\n",
      "\tspeed: 0.0180s/iter; left time: 171.2377s\n",
      "\titers: 300, epoch: 4 | loss: 0.0925769\n",
      "\tspeed: 0.0180s/iter; left time: 169.1057s\n",
      "\titers: 400, epoch: 4 | loss: 0.0831985\n",
      "\tspeed: 0.0180s/iter; left time: 167.3569s\n",
      "\titers: 500, epoch: 4 | loss: 0.1476100\n",
      "\tspeed: 0.0180s/iter; left time: 165.8809s\n",
      "Epoch: 4 cost time: 10.598434209823608\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1103549 Vali Loss: 0.0322846 Test Loss: 0.1100513\n",
      "Validation loss decreased (0.032544 --> 0.032285).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1093887\n",
      "\tspeed: 0.0537s/iter; left time: 484.2527s\n",
      "\titers: 200, epoch: 5 | loss: 0.1200557\n",
      "\tspeed: 0.0180s/iter; left time: 160.3581s\n",
      "\titers: 300, epoch: 5 | loss: 0.1096876\n",
      "\tspeed: 0.0180s/iter; left time: 158.3592s\n",
      "\titers: 400, epoch: 5 | loss: 0.0723158\n",
      "\tspeed: 0.0180s/iter; left time: 156.6069s\n",
      "\titers: 500, epoch: 5 | loss: 0.1374758\n",
      "\tspeed: 0.0180s/iter; left time: 154.7481s\n",
      "Epoch: 5 cost time: 10.53821587562561\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1053676 Vali Loss: 0.0319508 Test Loss: 0.1103467\n",
      "Validation loss decreased (0.032285 --> 0.031951).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0999631\n",
      "\tspeed: 0.0542s/iter; left time: 457.9758s\n",
      "\titers: 200, epoch: 6 | loss: 0.1338409\n",
      "\tspeed: 0.0180s/iter; left time: 150.1585s\n",
      "\titers: 300, epoch: 6 | loss: 0.1689444\n",
      "\tspeed: 0.0180s/iter; left time: 148.4268s\n",
      "\titers: 400, epoch: 6 | loss: 0.1071086\n",
      "\tspeed: 0.0180s/iter; left time: 146.6259s\n",
      "\titers: 500, epoch: 6 | loss: 0.1048287\n",
      "\tspeed: 0.0180s/iter; left time: 144.5680s\n",
      "Epoch: 6 cost time: 10.576935291290283\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1028331 Vali Loss: 0.0320684 Test Loss: 0.1101003\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1261138\n",
      "\tspeed: 0.0549s/iter; left time: 432.6900s\n",
      "\titers: 200, epoch: 7 | loss: 0.0776426\n",
      "\tspeed: 0.0180s/iter; left time: 140.1582s\n",
      "\titers: 300, epoch: 7 | loss: 0.0916797\n",
      "\tspeed: 0.0180s/iter; left time: 138.3997s\n",
      "\titers: 400, epoch: 7 | loss: 0.1496750\n",
      "\tspeed: 0.0168s/iter; left time: 127.4012s\n",
      "\titers: 500, epoch: 7 | loss: 0.0992560\n",
      "\tspeed: 0.0164s/iter; left time: 122.5777s\n",
      "Epoch: 7 cost time: 10.1607186794281\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1012986 Vali Loss: 0.0322577 Test Loss: 0.1099045\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0672307\n",
      "\tspeed: 0.0505s/iter; left time: 369.3203s\n",
      "\titers: 200, epoch: 8 | loss: 0.1146743\n",
      "\tspeed: 0.0179s/iter; left time: 129.0838s\n",
      "\titers: 300, epoch: 8 | loss: 0.1142103\n",
      "\tspeed: 0.0179s/iter; left time: 127.2680s\n",
      "\titers: 400, epoch: 8 | loss: 0.1174164\n",
      "\tspeed: 0.0180s/iter; left time: 126.0899s\n",
      "\titers: 500, epoch: 8 | loss: 0.0964760\n",
      "\tspeed: 0.0180s/iter; left time: 124.4794s\n",
      "Epoch: 8 cost time: 10.541171789169312\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.0999613 Vali Loss: 0.0321173 Test Loss: 0.1101281\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11049826443195343, mae:0.2042028158903122\n",
      ">>> LR=1e-4,DO=0.1,EL=3,DM=512,PL=48\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            512                 \n",
      "  n heads:            8                   e layers:           3                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.2063830\n",
      "\tspeed: 0.0291s/iter; left time: 329.3293s\n",
      "\titers: 200, epoch: 1 | loss: 0.1870340\n",
      "\tspeed: 0.0167s/iter; left time: 187.3994s\n",
      "\titers: 300, epoch: 1 | loss: 0.1797046\n",
      "\tspeed: 0.0154s/iter; left time: 170.8527s\n",
      "\titers: 400, epoch: 1 | loss: 0.1959289\n",
      "\tspeed: 0.0154s/iter; left time: 169.1877s\n",
      "\titers: 500, epoch: 1 | loss: 0.3154422\n",
      "\tspeed: 0.0154s/iter; left time: 168.0261s\n",
      "Epoch: 1 cost time: 10.338882446289062\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2134551 Vali Loss: 0.0435308 Test Loss: 0.1349582\n",
      "Validation loss decreased (inf --> 0.043531).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2284414\n",
      "\tspeed: 0.0485s/iter; left time: 520.7385s\n",
      "\titers: 200, epoch: 2 | loss: 0.1208357\n",
      "\tspeed: 0.0153s/iter; left time: 162.9941s\n",
      "\titers: 300, epoch: 2 | loss: 0.1812132\n",
      "\tspeed: 0.0153s/iter; left time: 161.2311s\n",
      "\titers: 400, epoch: 2 | loss: 0.1583438\n",
      "\tspeed: 0.0153s/iter; left time: 159.9648s\n",
      "\titers: 500, epoch: 2 | loss: 0.2020065\n",
      "\tspeed: 0.0153s/iter; left time: 158.4763s\n",
      "Epoch: 2 cost time: 9.057844161987305\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1656884 Vali Loss: 0.0411948 Test Loss: 0.1304179\n",
      "Validation loss decreased (0.043531 --> 0.041195).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1608097\n",
      "\tspeed: 0.0503s/iter; left time: 510.7273s\n",
      "\titers: 200, epoch: 3 | loss: 0.0874227\n",
      "\tspeed: 0.0154s/iter; left time: 155.0900s\n",
      "\titers: 300, epoch: 3 | loss: 0.1694377\n",
      "\tspeed: 0.0154s/iter; left time: 153.4541s\n",
      "\titers: 400, epoch: 3 | loss: 0.1177407\n",
      "\tspeed: 0.0154s/iter; left time: 151.7225s\n",
      "\titers: 500, epoch: 3 | loss: 0.1368724\n",
      "\tspeed: 0.0154s/iter; left time: 150.1399s\n",
      "Epoch: 3 cost time: 9.103183269500732\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1359576 Vali Loss: 0.0376442 Test Loss: 0.1299606\n",
      "Validation loss decreased (0.041195 --> 0.037644).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0844448\n",
      "\tspeed: 0.0497s/iter; left time: 477.0565s\n",
      "\titers: 200, epoch: 4 | loss: 0.1618553\n",
      "\tspeed: 0.0154s/iter; left time: 146.3004s\n",
      "\titers: 300, epoch: 4 | loss: 0.1517792\n",
      "\tspeed: 0.0154s/iter; left time: 144.5426s\n",
      "\titers: 400, epoch: 4 | loss: 0.1055960\n",
      "\tspeed: 0.0154s/iter; left time: 143.1420s\n",
      "\titers: 500, epoch: 4 | loss: 0.1550536\n",
      "\tspeed: 0.0154s/iter; left time: 141.9377s\n",
      "Epoch: 4 cost time: 9.098902702331543\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1223496 Vali Loss: 0.0364399 Test Loss: 0.1311539\n",
      "Validation loss decreased (0.037644 --> 0.036440).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1185952\n",
      "\tspeed: 0.0503s/iter; left time: 454.0731s\n",
      "\titers: 200, epoch: 5 | loss: 0.0921470\n",
      "\tspeed: 0.0154s/iter; left time: 137.7245s\n",
      "\titers: 300, epoch: 5 | loss: 0.0994880\n",
      "\tspeed: 0.0154s/iter; left time: 135.9234s\n",
      "\titers: 400, epoch: 5 | loss: 0.0897295\n",
      "\tspeed: 0.0155s/iter; left time: 134.8051s\n",
      "\titers: 500, epoch: 5 | loss: 0.0979272\n",
      "\tspeed: 0.0154s/iter; left time: 132.7243s\n",
      "Epoch: 5 cost time: 9.12540054321289\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1141438 Vali Loss: 0.0361733 Test Loss: 0.1294257\n",
      "Validation loss decreased (0.036440 --> 0.036173).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0919634\n",
      "\tspeed: 0.0491s/iter; left time: 414.8060s\n",
      "\titers: 200, epoch: 6 | loss: 0.0713519\n",
      "\tspeed: 0.0160s/iter; left time: 133.9540s\n",
      "\titers: 300, epoch: 6 | loss: 0.0640771\n",
      "\tspeed: 0.0170s/iter; left time: 140.4026s\n",
      "\titers: 400, epoch: 6 | loss: 0.0877847\n",
      "\tspeed: 0.0170s/iter; left time: 138.7133s\n",
      "\titers: 500, epoch: 6 | loss: 0.0983191\n",
      "\tspeed: 0.0170s/iter; left time: 136.7859s\n",
      "Epoch: 6 cost time: 9.811263799667358\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1107003 Vali Loss: 0.0353952 Test Loss: 0.1319505\n",
      "Validation loss decreased (0.036173 --> 0.035395).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1243195\n",
      "\tspeed: 0.0499s/iter; left time: 392.8872s\n",
      "\titers: 200, epoch: 7 | loss: 0.0919826\n",
      "\tspeed: 0.0154s/iter; left time: 119.7491s\n",
      "\titers: 300, epoch: 7 | loss: 0.1036018\n",
      "\tspeed: 0.0154s/iter; left time: 118.3657s\n",
      "\titers: 400, epoch: 7 | loss: 0.1400468\n",
      "\tspeed: 0.0154s/iter; left time: 116.7107s\n",
      "\titers: 500, epoch: 7 | loss: 0.1277194\n",
      "\tspeed: 0.0154s/iter; left time: 114.9853s\n",
      "Epoch: 7 cost time: 9.076128244400024\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1090898 Vali Loss: 0.0352193 Test Loss: 0.1302587\n",
      "Validation loss decreased (0.035395 --> 0.035219).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0812892\n",
      "\tspeed: 0.0488s/iter; left time: 356.5319s\n",
      "\titers: 200, epoch: 8 | loss: 0.1107979\n",
      "\tspeed: 0.0159s/iter; left time: 114.4833s\n",
      "\titers: 300, epoch: 8 | loss: 0.0703831\n",
      "\tspeed: 0.0176s/iter; left time: 125.4983s\n",
      "\titers: 400, epoch: 8 | loss: 0.1055723\n",
      "\tspeed: 0.0176s/iter; left time: 123.5370s\n",
      "\titers: 500, epoch: 8 | loss: 0.0826987\n",
      "\tspeed: 0.0176s/iter; left time: 121.7786s\n",
      "Epoch: 8 cost time: 9.951010465621948\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1076544 Vali Loss: 0.0354851 Test Loss: 0.1311845\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0923762\n",
      "\tspeed: 0.0511s/iter; left time: 344.1977s\n",
      "\titers: 200, epoch: 9 | loss: 0.0993528\n",
      "\tspeed: 0.0159s/iter; left time: 105.8591s\n",
      "\titers: 300, epoch: 9 | loss: 0.1253577\n",
      "\tspeed: 0.0153s/iter; left time: 100.2726s\n",
      "\titers: 400, epoch: 9 | loss: 0.0924837\n",
      "\tspeed: 0.0154s/iter; left time: 98.9087s\n",
      "\titers: 500, epoch: 9 | loss: 0.1125001\n",
      "\tspeed: 0.0153s/iter; left time: 97.2605s\n",
      "Epoch: 9 cost time: 9.26792573928833\n",
      "Epoch: 9, Steps: 570 | Train Loss: 0.1072153 Vali Loss: 0.0354216 Test Loss: 0.1312207\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.1162508\n",
      "\tspeed: 0.0473s/iter; left time: 291.8640s\n",
      "\titers: 200, epoch: 10 | loss: 0.1318796\n",
      "\tspeed: 0.0154s/iter; left time: 93.2581s\n",
      "\titers: 300, epoch: 10 | loss: 0.0907145\n",
      "\tspeed: 0.0154s/iter; left time: 91.9004s\n",
      "\titers: 400, epoch: 10 | loss: 0.1265460\n",
      "\tspeed: 0.0154s/iter; left time: 90.3571s\n",
      "\titers: 500, epoch: 10 | loss: 0.1289427\n",
      "\tspeed: 0.0154s/iter; left time: 88.6621s\n",
      "Epoch: 10 cost time: 9.036757469177246\n",
      "Epoch: 10, Steps: 570 | Train Loss: 0.1082149 Vali Loss: 0.0353202 Test Loss: 0.1312223\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm512_nh8_el3_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.13045193254947662, mae:0.23218737542629242\n",
      ">>> LR=1e-4,DO=0.1,EL=4,DM=256,PL=16\n",
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data              \n",
      "  Data Path:          causal_data.csv     Features:           MS                  \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             4                   Dec In:             4                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           4                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftMS_sl168_ll48_pl24_dm256_nh8_el4_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1391820\n",
      "\tspeed: 0.0304s/iter; left time: 344.0274s\n",
      "\titers: 200, epoch: 1 | loss: 0.2029080\n",
      "\tspeed: 0.0186s/iter; left time: 208.8357s\n",
      "\titers: 300, epoch: 1 | loss: 0.3258815\n",
      "\tspeed: 0.0186s/iter; left time: 206.9559s\n",
      "\titers: 400, epoch: 1 | loss: 0.1845980\n",
      "\tspeed: 0.0187s/iter; left time: 205.2170s\n",
      "\titers: 500, epoch: 1 | loss: 0.1627738\n",
      "\tspeed: 0.0186s/iter; left time: 203.1106s\n",
      "Epoch: 1 cost time: 11.852343320846558\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.2102097 Vali Loss: 0.0448969 Test Loss: 0.1337774\n",
      "Validation loss decreased (inf --> 0.044897).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2993554\n",
      "\tspeed: 0.0566s/iter; left time: 606.9139s\n",
      "\titers: 200, epoch: 2 | loss: 0.1644480\n",
      "\tspeed: 0.0185s/iter; left time: 196.6065s\n",
      "\titers: 300, epoch: 2 | loss: 0.1396863\n",
      "\tspeed: 0.0185s/iter; left time: 194.5998s\n",
      "\titers: 400, epoch: 2 | loss: 0.1331915\n",
      "\tspeed: 0.0185s/iter; left time: 192.8035s\n",
      "\titers: 500, epoch: 2 | loss: 0.1542030\n",
      "\tspeed: 0.0185s/iter; left time: 191.1217s\n",
      "Epoch: 2 cost time: 10.91523814201355\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1688071 Vali Loss: 0.0415520 Test Loss: 0.1278588\n",
      "Validation loss decreased (0.044897 --> 0.041552).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1257951\n",
      "\tspeed: 0.0558s/iter; left time: 566.6252s\n",
      "\titers: 200, epoch: 3 | loss: 0.1143206\n",
      "\tspeed: 0.0184s/iter; left time: 185.0097s\n",
      "\titers: 300, epoch: 3 | loss: 0.1279311\n",
      "\tspeed: 0.0184s/iter; left time: 183.2518s\n",
      "\titers: 400, epoch: 3 | loss: 0.1258806\n",
      "\tspeed: 0.0185s/iter; left time: 182.2606s\n",
      "\titers: 500, epoch: 3 | loss: 0.0975429\n",
      "\tspeed: 0.0185s/iter; left time: 180.2230s\n",
      "Epoch: 3 cost time: 10.785451650619507\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1449889 Vali Loss: 0.0376083 Test Loss: 0.1195712\n",
      "Validation loss decreased (0.041552 --> 0.037608).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1677614\n",
      "\tspeed: 0.0580s/iter; left time: 555.9753s\n",
      "\titers: 200, epoch: 4 | loss: 0.1814447\n",
      "\tspeed: 0.0208s/iter; left time: 197.0320s\n",
      "\titers: 300, epoch: 4 | loss: 0.1515219\n",
      "\tspeed: 0.0207s/iter; left time: 194.3716s\n",
      "\titers: 400, epoch: 4 | loss: 0.1079124\n",
      "\tspeed: 0.0207s/iter; left time: 192.2521s\n",
      "\titers: 500, epoch: 4 | loss: 0.1271523\n",
      "\tspeed: 0.0188s/iter; left time: 172.5073s\n",
      "Epoch: 4 cost time: 11.750059843063354\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.1339066 Vali Loss: 0.0367072 Test Loss: 0.1212885\n",
      "Validation loss decreased (0.037608 --> 0.036707).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1465802\n",
      "\tspeed: 0.0581s/iter; left time: 524.2315s\n",
      "\titers: 200, epoch: 5 | loss: 0.1292708\n",
      "\tspeed: 0.0186s/iter; left time: 165.9253s\n",
      "\titers: 300, epoch: 5 | loss: 0.1207413\n",
      "\tspeed: 0.0185s/iter; left time: 163.4870s\n",
      "\titers: 400, epoch: 5 | loss: 0.1002995\n",
      "\tspeed: 0.0185s/iter; left time: 161.6094s\n",
      "\titers: 500, epoch: 5 | loss: 0.1350384\n",
      "\tspeed: 0.0185s/iter; left time: 159.5795s\n",
      "Epoch: 5 cost time: 10.874135732650757\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.1279273 Vali Loss: 0.0351292 Test Loss: 0.1215083\n",
      "Validation loss decreased (0.036707 --> 0.035129).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0720300\n",
      "\tspeed: 0.0567s/iter; left time: 479.3493s\n",
      "\titers: 200, epoch: 6 | loss: 0.1259978\n",
      "\tspeed: 0.0185s/iter; left time: 154.5039s\n",
      "\titers: 300, epoch: 6 | loss: 0.0900185\n",
      "\tspeed: 0.0185s/iter; left time: 152.3015s\n",
      "\titers: 400, epoch: 6 | loss: 0.1716787\n",
      "\tspeed: 0.0185s/iter; left time: 150.6424s\n",
      "\titers: 500, epoch: 6 | loss: 0.1148341\n",
      "\tspeed: 0.0185s/iter; left time: 148.6274s\n",
      "Epoch: 6 cost time: 10.821367025375366\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.1261035 Vali Loss: 0.0348515 Test Loss: 0.1216410\n",
      "Validation loss decreased (0.035129 --> 0.034851).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0961893\n",
      "\tspeed: 0.0574s/iter; left time: 452.6661s\n",
      "\titers: 200, epoch: 7 | loss: 0.1358449\n",
      "\tspeed: 0.0185s/iter; left time: 144.0250s\n",
      "\titers: 300, epoch: 7 | loss: 0.1310754\n",
      "\tspeed: 0.0184s/iter; left time: 141.6715s\n",
      "\titers: 400, epoch: 7 | loss: 0.1342554\n",
      "\tspeed: 0.0184s/iter; left time: 139.7000s\n",
      "\titers: 500, epoch: 7 | loss: 0.1742030\n",
      "\tspeed: 0.0185s/iter; left time: 138.2850s\n",
      "Epoch: 7 cost time: 10.831374168395996\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.1236303 Vali Loss: 0.0349391 Test Loss: 0.1203975\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1173634\n",
      "\tspeed: 0.0554s/iter; left time: 404.8872s\n",
      "\titers: 200, epoch: 8 | loss: 0.0968303\n",
      "\tspeed: 0.0186s/iter; left time: 134.1800s\n",
      "\titers: 300, epoch: 8 | loss: 0.0950346\n",
      "\tspeed: 0.0186s/iter; left time: 131.9296s\n",
      "\titers: 400, epoch: 8 | loss: 0.1503787\n",
      "\tspeed: 0.0186s/iter; left time: 130.1721s\n",
      "\titers: 500, epoch: 8 | loss: 0.0932398\n",
      "\tspeed: 0.0186s/iter; left time: 128.5278s\n",
      "Epoch: 8 cost time: 10.870176076889038\n",
      "Epoch: 8, Steps: 570 | Train Loss: 0.1230835 Vali Loss: 0.0348350 Test Loss: 0.1202207\n",
      "Validation loss decreased (0.034851 --> 0.034835).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0810930\n",
      "\tspeed: 0.0560s/iter; left time: 377.3352s\n",
      "\titers: 200, epoch: 9 | loss: 0.1697227\n",
      "\tspeed: 0.0184s/iter; left time: 122.4840s\n",
      "^C\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4e80696dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/connection.py\", line 935, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib64/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 200, in <module>\n",
      "    exp.train(setting)\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 135, in train\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 222, in forward\n",
      "    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "  File \"/home/RDC/inceemir/power/models/TimeXer.py\", line 181, in forecast\n",
      "    dec_out = dec_out * (stdev[:, 0, -1:].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/tune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b29239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           EPF_168_24          Model:              TimeXer             \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               custom              Root Path:          ./data/             \n",
      "  Data Path:          causal_data.csv     Features:           S                   \n",
      "  Target:             electricity_price   Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "  Predictor(s):       solar_forecast,wind_forecast,total_load\n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            168                 Label Len:          48                  \n",
      "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             1                   Dec In:             1                   \n",
      "  C Out:              1                   d model:            256                 \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               2048                \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.0                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        10                  Itr:                1                   \n",
      "  Train Epochs:       20                  Batch Size:         32                  \n",
      "  Patience:           3                   Learning Rate:      0.001               \n",
      "  Des:                tuning              Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "train 18219\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "val 2608\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "\titers: 100, epoch: 1 | loss: 0.1520236\n",
      "\tspeed: 0.0217s/iter; left time: 244.9773s\n",
      "\titers: 200, epoch: 1 | loss: 0.1548833\n",
      "\tspeed: 0.0097s/iter; left time: 108.8597s\n",
      "\titers: 300, epoch: 1 | loss: 0.2273367\n",
      "\tspeed: 0.0097s/iter; left time: 108.1119s\n",
      "\titers: 400, epoch: 1 | loss: 0.1600210\n",
      "\tspeed: 0.0097s/iter; left time: 106.4666s\n",
      "\titers: 500, epoch: 1 | loss: 0.1187159\n",
      "\tspeed: 0.0097s/iter; left time: 105.8723s\n",
      "Epoch: 1 cost time: 6.775237083435059\n",
      "Epoch: 1, Steps: 570 | Train Loss: 0.1564833 Vali Loss: 0.0359156 Test Loss: 0.1361597\n",
      "Validation loss decreased (inf --> 0.035916).  Saving model ...\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 0.1301072\n",
      "\tspeed: 0.0348s/iter; left time: 372.9193s\n",
      "\titers: 200, epoch: 2 | loss: 0.1636493\n",
      "\tspeed: 0.0097s/iter; left time: 102.6531s\n",
      "\titers: 300, epoch: 2 | loss: 0.0818086\n",
      "\tspeed: 0.0096s/iter; left time: 101.2879s\n",
      "\titers: 400, epoch: 2 | loss: 0.0922860\n",
      "\tspeed: 0.0097s/iter; left time: 100.9665s\n",
      "\titers: 500, epoch: 2 | loss: 0.1375351\n",
      "\tspeed: 0.0097s/iter; left time: 100.0757s\n",
      "Epoch: 2 cost time: 5.786832571029663\n",
      "Epoch: 2, Steps: 570 | Train Loss: 0.1320662 Vali Loss: 0.0341317 Test Loss: 0.1265445\n",
      "Validation loss decreased (0.035916 --> 0.034132).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 3 | loss: 0.0666047\n",
      "\tspeed: 0.0348s/iter; left time: 354.0658s\n",
      "\titers: 200, epoch: 3 | loss: 0.1220048\n",
      "\tspeed: 0.0096s/iter; left time: 96.7110s\n",
      "\titers: 300, epoch: 3 | loss: 0.2379576\n",
      "\tspeed: 0.0097s/iter; left time: 96.3995s\n",
      "\titers: 400, epoch: 3 | loss: 0.0842706\n",
      "\tspeed: 0.0097s/iter; left time: 95.3143s\n",
      "\titers: 500, epoch: 3 | loss: 0.1083356\n",
      "\tspeed: 0.0096s/iter; left time: 93.8907s\n",
      "Epoch: 3 cost time: 5.784637451171875\n",
      "Epoch: 3, Steps: 570 | Train Loss: 0.1100980 Vali Loss: 0.0336505 Test Loss: 0.1192551\n",
      "Validation loss decreased (0.034132 --> 0.033651).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 4 | loss: 0.0549970\n",
      "\tspeed: 0.0345s/iter; left time: 331.3342s\n",
      "\titers: 200, epoch: 4 | loss: 0.0732935\n",
      "\tspeed: 0.0097s/iter; left time: 92.3929s\n",
      "\titers: 300, epoch: 4 | loss: 0.0679300\n",
      "\tspeed: 0.0097s/iter; left time: 91.3315s\n",
      "\titers: 400, epoch: 4 | loss: 0.0597613\n",
      "\tspeed: 0.0097s/iter; left time: 90.2642s\n",
      "\titers: 500, epoch: 4 | loss: 0.0913851\n",
      "\tspeed: 0.0097s/iter; left time: 89.4023s\n",
      "Epoch: 4 cost time: 5.834489822387695\n",
      "Epoch: 4, Steps: 570 | Train Loss: 0.0884700 Vali Loss: 0.0320455 Test Loss: 0.1141491\n",
      "Validation loss decreased (0.033651 --> 0.032045).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 5 | loss: 0.0925790\n",
      "\tspeed: 0.0344s/iter; left time: 310.4071s\n",
      "\titers: 200, epoch: 5 | loss: 0.1078071\n",
      "\tspeed: 0.0108s/iter; left time: 96.6592s\n",
      "\titers: 300, epoch: 5 | loss: 0.0739578\n",
      "\tspeed: 0.0109s/iter; left time: 95.8111s\n",
      "\titers: 400, epoch: 5 | loss: 0.0925858\n",
      "\tspeed: 0.0109s/iter; left time: 94.7201s\n",
      "\titers: 500, epoch: 5 | loss: 0.0755817\n",
      "\tspeed: 0.0109s/iter; left time: 93.7523s\n",
      "Epoch: 5 cost time: 6.359644174575806\n",
      "Epoch: 5, Steps: 570 | Train Loss: 0.0719178 Vali Loss: 0.0340264 Test Loss: 0.1210886\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0458987\n",
      "\tspeed: 0.0371s/iter; left time: 313.4474s\n",
      "\titers: 200, epoch: 6 | loss: 0.0881008\n",
      "\tspeed: 0.0109s/iter; left time: 91.0794s\n",
      "\titers: 300, epoch: 6 | loss: 0.0822319\n",
      "\tspeed: 0.0108s/iter; left time: 89.1511s\n",
      "\titers: 400, epoch: 6 | loss: 0.0473277\n",
      "\tspeed: 0.0097s/iter; left time: 78.9059s\n",
      "\titers: 500, epoch: 6 | loss: 0.0464111\n",
      "\tspeed: 0.0097s/iter; left time: 78.0683s\n",
      "Epoch: 6 cost time: 6.1555540561676025\n",
      "Epoch: 6, Steps: 570 | Train Loss: 0.0601371 Vali Loss: 0.0354510 Test Loss: 0.1255366\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0432866\n",
      "\tspeed: 0.0341s/iter; left time: 268.6332s\n",
      "\titers: 200, epoch: 7 | loss: 0.0522508\n",
      "\tspeed: 0.0096s/iter; left time: 74.9360s\n",
      "\titers: 300, epoch: 7 | loss: 0.0912942\n",
      "\tspeed: 0.0096s/iter; left time: 74.0907s\n",
      "\titers: 400, epoch: 7 | loss: 0.1045592\n",
      "\tspeed: 0.0096s/iter; left time: 73.0918s\n",
      "\titers: 500, epoch: 7 | loss: 0.0638355\n",
      "\tspeed: 0.0097s/iter; left time: 72.2391s\n",
      "Epoch: 7 cost time: 5.80290961265564\n",
      "Epoch: 7, Steps: 570 | Train Loss: 0.0534178 Vali Loss: 0.0350934 Test Loss: 0.1249684\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : long_term_forecast_EPF_168_24_TimeXer_custom_ftS_sl168_ll48_pl24_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_tuning_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "[DEBUG] kept columns: ['solar_forecast', 'total_load', 'wind_forecast']\n",
      "test 5237\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "test shape: (5237, 24, 1) (5237, 24, 1)\n",
      "mse:0.11430096626281738, mae:0.20448043942451477\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeXer.sh \\\n",
    "  --is_training 1 \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --model_id  EPF_168_24 \\\n",
    "  --model     TimeXer \\\n",
    "  --data      custom \\\n",
    "  --root_path ./data/ \\\n",
    "  --data_path causal_data.csv \\\n",
    "  --features  S \\\n",
    "  --predictor solar_forecast,wind_forecast,total_load \\\n",
    "  --seq_len   168 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len  24 \\\n",
    "  --enc_in    1 \\\n",
    "  --dec_in    1 \\\n",
    "  --c_out     1 \\\n",
    "  --learning_rate 1e-3 \\\n",
    "  --dropout       0.0 \\\n",
    "  --e_layers      2 \\\n",
    "  --d_model       256 \\\n",
    "  --n_heads       8 \\\n",
    "  --d_layers      1 \\\n",
    "  --d_ff          2048 \\\n",
    "  --expand        2 \\\n",
    "  --d_conv        4 \\\n",
    "  --factor        1 \\\n",
    "  --embed         timeF \\\n",
    "  --des           tuning \\\n",
    "  --patch_len     24 \\\n",
    "  --batch_size    32 \\\n",
    "  --train_epochs  20 \\\n",
    "  \"$@\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
