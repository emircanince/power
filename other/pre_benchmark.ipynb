{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b04bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 4, in <module>\n",
      "    from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 1, in <module>\n",
      "    from data_provider.data_factory import data_provider\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_factory.py\", line 3, in <module>\n",
      "    from data_provider.data_loader import Dataset_Custom\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_loader.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/__init__.py\", line 22, in <module>\n",
      "    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/compat/__init__.py\", line 18, in <module>\n",
      "    from pandas.compat.numpy import (\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/compat/numpy/__init__.py\", line 4, in <module>\n",
      "    from pandas.util.version import Version\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/util/__init__.py\", line 2, in <module>\n",
      "    from pandas.util._decorators import (  # noqa:F401\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/util/_decorators.py\", line 14, in <module>\n",
      "    from pandas._libs.properties import cache_readonly\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/_libs/__init__.py\", line 13, in <module>\n",
      "    from pandas._libs.interval import Interval\n",
      "  File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/LSTM.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf5261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  |  val MSE = 0.6986\n",
      "Epoch 02  |  val MSE = 0.3234\n",
      "Epoch 03  |  val MSE = 0.2495\n",
      "Epoch 04  |  val MSE = 0.3970\n",
      "Epoch 05  |  val MSE = 0.4434\n",
      "Epoch 06  |  val MSE = 0.3555\n",
      "Early stop triggered.\n",
      "\n",
      "====  LSTM baseline  ====\n",
      "MAE = 37.913  |  MSE = 3691.247\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- imports ----------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ------------------------ reproducibility ------------------------\n",
    "torch.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "\n",
    "# ---------------------- 1. load the data -------------------------\n",
    "CSV_PATH = \"./data/causal_data.csv\"        # adjust if needed\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"date\"])\n",
    "\n",
    "# keep just the columns we need\n",
    "cols_x = [\"solar_forecast\", \"wind_forecast\", \"total_load\"]\n",
    "col_y  = \"electricity_price\"\n",
    "df = df[[\"date\"] + cols_x + [col_y]].dropna()\n",
    "\n",
    "# ------------------ 2. chronological splits ----------------------\n",
    "train_end = \"2023-12-31 23:00:00\"\n",
    "val_end   = \"2024-06-30 23:00:00\"\n",
    "\n",
    "train_df = df[df[\"date\"] <= train_end]\n",
    "val_df   = df[(df[\"date\"] > train_end) & (df[\"date\"] <= val_end)]\n",
    "test_df  = df[df[\"date\"] > val_end]\n",
    "\n",
    "# ------------------ 3. standardise each split --------------------\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "def scale_xy(split_df, fit=False):\n",
    "    x = split_df[cols_x].values\n",
    "    y = split_df[[col_y]].values            # 2-D for sklearn\n",
    "    if fit:\n",
    "        x = scaler_x.fit_transform(x)\n",
    "        y = scaler_y.fit_transform(y)\n",
    "    else:\n",
    "        x = scaler_x.transform(x)\n",
    "        y = scaler_y.transform(y)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "x_train, y_train = scale_xy(train_df, fit=True)\n",
    "x_val,   y_val   = scale_xy(val_df)\n",
    "x_test,  y_test  = scale_xy(test_df)\n",
    "\n",
    "# --------------- 4. make (X,Y) sequences for LSTM ---------------\n",
    "SEQ_LEN  = 168     # look-back (hours)\n",
    "HORIZON  = 24      # predict next 24 hours\n",
    "\n",
    "def make_sequences(x, y):\n",
    "    X_seq, Y_seq = [], []\n",
    "    for i in range(len(x) - SEQ_LEN - HORIZON + 1):\n",
    "        X_seq.append(x[i : i + SEQ_LEN])\n",
    "        Y_seq.append(y[i + SEQ_LEN : i + SEQ_LEN + HORIZON].T)  # -> shape (1,24)\n",
    "    X_seq = np.stack(X_seq)                    # [N, 168, 3]\n",
    "    Y_seq = np.concatenate(Y_seq, axis=0)      # [N, 24]\n",
    "    return torch.tensor(X_seq), torch.tensor(Y_seq)\n",
    "\n",
    "Xtr, Ytr = make_sequences(x_train, y_train)\n",
    "Xvl, Yvl = make_sequences(x_val,   y_val)\n",
    "Xts, Yts = make_sequences(x_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr, Ytr), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xvl, Yvl), batch_size=32)\n",
    "\n",
    "# --------------------- 5. LSTM baseline --------------------------\n",
    "class PriceLSTM(nn.Module):\n",
    "    def __init__(self, n_in: int, hidden: int = 64, n_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_in,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.proj = nn.Linear(hidden, HORIZON)   # 24-step head\n",
    "\n",
    "    def forward(self, x):             # x : [B, 168, 3]\n",
    "        _, (h_n, _) = self.lstm(x)    # use last hidden state\n",
    "        out = self.proj(h_n[-1])      # [B, 24]\n",
    "        return out\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = PriceLSTM(n_in=len(cols_x)).to(device)\n",
    "\n",
    "# optimise MSE, early-stop on val MSE\n",
    "criterion = nn.MSELoss()\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "PATIENCE  = 3\n",
    "best_val  = np.inf\n",
    "pat_cnt   = 0\n",
    "\n",
    "# -------------------- 6. training loop ---------------------------\n",
    "for epoch in range(1, 51):   # hard cap 50 epochs\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # ----- validation -----\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_losses.append(criterion(model(xb), yb).item())\n",
    "    val_mse = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch:02d}  |  val MSE = {val_mse:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_mse < best_val - 1e-4:\n",
    "        best_val = val_mse\n",
    "        pat_cnt  = 0\n",
    "        best_state = model.state_dict()\n",
    "    else:\n",
    "        pat_cnt += 1\n",
    "        if pat_cnt >= PATIENCE:\n",
    "            print(\"Early stop triggered.\")\n",
    "            break\n",
    "\n",
    "# reload best weights\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# ---------------- 7. evaluate on *test* set ----------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_std = model(Xts.to(device)).cpu().numpy()   # std-scale\n",
    "y_true_std = Yts.numpy()\n",
    "\n",
    "# inverse scaling\n",
    "yhat = scaler_y.inverse_transform(yhat_std)\n",
    "ytrue = scaler_y.inverse_transform(y_true_std)\n",
    "\n",
    "mae = mean_absolute_error(ytrue, yhat)\n",
    "mse = mean_squared_error(ytrue, yhat)\n",
    "\n",
    "print(f\"\\n====  LSTM baseline  ====\")\n",
    "print(f\"MAE = {mae:.3f}  |  MSE = {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a08305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/inceemir/power/run.py\", line 4, in <module>\n",
      "    from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
      "  File \"/home/RDC/inceemir/power/exp/exp_long_term_forecasting.py\", line 1, in <module>\n",
      "    from data_provider.data_factory import data_provider\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_factory.py\", line 3, in <module>\n",
      "    from data_provider.data_loader import Dataset_Custom\n",
      "  File \"/home/RDC/inceemir/power/data_provider/data_loader.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/__init__.py\", line 22, in <module>\n",
      "    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/compat/__init__.py\", line 18, in <module>\n",
      "    from pandas.compat.numpy import (\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/compat/numpy/__init__.py\", line 4, in <module>\n",
      "    from pandas.util.version import Version\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/util/__init__.py\", line 2, in <module>\n",
      "    from pandas.util._decorators import (  # noqa:F401\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/util/_decorators.py\", line 14, in <module>\n",
      "    from pandas._libs.properties import cache_readonly\n",
      "  File \"/home/RDC/inceemir/power/.venv/lib64/python3.9/site-packages/pandas/_libs/__init__.py\", line 13, in <module>\n",
      "    from pandas._libs.interval import Interval\n",
      "  File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "scripts/LSTM.sh: Zeile 26: --lstm_hidden: Kommando nicht gefunden.\n",
      "scripts/LSTM.sh: Zeile 27: --lstm_num_layers: Kommando nicht gefunden.\n",
      "scripts/LSTM.sh: Zeile 28: --lstm_bidirectional: Kommando nicht gefunden.\n",
      "scripts/LSTM.sh: Zeile 31: --enc_in: Kommando nicht gefunden.\n",
      "scripts/LSTM.sh: Zeile 32: --dec_in: Kommando nicht gefunden.\n",
      "scripts/LSTM.sh: Zeile 33: --c_out: Kommando nicht gefunden.\n",
      "scripts/LSTM.sh: Zeile 35: --des: Kommando nicht gefunden.\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/LSTM.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "235c2375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | val MSE = 0.5682\n",
      "Epoch 002 | val MSE = 0.6094\n",
      "Epoch 003 | val MSE = 0.6481\n",
      "Epoch 004 | val MSE = 0.8603\n",
      "Epoch 005 | val MSE = 0.7203\n",
      "Epoch 006 | val MSE = 0.5282\n",
      "Epoch 007 | val MSE = 0.5844\n",
      "Epoch 008 | val MSE = 0.1443\n",
      "Epoch 009 | val MSE = 0.3995\n",
      "Epoch 010 | val MSE = 0.4903\n",
      "Epoch 011 | val MSE = 0.4469\n",
      "Epoch 012 | val MSE = 0.3456\n",
      "Epoch 013 | val MSE = 0.3528\n",
      "Early stop.\n",
      "\n",
      "=====  LSTM baseline (standardised scale)  =====\n",
      "MAE = 0.2951   |   MSE = 0.2468\n",
      "\n",
      "(inverse-transformed – for intuition only)\n",
      "MAE = 37.62 €   |   MSE = 4010.33  €²\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- imports ----------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ------------------------ reproducibility ------------------------\n",
    "torch.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "\n",
    "# ---------------------- 1. load the data -------------------------\n",
    "CSV_PATH = \"./data/causal_data.csv\"        # adjust if needed\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"date\"])\n",
    "\n",
    "cols_x = [\"solar_forecast\", \"wind_forecast\", \"total_load\"]\n",
    "col_y  = \"electricity_price\"\n",
    "df     = df[[\"date\"] + cols_x + [col_y]].dropna()\n",
    "\n",
    "# ------------------ 2. chronological splits ----------------------\n",
    "train_end = \"2023-12-31 23:00:00\"\n",
    "val_end   = \"2024-06-30 23:00:00\"\n",
    "\n",
    "train_df = df[df[\"date\"] <= train_end]\n",
    "val_df   = df[(df[\"date\"] > train_end) & (df[\"date\"] <= val_end)]\n",
    "test_df  = df[df[\"date\"] > val_end]\n",
    "\n",
    "# ------------------ 3. standardise each split --------------------\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "def scale_xy(split_df, fit=False):\n",
    "    x = split_df[cols_x].values\n",
    "    y = split_df[[col_y]].values            # keep 2-D shape\n",
    "    if fit:\n",
    "        x = scaler_x.fit_transform(x)\n",
    "        y = scaler_y.fit_transform(y)\n",
    "    else:\n",
    "        x = scaler_x.transform(x)\n",
    "        y = scaler_y.transform(y)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "x_train, y_train = scale_xy(train_df, fit=True)\n",
    "x_val,   y_val   = scale_xy(val_df)\n",
    "x_test,  y_test  = scale_xy(test_df)\n",
    "\n",
    "# --------------- 4. make (X,Y) sequences for LSTM ---------------\n",
    "SEQ_LEN  = 168     # look-back (hours)\n",
    "HORIZON  = 24      # forecast window\n",
    "\n",
    "def make_sequences(x, y):\n",
    "    X_seq, Y_seq = [], []\n",
    "    for i in range(len(x) - SEQ_LEN - HORIZON + 1):\n",
    "        X_seq.append(x[i : i + SEQ_LEN])\n",
    "        # collect the *vector* of next-24 values of y\n",
    "        Y_seq.append(y[i + SEQ_LEN : i + SEQ_LEN + HORIZON].T)  # shape (1,24)\n",
    "    return torch.tensor(np.stack(X_seq)), torch.tensor(np.concatenate(Y_seq))\n",
    "\n",
    "Xtr, Ytr = make_sequences(x_train, y_train)\n",
    "Xvl, Yvl = make_sequences(x_val,   y_val)\n",
    "Xts, Yts = make_sequences(x_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr, Ytr), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xvl, Yvl), batch_size=64)\n",
    "\n",
    "# --------------------- 5. LSTM baseline --------------------------\n",
    "class PriceLSTM(nn.Module):\n",
    "    def __init__(self, n_in: int, hidden: int = 128, n_layers: int = 2,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_in,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.proj = nn.Linear(hidden, HORIZON)   # 24-step head\n",
    "\n",
    "    def forward(self, x):             # x : [B, 168, 3]\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.proj(h_n[-1])      # [B, 24]\n",
    "        return out\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = PriceLSTM(n_in=len(cols_x)).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "PATIENCE  = 5\n",
    "best_val  = np.inf\n",
    "pat_cnt   = 0\n",
    "\n",
    "# -------------------- 6. training loop ---------------------------\n",
    "for epoch in range(1, 101):   # up to 100 epochs\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # ---- validate ----\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_losses.append( criterion(model(xb), yb).item() )\n",
    "    val_mse = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch:03d} | val MSE = {val_mse:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_mse < best_val - 1e-4:\n",
    "        best_val = val_mse\n",
    "        pat_cnt  = 0\n",
    "        best_state = model.state_dict()\n",
    "    else:\n",
    "        pat_cnt += 1\n",
    "        if pat_cnt >= PATIENCE:\n",
    "            print(\"Early stop.\")\n",
    "            break\n",
    "\n",
    "# reload best weights\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# ---------------- 7. evaluate on *test* set ----------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_std = model(Xts.to(device)).cpu().numpy()\n",
    "y_true_std = Yts.numpy()\n",
    "\n",
    "# ---- metrics in standardised space (same as other models) ----\n",
    "mae_std = mean_absolute_error(y_true_std, yhat_std)\n",
    "mse_std = mean_squared_error(y_true_std, yhat_std)\n",
    "print(\"\\n=====  LSTM baseline (standardised scale)  =====\")\n",
    "print(f\"MAE = {mae_std:.4f}   |   MSE = {mse_std:.4f}\")\n",
    "\n",
    "# ------------- optional: metrics in real € scale ----------------\n",
    "yhat_real  = scaler_y.inverse_transform(yhat_std)\n",
    "ytrue_real = scaler_y.inverse_transform(y_true_std)\n",
    "mae_real   = mean_absolute_error(ytrue_real, yhat_real)\n",
    "mse_real   = mean_squared_error(ytrue_real, yhat_real)\n",
    "print(\"\\n(inverse-transformed – for intuition only)\")\n",
    "print(f\"MAE = {mae_real:.2f} €   |   MSE = {mse_real:.2f}  €²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c660115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV …\n",
      "\n",
      "Training LinearRegression …\n",
      "[Linear]  MAE = 25.9635   MSE = 1894.7248\n",
      "\n",
      "Training 1-hidden-layer MLP …\n",
      "Epoch  1  val-MSE = 11941.0274\n",
      "Epoch  2  val-MSE = 7718.5826\n",
      "Epoch  3  val-MSE = 6999.1241\n",
      "Epoch  4  val-MSE = 6673.1845\n",
      "Epoch  5  val-MSE = 6491.6193\n",
      "Epoch  6  val-MSE = 6383.7500\n",
      "Epoch  7  val-MSE = 6306.0333\n",
      "Epoch  8  val-MSE = 6250.1228\n",
      "Epoch  9  val-MSE = 6204.0284\n",
      "Epoch 10  val-MSE = 6179.6426\n",
      "Epoch 11  val-MSE = 6169.9750\n",
      "Epoch 12  val-MSE = 6150.2302\n",
      "Epoch 13  val-MSE = 6133.3141\n",
      "Epoch 14  val-MSE = 6117.1433\n",
      "Epoch 15  val-MSE = 6114.1509\n",
      "Epoch 16  val-MSE = 6136.4535\n",
      "Epoch 17  val-MSE = 6123.0418\n",
      "Epoch 18  val-MSE = 6112.8399\n",
      "Epoch 19  val-MSE = 6119.3832\n",
      "Epoch 20  val-MSE = 6123.4493\n",
      "Epoch 21  val-MSE = 6114.8088\n",
      "Early-stopping.\n",
      "[Tiny-MLP] MAE = 92.3062   MSE = 12717.9125\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "#  baseline_regression_and_mlp.py\n",
    "#  ------------------------------------------------------------\n",
    "#  - LinearRegression (scikit-learn)\n",
    "#  - One-hidden-layer MLP (PyTorch)\n",
    "# -------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# libraries for the two baselines\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------- SETTINGS ------------------------------\n",
    "CSV_PATH   = Path(\"data/causal_data.csv\")\n",
    "SEQ_LEN    = 168          # hours fed into the model\n",
    "PRED_LEN   = 24           # hours predicted\n",
    "BATCH_SIZE = 256          # only for the small MLP\n",
    "EPOCHS     = 30\n",
    "LR         = 1e-3\n",
    "PATIENCE   = 3\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------ LOAD & PREP DATA ------------------------\n",
    "print(\"Loading CSV …\")\n",
    "df = (pd.read_csv(CSV_PATH, parse_dates=[\"date\"])\n",
    "        .sort_values(\"date\")\n",
    "        .set_index(\"date\"))\n",
    "\n",
    "cols_in   = [\"electricity_price\", \"solar_forecast\",\n",
    "             \"wind_forecast\", \"total_load\"]\n",
    "target_col = \"electricity_price\"\n",
    "\n",
    "# -------------- split into train / val / test chronologically -----\n",
    "train_end = \"2023-12-31 23:00:00\"\n",
    "val_end   = \"2024-06-30 23:00:00\"\n",
    "\n",
    "df_train = df.loc[:train_end,  cols_in]\n",
    "df_val   = df.loc[train_end:val_end,  cols_in]\n",
    "df_test  = df.loc[val_end:,  cols_in]\n",
    "\n",
    "# --------------------------- SCALING ------------------------------\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "def scale_block(block):\n",
    "    X = scaler_x.transform(block[cols_in])\n",
    "    y = scaler_y.transform(block[[target_col]])\n",
    "    return X, y\n",
    "\n",
    "# fit on *training* only\n",
    "scaler_x.fit(df_train[cols_in])\n",
    "scaler_y.fit(df_train[[target_col]])\n",
    "\n",
    "# ---------------- create rolling windows --------------------------\n",
    "def make_windows(block):\n",
    "    Xs, ys = [], []\n",
    "    a = block.values\n",
    "    for i in range(len(a) - SEQ_LEN - PRED_LEN + 1):\n",
    "        seq_x = a[i:i+SEQ_LEN]                 # 168 × 4\n",
    "        seq_y = a[i+SEQ_LEN:i+SEQ_LEN+PRED_LEN, 0]  # 24 ×  (price column)\n",
    "        Xs.append(seq_x)                       # keep 2-D shape\n",
    "        ys.append(seq_y)\n",
    "    return np.stack(Xs), np.stack(ys)\n",
    "\n",
    "X_tr, y_tr = make_windows(df_train)\n",
    "X_va, y_va = make_windows(df_val)\n",
    "X_te, y_te = make_windows(df_test)\n",
    "\n",
    "# flatten sequences for the regression baseline\n",
    "X_tr_flat = X_tr.reshape(len(X_tr), -1)\n",
    "X_va_flat = X_va.reshape(len(X_va), -1)\n",
    "X_te_flat = X_te.reshape(len(X_te), -1)\n",
    "\n",
    "# ================================================================\n",
    "# 1)  LINEAR-REGRESSION BASELINE\n",
    "# ================================================================\n",
    "print(\"\\nTraining LinearRegression …\")\n",
    "lin = LinearRegression(n_jobs=-1).fit(X_tr_flat, y_tr)\n",
    "\n",
    "y_pred_lr = lin.predict(X_te_flat)\n",
    "mae_lr = mean_absolute_error(y_te, y_pred_lr)\n",
    "mse_lr = mean_squared_error(y_te, y_pred_lr)\n",
    "\n",
    "print(f\"[Linear]  MAE = {mae_lr:.4f}   MSE = {mse_lr:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# 2)  MLP BASELINE  (one hidden layer)\n",
    "# ================================================================\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "in_dim  = SEQ_LEN * len(cols_in)      # 168 × 4  -> 672\n",
    "out_dim = PRED_LEN                    # 24\n",
    "\n",
    "model = TinyMLP(in_dim, out_dim).to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def to_loader(X, y, shuffle):\n",
    "    X_t = torch.tensor(X.reshape(len(X), -1), dtype=torch.float32)\n",
    "    y_t = torch.tensor(y,                 dtype=torch.float32)\n",
    "    return DataLoader(TensorDataset(X_t, y_t),\n",
    "                      batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "\n",
    "dl_tr = to_loader(X_tr, y_tr, shuffle=True)\n",
    "dl_va = to_loader(X_va, y_va, shuffle=False)\n",
    "\n",
    "best_val = np.inf\n",
    "stuck    = 0\n",
    "\n",
    "print(\"\\nTraining 1-hidden-layer MLP …\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in dl_tr:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # ---- validation ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = []\n",
    "        for xb, _ in dl_va:\n",
    "            xb = xb.to(DEVICE)\n",
    "            val_pred.append(model(xb).cpu().numpy())\n",
    "        val_pred = np.concatenate(val_pred, axis=0)\n",
    "        val_loss = mean_squared_error(y_va, val_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  val-MSE = {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val = val_loss\n",
    "        best_state = model.state_dict()\n",
    "        stuck = 0\n",
    "    else:\n",
    "        stuck += 1\n",
    "        if stuck >= PATIENCE:\n",
    "            print(\"Early-stopping.\")\n",
    "            break\n",
    "\n",
    "# ---------- test set ----------\n",
    "model.load_state_dict(best_state)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_te_flat = torch.tensor(X_te.reshape(len(X_te), -1),\n",
    "                             dtype=torch.float32).to(DEVICE)\n",
    "    y_pred_mlp = model(X_te_flat).cpu().numpy()\n",
    "\n",
    "mae_mlp = mean_absolute_error(y_te, y_pred_mlp)\n",
    "mse_mlp = mean_squared_error(y_te, y_pred_mlp)\n",
    "\n",
    "print(f\"[Tiny-MLP] MAE = {mae_mlp:.4f}   MSE = {mse_mlp:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "#  The MAE / MSE numbers above are what you put into the results\n",
    "#  tables next to TimeXer, iTransformer, PatchTST, Crossformer …\n",
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04221cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseasonal_naive\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seasonal_naive\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/power/.venv/lib64/python3.9/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seasonal_naive import seasonal_naive\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) Load your CSV exactly as you do for the neural models\n",
    "# ----------------------------------------------------------\n",
    "CSV_PATH = \"./data/causal_data.csv\"\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"date\"])\n",
    "\n",
    "# keep only the price column of the *test* period\n",
    "test_start = \"2024-07-01 00:00:00\"\n",
    "test_prices = (\n",
    "    df.loc[df[\"date\"] >= test_start, \"electricity_price\"]\n",
    "      .astype(np.float32)\n",
    "      .values            # → 1-D ndarray\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) Generate forecasts with the helper\n",
    "# ----------------------------------------------------------\n",
    "y_hat, y_true = seasonal_naive(\n",
    "    series      = test_prices,\n",
    "    horizon     = 24,       # predict 24 hours\n",
    "    season_lag  = 24        # use yesterday’s values\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) Evaluate\n",
    "# ----------------------------------------------------------\n",
    "mae = mean_absolute_error(y_true, y_hat)\n",
    "mse = mean_squared_error(y_true, y_hat)\n",
    "\n",
    "print(f\"Seasonal-naïve  MAE = {mae:.2f} | MSE = {mse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
